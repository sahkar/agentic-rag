{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4.1-mini\")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "try: \n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir='./storage/vllm'\n",
    "    )\n",
    "\n",
    "    vllm_index = load_index_from_storage(storage_context)\n",
    "    index_loaded = True\n",
    "except: \n",
    "    index_loaded = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "query_engine_tools = []\n",
    "\n",
    "try: \n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir='./storage/'\n",
    "    )\n",
    "\n",
    "    indices = load_index_from_storage(storage_context)\n",
    "    index_loaded = True\n",
    "except: \n",
    "    index_loaded = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pipeswitch.pdf\n",
      "Processing vLLM.pdf\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "if not index_loaded:\n",
    "    # Group documents by filename\n",
    "    docs_by_file = {}\n",
    "    docs = SimpleDirectoryReader(\n",
    "        input_dir='./data',\n",
    "        recursive=True,\n",
    "        filename_as_id=True\n",
    "    ).load_data()\n",
    "    \n",
    "    # Group documents by their filename\n",
    "    for doc in docs:\n",
    "        file_name = doc.metadata.get('file_name', 'unknown')\n",
    "        if file_name not in docs_by_file:\n",
    "            docs_by_file[file_name] = []\n",
    "        docs_by_file[file_name].append(doc)\n",
    "    \n",
    "    # Create one index and tool per file\n",
    "    for file_name, file_docs in docs_by_file.items():\n",
    "        print(f\"Processing {file_name}\")\n",
    "        \n",
    "        # Create index from all chunks of the same file\n",
    "        doc_index = VectorStoreIndex.from_documents(file_docs)\n",
    "        \n",
    "        persist_dir = f'./storage/{file_name}'\n",
    "        doc_index.storage_context.persist(persist_dir=persist_dir)\n",
    "        \n",
    "        doc_engine = doc_index.as_query_engine(similarity_top_k=3)\n",
    "        tool = QueryEngineTool.from_defaults(\n",
    "            query_engine=doc_engine,\n",
    "            name=file_name,\n",
    "            description=f\"A tool to answer questions about the {file_name} paper. If asked about a specific part, provide the exact text from the paper.\"\n",
    "        )\n",
    "        \n",
    "        query_engine_tools.append(tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.query_engine.QueryEngineTool object at 0x3108411f0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x3106114f0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x3101dff50>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310808a10>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310808c50>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809070>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809430>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809610>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809790>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310808710>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809a90>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809eb0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809fd0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080a390>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809f10>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809190>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080a210>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080acf0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809f70>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310808e30>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080aff0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809b50>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080b230>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080b470>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310809010>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080b770>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080b950>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080bc50>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080b5f0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080bf50>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x310808fb0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080b830>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x31080bef0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x311542bd0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x311556150>]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "print(query_engine_tools)\n",
    "agent = ReActAgent(\n",
    "    tools=query_engine_tools,\n",
    "    llm=Settings.llm,\n",
    "    system_prompt=\"\"\"You are a helpful RAG agent that can answer questions about multiple documents. \n",
    "    When answering questions:\n",
    "    1. First determine which document(s) are most relevant to the question\n",
    "    2. Use the appropriate tool(s) to search those documents\n",
    "    3. Always specify which document the information came from\n",
    "    4. If information comes from multiple documents, clearly indicate this\n",
    "    5. Keep all text in English\n",
    "    6. Provide exact quotes when relevant\"\"\"\n",
    ")\n",
    "\n",
    "ctx = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The user is asking about \"pipeswitch.\" I need to use a tool to help me answer the question.\n",
      "Action: pipeswitch.pdf\n",
      "Action Input: {\"input\":\"What is pipeswitch?\"}\n",
      "Call pipeswitch.pdf with {'input': 'What is pipeswitch?'}\n",
      "Returned: PipeSwitch is a system designed to enable fast pipelined context switching specifically for deep learning applications. It optimizes the process of switching tasks on GPUs by pipelining model transmission and task execution, reducing overhead and latency. PipeSwitch achieves efficient task switching by grouping model layers in a model-aware manner to balance the trade-off between pipelining overhead and efficiency. This grouping minimizes the number of PCIe calls and synchronization overhead, allowing computation to start as soon as parameters for a group are transmitted. Additionally, PipeSwitch incorporates unified memory management to further reduce overhead during task switching. Overall, it significantly improves throughput and latency for deep learning workloads by reducing the time required to preempt one task and start another.\n",
      "Thought: I have enough information to answer the question about what PipeSwitch is.\n",
      "Answer: PipeSwitch is a system designed to enable fast pipelined context switching specifically for deep learning applications. It optimizes task switching on GPUs by pipelining model transmission and task execution, reducing overhead and latency. PipeSwitch groups model layers in a model-aware way to balance pipelining overhead and efficiency, minimizing PCIe calls and synchronization overhead. This allows computation to start as soon as parameters for a group are transmitted. It also uses unified memory management to further reduce overhead during task switching. Overall, PipeSwitch significantly improves throughput and latency for deep learning workloads by reducing the time needed to preempt one task and start another."
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import ToolCallResult, AgentStream\n",
    "\n",
    "handler = agent.run(input(\"Enter a question: \"), ctx=ctx)\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ToolCallResult):\n",
    "        print(f\"\\nCall {ev.tool_name} with {ev.tool_kwargs}\\nReturned: {ev.tool_output}\")\n",
    "    if isinstance(ev, AgentStream):\n",
    "        print(f\"{ev.delta}\", end=\"\", flush=True)\n",
    "\n",
    "response = await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
