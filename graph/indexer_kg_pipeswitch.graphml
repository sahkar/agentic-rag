<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d0" for="edge" attr.name="relationship" attr.type="string"/>
<graph edgedefault="directed"><node id="THE DOMINANT PRACTICE TODAY"/>
<node id="DEDICATED GPU CLUSTERS FOR TRAINING AND INFERENCE SEPARATELY"/>
<node id="GPU CLUSTERS"/>
<node id="SHARED GPU CLUSTERS"/>
<node id="FINE-GRAINED TIME-SHARING GPU"/>
<node id="DIFFERENT APPLICATIONS"/>
<node id="TRAINING AND INFERENCE"/>
<node id="PIPESWITCH"/>
<node id="AN INFERENCE APPLICATION TO BE FILLED BY TRAINING OR OTHER INFERENCE APPLICATIONS"/>
<node id="GPU UTILIZATION"/>
<node id="SLOS"/>
<node id="SINGLE-GPU TRAINING"/>
<node id="ASYNCHRONOUS MULTI-GPU TRAINING"/>
<node id="DATA PARALLEL STRATEGIES"/>
<node id="PREEMPTING ONE GPU"/>
<node id="OTHER GPUS"/>
<node id="A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS"/>
<node id="A SINGLE GPU"/>
<node id="A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS OUT OF THE BOX"/>
<node id="ELASTIC SYNCHRONOUS TRAINING"/>
<node id="NUMBER OF GPUS USED FOR TRAINING"/>
<node id="SYNCHRONOUS MULTI-GPU TRAINING"/>
<node id="ONE WAY"/>
<node id="PIPESWITCH FOR SYNCHRONOUS MULTI-GPU TRAINING"/>
<node id="WE"/>
<node id="PIPELINED CONTEXT SWITCHING"/>
<node id="THE KEY IDEA"/>
<node id="THE LAYERED STRUCTURE OF NEURAL NETWORK MODELS"/>
<node id="THE LAYER-BY-LAYER COMPUTATION PATTERN"/>
<node id="THE PCIE"/>
<node id="THE GPU"/>
<node id="MODEL-AWARE GROUPING"/>
<node id="ACTIVE-STANDBY MECHANISM"/>
<node id="FAST WORKER SWITCHING AND PROCESS-LEVEL ISOLATION"/>
<node id="SYSTEM PROTOTYPE FOR PIPESWITCH"/>
<node id="3600"/>
<node id="C AND PYTHON"/>
<node id="PYTORCH 21"/>
<node id="DEEP LEARNING"/>
<node id="AN EMERGING FAMILY OF INTELLIGENT APPLICATIONS"/>
<node id="INTELLIGENT APPLICATIONS"/>
<node id="RETAIL"/>
<node id="TRANSPORTATION"/>
<node id="FINANCE"/>
<node id="HEALTHCARE"/>
<node id="GPUS"/>
<node id="ACCELERATORS FOR DEEP LEARNING"/>
<node id="ASH CROWD"/>
<node id="APPLICATION SUDDENLY BECOMES POPULAR"/>
<node id="TRAINING CLUSTER"/>
<node id="INFERENCE TASKS"/>
<node id="PRODUCTION SYSTEMS"/>
<node id="EACH APPLICATION ON PER-GPU GRANULARITY"/>
<node id="PROVISIONING ON PER-GPU GRANULARITY"/>
<node id="THE INTERFERENCE BETWEEN APPLICATIONS"/>
<node id="PER-GPU GRANULARITY"/>
<node id="VMS, CONTAINERS OR PROCESSES OF AN APPLICATION"/>
<node id="BINDING GPUS TO APPLICATIONS"/>
<node id="INTERFERENCE BETWEEN DIFFERENT APPLICATIONS"/>
<node id="SLO REQUIREMENTS"/>
<node id="OPERATING SYSTEMS"/>
<node id="TASK SCHEDULING AND CONTEXT SWITCHING"/>
<node id="THE IDEA OF NE-GRAINED CPU TIME-SHARING"/>
<node id="CLUSTER SCHEDULING"/>
<node id="NE-GRAINED TIME-SHARING"/>
<node id="PROVISIONING DEDICATED RESOURCES"/>
<node id="PROCESS-LEVEL ISOLATION"/>
<node id="SCHEDULING CYCLES"/>
<node id="NE-GRAINED"/>
<node id="GPU"/>
<node id="TASKS"/>
<node id="THE GAP"/>
<node id="THE PRECIOUS GPU MEMORY AND SLOW SWITCHING"/>
<node id="NAIVE USE OF GPUS IN THE SAME WAY AS CPUS"/>
<node id="DL INFERENCE WITH STRICT SLOS IN TENS TO HUNDREDS OF MILLISECONDS"/>
<node id="DNN MODEL (E.G., RESNET)"/>
<node id="SWITCHING_TO_UNPRELOADED_DNN_MODEL_ON_GPU"/>
<node id="MULTIPLE SECONDS"/>
<node id="STATE-OF-THE-ART TRICKS LIKE CUDA UNIFIED MEMORY 4 (6)"/>
<node id="DELAY"/>
<node id="GPU MEMORY"/>
<node id="HOST MEMORY"/>
<node id="MANY APPLICATIONS"/>
<node id="MEMORY FOOTPRINTS OF INFERENCE TASKS"/>
<node id="NULL"/>
<node id="MODELS"/>
<node id="REQUEST BATCHING"/>
<node id="THROUGHPUT"/>
<node id="CONTEXT SWITCHING DESIGN"/>
<node id="SWITCHING OVERHEAD"/>
<node id="CONTENTS ON GPU MEMORY"/>
<node id="BETTER APPROACH FOR EFFICIENTLY TIME-SHARING GPUS"/>
<node id="A MAJOR CHALLENGE FAST GPU CONTEXT SWITCHING BETWEEN DIFFERENT PROCESSES"/>
<node id="DNN MODELS"/>
<node id="THE MODELS FOR TRAINING OR INFERENCE"/>
<node id="ENTERPRISES"/>
<node id="GPU CLUSTERS TO RUN DNN WORKLOADS IN LARGE SCALE"/>
<node id="EITHER PRIVATELY OR PUBLICLY SHARED BY MULTIPLE USERS"/>
<node id="11 M. JEON, S. VENKATARAMAN, A. PHANISHAYEE, U. QIAN, W. XIAO, AND F. YANG"/>
<node id="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS"/>
<node id="USENIX ATC"/>
<node id="2019"/>
<node id="512 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="USENIX ASSOCIATION"/>
<node id="M. JEON, S. VENKATARAMAN, A. PHANISHAYEE, J. QIAN, W. XIAO, AND F. YANG"/>
<node id="NUMBER OF APPLICATIONS THAT CAN BE MULTIPLEXED"/>
<node id="GPU MEMORY SIZE"/>
<node id="EACH APPLICATION"/>
<node id="GPU COMPUTE AND MEMORY RESOURCES DURING ITS TIME SLICE"/>
<node id="GPU SERVERS VIA FINE-GRAINED TIME-SHARING"/>
<node id="AS DEDICATED SERVERS"/>
<node id="MULTIPLE DEEP LEARNING APPLICATIONS"/>
<node id="MILLISECONDS"/>
<node id="HIGH THROUGHPUT"/>
<node id="PIPESWITCH SYSTEM"/>
<node id="GPU MEMORY SHARING AND SWITCHING"/>
<node id="AN EFFICIENT TIME-SHARING GPU CLUSTER FOR DL WORKLOADS"/>
<node id="MILLISECOND-SCALE TASK SWITCHING TIME"/>
<node id="DL APPLICATIONS ON TIME-SHARING GPUS"/>
<node id="STRICT SLOS"/>
<node id="A MEASUREMENT STUDY"/>
<node id="MEASUREMENT STUDY"/>
<node id="THE TASK SWITCHING OVERHEAD"/>
<node id="THE OVERHEAD OF EACH COMPONENT"/>
<node id="PROFILING OF TASK SWITCHING OVERHEAD"/>
<node id="TASK SWITCHING OVERHEAD"/>
<node id="STUDY"/>
<node id="EVERY COMPONENT"/>
<node id="CONSIDERABLE AMOUNT OF TIME"/>
<node id="TIME TAKEN BY EVERY COMPONENT"/>
<node id="FROM TENS OF MILLISECONDS TO SECONDS"/>
<node id="INFERENCE TASK"/>
<node id="TENS OF MILLISECONDS ON A GPU"/>
<node id="LATENCY SLOS"/>
<node id="INFERENCE TIME"/>
<node id="A HOLISTIC APPROACH"/>
<node id="THE CHARACTERISTICS OF DL APPLICATIONS"/>
<node id="THE OVERHEAD OF ALL THE COMPONENTS"/>
<node id="OUR DESIGN"/>
<node id="A KEY OBSERVATION"/>
<node id="A LAYERED STRUCTURE"/>
<node id="A LAYER-BY-LAYER COMPUTATION PATTERN"/>
<node id="COMPUTATION OF DNN MODELS"/>
<node id="LAYER BY LAYER"/>
<node id="COMPUTATION"/>
<node id="ENTIRE MODEL TO BE TRANSMITTED TO THE GPU"/>
<node id="ALGORITHM"/>
<node id="TWO INSIGHTS"/>
<node id="FIND OPTIMAL GROUPING STRATEGY"/>
<node id="OPTIMAL GROUPING STRATEGY"/>
<node id="GIVEN MODEL"/>
<node id="DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT"/>
<node id="CUDA UNIFIED MEMORY 4"/>
<node id="OVERKILL"/>
<node id="UNNECESSARY OVERHEAD"/>
<node id="MEMORY ALLOCATION FOR A DNN MODEL"/>
<node id="TRUE"/>
<node id="DETERMINISTIC MEMORY ALLOCATION"/>
<node id="EXTRA MEMORY COPIES BETWEEN THE DAEMON AND THE WORKERS"/>
<node id="ELIMINATION OF EXTRA MEMORY COPIES"/>
<node id="IPC OVERHEAD"/>
<node id="SERVER"/>
<node id="STANDBY WORKERS"/>
<node id="THE ACTIVE WORKER"/>
<node id="A TASK IN THE GPU"/>
<node id="ACTIVE WORKER"/>
<node id="CURRENT TASK"/>
<node id="CONTROLLER"/>
<node id="MEMORY DAEMON"/>
<node id="STANDBY WORKER"/>
<node id="MEMORY DAEMON AND STANDBY WORKER"/>
<node id="TASK TO GPU"/>
<node id="TASK"/>
<node id="PIPELINED MODEL TRANSMISSION (4.2)"/>
<node id="TABLE 2"/>
<node id="COMPARISON OF WORKER SWITCHING MECHANISMS"/>
<node id="WORKER SWITCHING MECHANISMS"/>
<node id="NO TASK PROCESS-CLEANING INITIALIZATION LEVEL OVERHEAD ISOLATION"/>
<node id="TWO PROCESSES"/>
<node id="ONE PROCESS ACTIVE-STANDBY"/>
<node id="ACTIVE AND STANDBY WORKER SWITCHING MECHANISM"/>
<node id="TO HIDE THE OVERHEAD OF TASK CLEANING AND TASK INITIALIZATION"/>
<node id="PROCESS-LEVEL ISOLATION WITH SEPARATE WORKER PROCESSES"/>
<node id="NEW TECHNICAL CHALLENGES ON MEMORY MANAGEMENT AND WORKER SWITCHING ACROSS DIFFERENT PROCESSES"/>
<node id="ACTIVE-STANDBY WORKER SWITCHING"/>
<node id="EFFECTIVENESS OF ACTIVE-STANDBY WORKER SWITCHING"/>
<node id="EVALUATION OF ACTIVE-STANDBY WORKER SWITCHING"/>
<node id="ALL OTHER COMPONENTS OF PIPESWITCH"/>
<node id="MECHANISMS DISCUSSED IN SECTION 4.4"/>
<node id="PIPELINING"/>
<node id="COMPUTER SYSTEMS"/>
<node id="SYSTEM PERFORMANCE"/>
<node id="RESOURCE UTILIZATION"/>
<node id="TWO SOURCES OF SYSTEM OVERHEADS"/>
<node id="INTRA-BATCH PIPELINING"/>
<node id="MODEL TRANSMISSION AND COMPUTATION"/>
<node id="OVERHEAD OF SWITCHING BETWEEN DIFFERENT DNN MODELS"/>
<node id="DIFFERENT DNN MODELS"/>
<node id="INFERENCE OR TRAINING"/>
<node id="DL APPLICATIONS"/>
<node id="PIPELINED MODEL TRANSMISSION"/>
<node id="UNIFIED MEMORY MANAGEMENT"/>
<node id="A SYSTEM PROTOTYPE"/>
<node id="PYTORCH"/>
<node id="THIS SECTION"/>
<node id="INEFFICIENCIES IN TODAY'S SHARED GPU CLUSTERS"/>
<node id="RUNNING DEEP LEARNING WORKLOADS ON GPUS IN THE NE-GRAINED TIME-SHARING MODEL"/>
<node id="500 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="SHARED CLUSTER"/>
<node id="DEDICATED CLUSTER FOR EACH USER"/>
<node id="TEXT"/>
<node id="REASONS TO BUILD A SHARED CLUSTER INSTEAD OF A DEDICATED ONE FOR EACH USER"/>
<node id="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="KUBERNETES"/>
<node id="THE MAIN REASON"/>
<node id="TO BRING DOWN THE COST"/>
<node id="DEMAND OF INFERENCE"/>
<node id="MORE PREDICTABLE PATTERN"/>
<node id="INFERENCE TASK FOR A PARTICULAR APPLICATION"/>
<node id="DAILY PERIODICAL PATTERN"/>
<node id="APPLICATION USAGE"/>
<node id="THE PATTERNS"/>
<node id="DIFFERENT TASKS"/>
<node id="TRAINING"/>
<node id="INFERENCE"/>
<node id="SHARED CLUSTERS"/>
<node id="GPUS DESIGNED FOR INFERENCE TASKS"/>
<node id="TRAINING TASKS"/>
<node id="NEW ALGORITHMS AND SYSTEMS FOR DISTRIBUTED TRAINING"/>
<node id="MULTIPLE GPUS TO ACCELERATE TRAINING"/>
<node id="COMPUTATION POWER ON BOTH SIDES"/>
<node id="SAME ORDER OF MAGNITUDE"/>
<node id="INFERENCE WORKLOAD"/>
<node id="NUMBER OF ACTIVE USERS"/>
<node id="CLEAR PEAKS AND VALLEYS WITHIN EACH DAY"/>
<node id="PEAK DEMAND DURING DAYTIME"/>
<node id="2 TIMES THE VALLEY DEMAND AT MIDNIGHT"/>
<node id="NE-TUNING BERT"/>
<node id="DAILY NEWS"/>
<node id="BORG-LIKE 1 SYSTEMS FOR GPUS"/>
<node id="GREAT OPPORTUNITY IN IMPROVING GPU UTILIZATION"/>
<node id="INFERENCE AND TRAINING WORKLOADS"/>
<node id="INFERENCE LOADS ON DIFFERENT MODELS"/>
<node id="DIFFERENT PATTERNS"/>
<node id="DIFFERENT PATTERNS OF INFERENCE LOADS"/>
<node id="TIME SHARING"/>
<node id="ANY SERVER"/>
<node id="ANY TASK"/>
<node id="RUNNING ANY TASK ON ANY SERVER"/>
<node id="THE DESIGN OF LOAD BALANCERS AND SCHEDULERS"/>
<node id="SWITCHING BETWEEN DIFFERENT APPLICATIONS"/>
<node id="LOW OVERHEAD"/>
<node id="MODERN SERVER"/>
<node id="SEVERAL TB OF HOST MEMORY"/>
<node id="MODERN SERVER TO LOAD MANY APPLICATIONS"/>
<node id="DL TASKS"/>
<node id="STATE-OF-THE-ART MODELS"/>
<node id="DEEPER AND LARGER"/>
<node id="IDLE APPLICATIONS"/>
<node id="LARGE MEMORY SPACE"/>
<node id="ONLINE INFERENCE WORKLOADS"/>
<node id="NAIVE MEMORY SWAPPING BETWEEN THE HOST MEMORY AND THE GPU MEMORY"/>
<node id="THE RST INFERENCE BATCH"/>
<node id="SEVERAL SECONDS"/>
<node id="NVIDIA MPS"/>
<node id="STOP-AND-START"/>
<node id="SEVERAL HUNDRED MILLISECONDS OVERHEAD"/>
<node id="NVIDIA MPS FROM MEETING STRICT SLOS"/>
<node id="PIPESWITCH ARCHITECTURE"/>
<node id="501 CONTROLLER"/>
<node id="GPU MEMORY DAEMON"/>
<node id="NEW TASK"/>
<node id="BATCHES PER SECOND"/>
<node id="UPPER BOUND"/>
<node id="MPS"/>
<node id="EIGHT P3.2XLARGE INSTANCES"/>
<node id="INTRA-BATCH PIPELINING TO FAST START TRAINING AND INFERENCE TASKS"/>
<node id="FAST SWITCHING ACROSS TASKS"/>
<node id="PIPELINE"/>
<node id="FEASIBLE AND EFFECTIVE"/>
<node id="OTHER CHALLENGES LIKE MEMORY MANAGEMENT AND WORKER SWITCHING"/>
<node id="ARCHITECTURE AND TASK EXECUTION"/>
<node id="FIGURE 1"/>
<node id="PIPESWITCH SERVER"/>
<node id="PIPESWITCH PIPELINES MODEL"/>
<node id="TRANSMISSION AND TASK EXECUTION"/>
<node id="THE CONTROLLER"/>
<node id="THE CENTRAL COMPONENT"/>
<node id="A SYSTEM PROCESS"/>
<node id="THE MEMORY DAEMON"/>
<node id="THE GPU MEMORY"/>
<node id="THE DNN MODELS"/>
<node id="WORKER ON STANDBY"/>
<node id="THE STANDBY WORKER"/>
<node id="THE NEW TASK"/>
<node id="A STANDBY WORKER"/>
<node id="THE PREVIOUS TASK"/>
<node id="A SET OF TASKS RECEIVED FROM THE CLIENTS"/>
<node id="FAST CONTEXT SWITCHING"/>
<node id="THE SPECIFIC SCHEDULING ALGORITHM"/>
<node id="THIS PAPER"/>
<node id="THE CURRENT TASK TO FINISH IF IT IS INFERENCE"/>
<node id="THE ACTIVE WORKER TO STOP IF IT IS TRAINING"/>
<node id="AN IDLE STANDBY WORKER TO INITIALIZE ITS ENVIRONMENT FOR THE NEW TASK"/>
<node id="HOST MEMORY TO GPU MEMORY"/>
<node id="MODEL TRANSMISSION"/>
<node id="TASK STARTUP"/>
<node id="DIRECT TRANSMISSION BY MEMORY DAEMON"/>
<node id="EXTRA MEMORY COPY FROM MEMORY DAEMON TO WORKER"/>
<node id="WORKER"/>
<node id="RELEVANT GPU MEMORY HANDLERS TO WORKER"/>
<node id="MODEL"/>
<node id="EXECUTE ITS TASK"/>
<node id="4 PIPESWITCH DESIGN"/>
<node id="TASK SWITCHING OVERHEAD INTO INDIVIDUAL COMPONENTS"/>
<node id="EVALUATION"/>
<node id="END-TO-END EXPERIMENTS"/>
<node id="BENEFITS OF PIPESWITCH"/>
<node id="EFFECTIVENESS OF DESIGN CHOICES ON EACH COMPONENT"/>
<node id="SYSTEMATICALLY MINIMIZE THE OVERHEAD OF EACH COMPONENT"/>
<node id="THE MEA-502 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="SUREMENT"/>
<node id="SERVER STOPS TRAINING TASK RUNNING ON GPU AND STARTS INFERENCE TASK"/>
<node id="TIME"/>
<node id="STARTING AND EXECUTING INFERENCE TASK ON GPU"/>
<node id="THE NETWORK TIME"/>
<node id="THE TASK QUEUEING TIME"/>
<node id="CLEANING"/>
<node id="TASK CLEANING"/>
<node id="THE INFERENCE TASK"/>
<node id="ITS ENVIRONMENT"/>
<node id="PROCESS LAUNCHING"/>
<node id="PYTORCH CUDA RUNTIME LOADING"/>
<node id="CUDA CONTEXT INITIALIZATION"/>
<node id="MEMORY ALLOCATION"/>
<node id="PROCESS OF ASSIGNING MEMORY RESOURCES"/>
<node id="GROUPED TRANSMISSION"/>
<node id="TRANSMISSION METHOD"/>
<node id="TASK SWITCHING ON T4"/>
<node id="CPU"/>
<node id="G4DN.2XLARGE"/>
<node id="P3.2XLARGE"/>
<node id="CPU IN G4DN.2XLARGE"/>
<node id="INTEL PLATINUM 8259CL"/>
<node id="CPU IN P3.2XLARGE"/>
<node id="INTEL XEON E5-2686 V4"/>
<node id="ALL THE COMPONENTS"/>
<node id="THE INFERENCE TIME"/>
<node id="ALL THE COMPONENTS SHOULD BE OPTIMIZED TO ACHIEVE MINIMAL SWITCHING OVERHEAD AND MEET THE SLOS"/>
<node id="PCIE BANDWIDTH"/>
<node id="HOW FAST AN ARBITRARY TASK CAN BE LOADED TO THE GPU"/>
<node id="AN INFERENCE TASK"/>
<node id="THE RST LAYER TO THE NAL LAYER TO MAKE A PREDICTION"/>
<node id="EACH ITERATION IN A TRAINING TASK"/>
<node id="IN THE TRAINING PROCESS"/>
<node id="AFTER THE FORWARD PASS"/>
<node id="THE TASK"/>
<node id="A LAYER AS SOON AS THE LAYER IS LOADED IN THE GPU AND THE INPUT OF THE LAYER IS READY"/>
<node id="THE INPUT OF THE LAYER"/>
<node id="THE PREVIOUS LAYERS HAVE FINISHED THEIR COMPUTATION"/>
<node id="ITS FOLLOWING LAYERS"/>
<node id="PCIE"/>
<node id="TASK EXECUTION"/>
<node id="PCIE GPU E0 E1 EN-1 E2 (B) PIPELINE MODEL"/>
<node id="ADDING HOOKS"/>
<node id="DNN FRAMEWORK"/>
<node id="MODEL STRUCTURE INFORMATION"/>
<node id="USERS AND CLUSTER MANAGERS"/>
<node id="OVERHEAD"/>
<node id="MULTIPLE CALLS TO PCIE TO TRANSMIT DATA"/>
<node id="TRANSMISSION OVERHEAD"/>
<node id="DATA SIZE"/>
<node id="DIVIDING THE MODEL INTO MANY LAYERS"/>
<node id="INVOKING A PCIE CALL FOR EACH LAYER"/>
<node id="SIGNIFICANT EXTRA OVERHEAD"/>
<node id="SOME LAYERS"/>
<node id="VERY SMALL"/>
<node id="SYNCHRONIZATION OVERHEAD"/>
<node id="TRANSMISSION AND COMPUTATION"/>
<node id="COMPUTATION TO KNOW WHEN A LAYER IS READY TO COMPUTE"/>
<node id="USING SMALL GROUPS"/>
<node id="MORE OVERLAP BETWEEN TRANSMISSION AND COMPUTATION"/>
<node id="PIPELINING EFFICIENCY"/>
<node id="MORE PIPELINING OVERHEAD"/>
<node id="USING BIG GROUPS"/>
<node id="MINIMAL PIPELINING OVERHEAD"/>
<node id="REDUCED CHANCE FOR OVERLAPPING"/>
<node id="ALL POSSIBLE COMBINATIONS"/>
<node id="THE OPTIMAL GROUPING STRATEGY"/>
<node id="TWO PRUNING TECHNIQUES"/>
<node id="OPTIMAL GROUPING STRATEGY EFFICIENTLY"/>
<node id="USENIX ASSOCIATION 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="PCIE GPU LOWER BOUND OF F(GROUP(0, I), I1)"/>
<node id="GROUP(0, I)"/>
<node id="GROUP(I1, J)"/>
<node id="J, J1"/>
<node id="N-1"/>
<node id="PRUNING CONDITION"/>
<node id="LOWER BOUND IS CURRENT OPTIMAL TIME"/>
<node id="PRUNING TECHNIQUES"/>
<node id="I TO J"/>
<node id="BATCH"/>
<node id="I1 TO J"/>
<node id="GROUP"/>
<node id="0 TO I"/>
<node id="I1 TO N-1"/>
<node id="(I 1) TO J J"/>
<node id="J J ALGORITHM"/>
<node id="THE PROBLEM"/>
<node id="F(B,I)"/>
<node id="TOTAL TIME OF THE OPTIMAL GROUPING STRATEGY FROM LAYER I TO N-1"/>
<node id="LAYER 0 TO I-1"/>
<node id="GROUPS REPRESENTED BY B"/>
<node id="NUMBER OF LAYERS"/>
<node id="N"/>
<node id="ENTIRE MODEL F(,0)"/>
<node id="N CASES"/>
<node id="FORMATION OF THE FIRST GROUP"/>
<node id="CASE I"/>
<node id="FIRST GROUP CONTAINS LAYER 0 TO I"/>
<node id="THIS FORMULA"/>
<node id="F(GROUP(0,I),I1)"/>
<node id="MULTIPLE LAYERS IN A GROUP"/>
<node id="PROGRESS OF COMPUTATION"/>
<node id="PACKING MULTIPLE LAYERS IN A GROUP"/>
<node id="PIPELINE EFFICIENCY"/>
<node id="RST GROUP"/>
<node id="SAFE PACKING OF MULTIPLE LAYERS"/>
<node id="T(I, J)"/>
<node id="TRANSMISSION TIME"/>
<node id="E(I, J)"/>
<node id="EXECUTION TIME"/>
<node id="SIZE OF LAYER I TO J AND PCIE BANDWIDTH"/>
<node id="FIGURE 3(B)"/>
<node id="AN EXAMPLE FOR THIS INSIGHT"/>
<node id="TRANSMISSION OF THE SECOND GROUP"/>
<node id="COMPUTATION OF THE RST GROUP"/>
<node id="TRANSMISSION"/>
<node id="THE LEAST NUMBER OF LAYERS TO GROUP"/>
<node id="THE FOLLOWING EQUATION"/>
<node id="J ARGMAX J T(I1, J) E(0,I) (3)"/>
<node id="JIS"/>
<node id="GROUPING FROM I1 TO J"/>
<node id="HIGHER PIPELINE OVERHEAD"/>
<node id="ALGORITHM 1"/>
<node id="PSEUDO CODE"/>
<node id="X TO N1 FOR CASE I K"/>
<node id="B"/>
<node id="MULTIPLE GROUPS FORMED BY PREVIOUS LAYERS"/>
<node id="B.DELAY"/>
<node id="THE TIME TO WHICH THE GROUP CAN BE FORMED"/>
<node id="THE ALGORITHM NDS"/>
<node id="B.DELAY (LINE 4-9)"/>
<node id="THE ENUMERATION FOR I"/>
<node id="X TO J-1 (LINE 11)"/>
<node id="O(2N)"/>
<node id="NUMBER OF LAYERS N"/>
<node id="2N1"/>
<node id="ALL STRATEGIES IN WORST CASE"/>
<node id="THE TWO PRUNING TECHNIQUES"/>
<node id="MOST OF THE STRATEGIES"/>
<node id="THE OPTIMAL ONE"/>
<node id="M N X"/>
<node id="NUMBER OF LAYERS THE FUNCTION CONSIDERS"/>
<node id="FINDOPTGROUPING(B GROUP(X,X I),X I 1)"/>
<node id="K I K LAYERS"/>
<node id="THE OPTIMAL GROUPING STRATEGY FOR CASE I"/>
<node id="THE ASSUMPTION"/>
<node id="THE ALGORITHM"/>
<node id="K1"/>
<node id="THE OPTIMAL STRATEGY FOR THIS CASE"/>
<node id="ONE GROUP"/>
<node id="THESE CASES"/>
<node id="THE ENTIRE SEARCH SPACE"/>
<node id="THE OPTIMAL GROUPING STRATEGY FOR M K 1"/>
<node id="THIS TECHNIQUE"/>
<node id="THE OPTIMALITY"/>
<node id="GROUPING FROM X TO AT LEAST J"/>
<node id="PRUNING THESE CASES"/>
<node id="GROUPING THE LAYERS"/>
<node id="HIGH PIPELINING EFFICIENCY AND LOW PIPELINING OVERHEAD"/>
<node id="THE ORDER"/>
<node id="CORRECTNESS"/>
<node id="AN OPERATOR"/>
<node id="IT IS TRANSMITTED TO THE GPU AND THE INPUT IS READY"/>
<node id="OUR PIPELINED MODEL TRANSMISSION"/>
<node id="THE GENERAL CASE"/>
<node id="PIPELINED MODEL TRANSMISSION EFFECTIVENESS"/>
<node id="MECHANISMS IN 4.2"/>
<node id="FIGURE 7"/>
<node id="EFFECTIVENESS OF PIPELINED MODEL TRANSMISSION"/>
<node id="FUNCTIONS"/>
<node id="WORKERS THROUGH CUDA IPC API"/>
<node id="SHARED GPU MEMORY"/>
<node id="THIS SOLUTION"/>
<node id="HIGH OVERHEAD FOR DL APPLICATIONS"/>
<node id="HIGH OVERHEAD"/>
<node id="TWO REASONS"/>
<node id="MINIMIZATION OF GPU MEMORY MANAGEMENT OVERHEAD"/>
<node id="TRAINING TASK"/>
<node id="MODEL PARAMETERS"/>
<node id="WEIGHTS OF THE NEURAL NETWORK"/>
<node id="DNN STRUCTURE"/>
<node id="AMOUNT OF MEMORY NEEDED TO STORE MODEL PARAMETERS"/>
<node id="THE SAME"/>
<node id="MODEL ITSELF"/>
<node id="INTERMEDIATE RESULTS"/>
<node id="SIMPLE REGULAR PATTERN"/>
<node id="MEMORY FRAGMENTATION"/>
<node id="A TRAINING TASK"/>
<node id="THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS CANNOT BE IMMEDIATELY FREED"/>
<node id="THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS"/>
<node id="THE BACKWARD PASS TO UPDATE THE WEIGHTS"/>
<node id="THE BACKWARD PASS"/>
<node id="THE FORWARD PASS GENERATES THEM"/>
<node id="THE INTERMEDIATE RESULTS"/>
<node id="RST-IN-LAST-OUT"/>
<node id="MEMORY MANAGEMENT MECHANISM"/>
<node id="WORKERS"/>
<node id="OFFSET"/>
<node id="64-BIT INTEGER"/>
<node id="223 MS"/>
<node id="WITH THE MEMORY DAEMON"/>
<node id="EACH WORKER"/>
<node id="MEMORY TO STORE MODEL AND INTERMEDIATE RESULTS"/>
<node id="MEMORY AFTER INTERMEDIATE RESULTS ARE NO LONGER NEEDED"/>
<node id="MEMORY MANAGEMENT IN PYTORCH"/>
<node id="MEMORY ALLOCATION FOR A TASK ITSELF"/>
<node id="STORING THE MODELS IN A DEDICATED PROCESS"/>
<node id="MINIMAL MEMORY FOOTPRINT"/>
<node id="EACH MODEL"/>
<node id="ONLY ONCE"/>
<node id="AN EXTRA MEMORY COPY FROM THIS PROCESS TO A WORKER TO START A TASK"/>
<node id="AN EXTRA MEMORY COPY"/>
<node id="TASK SWITCHING TIME"/>
<node id="IPC"/>
<node id="DL APPLICATIONS TO MINIMIZE IPC OVERHEAD"/>
<node id="THE OVERHEAD"/>
<node id="THE PIPELINE"/>
<node id="THE IPCS FREQUENTLY"/>
<node id="MODEL TRANSMISSION AND TASK EXECUTION FOR EVERY PIPELINE GROUP"/>
<node id="THE IPC ONLY ONCE FOR THE ENTIRE MODEL TRANSMISSION"/>
<node id="NEURAL NETWORK MODEL"/>
<node id="EXPENSIVE GPU IPCS"/>
<node id="LATENCY"/>
<node id="IPC OPTIMIZATION"/>
<node id="NO UNIFIED MEMORY MANAGEMENT"/>
<node id="THE WORKER"/>
<node id="WHICH PIPELINE GROUP HAS BEEN TRANSMITTED"/>
<node id="PIN MEMORY"/>
<node id="A TECHNIQUE IN COMPUTING"/>
<node id="NO PIN"/>
<node id="MEMORY"/>
<node id="A NAIVE SOLUTION"/>
<node id="SEPARATE PROCESSES"/>
<node id="CURRENT TASK IS STOPPED"/>
<node id="CURRENT AND NEW TASKS"/>
<node id="THE SAME PROCESS WITH A WARM CUDA CONTEXT"/>
<node id="THE GPU ENVIRONMENT OF THE CURRENT TASK"/>
<node id="THE PROCESS OF THE OLD TASK"/>
<node id="THE GPU ENVIRONMENT"/>
<node id="ANOTHER PROCESS"/>
<node id="THE CURRENT TASK"/>
<node id="OVERHEAD TO CLEAN ITS STATUS"/>
<node id="ANOTHER JOB"/>
<node id="FREE ITS GPU MEMORY"/>
<node id="THE CLEANING PROCEDURE"/>
<node id="THE CONTENT OF THE MEMORY"/>
<node id="THE METADATA"/>
<node id="GPU MEMORY POINTERS"/>
<node id="CLEANING PROCEDURE"/>
<node id="POINTERS POINTING TO THE TENSOR DATA"/>
<node id="ACTUAL DATA"/>
<node id="THE GPU MEMORY AT THE SAME TIME"/>
<node id="AN ADDITIONAL ZERO-OUT OPERATION"/>
<node id="A CONCERN"/>
<node id="900GBS_FOR_V100"/>
<node id="TRUSTED ENVIRONMENT"/>
<node id="WHEN NEW PROCESS DOES NOT REQUIRE ENTIRE GPU MEMORY"/>
<node id="ACHIEVING MEMORY MANAGEMENT WITHOUT FULL RELEASE"/>
<node id="SIMPLE COORDINATION"/>
<node id="DIFFERENCES BETWEEN THREE SOLUTIONS"/>
<node id="MANY STANDBY WORKERS"/>
<node id="AT LEAST ONE IDLE STANDBY WORKER"/>
<node id="A TRANSACTION"/>
<node id="ENABLE OR DISABLE INFERENCE ON THIS MODEL"/>
<node id="PRODUCTION GPU TRAINING TRACE"/>
<node id="MICROSOFT"/>
<node id="THESE SCHEDULING SOLUTIONS"/>
<node id="THESE SOLUTIONS"/>
<node id="HTTPS"/>
<node id="PYTORCH.ORG"/>
<node id="THE SCHEDULER AND THE MEMORY DAEMON"/>
<node id="BETTER PERFORMANCE"/>
<node id="THE TCP THREAD"/>
<node id="CLIENTS"/>
<node id="THE SCHEDULER THREAD"/>
<node id="PARAMETERS"/>
<node id="GROUPS"/>
<node id="WORKER TO START COMPUTING CORRESPONDING LAYERS AFTER EACH GROUP IS TRANSFERRED"/>
<node id="THE WORKER PROCESS"/>
<node id="TWO THREADS"/>
<node id="8 VCPUS (INTEL XEON E5-2686 V4)"/>
<node id="1 GPU (NVIDIA V100 WITH 16 GB GPU MEMORY)"/>
<node id="PCIE 3.0 16"/>
<node id="61 GB MEMORY"/>
<node id="G4DN.2XLARGE INSTANCE"/>
<node id="8 VCPUS (INTEL PLATINUM 8259CL)"/>
<node id="NVIDIA T4"/>
<node id="16 GB GPU MEMORY"/>
<node id="PCIE 3.0 8"/>
<node id="32 GB MEMORY"/>
<node id="THE SOFTWARE ENVIRONMENT"/>
<node id="PYTORCH-1.3.0"/>
<node id="TORCHVISION-0.4.2"/>
<node id="SCIPY-1.3.2"/>
<node id="CUDA-10.1"/>
<node id="THE MODELS"/>
<node id="RESNET152 17"/>
<node id="INCEPTIONV3 22"/>
<node id="BERTBASE 23"/>
<node id="STANDARD BENCHMARK FOR EVALUATING DL SYSTEMS"/>
<node id="THE EXPERIMENTS"/>
<node id="BOTH TRAINING AND INFERENCE"/>
<node id="CHECKPOINTING FREQUENCY OF TRAINING TASKS"/>
<node id="SCHEDULING CYCLE"/>
<node id="CHECKPOINTING OVERHEAD"/>
<node id="THROUGHPUT AND LATENCY"/>
<node id="THE END-TO-END LATENCY EXPERIENCED BY THE CLIENT"/>
<node id="FIGURE 5"/>
<node id="TOTAL LATENCY EXPERIENCED BY THE CLIENT FOR DIFFERENT MECHANISMS"/>
<node id="EACH NUMBER"/>
<node id="100 RUNS"/>
<node id="END-TO-END OVERHEAD"/>
<node id="READY MODEL"/>
<node id="A MODEL THAT IS PREPARED OR SET"/>
<node id="ANY"/>
<node id="THE LOWER BOUND"/>
<node id="THE LOWEST LATENCY ACHIEVABLE FOR AN INFERENCE TASK"/>
<node id="A TECHNOLOGY DEVELOPED BY NVIDIA"/>
<node id="SEPARATE PROCESSES IN ADVANCE"/>
<node id="CUDA UNIFIED MEMORY"/>
<node id="A MEMORY MANAGEMENT FEATURE IN CUDA"/>
<node id="4"/>
<node id="DEVBLOGS.NVIDIA.COM"/>
<node id="UNIFIED MEMORY CUDA FOR BEGINNERS"/>
<node id="THE PROPERTIES"/>
<node id="NVIDIA V100 GPU"/>
<node id="READY MODEL, PIPESWITCH, MPS, STOP-AND-START"/>
<node id="RESNET152, INCEPTIONV3, BERTBASE"/>
<node id="NVIDIA T4 GPU"/>
<node id="LATENCY (MS)"/>
<node id="5000 TO 10000"/>
<node id="METRIC"/>
<node id="READY MODEL PIPESWITCH MPS STOP-AND-START LATENCY"/>
<node id="USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="510 14TH"/>
<node id="RESNET152"/>
<node id="INCEPTIONV3"/>
<node id="BERTBASE"/>
<node id="NO MEMORY MANAGEMENT"/>
<node id="NO IPC OPTIMIZATION"/>
<node id="NO PIN MEMORY"/>
<node id="NVIDIA V100"/>
<node id="0 TO 400"/>
<node id="PIPE SWITCH"/>
<node id="DISABLED"/>
<node id="MEMORY MANAGEMENT"/>
<node id="P3.2XLARGE INSTANCE"/>
<node id="NVIDIA V100 PCIE 3.0 16"/>
<node id="LATENCY MEASUREMENT"/>
<node id="6000 MS TO 8000 MS"/>
<node id="LATENCY METRIC"/>
<node id="PIPESWITCH WITH ONE PROCESS AND TWO PROCESSES"/>
<node id="5000 MS TO 7500 MS"/>
<node id="ONE PROCESS"/>
<node id="INSTANCE_TYPE"/>
<node id="RESNET152 ON P3.2XLARGE"/>
<node id="3.62 MS"/>
<node id="INCEPTIONV3 ON P3.2XLARGE"/>
<node id="4.82 MS"/>
<node id="BERTBASE ON P3.2XLARGE"/>
<node id="RESNET152 ON G4DN.2XLARGE"/>
<node id="2.53 MS"/>
<node id="INCEPTIONV3 ON G4DN.2XLARGE"/>
<node id="5.49 MS"/>
<node id="BERTBASE ON G4DN.2XLARGE"/>
<node id="6.57 MS"/>
<node id="TABLE 4"/>
<node id="STARTUP OVERHEAD FOR PIPESWITCH TO START COMPUTING THE RST LAYER"/>
<node id="TASK STARTUP OVERHEAD FOR PIPESWITCH"/>
<node id="RESNET152 INCEPTIONV3 BERTBASE"/>
<node id="LAYERS"/>
<node id="464 189 139"/>
<node id="1.33 S 0.18 S 0.34 S"/>
<node id="ONLY PRUNING 1"/>
<node id="2.09 S 0.30 S 0.88 S"/>
<node id="ONLY PRUNING 2"/>
<node id="3.44 H 5.07 S"/>
<node id="NO PRUNING"/>
<node id="24 H 24 H 24 H"/>
<node id="TABLE 5"/>
<node id="EFFECTIVENESS OF TWO PRUNING TECHNIQUES"/>
<node id="SALUS 7"/>
<node id="SECTION 2.2"/>
<node id="THE MAIN SOURCE OF THE OVERHEAD"/>
<node id="CUDA CONTEXT INITIALIZATION AND RST-TIME LIBRARY LOADING OPERATIONS IN PYTORCH"/>
<node id="MEMORY SWAPPING"/>
<node id="PIPESWITCH OVERHEAD FOR MOST CONFIGURATIONS"/>
<node id="10MS"/>
<node id="BERT ON T4 OVERHEAD"/>
<node id="LARGE MODEL SIZE"/>
<node id="SMALLER PCIE BANDWIDTH ON T4 COMPARED TO V100"/>
<node id="THIS EXPERIMENT"/>
<node id="THROUGHPUT AND END-TO-END LATENCY OF DIFFERENT MECHANISMS"/>
<node id="DIFFERENT SCHEDULING CYCLES"/>
<node id="THE DASHED LINE"/>
<node id="THE THROUGHPUT OF THE READY MODEL ASSUMING NO TASK SWITCHING"/>
<node id="THE ERROR BAR"/>
<node id="MINIMUM AND MAXIMUM LATENCY"/>
<node id="7500 TO 10000"/>
<node id="PIPESWITCH MPS STOP-AND-START"/>
<node id="1S 2S 5S 10S 30S"/>
<node id="LOWER BOUND (B) LATENCY"/>
<node id="IT GROUPS THE ENTIRE MODEL"/>
<node id="ONE TRANSMISSION"/>
<node id="PER-LAYER PIPELINE"/>
<node id="PIPELINE ORGANIZATION AT EACH LAYER"/>
<node id="PARAMETERS ARE TRANSMITTED"/>
<node id="NO OPTIMIZATION"/>
<node id="LAYERS OF THE MODEL INTO ONE BIG TENSOR"/>
<node id="ONE BIG TENSOR IN ONE GROUP"/>
<node id="REDUCTION"/>
<node id="OPTIMIZATIONS ON MEMORY MANAGEMENT AND WORKER SWITCHING HAVE ALREADY BEEN APPLIED"/>
<node id="MEETING STRICT SLOS"/>
<node id="ALL OVERHEADS FOR TASK SWITCHING"/>
<node id="THE PARAMETER SIZE FOR EACH LAYER IN ADVANCE"/>
<node id="THE RUNNING TIME FOR EACH LAYER IN ADVANCE"/>
<node id="THE EFFECTIVENESS OF UNIFIED MEMORY MANAGEMENT"/>
<node id="UNIFIED STRUCTURE"/>
<node id="FIGURE 8"/>
<node id="EFFECTIVENESS OF UNIFIED MEMORY MANAGEMENT"/>
<node id="PIPESWITCH THE SAME"/>
<node id="THE FOLLOWING VE MECHANISMS DISCUSSED IN 4.3"/>
<node id="OPTIMIZATION"/>
<node id="PAGES OF THE MEMORY DAEMON"/>
<node id="THE MAIN MEMORY"/>
<node id="ALL THE OPTIMIZATIONS ON MEMORY MANAGEMENT"/>
<node id="UNIFIED MEMORY MANAGEMENT MECHANISM"/>
<node id="1648 MS"/>
<node id="FIGURE 9"/>
<node id="EFFECTIVENESS OF ACTIVE-STANDBY SWITCHING"/>
<node id="THE RESULTS"/>
<node id="THE NEW PROCESS"/>
<node id="A NEW CUDA ENVIRONMENT"/>
<node id="THE TOTAL TIME"/>
<node id="ALGORITHMS AND SYSTEMS"/>
<node id="DEEP LEARNING TASKS ON CLUSTERS"/>
<node id="DEEP LEARNING TASKS"/>
<node id="TRAINING AND INFERENCE TASKS"/>
<node id="MANY TECHNIQUES AND SYSTEMS"/>
<node id="COMMUNICATION"/>
<node id="DISTRIBUTED TRAINING"/>
<node id="OTHER WORKS LIKE VDNN 43 AND SWAPADVISOR 44"/>
<node id="GPU MEMORY MANAGEMENT MODULE"/>
<node id="MEMORY MANAGEMENT FOR A SINGLE TRAINING TASK OF LARGE MODELS"/>
<node id="CLUSTER MANAGERS 4548"/>
<node id="GPUS TO VMS OR CONTAINERS AT DEVICE GRANULARITY"/>
<node id="GPU OPTIMIZATION EFFORTS"/>
<node id="RUNNING A SINGLE TASK"/>
<node id="TENSOR FUSION"/>
<node id="KERNEL-LEVEL CONCURRENCY"/>
<node id="KERNEL-LEVEL SCHEDULING"/>
<node id="A. VERMA, L. PEDROSA, M. KORUPOLU, D. OPPENHEIMER, E. TUNE, AND J. WILKES"/>
<node id="LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG"/>
<node id="EUROSYS"/>
<node id="2015"/>
<node id="DEAN AND L. A. BARROSO"/>
<node id="THE TAIL AT SCALE"/>
<node id="COMMUNICATIONS OF THE ACM"/>
<node id="VOL."/>
<node id="NEXUS"/>
<node id="GPU CLUSTER ENGINE FOR ACCELERATING DNN-BASED VIDEO ANALYSIS"/>
<node id="ACM SOSP 2019"/>
<node id="H. SHEN, L. CHEN, Y. JIN, L. ZHAO, B. KONG, M. PHILIPOSE, A. KRISHNAMURTHY, AND R. SUNDARAM"/>
<node id="5 A. OUSTERHOUT, J."/>
<node id="AUTHOR NAME"/>
<node id="FRIED, J. BEHRENS, A. BELAY, AND H. BALAKRISHNAN"/>
<node id="SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS"/>
<node id="SHENANGO"/>
<node id="HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS"/>
<node id="PAPER"/>
<node id="USENIX NSDI"/>
<node id="CUDA MULTI-PROCESS SERVICE"/>
<node id="6"/>
<node id="DOCUMENT HTTPS://DOCS.NVIDIA.COM/DEPLOY/PDF/CUDA_MULTIPROCESS_SERVICE_OVERVIEW.PDF"/>
<node id="HTTPS://DOCS.NVIDIA.COM/DEPLOY/PDF/CUDA_MULTIPROCESS_SERVICE_OVERVIEW.PDF"/>
<node id="SALUS"/>
<node id="P. YU AND M. CHOWDHURY"/>
<node id="FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS"/>
<node id="CONFERENCE ON MACHINE LEARNING AND SYSTEMS"/>
<node id="2020"/>
<node id="PIPEDREAM"/>
<node id="GENERALIZED PIPELINE PARALLELISM FOR DNN TRAINING"/>
<node id="D. NARAYANAN, A. HARLAP, A. PHANISHAYEE, V. SESHADRI, N. R. DEVANUR, G. R. GANGER, P. B. GIBBONS, AND M. ZAHARIA"/>
<node id="TIRESIAS"/>
<node id="GPU CLUSTER MANAGER FOR DISTRIBUTED DEEP LEARNING"/>
<node id="USENIX NSDI 2019"/>
<node id="J. GU, M. CHOWDHURY, K. G. SHIN, Y. ZHU, M. JEON, J. QIAN, H. LIU, AND C. GUO"/>
<node id="POSEIDON"/>
<node id="EFFICIENT COMMUNICATION ARCHITECTURE FOR DISTRIBUTED DEEP LEARNING ON GPU CLUSTERS"/>
<node id="USENIX ATC 2017"/>
<node id="H. ZHANG, Z. ZHENG, S. XU, W. DAI, Q. HO, X. LIANG, Z. HU, J. WEI, P. XIE, AND E. P. XING"/>
<node id="AMAZON WEB SERVICES"/>
<node id="12"/>
<node id="AWS.AMAZON.COM"/>
<node id="MICROSOFT AZURE"/>
<node id="CLOUD COMPUTING PLATFORM"/>
<node id="AZURE.MICROSOFT.COM"/>
<node id="GOOGLE CLOUD PLATFORM"/>
<node id="14"/>
<node id="CLOUD.GOOGLE.COM"/>
<node id="HOROVOD"/>
<node id="FAST AND EASY DISTRIBUTED DEEP LEARNING FRAMEWORK"/>
<node id="TENSORFLOW"/>
<node id="A. SERGEEV AND M. DEL BALSO"/>
<node id="HOROVOD PAPER"/>
<node id="ARXIV PREPRINT ARXIV:1802.05799"/>
<node id="2018"/>
<node id="SU"/>
<node id="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER"/>
<node id="USENIX OSDI"/>
<node id="2014"/>
<node id="17 K. HE, X. ZHANG, S. REN, AND J."/>
<node id="AUTHORS"/>
<node id="NVIDIA DATA CENTER DEEP LEARNING PRODUCT"/>
<node id="PERFORMANCE"/>
<node id="DEVELOPER.NVIDIA.COM/DEEP-LEARNING-PERFORMANCE-TRAINING-INFERENCE"/>
<node id="PHILLY TRACES"/>
<node id="20"/>
<node id="GITHUB REPOSITORY"/>
<node id="HTTPS://GITHUB.COM/MSR-FIDDLE/PHILLY-TRACES"/>
<node id="21"/>
<node id="SZEGEDY ET AL."/>
<node id="RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION"/>
<node id="IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"/>
<node id="2016"/>
<node id="J. DEVLIN, M.-W. CHANG, K. LEE, AND K. TOUTANOVA"/>
<node id="BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING"/>
<node id="PROCEEDINGS OF THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, VOLUME 1 (LONG AND SHORT PAPERS)"/>
<node id="GANDIVA"/>
<node id="INTROSPECTIVE CLUSTER SCHEDULING FOR DEEP LEARNING"/>
<node id="24 W. XIAO, R. BHARDWAJ, R. RAMJEE, M. SIVATHANU, N. KWATRA, Z. HAN, P. PATEL, X. PENG, H. ZHAO, Q. ZHANG, ET AL."/>
<node id="25"/>
<node id="TENSORFLOW XLA"/>
<node id="54"/>
<node id="HTTPS://WWW.TENSORFLOW.ORG"/>
<node id="THE OFFICIAL WEBSITE OF TENSORFLOW"/>
<node id="XLA"/>
<node id="MXNET"/>
<node id="26"/>
<node id="MXNET.APACHE.ORG"/>
<node id="SLAQ"/>
<node id="QUALITY-DRIVEN SCHEDULING FOR DISTRIBUTED MACHINE LEARNING"/>
<node id="M. J. FREEDMAN"/>
<node id="ACM SYMPOSIUM ON CLOUD COMPUTING"/>
<node id="2017"/>
<node id="OPTIMUS"/>
<node id="EFFICIENT DYNAMIC RESOURCE SCHEDULER FOR DEEP LEARNING CLUSTERS"/>
<node id="EUROSYS 2018"/>
<node id="Y. PENG, Y. BAO, Y. CHEN, C. WU, AND C. GUO"/>
<node id="THEMIS"/>
<node id="USENIX NSDI 2020"/>
<node id="FAIR AND EFFICIENT GPU CLUSTER SCHEDULING"/>
<node id="K. MAHAJAN, A. BALASUBRAMANIAN, A. SINGHVI, S. VENKATARAMAN, A. AKELLA, A. PHANISHAYEE, S. CHAWLA"/>
<node id="HYPERSCHED"/>
<node id="R. LIAW, R. BHARDWAJ, L. DUNLAP, Y. ZOU, J. E. GONZALEZ, I. STOICA, A. TUMANOV"/>
<node id="DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT"/>
<node id="CHET"/>
<node id="FULLY-HOMOMORPHIC_NEURAL-NETWORK_INFERENCING"/>
<node id="ACM_CONFERENCE_ON_PROGRAMMING_LANGUAGE_DESIGN_AND_IMPLEMENTATION"/>
<node id="R. DATHATHRI, O. SAARIKIVI, H. CHEN, K. LAINE, K. LAUTER, S. MALEKI, M. MUSUVATHI, AND T. MYTKOWICZ"/>
<node id="TVM"/>
<node id="deep learning"/>
<node id="USENIX OSDI 2018"/>
<node id="T. CHEN, T. MOREAU, Z. JIANG, L. ZHENG, E. YAN, H. SHEN, M. COWAN, L. WANG, Y. HU, L. CEZE, ET AL."/>
<node id="GPIPE"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 2019"/>
<node id="EFFICIENT TRAINING OF GIANT NEURAL NETWORKS"/>
<node id="PIPELINE PARALLELISM"/>
<node id="33 Y. HUANG, Y. CHENG, A. BAPNA, O. FIRAT, D. CHEN, M. CHEN, H. LEE, J. NGIAM, Q. V. LE, Y. WU, ET AL."/>
<node id="BLINK"/>
<node id="FAST AND GENERIC COLLECTIVES FOR DISTRIBUTED MACHINE LEARNING"/>
<node id="G. WANG, S. VENKATARAMAN, A. PHANISHAYEE, J. THELIN, N. DEVANUR, AND I. STOICA"/>
<node id="DEVELOPER.NVIDIA.COM/NCCL"/>
<node id="36 J. LIU, J. WU, AND D. K. PANDA"/>
<node id="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION OVER INNIBAND"/>
<node id="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION"/>
<node id="INT."/>
<node id="37 Q. HO, J. CIPAR, H. CUI, S. LEE, J. K. KIM, P. B. GIBBONS, G. A. GIBSON, G. GANGER, AND E. P. XING"/>
<node id="MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS"/>
<node id="2013"/>
<node id="A. AWAN, C.-H. CHU, H. SUBRAMONI, AND D. K. PANDA"/>
<node id="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?"/>
<node id="PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING"/>
<node id="GOSSIPGRAD"/>
<node id="SCALABLE DEEP LEARNING USING GOSSIP COMMUNICATION BASED ASYNCHRONOUS GRADIENT DESCENT"/>
<node id="CORR VOL."/>
<node id="A. VISHNU"/>
<node id="GOSSIPGRAD PAPER"/>
<node id="C. SIEGEL"/>
<node id="T. WARFEL"/>
<node id="V. AMATYA"/>
<node id="41 Z. ZHANG, C. CHANG, H. LIN, Y. WANG, R. ARORA, AND X. JIN"/>
<node id="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?"/>
<node id="PAPER IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?"/>
<node id="ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)"/>
<node id="AUGUST 2020"/>
<node id="42 Y. CHEN, Z. LIU, B. REN, AND X. JIN"/>
<node id="EFFICIENT CONSTRUCTIONS OF CHECKPOINTS"/>
<node id="PAPER EFFICIENT CONSTRUCTIONS OF CHECKPOINTS"/>
<node id="INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)"/>
<node id="JULY 2020"/>
<node id="VDNN"/>
<node id="VIRTUALIZED DEEP NEURAL NETWORKS FOR SCALABLE MEMORY-EFFICIENT NEURAL NETWORK DESIGN"/>
<node id="2016 49TH ANNUAL IEEEACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO)"/>
<node id="M. RHU, N. GIMELSHEIN, J. CLEMONS, A. ZULQAR, AND S. W. KECKLER"/>
<node id="KUBERNETES.IO"/>
<node id="NVIDIA CONTAINER RUNTIME"/>
<node id="DOCKER"/>
<node id="GITHUB.COM/NVIDIA/NVIDIA-DOCKER"/>
<node id="MESOS"/>
<node id="FINE-GRAINED RESOURCE SHARING IN THE DATA CENTER"/>
<node id="47 B. HINDMAN, A. KONWINSKI, M. ZAHARIA, A. GHODSI, A. D. JOSEPH, R. H. KATZ, S. SHENKER, AND I. STOICA"/>
<node id="MESOS: A PLATFORM FOR FINE-GRAINED RESOURCE SHARING IN THE DATA CENTER"/>
<node id="USENIX NSDI 2011"/>
<node id="APACHE HADOOP YARN"/>
<node id="YET ANOTHER RESOURCE NEGOTIATOR"/>
<node id="48 V. K. VAVILAPALLI, A. C. MURTHY, C. DOUGLAS, S. AGARWAL, M. KONAR, R. EVANS, T. GRAVES, J. LOWE, H. SHAH, S. SETH, ET AL."/>
<node id="APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR"/>
<node id="GPGPU TRANSPARENT VIRTUALIZATION COMPONENT"/>
<node id="HIGH PERFORMANCE COMPUTING CLOUDS"/>
<node id="EUROPEAN CONFERENCE ON PARALLEL PROCESSING"/>
<node id="2010"/>
<node id="G. GIUNTA, R. MONTELLA, G. AGRILLO, AND G. COVIELLO"/>
<node id="V. T. RAVI, M. BECCHI, G. AGRAWAL, AND S. CHAKRADHAR"/>
<node id="SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK"/>
<node id="PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING"/>
<node id="2011"/>
<node id="RCUDA"/>
<node id="GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS"/>
<node id="2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION"/>
<node id="J. DUATO, A. J. PENA, F. SILLA, R. MAYO, AND E. S. QUINTANA-ORT"/>
<node id="VCUDA"/>
<node id="IN VIRTUAL MACHINES"/>
<node id="SUN AND K. LI"/>
<node id="IEEE TRANSACTIONS ON COMPUTERS"/>
<node id="A FLEXIBLE AND EFFICIENT MACHINE LEARNING LIBRARY"/>
<node id="HETEROGENEOUS DISTRIBUTED SYSTEMS"/>
<node id="T. CHEN, M. LI, Y. LI, M. LIN, N. WANG, M. WANG, T. XIAO, B. XU, C. ZHANG, AND Z. ZHANG"/>
<node id="MXNET: A FLEXIBLE AND EFFICIENT MACHINE LEARNING LIBRARY FOR HETEROGENEOUS DISTRIBUTED SYSTEMS"/>
<node id="MXNET PAPER"/>
<node id="ARXIV PREPRINT ARXIV:1512.01274"/>
<node id="56 C. GREGG, J. DORN, K. HAZELWOOD, AND K. SKADRON"/>
<node id="FINE-GRAINED RESOURCE SHARING FOR CONCURRENT GPGPU KERNELS"/>
<node id="4TH USENIX WORKSHOP ON HOT TOPICS IN PARALLELISM"/>
<node id="2012"/>
<node id="57 S. PAI, M. J. THAZHUTHAVEETIL, AND R. GOVINDARAJAN"/>
<node id="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS"/>
<node id="ACM SIGARCH COMPUTER ARCHITECTURE NEWS"/>
<node id="TASO"/>
<node id="DEEP LEARNING COMPUTATION"/>
<node id="AUTOMATIC GENERATION OF GRAPH SUBSTITUTIONS"/>
<node id="Z. JIA, O. PADON, J. THOMAS, T. WARSZAWSKI, M. ZAHARIA, AND A. AIKEN"/>
<edge source="THE DOMINANT PRACTICE TODAY" target="DEDICATED GPU CLUSTERS FOR TRAINING AND INFERENCE SEPARATELY">
  <data key="d0">PROVISIONS</data>
</edge>
<edge source="GPU CLUSTERS" target="SHARED GPU CLUSTERS">
  <data key="d0">ARE DESCRIBED AS</data>
</edge>
<edge source="GPU CLUSTERS" target="DIFFERENT APPLICATIONS">
  <data key="d0">CAN BE SHARED ACROSS</data>
</edge>
<edge source="GPU CLUSTERS" target="EITHER PRIVATELY OR PUBLICLY SHARED BY MULTIPLE USERS">
  <data key="d0">ARE</data>
</edge>
<edge source="FINE-GRAINED TIME-SHARING GPU" target="GPU CLUSTERS">
  <data key="d0">IS ENVISIONED TO BUILD</data>
</edge>
<edge source="DIFFERENT APPLICATIONS" target="TRAINING AND INFERENCE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="PIPESWITCH" target="AN INFERENCE APPLICATION TO BE FILLED BY TRAINING OR OTHER INFERENCE APPLICATIONS">
  <data key="d0">ENABLES UNUSED CYCLES OF</data>
</edge>
<edge source="PIPESWITCH" target="GPU UTILIZATION">
  <data key="d0">CAN SIGNIFICANTLY IMPROVE</data>
</edge>
<edge source="PIPESWITCH" target="SLOS">
  <data key="d0">CAN IMPROVE GPU UTILIZATION WITHOUT SACRIFICING</data>
</edge>
<edge source="PIPESWITCH" target="SINGLE-GPU TRAINING">
  <data key="d0">SUPPORTS TRAINING USING SINGLE GPU</data>
</edge>
<edge source="PIPESWITCH" target="ASYNCHRONOUS MULTI-GPU TRAINING">
  <data key="d0">SUPPORTS TRAINING USING ASYNCHRONOUS MULTI-GPU</data>
</edge>
<edge source="PIPESWITCH" target="A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS OUT OF THE BOX">
  <data key="d0">IS APPLICABLE TO</data>
</edge>
<edge source="PIPESWITCH" target="SYNCHRONOUS MULTI-GPU TRAINING">
  <data key="d0">CAN BE USED FOR</data>
</edge>
<edge source="PIPESWITCH" target="GPU SERVERS VIA FINE-GRAINED TIME-SHARING">
  <data key="d0">ENABLES GPU-EFFICIENT MULTIPLEXING OF MANY DL APPLICATIONS</data>
</edge>
<edge source="PIPESWITCH" target="AS DEDICATED SERVERS">
  <data key="d0">ACHIEVES HIGH THROUGHPUT</data>
</edge>
<edge source="PIPESWITCH" target="MULTIPLE DEEP LEARNING APPLICATIONS">
  <data key="d0">ENABLES GPU-EFFICIENT FINE-GRAINED TIME-SHARING FOR</data>
</edge>
<edge source="PIPESWITCH" target="MILLISECONDS">
  <data key="d0">ACHIEVES CONTEXT SWITCHING LATENCIES ON THE SCALE OF</data>
</edge>
<edge source="PIPESWITCH" target="HIGH THROUGHPUT">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="PIPESWITCH" target="MILLISECOND-SCALE TASK SWITCHING TIME">
  <data key="d0">ACHIEVES TASK SWITCHING TIME ON THE SCALE OF MILLISECONDS</data>
</edge>
<edge source="PIPESWITCH" target="DL APPLICATIONS ON TIME-SHARING GPUS">
  <data key="d0">ENABLES DL APPLICATIONS TO OPERATE ON TIME-SHARING GPUS</data>
</edge>
<edge source="PIPESWITCH" target="PROCESS-LEVEL ISOLATION WITH SEPARATE WORKER PROCESSES">
  <data key="d0">ENFORCES</data>
</edge>
<edge source="PIPESWITCH" target="NEW TECHNICAL CHALLENGES ON MEMORY MANAGEMENT AND WORKER SWITCHING ACROSS DIFFERENT PROCESSES">
  <data key="d0">REQUIRES ADDRESSING</data>
</edge>
<edge source="PIPESWITCH" target="INTRA-BATCH PIPELINING">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="PIPESWITCH" target="INTRA-BATCH PIPELINING TO FAST START TRAINING AND INFERENCE TASKS">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="PIPESWITCH" target="FAST SWITCHING ACROSS TASKS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PIPESWITCH" target="MODELS">
  <data key="d0">REQUIRES_KNOWLEDGE_OF</data>
</edge>
<edge source="PIPESWITCH" target="DNN FRAMEWORK">
  <data key="d0">CAN BE IMPLEMENTED AS PART OF</data>
</edge>
<edge source="PIPESWITCH" target="MODEL STRUCTURE INFORMATION">
  <data key="d0">CAN GATHER</data>
</edge>
<edge source="PIPESWITCH" target="USERS AND CLUSTER MANAGERS">
  <data key="d0">REMAINS TRANSPARENT TO</data>
</edge>
<edge source="PIPESWITCH" target="WORKERS">
  <data key="d0">SENDS OFFSET FOR SHARED GPU MEMORY TO</data>
</edge>
<edge source="PIPESWITCH" target="223 MS">
  <data key="d0">SAVES TIME COMPARED TO NO UNIFIED MEMORY MANAGEMENT</data>
</edge>
<edge source="PIPESWITCH" target="WITH THE MEMORY DAEMON">
  <data key="d0">ELIMINATES MEMORY ALLOCATION OVERHEAD</data>
</edge>
<edge source="PIPESWITCH" target="MEMORY DAEMON">
  <data key="d0">STORES MODELS IN</data>
</edge>
<edge source="PIPESWITCH" target="NO MEMORY MANAGEMENT">
  <data key="d0">OPERATES WITH</data>
</edge>
<edge source="PIPESWITCH" target="NO IPC OPTIMIZATION">
  <data key="d0">OPERATES WITH</data>
</edge>
<edge source="PIPESWITCH" target="NO PIN MEMORY">
  <data key="d0">OPERATES WITH</data>
</edge>
<edge source="PIPESWITCH" target="CUDA UNIFIED MEMORY">
  <data key="d0">OPERATES WITH</data>
</edge>
<edge source="PIPESWITCH" target="ONE PROCESS">
  <data key="d0">SUPPORTS_NUMBER_OF_PROCESSES</data>
</edge>
<edge source="PIPESWITCH" target="TWO PROCESSES">
  <data key="d0">SUPPORTS_NUMBER_OF_PROCESSES</data>
</edge>
<edge source="ASYNCHRONOUS MULTI-GPU TRAINING" target="DATA PARALLEL STRATEGIES">
  <data key="d0">APPLIES TO</data>
</edge>
<edge source="PREEMPTING ONE GPU" target="OTHER GPUS">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS" target="A SINGLE GPU">
  <data key="d0">CURRENTLY USE</data>
</edge>
<edge source="ELASTIC SYNCHRONOUS TRAINING" target="NUMBER OF GPUS USED FOR TRAINING">
  <data key="d0">ALLOWS DYNAMIC CHANGING OF</data>
</edge>
<edge source="ONE WAY" target="PIPESWITCH FOR SYNCHRONOUS MULTI-GPU TRAINING">
  <data key="d0">TO SEAMLESSLY USE</data>
</edge>
<edge source="WE" target="PIPELINED CONTEXT SWITCHING">
  <data key="d0">ACHIEVE BY INTRODUCING</data>
</edge>
<edge source="WE" target="A MAJOR CHALLENGE FAST GPU CONTEXT SWITCHING BETWEEN DIFFERENT PROCESSES">
  <data key="d0">FACE</data>
</edge>
<edge source="WE" target="A MEASUREMENT STUDY">
  <data key="d0">PERFORM</data>
</edge>
<edge source="WE" target="A HOLISTIC APPROACH">
  <data key="d0">TAKE</data>
</edge>
<edge source="WE" target="THE CHARACTERISTICS OF DL APPLICATIONS">
  <data key="d0">EXPLOIT</data>
</edge>
<edge source="WE" target="A SYSTEM PROTOTYPE">
  <data key="d0">IMPLEMENTED</data>
</edge>
<edge source="WE" target="PYTORCH">
  <data key="d0">INTEGRATED SYSTEM PROTOTYPE WITH</data>
</edge>
<edge source="WE" target="OTHER CHALLENGES LIKE MEMORY MANAGEMENT AND WORKER SWITCHING">
  <data key="d0">NEED TO RESOLVE</data>
</edge>
<edge source="WE" target="FAST CONTEXT SWITCHING">
  <data key="d0">FOCUS_ON</data>
</edge>
<edge source="WE" target="END-TO-END EXPERIMENTS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="EFFECTIVENESS OF DESIGN CHOICES ON EACH COMPONENT">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="OUR DESIGN">
  <data key="d0">DESCRIBE</data>
</edge>
<edge source="WE" target="THE NETWORK TIME">
  <data key="d0">EXCLUDE</data>
</edge>
<edge source="WE" target="THE TASK QUEUEING TIME">
  <data key="d0">EXCLUDE</data>
</edge>
<edge source="WE" target="ALL THE COMPONENTS SHOULD BE OPTIMIZED TO ACHIEVE MINIMAL SWITCHING OVERHEAD AND MEET THE SLOS">
  <data key="d0">EMPHASIZE THAT</data>
</edge>
<edge source="WE" target="ALL POSSIBLE COMBINATIONS">
  <data key="d0">CAN ENUMERATE</data>
</edge>
<edge source="WE" target="TWO PRUNING TECHNIQUES">
  <data key="d0">INTRODUCE</data>
</edge>
<edge source="WE" target="(I 1) TO J J">
  <data key="d0">CAN PRUNE THE CASES THAT GROUP FROM LAYER</data>
</edge>
<edge source="WE" target="J J ALGORITHM">
  <data key="d0">CAN ONLY SEARCH FOR</data>
</edge>
<edge source="WE" target="THE PROBLEM">
  <data key="d0">INTEND TO FORMULATE</data>
</edge>
<edge source="WE" target="DL APPLICATIONS TO MINIMIZE IPC OVERHEAD">
  <data key="d0">LEVERAGE PROPERTY OF</data>
</edge>
<edge source="WE" target="EACH MODEL">
  <data key="d0">USE REPRESENTATIVE CONFIGURATIONS FOR</data>
</edge>
<edge source="WE" target="THROUGHPUT AND LATENCY">
  <data key="d0">USE_AS_EVALUATION_METRICS</data>
</edge>
<edge source="WE" target="THE END-TO-END LATENCY EXPERIENCED BY THE CLIENT">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="SEPARATE PROCESSES IN ADVANCE">
  <data key="d0">INITIALIZE</data>
</edge>
<edge source="WE" target="THE PARAMETER SIZE FOR EACH LAYER IN ADVANCE">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="THE RUNNING TIME FOR EACH LAYER IN ADVANCE">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="PIPESWITCH THE SAME">
  <data key="d0">KEEP ALL OTHER COMPONENTS OF</data>
</edge>
<edge source="WE" target="THE FOLLOWING VE MECHANISMS DISCUSSED IN 4.3">
  <data key="d0">COMPARE</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="DL APPLICATIONS">
  <data key="d0">EXPLOITS CHARACTERISTICS OF</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="PIPELINED MODEL TRANSMISSION">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="UNIFIED MEMORY MANAGEMENT">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="ACTIVE-STANDBY WORKER SWITCHING">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="SWITCHING OVERHEAD">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">ENFORCES</data>
</edge>
<edge source="THE KEY IDEA" target="THE LAYERED STRUCTURE OF NEURAL NETWORK MODELS">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="THE KEY IDEA" target="THE LAYER-BY-LAYER COMPUTATION PATTERN">
  <data key="d0">UTILIZES</data>
</edge>
<edge source="THE KEY IDEA" target="THE PCIE">
  <data key="d0">PIPELINES MODEL TRANSMISSION OVER</data>
</edge>
<edge source="THE KEY IDEA" target="THE GPU">
  <data key="d0">PIPELINES TASK EXECUTION IN</data>
</edge>
<edge source="THE KEY IDEA" target="MODEL-AWARE GROUPING">
  <data key="d0">APPLIES</data>
</edge>
<edge source="ACTIVE-STANDBY MECHANISM" target="FAST WORKER SWITCHING AND PROCESS-LEVEL ISOLATION">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="SYSTEM PROTOTYPE FOR PIPESWITCH" target="3600">
  <data key="d0">CONTAINS NUMBER OF LINES OF CODE</data>
</edge>
<edge source="SYSTEM PROTOTYPE FOR PIPESWITCH" target="C AND PYTHON">
  <data key="d0">IS IMPLEMENTED IN PROGRAMMING LANGUAGES</data>
</edge>
<edge source="SYSTEM PROTOTYPE FOR PIPESWITCH" target="PYTORCH 21">
  <data key="d0">IS INTEGRATED WITH</data>
</edge>
<edge source="DEEP LEARNING" target="AN EMERGING FAMILY OF INTELLIGENT APPLICATIONS">
  <data key="d0">POWERS</data>
</edge>
<edge source="INTELLIGENT APPLICATIONS" target="RETAIL">
  <data key="d0">APPLY_IN_DOMAINS</data>
</edge>
<edge source="INTELLIGENT APPLICATIONS" target="TRANSPORTATION">
  <data key="d0">APPLY_IN_DOMAINS</data>
</edge>
<edge source="INTELLIGENT APPLICATIONS" target="FINANCE">
  <data key="d0">APPLY_IN_DOMAINS</data>
</edge>
<edge source="INTELLIGENT APPLICATIONS" target="HEALTHCARE">
  <data key="d0">APPLY_IN_DOMAINS</data>
</edge>
<edge source="GPUS" target="ACCELERATORS FOR DEEP LEARNING">
  <data key="d0">ARE ONE OF THE MOST WIDELY-USED CLASSES OF</data>
</edge>
<edge source="ASH CROWD" target="APPLICATION SUDDENLY BECOMES POPULAR">
  <data key="d0">CAUSES DEMAND TO EXCEED OPERATORS EXPECTATION</data>
</edge>
<edge source="TRAINING CLUSTER" target="INFERENCE TASKS">
  <data key="d0">CANNOT PREEMPT TRAINING TASKS FOR</data>
</edge>
<edge source="PRODUCTION SYSTEMS" target="EACH APPLICATION ON PER-GPU GRANULARITY">
  <data key="d0">ARE PROVISIONED TO</data>
</edge>
<edge source="PRODUCTION SYSTEMS" target="PER-GPU GRANULARITY">
  <data key="d0">ALLOCATE GPUS TO APPLICATIONS ON</data>
</edge>
<edge source="PRODUCTION SYSTEMS" target="VMS, CONTAINERS OR PROCESSES OF AN APPLICATION">
  <data key="d0">BIND GPUS TO</data>
</edge>
<edge source="PROVISIONING ON PER-GPU GRANULARITY" target="THE INTERFERENCE BETWEEN APPLICATIONS">
  <data key="d0">LIMITS</data>
</edge>
<edge source="BINDING GPUS TO APPLICATIONS" target="INTERFERENCE BETWEEN DIFFERENT APPLICATIONS">
  <data key="d0">AIMS TO LIMIT</data>
</edge>
<edge source="BINDING GPUS TO APPLICATIONS" target="SLO REQUIREMENTS">
  <data key="d0">AIMS TO SATISFY</data>
</edge>
<edge source="OPERATING SYSTEMS" target="TASK SCHEDULING AND CONTEXT SWITCHING">
  <data key="d0">ACHIEVE HIGH CPU UTILIZATION VIA</data>
</edge>
<edge source="THE IDEA OF NE-GRAINED CPU TIME-SHARING" target="CLUSTER SCHEDULING">
  <data key="d0">HAS BEEN FURTHER EXTENDED TO</data>
</edge>
<edge source="NE-GRAINED TIME-SHARING" target="PROVISIONING DEDICATED RESOURCES">
  <data key="d0">PROVIDES BETTER UTILIZATION THAN</data>
</edge>
<edge source="NE-GRAINED TIME-SHARING" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">PROVIDES NECESSARY</data>
</edge>
<edge source="SCHEDULING CYCLES" target="NE-GRAINED">
  <data key="d0">ARE ENABLED TO BE</data>
</edge>
<edge source="GPU" target="TASKS">
  <data key="d0">EXPERIENCES HIGH OVERHEAD WHEN SWITCHING BETWEEN</data>
</edge>
<edge source="GPU" target="DNN MODEL (E.G., RESNET)">
  <data key="d0">SWITCHES_TO_DNN_MODEL_NOT_PRELOADED_ONTO_IT</data>
</edge>
<edge source="GPU" target="THE MODELS FOR TRAINING OR INFERENCE">
  <data key="d0">CAN QUICKLY CONTEXT-SWITCH BETWEEN</data>
</edge>
<edge source="GPU" target="900GBS_FOR_V100">
  <data key="d0">EXHIBITS_HIGH_MEMORY_BANDWIDTH</data>
</edge>
<edge source="GPU" target="PCIE 3.0 8">
  <data key="d0">USES_INTERFACE</data>
</edge>
<edge source="THE GAP" target="THE PRECIOUS GPU MEMORY AND SLOW SWITCHING">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="NAIVE USE OF GPUS IN THE SAME WAY AS CPUS" target="DL INFERENCE WITH STRICT SLOS IN TENS TO HUNDREDS OF MILLISECONDS">
  <data key="d0">WILL NOT SATISFY REQUIREMENTS OF</data>
</edge>
<edge source="SWITCHING_TO_UNPRELOADED_DNN_MODEL_ON_GPU" target="MULTIPLE SECONDS">
  <data key="d0">CAUSES_DELAY_BEFORE_SERVING_INFERENCE_REQUEST</data>
</edge>
<edge source="STATE-OF-THE-ART TRICKS LIKE CUDA UNIFIED MEMORY 4 (6)" target="DELAY">
  <data key="d0">DO_NOT_ELIMINATE_DELAY_IN_SERVING_INFERENCE_REQUESTS_AFTER_SWITCHING_DNN_MODEL</data>
</edge>
<edge source="GPU MEMORY" target="HOST MEMORY">
  <data key="d0">HAS MUCH MORE LIMITED CAPACITY THAN</data>
</edge>
<edge source="GPU MEMORY" target="MANY APPLICATIONS">
  <data key="d0">CANNOT PRELOAD</data>
</edge>
<edge source="GPU MEMORY" target="PIPESWITCH">
  <data key="d0">IS MANAGED BY</data>
</edge>
<edge source="GPU MEMORY" target="MEMORY SWAPPING">
  <data key="d0">CAUSES</data>
</edge>
<edge source="HOST MEMORY" target="GPU MEMORY">
  <data key="d0">IS LARGER AND CHEAPER THAN</data>
</edge>
<edge source="MEMORY FOOTPRINTS OF INFERENCE TASKS" target="NULL">
  <data key="d0">ARE INCREASING</data>
</edge>
<edge source="MODELS" target="NULL">
  <data key="d0">ARE GETTING LARGER</data>
</edge>
<edge source="MODELS" target="RESNET152, INCEPTIONV3, BERTBASE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MODELS" target="RESNET152">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MODELS" target="INCEPTIONV3">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MODELS" target="BERTBASE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="REQUEST BATCHING" target="THROUGHPUT">
  <data key="d0">IS PREVALENTLY USED TO INCREASE</data>
</edge>
<edge source="THROUGHPUT" target="BATCHES PER SECOND">
  <data key="d0">IS_MEASURED_IN</data>
</edge>
<edge source="THROUGHPUT" target="UPPER BOUND">
  <data key="d0">INCLUDES_MEASUREMENTS_FOR</data>
</edge>
<edge source="THROUGHPUT" target="PIPESWITCH">
  <data key="d0">INCLUDES_MEASUREMENTS_FOR</data>
</edge>
<edge source="THROUGHPUT" target="MPS">
  <data key="d0">INCLUDES_MEASUREMENTS_FOR</data>
</edge>
<edge source="THROUGHPUT" target="STOP-AND-START">
  <data key="d0">INCLUDES_MEASUREMENTS_FOR</data>
</edge>
<edge source="THROUGHPUT" target="EIGHT P3.2XLARGE INSTANCES">
  <data key="d0">IS_REPORTED_FOR</data>
</edge>
<edge source="CONTEXT SWITCHING DESIGN" target="SWITCHING OVERHEAD">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="CONTEXT SWITCHING DESIGN" target="CONTENTS ON GPU MEMORY">
  <data key="d0">QUICKLY SWITCHES</data>
</edge>
<edge source="CONTEXT SWITCHING DESIGN" target="BETTER APPROACH FOR EFFICIENTLY TIME-SHARING GPUS">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="DNN MODELS" target="HOST MEMORY">
  <data key="d0">CAN BE HELD IN</data>
</edge>
<edge source="DNN MODELS" target="A LAYERED STRUCTURE">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="DNN MODELS" target="A LAYER-BY-LAYER COMPUTATION PATTERN">
  <data key="d0">FOLLOW</data>
</edge>
<edge source="ENTERPRISES" target="GPU CLUSTERS TO RUN DNN WORKLOADS IN LARGE SCALE">
  <data key="d0">BUILD</data>
</edge>
<edge source="11 M. JEON, S. VENKATARAMAN, A. PHANISHAYEE, U. QIAN, W. XIAO, AND F. YANG" target="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS" target="USENIX ATC">
  <data key="d0">WAS_PUBLISHED_IN</data>
</edge>
<edge source="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS" target="2019">
  <data key="d0">WAS_PUBLISHED_IN_YEAR</data>
</edge>
<edge source="512 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">WAS_ORGANIZED_BY</data>
</edge>
<edge source="USENIX ASSOCIATION" target="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">ORGANIZES</data>
</edge>
<edge source="M. JEON, S. VENKATARAMAN, A. PHANISHAYEE, J. QIAN, W. XIAO, AND F. YANG" target="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS">
  <data key="d0">AUTHORED_PAPER</data>
</edge>
<edge source="NUMBER OF APPLICATIONS THAT CAN BE MULTIPLEXED" target="GPU MEMORY SIZE">
  <data key="d0">IS NOT LIMITED BY</data>
</edge>
<edge source="EACH APPLICATION" target="GPU COMPUTE AND MEMORY RESOURCES DURING ITS TIME SLICE">
  <data key="d0">USES ENTIRE</data>
</edge>
<edge source="PIPESWITCH SYSTEM" target="GPU MEMORY SHARING AND SWITCHING">
  <data key="d0">CLOSES THE GAP OF</data>
</edge>
<edge source="PIPESWITCH SYSTEM" target="AN EFFICIENT TIME-SHARING GPU CLUSTER FOR DL WORKLOADS">
  <data key="d0">ENABLES THE DESIGN OF</data>
</edge>
<edge source="DL APPLICATIONS ON TIME-SHARING GPUS" target="STRICT SLOS">
  <data key="d0">MEET PERFORMANCE REQUIREMENTS</data>
</edge>
<edge source="MEASUREMENT STUDY" target="THE TASK SWITCHING OVERHEAD">
  <data key="d0">PROBES</data>
</edge>
<edge source="MEASUREMENT STUDY" target="THE OVERHEAD OF EACH COMPONENT">
  <data key="d0">ANALYZES</data>
</edge>
<edge source="MEASUREMENT STUDY" target="PROFILING OF TASK SWITCHING OVERHEAD">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="MEASUREMENT STUDY" target="TASK SWITCHING OVERHEAD INTO INDIVIDUAL COMPONENTS">
  <data key="d0">BREAKS DOWN</data>
</edge>
<edge source="TASK SWITCHING OVERHEAD" target="STUDY">
  <data key="d0">IS THE PROBLEM TO UNDERSTAND</data>
</edge>
<edge source="EVERY COMPONENT" target="CONSIDERABLE AMOUNT OF TIME">
  <data key="d0">TAKES AMOUNT OF TIME</data>
</edge>
<edge source="TIME TAKEN BY EVERY COMPONENT" target="FROM TENS OF MILLISECONDS TO SECONDS">
  <data key="d0">VARIES IN DURATION</data>
</edge>
<edge source="INFERENCE TASK" target="TENS OF MILLISECONDS ON A GPU">
  <data key="d0">TAKES TIME ON</data>
</edge>
<edge source="INFERENCE TASK" target="SERVER">
  <data key="d0">ARRIVED AT</data>
</edge>
<edge source="INFERENCE TASK" target="INFERENCE">
  <data key="d0">USES MODEL FOR</data>
</edge>
<edge source="INFERENCE TASK" target="MODEL ITSELF">
  <data key="d0">DOES NOT MODIFY</data>
</edge>
<edge source="LATENCY SLOS" target="INFERENCE TIME">
  <data key="d0">ARE TYPICALLY MULTIPLES OF</data>
</edge>
<edge source="THE CHARACTERISTICS OF DL APPLICATIONS" target="THE OVERHEAD OF ALL THE COMPONENTS">
  <data key="d0">ARE USED TO MINIMIZE</data>
</edge>
<edge source="OUR DESIGN" target="A KEY OBSERVATION">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="OUR DESIGN" target="SYSTEMATICALLY MINIMIZE THE OVERHEAD OF EACH COMPONENT">
  <data key="d0">AIMS TO</data>
</edge>
<edge source="COMPUTATION OF DNN MODELS" target="LAYER BY LAYER">
  <data key="d0">TAKES PLACE</data>
</edge>
<edge source="COMPUTATION" target="ENTIRE MODEL TO BE TRANSMITTED TO THE GPU">
  <data key="d0">DOES NOT REQUIRE WAITING FOR</data>
</edge>
<edge source="COMPUTATION" target="PARAMETERS ARE TRANSMITTED">
  <data key="d0">STARTS WHEN</data>
</edge>
<edge source="ALGORITHM" target="TWO INSIGHTS">
  <data key="d0">DESIGNED BASED ON</data>
</edge>
<edge source="ALGORITHM" target="FIND OPTIMAL GROUPING STRATEGY">
  <data key="d0">PURPOSE</data>
</edge>
<edge source="OPTIMAL GROUPING STRATEGY" target="GIVEN MODEL">
  <data key="d0">APPLIES TO</data>
</edge>
<edge source="OPTIMAL GROUPING STRATEGY" target="ENTIRE MODEL F(,0)">
  <data key="d0">APPLIES_TO</data>
</edge>
<edge source="DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT" target="CUDA UNIFIED MEMORY 4">
  <data key="d0">INCLUDES EXAMPLE</data>
</edge>
<edge source="DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT" target="OVERKILL">
  <data key="d0">CAUSES</data>
</edge>
<edge source="DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT" target="UNNECESSARY OVERHEAD">
  <data key="d0">INCURS</data>
</edge>
<edge source="MEMORY ALLOCATION FOR A DNN MODEL" target="TRUE">
  <data key="d0">EXHIBITS DETERMINISTIC BEHAVIOR</data>
</edge>
<edge source="DETERMINISTIC MEMORY ALLOCATION" target="EXTRA MEMORY COPIES BETWEEN THE DAEMON AND THE WORKERS">
  <data key="d0">ENABLES ELIMINATION OF</data>
</edge>
<edge source="ELIMINATION OF EXTRA MEMORY COPIES" target="IPC OVERHEAD">
  <data key="d0">CONTRIBUTES TO REDUCTION OF</data>
</edge>
<edge source="SERVER" target="STANDBY WORKERS">
  <data key="d0">INCLUDES_ONE_OR_MORE</data>
</edge>
<edge source="SERVER" target="HOST MEMORY">
  <data key="d0">REQUIRES TO KEEP ONLY ONE COPY OF EACH MODEL IN</data>
</edge>
<edge source="THE ACTIVE WORKER" target="A TASK IN THE GPU">
  <data key="d0">CURRENTLY EXECUTES</data>
</edge>
<edge source="THE ACTIVE WORKER" target="A STANDBY WORKER">
  <data key="d0">BECOMES</data>
</edge>
<edge source="THE ACTIVE WORKER" target="THE PREVIOUS TASK">
  <data key="d0">CLEANS THE ENVIRONMENT FOR</data>
</edge>
<edge source="ACTIVE WORKER" target="CURRENT TASK">
  <data key="d0">COMPLETES OR STOPS</data>
</edge>
<edge source="CONTROLLER" target="MEMORY DAEMON">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="CONTROLLER" target="STANDBY WORKER">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="CONTROLLER" target="WORKER TO START COMPUTING CORRESPONDING LAYERS AFTER EACH GROUP IS TRANSFERRED">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="MEMORY DAEMON" target="A SYSTEM PROCESS">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY DAEMON" target="GPU MEMORY">
  <data key="d0">MANAGES</data>
</edge>
<edge source="MEMORY DAEMON" target="HOST MEMORY TO GPU MEMORY">
  <data key="d0">TRANSMITS MODEL FROM</data>
</edge>
<edge source="MEMORY DAEMON" target="WORKER">
  <data key="d0">USES THE SAME ORDER TO TRANSMIT THE MODEL AS</data>
</edge>
<edge source="MEMORY DAEMON" target="RELEVANT GPU MEMORY HANDLERS TO WORKER">
  <data key="d0">NEEDS TO EXPORT</data>
</edge>
<edge source="MEMORY DAEMON" target="EXPENSIVE GPU IPCS">
  <data key="d0">CAN MINIMIZE USAGE OF</data>
</edge>
<edge source="STANDBY WORKER" target="WORKER ON STANDBY">
  <data key="d0">DESCRIBES_ROLE</data>
</edge>
<edge source="MEMORY DAEMON AND STANDBY WORKER" target="TASK TO GPU">
  <data key="d0">LOAD</data>
</edge>
<edge source="TASK" target="PIPELINED MODEL TRANSMISSION (4.2)">
  <data key="d0">EXECUTES WITH</data>
</edge>
<edge source="TASK" target="CLEANING">
  <data key="d0">INVOLVES</data>
</edge>
<edge source="TASK" target="GPU">
  <data key="d0">IS_EXECUTED_ON</data>
</edge>
<edge source="TABLE 2" target="COMPARISON OF WORKER SWITCHING MECHANISMS">
  <data key="d0">PRESENTS</data>
</edge>
<edge source="TABLE 2" target="DIFFERENCES BETWEEN THREE SOLUTIONS">
  <data key="d0">SUMMARIZES</data>
</edge>
<edge source="WORKER SWITCHING MECHANISMS" target="NO TASK PROCESS-CLEANING INITIALIZATION LEVEL OVERHEAD ISOLATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="WORKER SWITCHING MECHANISMS" target="TWO PROCESSES">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="WORKER SWITCHING MECHANISMS" target="ONE PROCESS ACTIVE-STANDBY">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ACTIVE AND STANDBY WORKER SWITCHING MECHANISM" target="TO HIDE THE OVERHEAD OF TASK CLEANING AND TASK INITIALIZATION">
  <data key="d0">DESIGNS</data>
</edge>
<edge source="ACTIVE AND STANDBY WORKER SWITCHING MECHANISM" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">ENSURES</data>
</edge>
<edge source="ACTIVE-STANDBY WORKER SWITCHING" target="EFFECTIVENESS OF ACTIVE-STANDBY WORKER SWITCHING">
  <data key="d0">IS EVALUATED TO ASSESS</data>
</edge>
<edge source="EVALUATION OF ACTIVE-STANDBY WORKER SWITCHING" target="ALL OTHER COMPONENTS OF PIPESWITCH">
  <data key="d0">KEEPS CONSTANT</data>
</edge>
<edge source="EVALUATION OF ACTIVE-STANDBY WORKER SWITCHING" target="MECHANISMS DISCUSSED IN SECTION 4.4">
  <data key="d0">COMPARES</data>
</edge>
<edge source="PIPELINING" target="COMPUTER SYSTEMS">
  <data key="d0">IS A CANONICAL TECHNIQUE USED IN</data>
</edge>
<edge source="PIPELINING" target="SYSTEM PERFORMANCE">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="PIPELINING" target="RESOURCE UTILIZATION">
  <data key="d0">MAXIMIZES</data>
</edge>
<edge source="PIPELINING" target="TWO SOURCES OF SYSTEM OVERHEADS">
  <data key="d0">BRINGS</data>
</edge>
<edge source="INTRA-BATCH PIPELINING" target="MODEL TRANSMISSION AND COMPUTATION">
  <data key="d0">OVERLAPS</data>
</edge>
<edge source="INTRA-BATCH PIPELINING" target="OVERHEAD OF SWITCHING BETWEEN DIFFERENT DNN MODELS">
  <data key="d0">REDUCES</data>
</edge>
<edge source="DIFFERENT DNN MODELS" target="INFERENCE OR TRAINING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="DL APPLICATIONS" target="MINIMIZATION OF GPU MEMORY MANAGEMENT OVERHEAD">
  <data key="d0">HAVE CHARACTERISTICS THAT ENABLE</data>
</edge>
<edge source="PIPELINED MODEL TRANSMISSION" target="PIPELINED MODEL TRANSMISSION EFFECTIVENESS">
  <data key="d0">IS EVALUATED BY KEEPING ALL OTHER COMPONENTS OF PIPESWITCH THE SAME</data>
</edge>
<edge source="PIPELINED MODEL TRANSMISSION" target="MECHANISMS IN 4.2">
  <data key="d0">IS COMPARED USING MECHANISMS DISCUSSED IN SECTION 4.2</data>
</edge>
<edge source="UNIFIED MEMORY MANAGEMENT" target="THE EFFECTIVENESS OF UNIFIED MEMORY MANAGEMENT">
  <data key="d0">IS USED TO EVALUATE</data>
</edge>
<edge source="PYTORCH" target="21">
  <data key="d0">IS_IDENTIFIED_BY</data>
</edge>
<edge source="THIS SECTION" target="INEFFICIENCIES IN TODAY'S SHARED GPU CLUSTERS">
  <data key="d0">IDENTIFIES</data>
</edge>
<edge source="THIS SECTION" target="RUNNING DEEP LEARNING WORKLOADS ON GPUS IN THE NE-GRAINED TIME-SHARING MODEL">
  <data key="d0">MOTIVATES</data>
</edge>
<edge source="THIS SECTION" target="EVALUATION">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="500 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">IS ORGANIZED BY</data>
</edge>
<edge source="SHARED CLUSTER" target="DEDICATED CLUSTER FOR EACH USER">
  <data key="d0">IS CONTRASTED WITH</data>
</edge>
<edge source="TEXT" target="REASONS TO BUILD A SHARED CLUSTER INSTEAD OF A DEDICATED ONE FOR EACH USER">
  <data key="d0">POSES QUESTION ABOUT</data>
</edge>
<edge source="TEXT" target="ARCHITECTURE AND TASK EXECUTION">
  <data key="d0">PROVIDES_OVERVIEW_OF</data>
</edge>
<edge source="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="KUBERNETES">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">ORGANIZED_BY</data>
</edge>
<edge source="THE MAIN REASON" target="TO BRING DOWN THE COST">
  <data key="d0">EXPLAINS THE PURPOSE</data>
</edge>
<edge source="DEMAND OF INFERENCE" target="MORE PREDICTABLE PATTERN">
  <data key="d0">EXHIBITS</data>
</edge>
<edge source="INFERENCE TASK FOR A PARTICULAR APPLICATION" target="DAILY PERIODICAL PATTERN">
  <data key="d0">DISPLAYS</data>
</edge>
<edge source="DAILY PERIODICAL PATTERN" target="APPLICATION USAGE">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="THE PATTERNS" target="DIFFERENT TASKS">
  <data key="d0">CAN VARY ACROSS</data>
</edge>
<edge source="TRAINING" target="INFERENCE">
  <data key="d0">DOES NOT SHARE DATA WITH</data>
</edge>
<edge source="TRAINING" target="GPU">
  <data key="d0">REQUIRES LARGE AMOUNTS OF MEMORY ON</data>
</edge>
<edge source="SHARED CLUSTERS" target="TRAINING AND INFERENCE">
  <data key="d0">ARE NOT SHARED BETWEEN</data>
</edge>
<edge source="GPUS DESIGNED FOR INFERENCE TASKS" target="TRAINING TASKS">
  <data key="d0">MIGHT BE INSUFFICIENT FOR</data>
</edge>
<edge source="NEW ALGORITHMS AND SYSTEMS FOR DISTRIBUTED TRAINING" target="MULTIPLE GPUS TO ACCELERATE TRAINING">
  <data key="d0">ENABLE</data>
</edge>
<edge source="COMPUTATION POWER ON BOTH SIDES" target="SAME ORDER OF MAGNITUDE">
  <data key="d0">FALLS WITHIN ORDER OF MAGNITUDE</data>
</edge>
<edge source="INFERENCE WORKLOAD" target="NUMBER OF ACTIVE USERS">
  <data key="d0">FLUCTUATES IN CORRELATION WITH</data>
</edge>
<edge source="INFERENCE WORKLOAD" target="CLEAR PEAKS AND VALLEYS WITHIN EACH DAY">
  <data key="d0">DISPLAYS</data>
</edge>
<edge source="PEAK DEMAND DURING DAYTIME" target="2 TIMES THE VALLEY DEMAND AT MIDNIGHT">
  <data key="d0">IS</data>
</edge>
<edge source="NE-TUNING BERT" target="DAILY NEWS">
  <data key="d0">USES DATA SOURCE</data>
</edge>
<edge source="BORG-LIKE 1 SYSTEMS FOR GPUS" target="GREAT OPPORTUNITY IN IMPROVING GPU UTILIZATION">
  <data key="d0">MEAN</data>
</edge>
<edge source="INFERENCE AND TRAINING WORKLOADS" target="RESOURCE UTILIZATION">
  <data key="d0">HAVE COMPLEMENTARY USAGE PATTERNS</data>
</edge>
<edge source="INFERENCE LOADS ON DIFFERENT MODELS" target="DIFFERENT PATTERNS">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="DIFFERENT PATTERNS OF INFERENCE LOADS" target="TIME SHARING">
  <data key="d0">BENEFIT FROM</data>
</edge>
<edge source="ANY SERVER" target="ANY TASK">
  <data key="d0">WOULD BE ABLE TO RUN</data>
</edge>
<edge source="RUNNING ANY TASK ON ANY SERVER" target="THE DESIGN OF LOAD BALANCERS AND SCHEDULERS">
  <data key="d0">WOULD GREATLY SIMPLIFY</data>
</edge>
<edge source="SWITCHING BETWEEN DIFFERENT APPLICATIONS" target="LOW OVERHEAD">
  <data key="d0">WOULD HAVE</data>
</edge>
<edge source="MODERN SERVER" target="SEVERAL TB OF HOST MEMORY">
  <data key="d0">CAN BE EQUIPPED WITH</data>
</edge>
<edge source="SEVERAL TB OF HOST MEMORY" target="MODERN SERVER TO LOAD MANY APPLICATIONS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="DL TASKS" target="GPU">
  <data key="d0">REQUIRE LARGE AMOUNTS OF MEMORY ON</data>
</edge>
<edge source="STATE-OF-THE-ART MODELS" target="DEEPER AND LARGER">
  <data key="d0">ARE BECOMING</data>
</edge>
<edge source="IDLE APPLICATIONS" target="LARGE MEMORY SPACE">
  <data key="d0">CAN OCCUPY</data>
</edge>
<edge source="ONLINE INFERENCE WORKLOADS" target="STRICT SLOS">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="NAIVE MEMORY SWAPPING BETWEEN THE HOST MEMORY AND THE GPU MEMORY" target="STRICT SLOS">
  <data key="d0">CANNOT MEET</data>
</edge>
<edge source="THE RST INFERENCE BATCH" target="SEVERAL SECONDS">
  <data key="d0">WOULD REQUIRE DURATION TO FINISH</data>
</edge>
<edge source="NVIDIA MPS" target="STOP-AND-START">
  <data key="d0">INCURS LOWER OVERHEAD COMPARED TO</data>
</edge>
<edge source="NVIDIA MPS" target="NULL">
  <data key="d0">INCURS SEVERAL HUNDRED MILLISECONDS OVERHEAD</data>
</edge>
<edge source="NVIDIA MPS" target="A TECHNOLOGY DEVELOPED BY NVIDIA">
  <data key="d0">REFERS TO</data>
</edge>
<edge source="SEVERAL HUNDRED MILLISECONDS OVERHEAD" target="NVIDIA MPS FROM MEETING STRICT SLOS">
  <data key="d0">PREVENTS</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="501 CONTROLLER">
  <data key="d0">INCLUDES COMPONENT</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="ACTIVE WORKER">
  <data key="d0">INCLUDES COMPONENT</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="GPU MEMORY DAEMON">
  <data key="d0">INCLUDES COMPONENT</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="STANDBY WORKER">
  <data key="d0">INCLUDES COMPONENT</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="NEW TASK">
  <data key="d0">HANDLES</data>
</edge>
<edge source="NEW TASK" target="THE GPU ENVIRONMENT OF THE CURRENT TASK">
  <data key="d0">REUSES</data>
</edge>
<edge source="PIPELINE" target="FEASIBLE AND EFFECTIVE">
  <data key="d0">WILL BE SHOWN AS</data>
</edge>
<edge source="FIGURE 1" target="PIPESWITCH SERVER">
  <data key="d0">SHOWS ARCHITECTURE OF</data>
</edge>
<edge source="PIPESWITCH PIPELINES MODEL" target="TRANSMISSION AND TASK EXECUTION">
  <data key="d0">MODELS</data>
</edge>
<edge source="THE CONTROLLER" target="THE CENTRAL COMPONENT">
  <data key="d0">FUNCTIONS AS</data>
</edge>
<edge source="THE CONTROLLER" target="A SET OF TASKS RECEIVED FROM THE CLIENTS">
  <data key="d0">QUEUES</data>
</edge>
<edge source="THE CONTROLLER" target="THE CURRENT TASK TO FINISH IF IT IS INFERENCE">
  <data key="d0">WAITS FOR</data>
</edge>
<edge source="THE CONTROLLER" target="THE ACTIVE WORKER TO STOP IF IT IS TRAINING">
  <data key="d0">PREEMPTS THE CURRENT TASK BY NOTIFYING</data>
</edge>
<edge source="THE CONTROLLER" target="AN IDLE STANDBY WORKER TO INITIALIZE ITS ENVIRONMENT FOR THE NEW TASK">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE GPU MEMORY">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE DNN MODELS">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE STANDBY WORKER" target="THE NEW TASK">
  <data key="d0">BECOMES THE NEW ACTIVE WORKER TO EXECUTE</data>
</edge>
<edge source="THE NEW TASK" target="THE GPU MEMORY AT THE SAME TIME">
  <data key="d0">CAN TRANSMIT ITS MODEL TO</data>
</edge>
<edge source="THE SPECIFIC SCHEDULING ALGORITHM" target="THIS PAPER">
  <data key="d0">IS_ORTHOGONAL_TO</data>
</edge>
<edge source="MODEL TRANSMISSION" target="TASK STARTUP">
  <data key="d0">OCCURS DURING</data>
</edge>
<edge source="MODEL TRANSMISSION" target="PCIE">
  <data key="d0">OCCURS_OVER</data>
</edge>
<edge source="DIRECT TRANSMISSION BY MEMORY DAEMON" target="EXTRA MEMORY COPY FROM MEMORY DAEMON TO WORKER">
  <data key="d0">ELIMINATES</data>
</edge>
<edge source="WORKER" target="MODEL">
  <data key="d0">CAN ACCESS</data>
</edge>
<edge source="WORKER" target="EXECUTE ITS TASK">
  <data key="d0">USES MODEL TO</data>
</edge>
<edge source="MODEL" target="GPU">
  <data key="d0">IS_TRANSMITTED_TO</data>
</edge>
<edge source="4 PIPESWITCH DESIGN" target="MEASUREMENT STUDY">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="END-TO-END EXPERIMENTS" target="BENEFITS OF PIPESWITCH">
  <data key="d0">DEMONSTRATE</data>
</edge>
<edge source="END-TO-END EXPERIMENTS" target="END-TO-END OVERHEAD">
  <data key="d0">FOCUS ON MINIMIZING</data>
</edge>
<edge source="THE MEA-502 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">IS_ORGANIZED_BY</data>
</edge>
<edge source="SUREMENT" target="SERVER STOPS TRAINING TASK RUNNING ON GPU AND STARTS INFERENCE TASK">
  <data key="d0">CONSIDERS_SCENARIO</data>
</edge>
<edge source="TIME" target="STARTING AND EXECUTING INFERENCE TASK ON GPU">
  <data key="d0">MEASURED FOR</data>
</edge>
<edge source="TASK CLEANING" target="TIME">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="THE INFERENCE TASK" target="ITS ENVIRONMENT">
  <data key="d0">CREATES AND INITIALIZES</data>
</edge>
<edge source="ITS ENVIRONMENT" target="PROCESS LAUNCHING">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ITS ENVIRONMENT" target="PYTORCH CUDA RUNTIME LOADING">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ITS ENVIRONMENT" target="CUDA CONTEXT INITIALIZATION">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MEMORY ALLOCATION" target="PROCESS OF ASSIGNING MEMORY RESOURCES">
  <data key="d0">DESCRIBES_CONCEPT</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="TRANSMISSION METHOD">
  <data key="d0">DESCRIBES_CONCEPT</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="NO OPTIMIZATION">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="LAYERS OF THE MODEL INTO ONE BIG TENSOR">
  <data key="d0">COMBINES</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="ONE BIG TENSOR IN ONE GROUP">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="TASK SWITCHING ON T4" target="CPU">
  <data key="d0">DEPENDS LARGELY ON</data>
</edge>
<edge source="G4DN.2XLARGE" target="P3.2XLARGE">
  <data key="d0">IS EQUIPPED WITH BETTER CPU THAN</data>
</edge>
<edge source="P3.2XLARGE" target="8 VCPUS (INTEL XEON E5-2686 V4)">
  <data key="d0">IS CONFIGURED WITH</data>
</edge>
<edge source="P3.2XLARGE" target="1 GPU (NVIDIA V100 WITH 16 GB GPU MEMORY)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="P3.2XLARGE" target="PCIE 3.0 16">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="P3.2XLARGE" target="61 GB MEMORY">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="P3.2XLARGE" target="NVIDIA V100 GPU">
  <data key="d0">INCLUDES_HARDWARE</data>
</edge>
<edge source="P3.2XLARGE" target="NVIDIA V100">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="CPU IN G4DN.2XLARGE" target="INTEL PLATINUM 8259CL">
  <data key="d0">IS</data>
</edge>
<edge source="CPU IN P3.2XLARGE" target="INTEL XEON E5-2686 V4">
  <data key="d0">IS</data>
</edge>
<edge source="ALL THE COMPONENTS" target="THE INFERENCE TIME">
  <data key="d0">TAKE CONSIDERABLE TIME COMPARED TO</data>
</edge>
<edge source="PCIE BANDWIDTH" target="HOW FAST AN ARBITRARY TASK CAN BE LOADED TO THE GPU">
  <data key="d0">REPRESENTS THE PHYSICAL LIMIT ON</data>
</edge>
<edge source="AN INFERENCE TASK" target="THE RST LAYER TO THE NAL LAYER TO MAKE A PREDICTION">
  <data key="d0">PERFORMS A FORWARD PASS FROM</data>
</edge>
<edge source="EACH ITERATION IN A TRAINING TASK" target="IN THE TRAINING PROCESS">
  <data key="d0">PERFORMS A FORWARD PASS</data>
</edge>
<edge source="EACH ITERATION IN A TRAINING TASK" target="AFTER THE FORWARD PASS">
  <data key="d0">PERFORMS A BACKWARD PASS</data>
</edge>
<edge source="THE TASK" target="A LAYER AS SOON AS THE LAYER IS LOADED IN THE GPU AND THE INPUT OF THE LAYER IS READY">
  <data key="d0">CAN START THE COMPUTATION OF</data>
</edge>
<edge source="THE TASK" target="ITS FOLLOWING LAYERS">
  <data key="d0">STARTS COMPUTATION REGARDLESS OF</data>
</edge>
<edge source="THE INPUT OF THE LAYER" target="THE PREVIOUS LAYERS HAVE FINISHED THEIR COMPUTATION">
  <data key="d0">BECOMES READY WHEN</data>
</edge>
<edge source="TASK EXECUTION" target="GPU">
  <data key="d0">TAKES_PLACE_ON</data>
</edge>
<edge source="PCIE GPU E0 E1 EN-1 E2 (B) PIPELINE MODEL" target="TRANSMISSION AND TASK EXECUTION">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="ADDING HOOKS" target="TRUE">
  <data key="d0">CAN BE AUTOMATED</data>
</edge>
<edge source="DNN FRAMEWORK" target="PYTORCH">
  <data key="d0">INCLUDES EXAMPLES SUCH AS</data>
</edge>
<edge source="OVERHEAD" target="MULTIPLE CALLS TO PCIE TO TRANSMIT DATA">
  <data key="d0">CAUSES</data>
</edge>
<edge source="TRANSMISSION OVERHEAD" target="DATA SIZE">
  <data key="d0">IS DOMINATED BY</data>
</edge>
<edge source="DIVIDING THE MODEL INTO MANY LAYERS" target="INVOKING A PCIE CALL FOR EACH LAYER">
  <data key="d0">CAUSES</data>
</edge>
<edge source="INVOKING A PCIE CALL FOR EACH LAYER" target="SIGNIFICANT EXTRA OVERHEAD">
  <data key="d0">CAUSES</data>
</edge>
<edge source="SOME LAYERS" target="VERY SMALL">
  <data key="d0">CAN BE</data>
</edge>
<edge source="SYNCHRONIZATION OVERHEAD" target="TRANSMISSION AND COMPUTATION">
  <data key="d0">OCCURS BETWEEN</data>
</edge>
<edge source="SYNCHRONIZATION OVERHEAD" target="COMPUTATION TO KNOW WHEN A LAYER IS READY TO COMPUTE">
  <data key="d0">IS NECESSARY FOR</data>
</edge>
<edge source="USING SMALL GROUPS" target="TRANSMISSION AND COMPUTATION">
  <data key="d0">ENABLES MORE OVERLAP BETWEEN</data>
</edge>
<edge source="USING SMALL GROUPS" target="MORE PIPELINING OVERHEAD">
  <data key="d0">CAUSES</data>
</edge>
<edge source="MORE OVERLAP BETWEEN TRANSMISSION AND COMPUTATION" target="PIPELINING EFFICIENCY">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="USING BIG GROUPS" target="MINIMAL PIPELINING OVERHEAD">
  <data key="d0">RESULTS IN</data>
</edge>
<edge source="USING BIG GROUPS" target="REDUCED CHANCE FOR OVERLAPPING">
  <data key="d0">CAUSES</data>
</edge>
<edge source="ALL POSSIBLE COMBINATIONS" target="THE OPTIMAL GROUPING STRATEGY">
  <data key="d0">ARE USED TO FIND</data>
</edge>
<edge source="ALL POSSIBLE COMBINATIONS" target="N CASES">
  <data key="d0">ARE_DIVIDED_INTO</data>
</edge>
<edge source="THE OPTIMAL GROUPING STRATEGY" target="THESE CASES">
  <data key="d0">IS CHOSEN FROM</data>
</edge>
<edge source="TWO PRUNING TECHNIQUES" target="TWO INSIGHTS">
  <data key="d0">ARE BASED ON</data>
</edge>
<edge source="TWO PRUNING TECHNIQUES" target="OPTIMAL GROUPING STRATEGY EFFICIENTLY">
  <data key="d0">AIM TO FIND</data>
</edge>
<edge source="USENIX ASSOCIATION 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="PCIE GPU LOWER BOUND OF F(GROUP(0, I), I1)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PCIE GPU LOWER BOUND OF F(GROUP(0, I), I1)" target="GROUP(0, I)">
  <data key="d0">INVOLVES</data>
</edge>
<edge source="GROUP(0, I)" target="GROUP(I1, J)">
  <data key="d0">IS_ASSOCIATED_WITH</data>
</edge>
<edge source="J, J1" target="N-1">
  <data key="d0">RANGE</data>
</edge>
<edge source="PRUNING CONDITION" target="LOWER BOUND IS CURRENT OPTIMAL TIME">
  <data key="d0">APPLIES_IF</data>
</edge>
<edge source="PRUNING TECHNIQUES" target="I TO J">
  <data key="d0">INCLUDE PRUNING CASES THAT GROUP FROM</data>
</edge>
<edge source="BATCH" target="I1 TO J">
  <data key="d0">OPERATES AT LEAST FROM LAYER</data>
</edge>
<edge source="GROUP" target="0 TO I">
  <data key="d0">INCLUDES RANGE</data>
</edge>
<edge source="GROUP" target="I1 TO N-1">
  <data key="d0">INCLUDES RANGE</data>
</edge>
<edge source="F(B,I)" target="TOTAL TIME OF THE OPTIMAL GROUPING STRATEGY FROM LAYER I TO N-1">
  <data key="d0">RETURNS</data>
</edge>
<edge source="LAYER 0 TO I-1" target="GROUPS REPRESENTED BY B">
  <data key="d0">HAVE FORMED</data>
</edge>
<edge source="NUMBER OF LAYERS" target="N">
  <data key="d0">IS DEFINED AS</data>
</edge>
<edge source="N CASES" target="FORMATION OF THE FIRST GROUP">
  <data key="d0">ARE_BASED_ON</data>
</edge>
<edge source="CASE I" target="FIRST GROUP CONTAINS LAYER 0 TO I">
  <data key="d0">MEANS</data>
</edge>
<edge source="THIS FORMULA" target="F(GROUP(0,I),I1)">
  <data key="d0">CAN BE APPLIED RECURSIVELY TO COMPUTE</data>
</edge>
<edge source="MULTIPLE LAYERS IN A GROUP" target="PROGRESS OF COMPUTATION">
  <data key="d0">CAN BE SAFELY PACKED BASED ON</data>
</edge>
<edge source="PACKING MULTIPLE LAYERS IN A GROUP" target="PIPELINE EFFICIENCY">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="RST GROUP" target="SAFE PACKING OF MULTIPLE LAYERS">
  <data key="d0">IS EXCLUDED FROM</data>
</edge>
<edge source="RST GROUP" target="X TO N1 FOR CASE I K">
  <data key="d0">CONTAINS ALL LAYERS FROM</data>
</edge>
<edge source="T(I, J)" target="TRANSMISSION TIME">
  <data key="d0">REPRESENTS TRANSMISSION TIME FOR A GROUP FROM LAYER I TO J</data>
</edge>
<edge source="T(I, J)" target="SIZE OF LAYER I TO J AND PCIE BANDWIDTH">
  <data key="d0">IS CALCULATED BASED ON SIZE OF LAYER I TO J AND PCIE BANDWIDTH</data>
</edge>
<edge source="E(I, J)" target="EXECUTION TIME">
  <data key="d0">REPRESENTS EXECUTION TIME FOR A GROUP FROM LAYER I TO J</data>
</edge>
<edge source="E(I, J)" target="GPU">
  <data key="d0">IS PROFILED ON THE GPU</data>
</edge>
<edge source="FIGURE 3(B)" target="AN EXAMPLE FOR THIS INSIGHT">
  <data key="d0">SHOWS</data>
</edge>
<edge source="TRANSMISSION OF THE SECOND GROUP" target="COMPUTATION OF THE RST GROUP">
  <data key="d0">CAN BE HIDDEN INTO</data>
</edge>
<edge source="TRANSMISSION" target="COMPUTATION OF THE RST GROUP">
  <data key="d0">MUST FINISH NO LATER THAN</data>
</edge>
<edge source="THE LEAST NUMBER OF LAYERS TO GROUP" target="THE FOLLOWING EQUATION">
  <data key="d0">CAN BE COMPUTED USING</data>
</edge>
<edge source="J ARGMAX J T(I1, J) E(0,I) (3)" target="I1 TO J">
  <data key="d0">GROUPS FROM LAYER</data>
</edge>
<edge source="JIS" target="I1 TO J">
  <data key="d0">IS NO BETTER THAN GROUPING FROM</data>
</edge>
<edge source="GROUPING FROM I1 TO J" target="PIPELINE EFFICIENCY">
  <data key="d0">DOES NOT INCREASE</data>
</edge>
<edge source="GROUPING FROM I1 TO J" target="HIGHER PIPELINE OVERHEAD">
  <data key="d0">CAUSES</data>
</edge>
<edge source="ALGORITHM 1" target="PSEUDO CODE">
  <data key="d0">DISPLAYS</data>
</edge>
<edge source="ALGORITHM 1" target="O(2N)">
  <data key="d0">HAS TIME COMPLEXITY OF</data>
</edge>
<edge source="ALGORITHM 1" target="ALL STRATEGIES IN WORST CASE">
  <data key="d0">REQUIRES ENUMERATION OF</data>
</edge>
<edge source="ALGORITHM 1" target="1.33 S 0.18 S 0.34 S">
  <data key="d0">REQUIRES TIME</data>
</edge>
<edge source="B" target="MULTIPLE GROUPS FORMED BY PREVIOUS LAYERS">
  <data key="d0">CAN CONTAIN</data>
</edge>
<edge source="B.DELAY" target="THE TIME TO WHICH THE GROUP CAN BE FORMED">
  <data key="d0">DENOTES</data>
</edge>
<edge source="THE ALGORITHM NDS" target="B.DELAY (LINE 4-9)">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="THE ENUMERATION FOR I" target="X TO J-1 (LINE 11)">
  <data key="d0">SKIPS LAYERS FROM</data>
</edge>
<edge source="NUMBER OF LAYERS N" target="2N1">
  <data key="d0">DETERMINES NUMBER OF DIFFERENT GROUPING STRATEGIES</data>
</edge>
<edge source="THE TWO PRUNING TECHNIQUES" target="MOST OF THE STRATEGIES">
  <data key="d0">ARE ABLE TO PRUNE</data>
</edge>
<edge source="THE TWO PRUNING TECHNIQUES" target="THE OPTIMAL ONE">
  <data key="d0">CAN QUICKLY FIND</data>
</edge>
<edge source="M N X" target="NUMBER OF LAYERS THE FUNCTION CONSIDERS">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="FINDOPTGROUPING(B GROUP(X,X I),X I 1)" target="K I K LAYERS">
  <data key="d0">ONLY CONSIDERS</data>
</edge>
<edge source="FINDOPTGROUPING(B GROUP(X,X I),X I 1)" target="THE OPTIMAL GROUPING STRATEGY FOR CASE I">
  <data key="d0">OUTPUTS</data>
</edge>
<edge source="THE OPTIMAL GROUPING STRATEGY FOR CASE I" target="THE ASSUMPTION">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="THE ALGORITHM" target="K1">
  <data key="d0">CONSIDERS NUMBER OF LAYERS</data>
</edge>
<edge source="THE ALGORITHM" target="THE OPTIMAL GROUPING STRATEGY FOR M K 1">
  <data key="d0">OUTPUTS</data>
</edge>
<edge source="THE OPTIMAL STRATEGY FOR THIS CASE" target="ONE GROUP">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="THESE CASES" target="THE ENTIRE SEARCH SPACE">
  <data key="d0">ARE EXCLUSIVE AND COVER</data>
</edge>
<edge source="THESE CASES" target="GROUPING FROM X TO AT LEAST J">
  <data key="d0">CANNOT ADVANCE THE COMPUTATION TO AN EARLIER POINT THAN</data>
</edge>
<edge source="THIS TECHNIQUE" target="THE OPTIMALITY">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="PRUNING THESE CASES" target="THE OPTIMALITY">
  <data key="d0">DO NOT AFFECT</data>
</edge>
<edge source="GROUPING THE LAYERS" target="HIGH PIPELINING EFFICIENCY AND LOW PIPELINING OVERHEAD">
  <data key="d0">AIMS TO ACHIEVE</data>
</edge>
<edge source="THE ORDER" target="CORRECTNESS">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="AN OPERATOR" target="IT IS TRANSMITTED TO THE GPU AND THE INPUT IS READY">
  <data key="d0">CAN BE EXECUTED ONLY WHEN</data>
</edge>
<edge source="OUR PIPELINED MODEL TRANSMISSION" target="THE GENERAL CASE">
  <data key="d0">APPLIES TO</data>
</edge>
<edge source="FIGURE 7" target="EFFECTIVENESS OF PIPELINED MODEL TRANSMISSION">
  <data key="d0">DEPICTS</data>
</edge>
<edge source="FUNCTIONS" target="GPU MEMORY">
  <data key="d0">ALLOCATE</data>
</edge>
<edge source="FUNCTIONS" target="WORKERS THROUGH CUDA IPC API">
  <data key="d0">SHARE GPU MEMORY TO</data>
</edge>
<edge source="FUNCTIONS" target="SHARED GPU MEMORY">
  <data key="d0">RETRIEVE</data>
</edge>
<edge source="THIS SOLUTION" target="HIGH OVERHEAD FOR DL APPLICATIONS">
  <data key="d0">INCURS</data>
</edge>
<edge source="THIS SOLUTION" target="THE LOWER BOUND">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="HIGH OVERHEAD" target="TWO REASONS">
  <data key="d0">OCCURS BECAUSE OF</data>
</edge>
<edge source="TRAINING TASK" target="MODEL PARAMETERS">
  <data key="d0">UPDATES</data>
</edge>
<edge source="TRAINING TASK" target="DNN STRUCTURE">
  <data key="d0">DOES NOT UPDATE</data>
</edge>
<edge source="TRAINING TASK" target="ANY">
  <data key="d0">DOES NOT EXIST</data>
</edge>
<edge source="MODEL PARAMETERS" target="WEIGHTS OF THE NEURAL NETWORK">
  <data key="d0">ARE</data>
</edge>
<edge source="AMOUNT OF MEMORY NEEDED TO STORE MODEL PARAMETERS" target="THE SAME">
  <data key="d0">REMAINS</data>
</edge>
<edge source="INTERMEDIATE RESULTS" target="SIMPLE REGULAR PATTERN">
  <data key="d0">CHANGE IN</data>
</edge>
<edge source="SIMPLE REGULAR PATTERN" target="MEMORY FRAGMENTATION">
  <data key="d0">DO NOT CAUSE</data>
</edge>
<edge source="A TRAINING TASK" target="THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS CANNOT BE IMMEDIATELY FREED">
  <data key="d0">DIFFERS IN THAT</data>
</edge>
<edge source="THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS" target="THE BACKWARD PASS TO UPDATE THE WEIGHTS">
  <data key="d0">ARE ALSO USED BY</data>
</edge>
<edge source="THE BACKWARD PASS" target="THE FORWARD PASS GENERATES THEM">
  <data key="d0">CONSUMES INTERMEDIATE RESULTS IN REVERSE ORDER OF</data>
</edge>
<edge source="THE INTERMEDIATE RESULTS" target="RST-IN-LAST-OUT">
  <data key="d0">FOLLOW ORDER</data>
</edge>
<edge source="MEMORY MANAGEMENT MECHANISM" target="DL APPLICATIONS">
  <data key="d0">IS DESIGNED BASED ON TWO CHARACTERISTICS</data>
</edge>
<edge source="OFFSET" target="64-BIT INTEGER">
  <data key="d0">IS REPRESENTED AS</data>
</edge>
<edge source="EACH WORKER" target="MEMORY TO STORE MODEL AND INTERMEDIATE RESULTS">
  <data key="d0">USES MEMORY POOL TO ALLOCATE MEMORY</data>
</edge>
<edge source="EACH WORKER" target="MEMORY AFTER INTERMEDIATE RESULTS ARE NO LONGER NEEDED">
  <data key="d0">RECYCLES MEMORY TO MEMORY POOL</data>
</edge>
<edge source="MEMORY MANAGEMENT IN PYTORCH" target="MEMORY ALLOCATION FOR A TASK ITSELF">
  <data key="d0">HANDLES</data>
</edge>
<edge source="STORING THE MODELS IN A DEDICATED PROCESS" target="MINIMAL MEMORY FOOTPRINT">
  <data key="d0">RESULTS IN</data>
</edge>
<edge source="STORING THE MODELS IN A DEDICATED PROCESS" target="AN EXTRA MEMORY COPY FROM THIS PROCESS TO A WORKER TO START A TASK">
  <data key="d0">CAUSES</data>
</edge>
<edge source="EACH MODEL" target="ONLY ONCE">
  <data key="d0">IS STORED</data>
</edge>
<edge source="AN EXTRA MEMORY COPY" target="TASK SWITCHING TIME">
  <data key="d0">NEGATIVELY AFFECTS</data>
</edge>
<edge source="IPC" target="OVERHEAD">
  <data key="d0">REQUIRES_MINIMIZATION_OF</data>
</edge>
<edge source="IPC" target="OPTIMIZATION">
  <data key="d0">LACKS</data>
</edge>
<edge source="THE OVERHEAD" target="THE PIPELINE">
  <data key="d0">IS EXACERBATED BY</data>
</edge>
<edge source="THE PIPELINE" target="THE IPCS FREQUENTLY">
  <data key="d0">NEEDS TO INVOKE</data>
</edge>
<edge source="THE PIPELINE" target="MODEL TRANSMISSION AND TASK EXECUTION FOR EVERY PIPELINE GROUP">
  <data key="d0">INVOKES THE IPCS TO SYNCHRONIZE</data>
</edge>
<edge source="THE PIPELINE" target="THE IPC ONLY ONCE FOR THE ENTIRE MODEL TRANSMISSION">
  <data key="d0">DOES NOT INVOKE</data>
</edge>
<edge source="NEURAL NETWORK MODEL" target="TRUE">
  <data key="d0">IS KNOWN AND GIVEN</data>
</edge>
<edge source="LATENCY" target="IPC OPTIMIZATION">
  <data key="d0">INCREASES WITHOUT</data>
</edge>
<edge source="LATENCY" target="NO UNIFIED MEMORY MANAGEMENT">
  <data key="d0">EXCEEDS LEVEL WITH</data>
</edge>
<edge source="LATENCY" target="MILLISECONDS">
  <data key="d0">MEASURED_IN</data>
</edge>
<edge source="LATENCY" target="READY MODEL, PIPESWITCH, MPS, STOP-AND-START">
  <data key="d0">RECORDED_FOR_MODELS</data>
</edge>
<edge source="LATENCY" target="RESNET152, INCEPTIONV3, BERTBASE">
  <data key="d0">RECORDED_FOR_NEURAL_NETWORKS</data>
</edge>
<edge source="LATENCY" target="5000 MS TO 7500 MS">
  <data key="d0">RANGES_FROM</data>
</edge>
<edge source="IPC OPTIMIZATION" target="DISABLED">
  <data key="d0">CONFIGURATION_STATUS</data>
</edge>
<edge source="IPC OPTIMIZATION" target="1648 MS">
  <data key="d0">REDUCES LATENCY BY</data>
</edge>
<edge source="THE WORKER" target="WHICH PIPELINE GROUP HAS BEEN TRANSMITTED">
  <data key="d0">RECEIVES NOTIFICATION ABOUT</data>
</edge>
<edge source="PIN MEMORY" target="A TECHNIQUE IN COMPUTING">
  <data key="d0">REFERS TO</data>
</edge>
<edge source="PIN MEMORY" target="DISABLED">
  <data key="d0">CONFIGURATION_STATUS</data>
</edge>
<edge source="NO PIN" target="MEMORY">
  <data key="d0">RELATES_TO</data>
</edge>
<edge source="A NAIVE SOLUTION" target="SEPARATE PROCESSES">
  <data key="d0">INVOLVES USING</data>
</edge>
<edge source="A NAIVE SOLUTION" target="CURRENT TASK IS STOPPED">
  <data key="d0">STARTS NEW TASK AFTER</data>
</edge>
<edge source="CURRENT AND NEW TASKS" target="THE SAME PROCESS WITH A WARM CUDA CONTEXT">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE PROCESS OF THE OLD TASK" target="THE GPU ENVIRONMENT">
  <data key="d0">CLEANS</data>
</edge>
<edge source="ANOTHER PROCESS" target="THE NEW TASK">
  <data key="d0">IS CREATED AND INITIALIZED FOR</data>
</edge>
<edge source="THE CURRENT TASK" target="OVERHEAD TO CLEAN ITS STATUS">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="ANOTHER JOB" target="FREE ITS GPU MEMORY">
  <data key="d0">PERFORMS_ACTION</data>
</edge>
<edge source="THE CLEANING PROCEDURE" target="THE CONTENT OF THE MEMORY">
  <data key="d0">DOES NOT MODIFY</data>
</edge>
<edge source="THE CLEANING PROCEDURE" target="THE METADATA">
  <data key="d0">ONLY CLEANS</data>
</edge>
<edge source="THE METADATA" target="GPU MEMORY POINTERS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="CLEANING PROCEDURE" target="POINTERS POINTING TO THE TENSOR DATA">
  <data key="d0">DELETES</data>
</edge>
<edge source="CLEANING PROCEDURE" target="ACTUAL DATA">
  <data key="d0">DOES NOT FREE</data>
</edge>
<edge source="AN ADDITIONAL ZERO-OUT OPERATION" target="A CONCERN">
  <data key="d0">CAN BE ADDED IF THERE IS A CONCERN</data>
</edge>
<edge source="TRUSTED ENVIRONMENT" target="WHEN NEW PROCESS DOES NOT REQUIRE ENTIRE GPU MEMORY">
  <data key="d0">DOES NOT REQUIRE RELEASING ALL ALLOCATED MEMORY FOR PREEMPTED PROCESS</data>
</edge>
<edge source="ACHIEVING MEMORY MANAGEMENT WITHOUT FULL RELEASE" target="SIMPLE COORDINATION">
  <data key="d0">CAN BE DONE BY</data>
</edge>
<edge source="MANY STANDBY WORKERS" target="AT LEAST ONE IDLE STANDBY WORKER">
  <data key="d0">ENABLE PRESENCE OF</data>
</edge>
<edge source="A TRANSACTION" target="ENABLE OR DISABLE INFERENCE ON THIS MODEL">
  <data key="d0">MEANS MODEL SWITCHED IN OR OUT ON ALL OF ITS GPUS</data>
</edge>
<edge source="PRODUCTION GPU TRAINING TRACE" target="MICROSOFT">
  <data key="d0">WAS ANALYZED BY</data>
</edge>
<edge source="THESE SCHEDULING SOLUTIONS" target="PIPESWITCH">
  <data key="d0">ARE COMPLEMENTARY TO</data>
</edge>
<edge source="THESE SOLUTIONS" target="PIPESWITCH">
  <data key="d0">ARE COMPLEMENTARY TO</data>
</edge>
<edge source="HTTPS" target="PYTORCH.ORG">
  <data key="d0">LOCATES_WEBSITE</data>
</edge>
<edge source="HTTPS" target="AWS.AMAZON.COM">
  <data key="d0">IS ASSOCIATED WITH DOMAIN</data>
</edge>
<edge source="HTTPS" target="AZURE.MICROSOFT.COM">
  <data key="d0">IS_ASSOCIATED_WITH_DOMAIN</data>
</edge>
<edge source="HTTPS" target="CLOUD.GOOGLE.COM">
  <data key="d0">USES_PROTOCOL_FOR_ACCESSING</data>
</edge>
<edge source="HTTPS" target="DEVELOPER.NVIDIA.COM/DEEP-LEARNING-PERFORMANCE-TRAINING-INFERENCE">
  <data key="d0">LOCATED_AT</data>
</edge>
<edge source="HTTPS" target="MXNET.APACHE.ORG">
  <data key="d0">POINTS_TO_DOMAIN</data>
</edge>
<edge source="HTTPS" target="DEVELOPER.NVIDIA.COM/NCCL">
  <data key="d0">LOCATES</data>
</edge>
<edge source="HTTPS" target="KUBERNETES.IO">
  <data key="d0">IS_ASSOCIATED_WITH</data>
</edge>
<edge source="THE SCHEDULER AND THE MEMORY DAEMON" target="BETTER PERFORMANCE">
  <data key="d0">ARE IMPLEMENTED TOGETHER FOR</data>
</edge>
<edge source="THE TCP THREAD" target="CLIENTS">
  <data key="d0">ACCEPTS TASK THROUGH TCP FROM</data>
</edge>
<edge source="THE TCP THREAD" target="THE SCHEDULER THREAD">
  <data key="d0">SENDS THE TASK TO</data>
</edge>
<edge source="PARAMETERS" target="GPU MEMORY">
  <data key="d0">ARE TRANSMITTED TO</data>
</edge>
<edge source="PARAMETERS" target="GROUPS">
  <data key="d0">ARE TRANSMITTED IN</data>
</edge>
<edge source="PARAMETERS" target="PIPELINE">
  <data key="d0">ARE TRANSMITTED USING</data>
</edge>
<edge source="THE WORKER PROCESS" target="TWO THREADS">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="G4DN.2XLARGE INSTANCE" target="8 VCPUS (INTEL PLATINUM 8259CL)">
  <data key="d0">IS CONFIGURED WITH NUMBER OF VCPUS</data>
</edge>
<edge source="G4DN.2XLARGE INSTANCE" target="NVIDIA T4">
  <data key="d0">INCLUDES GPU TYPE</data>
</edge>
<edge source="G4DN.2XLARGE INSTANCE" target="16 GB GPU MEMORY">
  <data key="d0">INCLUDES GPU MEMORY SIZE</data>
</edge>
<edge source="G4DN.2XLARGE INSTANCE" target="PCIE 3.0 8">
  <data key="d0">SUPPORTS PCIE VERSION</data>
</edge>
<edge source="G4DN.2XLARGE INSTANCE" target="32 GB MEMORY">
  <data key="d0">INCLUDES SYSTEM MEMORY SIZE</data>
</edge>
<edge source="G4DN.2XLARGE INSTANCE" target="NVIDIA T4 GPU">
  <data key="d0">EQUIPPED_WITH</data>
</edge>
<edge source="THE SOFTWARE ENVIRONMENT" target="PYTORCH-1.3.0">
  <data key="d0">INCLUDES SOFTWARE PACKAGE</data>
</edge>
<edge source="THE SOFTWARE ENVIRONMENT" target="TORCHVISION-0.4.2">
  <data key="d0">INCLUDES SOFTWARE PACKAGE</data>
</edge>
<edge source="THE SOFTWARE ENVIRONMENT" target="SCIPY-1.3.2">
  <data key="d0">INCLUDES SOFTWARE PACKAGE</data>
</edge>
<edge source="THE SOFTWARE ENVIRONMENT" target="CUDA-10.1">
  <data key="d0">INCLUDES SOFTWARE PACKAGE</data>
</edge>
<edge source="THE MODELS" target="RESNET152 17">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE MODELS" target="INCEPTIONV3 22">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE MODELS" target="BERTBASE 23">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="RESNET152 17" target="STANDARD BENCHMARK FOR EVALUATING DL SYSTEMS">
  <data key="d0">SERVE_AS</data>
</edge>
<edge source="INCEPTIONV3 22" target="STANDARD BENCHMARK FOR EVALUATING DL SYSTEMS">
  <data key="d0">SERVE_AS</data>
</edge>
<edge source="BERTBASE 23" target="STANDARD BENCHMARK FOR EVALUATING DL SYSTEMS">
  <data key="d0">SERVE_AS</data>
</edge>
<edge source="THE EXPERIMENTS" target="BOTH TRAINING AND INFERENCE">
  <data key="d0">COVER</data>
</edge>
<edge source="CHECKPOINTING FREQUENCY OF TRAINING TASKS" target="SCHEDULING CYCLE">
  <data key="d0">IS SET ACCORDING TO</data>
</edge>
<edge source="SCHEDULING CYCLE" target="CHECKPOINTING OVERHEAD">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="FIGURE 5" target="TOTAL LATENCY EXPERIENCED BY THE CLIENT FOR DIFFERENT MECHANISMS">
  <data key="d0">DEPICTS</data>
</edge>
<edge source="EACH NUMBER" target="100 RUNS">
  <data key="d0">IS REPORTED WITH AVERAGE VALUE OF</data>
</edge>
<edge source="READY MODEL" target="A MODEL THAT IS PREPARED OR SET">
  <data key="d0">REFERS TO</data>
</edge>
<edge source="THE LOWER BOUND" target="THE LOWEST LATENCY ACHIEVABLE FOR AN INFERENCE TASK">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="CUDA UNIFIED MEMORY" target="A MEMORY MANAGEMENT FEATURE IN CUDA">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="CUDA UNIFIED MEMORY" target="4">
  <data key="d0">IS NAMED AS</data>
</edge>
<edge source="CUDA UNIFIED MEMORY" target="B">
  <data key="d0">CONFIGURATION_TYPE</data>
</edge>
<edge source="DEVBLOGS.NVIDIA.COM" target="UNIFIED MEMORY CUDA FOR BEGINNERS">
  <data key="d0">PROVIDES_INFORMATION_ON</data>
</edge>
<edge source="THE PROPERTIES" target="4">
  <data key="d0">ARE DESCRIBED IN</data>
</edge>
<edge source="NVIDIA V100 GPU" target="PCIE 3.0 16">
  <data key="d0">USES_INTERFACE</data>
</edge>
<edge source="NVIDIA T4 GPU" target="PCIE 3.0 8">
  <data key="d0">CONNECTED_VIA</data>
</edge>
<edge source="LATENCY (MS)" target="5000 TO 10000">
  <data key="d0">RANGES FROM</data>
</edge>
<edge source="LATENCY (MS)" target="RESNET152">
  <data key="d0">IS MEASURED FOR</data>
</edge>
<edge source="LATENCY (MS)" target="INCEPTIONV3">
  <data key="d0">IS MEASURED FOR</data>
</edge>
<edge source="LATENCY (MS)" target="BERTBASE">
  <data key="d0">IS MEASURED FOR</data>
</edge>
<edge source="LATENCY (MS)" target="7500 TO 10000">
  <data key="d0">RANGES_FROM</data>
</edge>
<edge source="METRIC" target="READY MODEL PIPESWITCH MPS STOP-AND-START LATENCY">
  <data key="d0">MEASURES</data>
</edge>
<edge source="USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="510 14TH">
  <data key="d0">IS IDENTIFIED BY</data>
</edge>
<edge source="RESNET152" target="0 TO 400">
  <data key="d0">MEASURED_LATENCY_IN_MILLISECONDS</data>
</edge>
<edge source="INCEPTIONV3" target="0 TO 400">
  <data key="d0">MEASURED_LATENCY_IN_MILLISECONDS</data>
</edge>
<edge source="BERTBASE" target="0 TO 400">
  <data key="d0">MEASURED_LATENCY_IN_MILLISECONDS</data>
</edge>
<edge source="PIPE SWITCH" target="DISABLED">
  <data key="d0">CONFIGURATION_STATUS</data>
</edge>
<edge source="MEMORY MANAGEMENT" target="DISABLED">
  <data key="d0">CONFIGURATION_STATUS</data>
</edge>
<edge source="MEMORY MANAGEMENT" target="UNIFIED STRUCTURE">
  <data key="d0">LACKS</data>
</edge>
<edge source="P3.2XLARGE INSTANCE" target="NVIDIA V100 PCIE 3.0 16">
  <data key="d0">USES GPU MODEL</data>
</edge>
<edge source="LATENCY MEASUREMENT" target="6000 MS TO 8000 MS">
  <data key="d0">RANGES FROM</data>
</edge>
<edge source="LATENCY METRIC" target="PIPESWITCH WITH ONE PROCESS AND TWO PROCESSES">
  <data key="d0">MEASURES PERFORMANCE OF</data>
</edge>
<edge source="INSTANCE_TYPE" target="G4DN.2XLARGE">
  <data key="d0">IS</data>
</edge>
<edge source="INSTANCE_TYPE" target="NVIDIA T4">
  <data key="d0">CONTAINS_GPU</data>
</edge>
<edge source="RESNET152 ON P3.2XLARGE" target="3.62 MS">
  <data key="d0">EXHIBITS STARTUP OVERHEAD TIME</data>
</edge>
<edge source="INCEPTIONV3 ON P3.2XLARGE" target="4.82 MS">
  <data key="d0">EXHIBITS STARTUP OVERHEAD TIME</data>
</edge>
<edge source="BERTBASE ON P3.2XLARGE" target="3.62 MS">
  <data key="d0">EXHIBITS STARTUP OVERHEAD TIME</data>
</edge>
<edge source="RESNET152 ON G4DN.2XLARGE" target="2.53 MS">
  <data key="d0">EXHIBITS STARTUP OVERHEAD TIME</data>
</edge>
<edge source="INCEPTIONV3 ON G4DN.2XLARGE" target="5.49 MS">
  <data key="d0">EXHIBITS STARTUP OVERHEAD TIME</data>
</edge>
<edge source="BERTBASE ON G4DN.2XLARGE" target="6.57 MS">
  <data key="d0">EXHIBITS STARTUP OVERHEAD TIME</data>
</edge>
<edge source="TABLE 4" target="STARTUP OVERHEAD FOR PIPESWITCH TO START COMPUTING THE RST LAYER">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="TASK STARTUP OVERHEAD FOR PIPESWITCH" target="TABLE 4">
  <data key="d0">IS SHOWN IN</data>
</edge>
<edge source="TASK STARTUP OVERHEAD FOR PIPESWITCH" target="RESNET152 INCEPTIONV3 BERTBASE">
  <data key="d0">IS DEFINED AS THE DIFFERENCE BETWEEN TIME FOR</data>
</edge>
<edge source="LAYERS" target="464 189 139">
  <data key="d0">HAVE VALUES</data>
</edge>
<edge source="ONLY PRUNING 1" target="2.09 S 0.30 S 0.88 S">
  <data key="d0">REQUIRES TIME</data>
</edge>
<edge source="ONLY PRUNING 2" target="3.44 H 5.07 S">
  <data key="d0">REQUIRES TIME</data>
</edge>
<edge source="NO PRUNING" target="24 H 24 H 24 H">
  <data key="d0">REQUIRES TIME</data>
</edge>
<edge source="TABLE 5" target="EFFECTIVENESS OF TWO PRUNING TECHNIQUES">
  <data key="d0">PRESENTS</data>
</edge>
<edge source="SALUS 7" target="GPU">
  <data key="d0">REQUIRES MODELS TO BE PRELOADED TO</data>
</edge>
<edge source="SALUS 7" target="SECTION 2.2">
  <data key="d0">POSSESSES SEVERAL LIMITATIONS DESCRIBED IN</data>
</edge>
<edge source="THE MAIN SOURCE OF THE OVERHEAD" target="CUDA CONTEXT INITIALIZATION AND RST-TIME LIBRARY LOADING OPERATIONS IN PYTORCH">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="PIPESWITCH OVERHEAD FOR MOST CONFIGURATIONS" target="10MS">
  <data key="d0">REACHES UP TO</data>
</edge>
<edge source="BERT ON T4 OVERHEAD" target="LARGE MODEL SIZE">
  <data key="d0">RESULTS FROM</data>
</edge>
<edge source="BERT ON T4 OVERHEAD" target="SMALLER PCIE BANDWIDTH ON T4 COMPARED TO V100">
  <data key="d0">RESULTS FROM</data>
</edge>
<edge source="THIS EXPERIMENT" target="THROUGHPUT AND END-TO-END LATENCY OF DIFFERENT MECHANISMS">
  <data key="d0">COMPARES</data>
</edge>
<edge source="THIS EXPERIMENT" target="ALL THE OPTIMIZATIONS ON MEMORY MANAGEMENT">
  <data key="d0">DEMONSTRATES EFFECTIVENESS OF</data>
</edge>
<edge source="THROUGHPUT AND END-TO-END LATENCY OF DIFFERENT MECHANISMS" target="DIFFERENT SCHEDULING CYCLES">
  <data key="d0">ARE COMPARED UNDER</data>
</edge>
<edge source="THE DASHED LINE" target="THE THROUGHPUT OF THE READY MODEL ASSUMING NO TASK SWITCHING">
  <data key="d0">REPRESENTS THE UPPER BOUND</data>
</edge>
<edge source="THE ERROR BAR" target="MINIMUM AND MAXIMUM LATENCY">
  <data key="d0">INDICATES THE RANGE OF</data>
</edge>
<edge source="PIPESWITCH MPS STOP-AND-START" target="1S 2S 5S 10S 30S">
  <data key="d0">INCLUDES_LATENCY_MEASUREMENTS_AT</data>
</edge>
<edge source="LOWER BOUND (B) LATENCY" target="0 TO 400">
  <data key="d0">RANGES_FROM</data>
</edge>
<edge source="IT GROUPS THE ENTIRE MODEL" target="ONE TRANSMISSION">
  <data key="d0">GROUPS_INTO</data>
</edge>
<edge source="PER-LAYER PIPELINE" target="PIPELINE ORGANIZATION AT EACH LAYER">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="REDUCTION" target="OPTIMIZATIONS ON MEMORY MANAGEMENT AND WORKER SWITCHING HAVE ALREADY BEEN APPLIED">
  <data key="d0">IS SIGNIFICANT WHEN EVALUATED</data>
</edge>
<edge source="MEETING STRICT SLOS" target="ALL OVERHEADS FOR TASK SWITCHING">
  <data key="d0">REQUIRES REDUCING</data>
</edge>
<edge source="FIGURE 8" target="EFFECTIVENESS OF UNIFIED MEMORY MANAGEMENT">
  <data key="d0">DEPICTS</data>
</edge>
<edge source="PAGES OF THE MEMORY DAEMON" target="THE MAIN MEMORY">
  <data key="d0">ARE NOT PINNED TO</data>
</edge>
<edge source="UNIFIED MEMORY MANAGEMENT MECHANISM" target="PIPESWITCH">
  <data key="d0">IS USED BY</data>
</edge>
<edge source="FIGURE 9" target="EFFECTIVENESS OF ACTIVE-STANDBY SWITCHING">
  <data key="d0">DEPICTS</data>
</edge>
<edge source="FIGURE 9" target="THE RESULTS">
  <data key="d0">DISPLAYS</data>
</edge>
<edge source="THE NEW PROCESS" target="A NEW CUDA ENVIRONMENT">
  <data key="d0">NEEDS TO CREATE</data>
</edge>
<edge source="A NEW CUDA ENVIRONMENT" target="THE TOTAL TIME">
  <data key="d0">DOMINATES</data>
</edge>
<edge source="ALGORITHMS AND SYSTEMS" target="DEEP LEARNING TASKS ON CLUSTERS">
  <data key="d0">ARE DESIGNED FOR EXECUTING AND SCHEDULING</data>
</edge>
<edge source="DEEP LEARNING TASKS" target="TRAINING AND INFERENCE TASKS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MANY TECHNIQUES AND SYSTEMS" target="COMMUNICATION">
  <data key="d0">HAVE BEEN PROPOSED TO OPTIMIZE</data>
</edge>
<edge source="MANY TECHNIQUES AND SYSTEMS" target="DISTRIBUTED TRAINING">
  <data key="d0">HAVE BEEN PROPOSED TO IMPROVE</data>
</edge>
<edge source="OTHER WORKS LIKE VDNN 43 AND SWAPADVISOR 44" target="GPU MEMORY MANAGEMENT MODULE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="OTHER WORKS LIKE VDNN 43 AND SWAPADVISOR 44" target="MEMORY MANAGEMENT FOR A SINGLE TRAINING TASK OF LARGE MODELS">
  <data key="d0">FOCUS_ON</data>
</edge>
<edge source="MEMORY MANAGEMENT FOR A SINGLE TRAINING TASK OF LARGE MODELS" target="PIPESWITCH">
  <data key="d0">ARE_NOT_DIRECTLY_COMPARABLE_TO</data>
</edge>
<edge source="CLUSTER MANAGERS 4548" target="GPUS TO VMS OR CONTAINERS AT DEVICE GRANULARITY">
  <data key="d0">TYPICALLY ALLOCATE</data>
</edge>
<edge source="GPU OPTIMIZATION EFFORTS" target="RUNNING A SINGLE TASK">
  <data key="d0">FOCUS ON IMPROVING PERFORMANCE OF</data>
</edge>
<edge source="GPU OPTIMIZATION EFFORTS" target="TENSOR FUSION">
  <data key="d0">INCLUDE TECHNIQUES SUCH AS</data>
</edge>
<edge source="GPU OPTIMIZATION EFFORTS" target="KERNEL-LEVEL CONCURRENCY">
  <data key="d0">INCLUDE TECHNIQUES SUCH AS</data>
</edge>
<edge source="GPU OPTIMIZATION EFFORTS" target="KERNEL-LEVEL SCHEDULING">
  <data key="d0">INCLUDE TECHNIQUES SUCH AS</data>
</edge>
<edge source="A. VERMA, L. PEDROSA, M. KORUPOLU, D. OPPENHEIMER, E. TUNE, AND J. WILKES" target="LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG" target="EUROSYS">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG" target="2015">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="DEAN AND L. A. BARROSO" target="THE TAIL AT SCALE">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="THE TAIL AT SCALE" target="COMMUNICATIONS OF THE ACM">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="COMMUNICATIONS OF THE ACM" target="VOL.">
  <data key="d0">INCLUDES_VOLUME</data>
</edge>
<edge source="NEXUS" target="GPU CLUSTER ENGINE FOR ACCELERATING DNN-BASED VIDEO ANALYSIS">
  <data key="d0">IS A</data>
</edge>
<edge source="NEXUS" target="ACM SOSP 2019">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="H. SHEN, L. CHEN, Y. JIN, L. ZHAO, B. KONG, M. PHILIPOSE, A. KRISHNAMURTHY, AND R. SUNDARAM" target="NEXUS">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="5 A. OUSTERHOUT, J." target="AUTHOR NAME">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="FRIED, J. BEHRENS, A. BELAY, AND H. BALAKRISHNAN" target="SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="SHENANGO" target="HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS">
  <data key="d0">FOCUSES ON</data>
</edge>
<edge source="PAPER" target="USENIX NSDI">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="PAPER" target="2019">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="PAPER" target="USENIX OSDI">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="PAPER" target="2014">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="PAPER" target="PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="PAPER" target="2011">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="CUDA MULTI-PROCESS SERVICE" target="6">
  <data key="d0">IDENTIFIED_AS</data>
</edge>
<edge source="DOCUMENT HTTPS://DOCS.NVIDIA.COM/DEPLOY/PDF/CUDA_MULTIPROCESS_SERVICE_OVERVIEW.PDF" target="HTTPS://DOCS.NVIDIA.COM/DEPLOY/PDF/CUDA_MULTIPROCESS_SERVICE_OVERVIEW.PDF">
  <data key="d0">LOCATED_AT</data>
</edge>
<edge source="SALUS" target="P. YU AND M. CHOWDHURY">
  <data key="d0">PROPOSED_BY_AUTHORS</data>
</edge>
<edge source="SALUS" target="FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS">
  <data key="d0">DESCRIBED_AS</data>
</edge>
<edge source="SALUS" target="CONFERENCE ON MACHINE LEARNING AND SYSTEMS">
  <data key="d0">PRESENTED_IN</data>
</edge>
<edge source="SALUS" target="2020">
  <data key="d0">PUBLISHED_IN_YEAR</data>
</edge>
<edge source="PIPEDREAM" target="GENERALIZED PIPELINE PARALLELISM FOR DNN TRAINING">
  <data key="d0">IS A METHOD FOR</data>
</edge>
<edge source="PIPEDREAM" target="ACM SOSP 2019">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="D. NARAYANAN, A. HARLAP, A. PHANISHAYEE, V. SESHADRI, N. R. DEVANUR, G. R. GANGER, P. B. GIBBONS, AND M. ZAHARIA" target="PIPEDREAM">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="TIRESIAS" target="GPU CLUSTER MANAGER FOR DISTRIBUTED DEEP LEARNING">
  <data key="d0">IS A</data>
</edge>
<edge source="TIRESIAS" target="USENIX NSDI 2019">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="J. GU, M. CHOWDHURY, K. G. SHIN, Y. ZHU, M. JEON, J. QIAN, H. LIU, AND C. GUO" target="TIRESIAS">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="POSEIDON" target="EFFICIENT COMMUNICATION ARCHITECTURE FOR DISTRIBUTED DEEP LEARNING ON GPU CLUSTERS">
  <data key="d0">IS A</data>
</edge>
<edge source="POSEIDON" target="USENIX ATC 2017">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="H. ZHANG, Z. ZHENG, S. XU, W. DAI, Q. HO, X. LIANG, Z. HU, J. WEI, P. XIE, AND E. P. XING" target="POSEIDON">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="AMAZON WEB SERVICES" target="12">
  <data key="d0">IDENTIFIED_BY_NUMBER</data>
</edge>
<edge source="MICROSOFT AZURE" target="CLOUD COMPUTING PLATFORM">
  <data key="d0">IDENTIFIED_AS</data>
</edge>
<edge source="GOOGLE CLOUD PLATFORM" target="14">
  <data key="d0">IS NAMED AS</data>
</edge>
<edge source="HOROVOD" target="FAST AND EASY DISTRIBUTED DEEP LEARNING FRAMEWORK">
  <data key="d0">IS DESCRIBED AS</data>
</edge>
<edge source="HOROVOD" target="TENSORFLOW">
  <data key="d0">IS IMPLEMENTED IN</data>
</edge>
<edge source="TENSORFLOW" target="25">
  <data key="d0">IS_IDENTIFIED_BY</data>
</edge>
<edge source="A. SERGEEV AND M. DEL BALSO" target="HOROVOD PAPER">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="HOROVOD PAPER" target="ARXIV PREPRINT ARXIV:1802.05799">
  <data key="d0">IS PUBLISHED AS</data>
</edge>
<edge source="HOROVOD PAPER" target="2018">
  <data key="d0">IS PUBLISHED IN YEAR</data>
</edge>
<edge source="SU" target="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="17 K. HE, X. ZHANG, S. REN, AND J." target="AUTHORS">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="AUTHORS" target="Y. PENG, Y. BAO, Y. CHEN, C. WU, AND C. GUO">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="AUTHORS" target="M. RHU, N. GIMELSHEIN, J. CLEMONS, A. ZULQAR, AND S. W. KECKLER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="NVIDIA DATA CENTER DEEP LEARNING PRODUCT" target="PERFORMANCE">
  <data key="d0">EXHIBITS</data>
</edge>
<edge source="PHILLY TRACES" target="20">
  <data key="d0">QUANTITY_IS</data>
</edge>
<edge source="GITHUB REPOSITORY" target="HTTPS://GITHUB.COM/MSR-FIDDLE/PHILLY-TRACES">
  <data key="d0">HAS URL</data>
</edge>
<edge source="SZEGEDY ET AL." target="RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION" target="IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION">
  <data key="d0">WAS PRESENTED AT</data>
</edge>
<edge source="RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION" target="2016">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="J. DEVLIN, M.-W. CHANG, K. LEE, AND K. TOUTANOVA" target="BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING" target="PROCEEDINGS OF THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, VOLUME 1 (LONG AND SHORT PAPERS)">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="PROCEEDINGS OF THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, VOLUME 1 (LONG AND SHORT PAPERS)" target="2019">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="GANDIVA" target="INTROSPECTIVE CLUSTER SCHEDULING FOR DEEP LEARNING">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="GANDIVA" target="USENIX OSDI">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="GANDIVA" target="2018">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="24 W. XIAO, R. BHARDWAJ, R. RAMJEE, M. SIVATHANU, N. KWATRA, Z. HAN, P. PATEL, X. PENG, H. ZHAO, Q. ZHANG, ET AL." target="GANDIVA">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="TENSORFLOW XLA" target="54">
  <data key="d0">IDENTIFIED_BY_NUMBER</data>
</edge>
<edge source="HTTPS://WWW.TENSORFLOW.ORG" target="THE OFFICIAL WEBSITE OF TENSORFLOW">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="HTTPS://WWW.TENSORFLOW.ORG" target="XLA">
  <data key="d0">PROVIDES_ACCESS_TO</data>
</edge>
<edge source="MXNET" target="26">
  <data key="d0">IS IDENTIFIED BY</data>
</edge>
<edge source="MXNET" target="A FLEXIBLE AND EFFICIENT MACHINE LEARNING LIBRARY">
  <data key="d0">IS_DESCRIBED_AS</data>
</edge>
<edge source="MXNET" target="HETEROGENEOUS DISTRIBUTED SYSTEMS">
  <data key="d0">IS_DESIGNED_FOR</data>
</edge>
<edge source="SLAQ" target="QUALITY-DRIVEN SCHEDULING FOR DISTRIBUTED MACHINE LEARNING">
  <data key="d0">IS A</data>
</edge>
<edge source="SLAQ" target="ACM SYMPOSIUM ON CLOUD COMPUTING">
  <data key="d0">WAS PRESENTED AT</data>
</edge>
<edge source="M. J. FREEDMAN" target="SLAQ">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="ACM SYMPOSIUM ON CLOUD COMPUTING" target="2017">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="ACM SYMPOSIUM ON CLOUD COMPUTING" target="2013">
  <data key="d0">OCCURRED_IN_YEAR</data>
</edge>
<edge source="OPTIMUS" target="EFFICIENT DYNAMIC RESOURCE SCHEDULER FOR DEEP LEARNING CLUSTERS">
  <data key="d0">IS A</data>
</edge>
<edge source="OPTIMUS" target="EUROSYS 2018">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="THEMIS" target="USENIX NSDI 2020">
  <data key="d0">PRESENTED_IN_CONFERENCE</data>
</edge>
<edge source="THEMIS" target="FAIR AND EFFICIENT GPU CLUSTER SCHEDULING">
  <data key="d0">FOCUSES_ON</data>
</edge>
<edge source="THEMIS" target="K. MAHAJAN, A. BALASUBRAMANIAN, A. SINGHVI, S. VENKATARAMAN, A. AKELLA, A. PHANISHAYEE, S. CHAWLA">
  <data key="d0">AUTHORED_BY</data>
</edge>
<edge source="HYPERSCHED" target="R. LIAW, R. BHARDWAJ, L. DUNLAP, Y. ZOU, J. E. GONZALEZ, I. STOICA, A. TUMANOV">
  <data key="d0">PROPOSED_BY_AUTHORS</data>
</edge>
<edge source="HYPERSCHED" target="DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT">
  <data key="d0">FOCUSES_ON</data>
</edge>
<edge source="HYPERSCHED" target="ACM SYMPOSIUM ON CLOUD COMPUTING">
  <data key="d0">PRESENTED_AT</data>
</edge>
<edge source="HYPERSCHED" target="2019">
  <data key="d0">PUBLISHED_IN_YEAR</data>
</edge>
<edge source="CHET" target="FULLY-HOMOMORPHIC_NEURAL-NETWORK_INFERENCING">
  <data key="d0">IS_AN_OPTIMIZING_COMPILER_FOR</data>
</edge>
<edge source="CHET" target="ACM_CONFERENCE_ON_PROGRAMMING_LANGUAGE_DESIGN_AND_IMPLEMENTATION">
  <data key="d0">WAS_PRESENTED_IN</data>
</edge>
<edge source="ACM_CONFERENCE_ON_PROGRAMMING_LANGUAGE_DESIGN_AND_IMPLEMENTATION" target="2019">
  <data key="d0">OCCURRED_IN_YEAR</data>
</edge>
<edge source="R. DATHATHRI, O. SAARIKIVI, H. CHEN, K. LAINE, K. LAUTER, S. MALEKI, M. MUSUVATHI, AND T. MYTKOWICZ" target="CHET">
  <data key="d0">ARE_AUTHORS_OF</data>
</edge>
<edge source="TVM" target="deep learning">
  <data key="d0">is an automated end-to-end optimizing compiler for</data>
</edge>
<edge source="TVM" target="USENIX OSDI 2018">
  <data key="d0">was presented in</data>
</edge>
<edge source="T. CHEN, T. MOREAU, Z. JIANG, L. ZHENG, E. YAN, H. SHEN, M. COWAN, L. WANG, Y. HU, L. CEZE, ET AL." target="TVM">
  <data key="d0">are authors of</data>
</edge>
<edge source="GPIPE" target="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 2019">
  <data key="d0">PRESENTED_IN</data>
</edge>
<edge source="GPIPE" target="EFFICIENT TRAINING OF GIANT NEURAL NETWORKS">
  <data key="d0">FOCUSES_ON</data>
</edge>
<edge source="GPIPE" target="PIPELINE PARALLELISM">
  <data key="d0">USES_METHOD</data>
</edge>
<edge source="33 Y. HUANG, Y. CHENG, A. BAPNA, O. FIRAT, D. CHEN, M. CHEN, H. LEE, J. NGIAM, Q. V. LE, Y. WU, ET AL." target="GPIPE">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="BLINK" target="FAST AND GENERIC COLLECTIVES FOR DISTRIBUTED MACHINE LEARNING">
  <data key="d0">IS DESCRIBED AS</data>
</edge>
<edge source="BLINK" target="CONFERENCE ON MACHINE LEARNING AND SYSTEMS">
  <data key="d0">WAS PRESENTED AT</data>
</edge>
<edge source="BLINK" target="2020">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="G. WANG, S. VENKATARAMAN, A. PHANISHAYEE, J. THELIN, N. DEVANUR, AND I. STOICA" target="BLINK">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="36 J. LIU, J. WU, AND D. K. PANDA" target="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION OVER INNIBAND">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION" target="INT.">
  <data key="d0">IS PUBLISHED IN</data>
</edge>
<edge source="37 Q. HO, J. CIPAR, H. CUI, S. LEE, J. K. KIM, P. B. GIBBONS, G. A. GIBSON, G. GANGER, AND E. P. XING" target="MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER" target="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER" target="2013">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="A. AWAN, C.-H. CHU, H. SUBRAMONI, AND D. K. PANDA" target="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?" target="PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING">
  <data key="d0">WAS_PUBLISHED_IN</data>
</edge>
<edge source="PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING" target="2018">
  <data key="d0">OCCURRED_IN_YEAR</data>
</edge>
<edge source="GOSSIPGRAD" target="SCALABLE DEEP LEARNING USING GOSSIP COMMUNICATION BASED ASYNCHRONOUS GRADIENT DESCENT">
  <data key="d0">IS A DEEP LEARNING METHOD</data>
</edge>
<edge source="GOSSIPGRAD" target="CORR VOL.">
  <data key="d0">IS DESCRIBED IN</data>
</edge>
<edge source="A. VISHNU" target="GOSSIPGRAD PAPER">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="C. SIEGEL" target="GOSSIPGRAD PAPER">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="T. WARFEL" target="GOSSIPGRAD PAPER">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="V. AMATYA" target="GOSSIPGRAD PAPER">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="41 Z. ZHANG, C. CHANG, H. LIN, Y. WANG, R. ARORA, AND X. JIN" target="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="PAPER IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?" target="ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)">
  <data key="d0">WAS PRESENTED AT</data>
</edge>
<edge source="ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)" target="AUGUST 2020">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="42 Y. CHEN, Z. LIU, B. REN, AND X. JIN" target="EFFICIENT CONSTRUCTIONS OF CHECKPOINTS">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="PAPER EFFICIENT CONSTRUCTIONS OF CHECKPOINTS" target="INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)">
  <data key="d0">WAS PRESENTED AT</data>
</edge>
<edge source="PAPER EFFICIENT CONSTRUCTIONS OF CHECKPOINTS" target="JULY 2020">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="VDNN" target="VIRTUALIZED DEEP NEURAL NETWORKS FOR SCALABLE MEMORY-EFFICIENT NEURAL NETWORK DESIGN">
  <data key="d0">IS A</data>
</edge>
<edge source="VDNN" target="2016 49TH ANNUAL IEEEACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO)">
  <data key="d0">WAS PRESENTED AT</data>
</edge>
<edge source="VDNN" target="2016">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="NVIDIA CONTAINER RUNTIME" target="DOCKER">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="GITHUB.COM/NVIDIA/NVIDIA-DOCKER" target="HTTPS">
  <data key="d0">USES_PROTOCOL</data>
</edge>
<edge source="MESOS" target="FINE-GRAINED RESOURCE SHARING IN THE DATA CENTER">
  <data key="d0">IS A PLATFORM FOR</data>
</edge>
<edge source="MESOS" target="USENIX NSDI 2011">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="47 B. HINDMAN, A. KONWINSKI, M. ZAHARIA, A. GHODSI, A. D. JOSEPH, R. H. KATZ, S. SHENKER, AND I. STOICA" target="MESOS: A PLATFORM FOR FINE-GRAINED RESOURCE SHARING IN THE DATA CENTER">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="APACHE HADOOP YARN" target="YET ANOTHER RESOURCE NEGOTIATOR">
  <data key="d0">IS_DESCRIBED_AS</data>
</edge>
<edge source="APACHE HADOOP YARN" target="ACM SYMPOSIUM ON CLOUD COMPUTING">
  <data key="d0">WAS_PRESENTED_AT</data>
</edge>
<edge source="48 V. K. VAVILAPALLI, A. C. MURTHY, C. DOUGLAS, S. AGARWAL, M. KONAR, R. EVANS, T. GRAVES, J. LOWE, H. SHAH, S. SETH, ET AL." target="APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="GPGPU TRANSPARENT VIRTUALIZATION COMPONENT" target="HIGH PERFORMANCE COMPUTING CLOUDS">
  <data key="d0">FOCUSES_ON</data>
</edge>
<edge source="GPGPU TRANSPARENT VIRTUALIZATION COMPONENT" target="EUROPEAN CONFERENCE ON PARALLEL PROCESSING">
  <data key="d0">WAS PRESENTED_AT</data>
</edge>
<edge source="GPGPU TRANSPARENT VIRTUALIZATION COMPONENT" target="2010">
  <data key="d0">WAS PUBLISHED_IN_YEAR</data>
</edge>
<edge source="GPGPU TRANSPARENT VIRTUALIZATION COMPONENT" target="G. GIUNTA, R. MONTELLA, G. AGRILLO, AND G. COVIELLO">
  <data key="d0">WAS DEVELOPED_BY</data>
</edge>
<edge source="V. T. RAVI, M. BECCHI, G. AGRAWAL, AND S. CHAKRADHAR" target="SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="RCUDA" target="GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS">
  <data key="d0">REDUCES NUMBER OF</data>
</edge>
<edge source="RCUDA" target="2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION">
  <data key="d0">WAS PRESENTED AT</data>
</edge>
<edge source="J. DUATO, A. J. PENA, F. SILLA, R. MAYO, AND E. S. QUINTANA-ORT" target="RCUDA">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="VCUDA" target="IN VIRTUAL MACHINES">
  <data key="d0">IS GPU-ACCELERATED HIGH-PERFORMANCE COMPUTING FRAMEWORK</data>
</edge>
<edge source="VCUDA" target="IEEE TRANSACTIONS ON COMPUTERS">
  <data key="d0">IS PUBLISHED IN</data>
</edge>
<edge source="SUN AND K. LI" target="VCUDA">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="T. CHEN, M. LI, Y. LI, M. LIN, N. WANG, M. WANG, T. XIAO, B. XU, C. ZHANG, AND Z. ZHANG" target="MXNET: A FLEXIBLE AND EFFICIENT MACHINE LEARNING LIBRARY FOR HETEROGENEOUS DISTRIBUTED SYSTEMS">
  <data key="d0">ARE_AUTHORS_OF</data>
</edge>
<edge source="MXNET PAPER" target="ARXIV PREPRINT ARXIV:1512.01274">
  <data key="d0">WAS_PUBLISHED_AS</data>
</edge>
<edge source="MXNET PAPER" target="2015">
  <data key="d0">WAS_PUBLISHED_IN_YEAR</data>
</edge>
<edge source="56 C. GREGG, J. DORN, K. HAZELWOOD, AND K. SKADRON" target="FINE-GRAINED RESOURCE SHARING FOR CONCURRENT GPGPU KERNELS">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="FINE-GRAINED RESOURCE SHARING FOR CONCURRENT GPGPU KERNELS" target="4TH USENIX WORKSHOP ON HOT TOPICS IN PARALLELISM">
  <data key="d0">WAS PRESENTED AT</data>
</edge>
<edge source="4TH USENIX WORKSHOP ON HOT TOPICS IN PARALLELISM" target="2012">
  <data key="d0">OCCURRED IN YEAR</data>
</edge>
<edge source="57 S. PAI, M. J. THAZHUTHAVEETIL, AND R. GOVINDARAJAN" target="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS">
  <data key="d0">AUTHORED PAPER TITLED</data>
</edge>
<edge source="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS" target="ACM SIGARCH COMPUTER ARCHITECTURE NEWS">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="TASO" target="DEEP LEARNING COMPUTATION">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="TASO" target="AUTOMATIC GENERATION OF GRAPH SUBSTITUTIONS">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="TASO" target="ACM SOSP 2019">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="Z. JIA, O. PADON, J. THOMAS, T. WARSZAWSKI, M. ZAHARIA, AND A. AIKEN" target="TASO">
  <data key="d0">AUTHORED</data>
</edge>
</graph></graphml>