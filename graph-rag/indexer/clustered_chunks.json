{
  "418": [
    "This paper is included in the Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation November 46, 2020 978-1-939133-19-9 Open access to the Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation is sponsored by USENIX PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications Zhihao Bai and Zhen Zhang, Johns Hopkins University; Yibo Zhu, ByteDance Inc.; Xin Jin, Johns Hopkins University https:www.usenix.orgconferenceosdi20presentationbai PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications Zhihao Bai Zhen Zhang Yibo Zhu Xin Jin Johns Hopkins University ByteDance Inc. Abstract Deep learning (DL) workloads include throughput- intensive training tasks and latency-sensitive inference tasks."
  ],
  "25": [
    "The dominant practice today is to provision dedicated GPU clusters for training and inference separately.",
    "The dominant practice today is to provision dedicated GPU clusters for training and inference separately.",
    "2.1 GPU Clusters Shared GPU clusters.",
    "Even though training and inference both use GPUs, the current practice is to build dedicated clusters for training and inference separately.",
    "2.2 Fine-Grained Time-Sharing GPU We envision to build GPU clusters that can be shared across different applications including training and inference."
  ],
  "462": [
    "Due to the need to meet strict Service-Level Objectives (SLOs), GPU clus- ters are often over-provisioned based on the peak load with limited sharing between applications and task types."
  ],
  "76": [
    "We present PipeSwitch, a system that enables unused cycles of an inference application to be lled by training or other inference applications.",
    "PipeSwitch enables unused cycles of an inference application to be lled by training or other inference applica- tions."
  ],
  "465": [
    "It allows multiple DL applications to time-share the same GPU with the entire GPU memory and millisecond-scale switching overhead."
  ],
  "5": [
    "With PipeSwitch, GPU utilization can be signicantly improved without sacri- cing SLOs.",
    "Experiments on a variety of DL models and GPU cards show that PipeSwitch only incurs a task startup overhead of 3.66.6 ms and a total overhead of 5.434.6 ms (1050 better than NVIDIA MPS), and achieves near 100 GPU utilization.",
    "Experiments on a variety of DL models and GPU cards show that PipeSwitch only incurs a task startup over- head of 3.66.6 ms and a total overhead of 5.434.6 ms (1050 better than NVIDIA MPS), and achieves near 100 GPU utilization.",
    "4.5 Discussion PipeSwitch is focused on single-GPU tasks for training and inference.",
    "Multi-GPU inference tasks can be supported by performing PipeSwitch on each GPU with transactions.",
    "For training tasks, PipeSwitch supports single-GPU train- ing and asynchronous multi-GPU training for data parallel strategies, as preempting one GPU does not affect other GPUs.",
    "Thus, a signicant fraction of tasks in real- world workloads currently use a single GPU, and PipeSwitch is applicable to them out of the box.",
    "One way to seamlessly use PipeSwitch for synchronous multi-GPU training is to use elastic synchronous training, which allows the dynamic changing of the number of GPUs used for train- ing.",
    "PipeSwitch has high throughput close to the upper bound, achieving near 100 GPU utilization.",
    "We demonstrate the performance of PipeSwitch with experiments on a variety of DNN models and GPU cards.",
    "PipeSwitch can signicantly increase GPU utilization and improve the agility of DL applications."
  ],
  "453": [
    "We achieve so by introducing pipelined context switching."
  ],
  "20": [
    "The key idea is to leverage the layered structure of neural network models and their layer-by-layer computa- tion pattern to pipeline model transmission over the PCIe and task execution in the GPU with model-aware grouping.",
    "Based on this observation, we design a pipelined model transmission mechanism, which pipelines model transmission over the PCIe and model computation in the GPU.",
    "4.2 Pipelined Model Transmission Transmitting a task from CPU to GPU is bounded by the PCIe bandwidth."
  ],
  "238": [
    "We also design unied memory management and active-standby worker switching mechanisms to accompany the pipelining and ensure process-level isolation.",
    "We use an active-standby mechanism for fast worker switching and process-level isolation."
  ],
  "83": [
    "We have built a PipeSwitch prototype and integrated it with PyTorch.",
    "PipeSwitch does not modify the model structure, and only adds hooks for PyTorch to wait for transmission or synchronize the execution.",
    "5 Implementation We have implemented a system prototype for PipeSwitch with 3600 lines of code in C and Python, and we have integrated it with PyTorch 21."
  ],
  "467": [
    "1 Introduction Deep learning (DL) powers an emerging family of intelligent applications in many domains, from retail and transportation, to nance and healthcare."
  ],
  "298": [
    "GPUs are one of the most widely- used classes of accelerators for DL."
  ],
  "355": [
    "They provide better trade- off between performance, cost and energy consumption than CPUs for deep neural network (DNN) models."
  ],
  "409": [
    "DL workloads include throughput-intensive training tasks and latency-sensitive inference tasks."
  ],
  "38": [
    "Inference tasks cannot be served with training clusters under ash crowds, and training tasks cannot utilize inference clusters when the inference load is low.",
    "However, when there is a ash crowd (e.g., an ap- plication suddenly becomes popular and the demand grows beyond the operators expectation), the training cluster can- not preempt the training tasks for inference tasks."
  ],
  "85": [
    "Con- sequently, inference clusters are often over-provisioned for the peak load, in order to meet strict Service Level Objectives (SLOs).",
    "Inference clusters are over-provisioned for the peak load, because they directly serve user requests and need to meet strict SLOs."
  ],
  "65": [
    "Even for inference itself, production systems are typ- ically provisioned to each application on per-GPU granularity to limit the interference between applications.",
    "Even for inference tasks, production systems often allocate GPUs to applications on per-GPU granularity (e.g., binding GPUs to the VMs, containers or processes of an applica- tion), in order to limit the interference between different applications and satisfy the SLO requirements."
  ],
  "356": [
    "Ideally, multiple DL applications should be able to be packed to the same GPU server to maximize GPU utiliza- tion via time-sharing."
  ],
  "227": [
    "This is exactly how operating systems achieve high CPU utilization via task scheduling and con- text switching.",
    "This is inspired by the OS scheduler and context switching in the CPU world."
  ],
  "113": [
    "The idea of ne-grained CPU time-sharing has been further extended to cluster scheduling.",
    "Similar to CPU workloads, ne-grained time-sharing can provide better utilization than provisioning dedicated re- sources, while providing necessary process-level isolation.",
    "Enabling ne-grained scheduling cycles."
  ],
  "321": [
    "For example, Google Borg 1 packs online services and batch jobs, and saves 20-30 machines (compared with not packing them)."
  ],
  "434": [
    "Why cant we use GPUs in the same way?"
  ],
  "117": [
    "The gap is that GPU has high overhead when switching between tasks.",
    "The gap: the precious GPU memory and slow switching."
  ],
  "289": [
    "Consequently, naively using GPUs in the same way as CPUs will not satisfy the requirements of DL infer- ence that have strict SLOs in the range of tens to hundreds of milliseconds 2,3."
  ],
  "284": [
    "If a GPU switches to a DNN model (e.g., ResNet) that has not been preloaded onto the GPU, it can take multiple seconds before serving the rst inference request, even with state-of-the-art tricks like CUDA unied mem- ory 4 (6)."
  ],
  "343": [
    "In contrast, CPU applications can be switched in milliseconds or even microseconds 5."
  ],
  "2": [
    "To avoid such switching overhead, the existing solution is to spatially share the GPU memory.",
    "In addition, this approach does not provide strong GPU memory isolation between applications."
  ],
  "1": [
    "For example, although NVIDIA Multiple Process Sharing (MPS) 6 and Salus 7 allow multiple processes to use the same GPU, they require all processes data (e.g., DNN models) to be preloaded into the GPU memory.",
    "This is the multi-process support from NVIDIA which allows the inference process to share the GPU with the training process.",
    "NVIDIA MPS 6 pro- vides ofcial support for sharing a GPU between multiple processes."
  ],
  "455": [
    "Unfortunately, the GPU memory is much more limited than host memory and cannot preload many applica- tions."
  ],
  "49": [
    "Sometimes, just one single memory-intensive training task may consume all the GPU memory.",
    "The training task stops and cleans its GPU environment, such as freeing the GPU memory.",
    "It stops the training task in the GPU, and then starts the inference task.",
    "The training task occupies the entire GPU memory and does not stop when inference tasks come."
  ],
  "46": [
    "Moreover, the mem- ory footprints of inference tasks are also increasingthe mod- els are getting larger, and request batching is prevalently used to increase throughput 3.",
    "In addition, request batching is prevalently used to increase throughput 3, which further increases the GPU memory requirement of inference applications."
  ],
  "41": [
    "As such, we argue that a context switching design that min- imizes the switching overhead, especially quickly switching the contents on GPU memory, is a better approach for ef- ciently time-sharing GPUs.",
    "To our best knowledge, no existing solution offers such context switching abstraction for GPU.",
    "We achieve so by introducing a new technology called pipelined context switching that exploits the characteristics of DL applications to achieve millisecond-scale overhead for switching tasks on GPUs.",
    "To achieve this goal, however, we face a major challengefast GPU context switching between different processes.",
    "Thus, it is possible to build a pipeline that overlaps the computation and GPU memory swapping for fast context switching.",
    "There is no need for context switching if the application is already loaded in the GPU.",
    "We introduce pipelined context switching to minimize task switch- ing overhead on GPUs for DL applications."
  ],
  "24": [
    "The DNN models can be held in host memory, which is much larger and cheaper than GPU memory, and the GPU can quickly context-switch between USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 499 the models either for training or inference.",
    "To run DNN workloads in a large scale, enterprises build GPU clusters that are either pri- vately 10,11 or publicly 1214 shared by multiple users.",
    "11 M. Jeon, S. Venkataraman, A. Phanishayee, u. Qian, W. Xiao, and F. Yang, Analysis of large-scale multi- tenant GPU clusters for DNN training workloads, in USENIX ATC, 2019.",
    "512 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association 19 M. Jeon, S. Venkataraman, A. Phanishayee, J. Qian, W. Xiao, and F. Yang, Analysis of large-scale multi- tenant GPU clusters for DNN training workloads, in USENIX ATC, 2019."
  ],
  "400": [
    "This way, the num- ber of applications that can be multiplexed is not limited by the GPU memory size, and each application is able to use the entire GPU compute and memory resources during its time slice."
  ],
  "55": [
    "To this end, we propose PipeSwitch, a system that (i) en- ables GPU-efcient multiplexing of many DL applications on GPU servers via ne-grained time-sharing, and (ii) achieves millisecond-scale latencies and high throughput as dedicated servers.",
    "We propose PipeSwitch, a system that enables GPU- efcient ne-grained time-sharing for multiple DL appli- cations, and achieves millisecond-scale context switching latencies and high throughput.",
    "Combining all the ideas into our system, PipeSwitch, we close the gap of GPU memory sharing and switching, and enable the design of an efcient time-sharing GPU cluster for DL workloads.",
    "3 PipeSwitch Overview PipeSwitch enables GPU-efcient multiplexing of multiple DL applications on GPU servers.",
    "8 Conclusion We present PipeSwitch, a system that enables GPU-efcient ne-grained time-sharing for multiple DL applications.",
    "With these techniques, PipeSwitch is able to achieve millisecond-scale task switching time, and enables DL applications on time-sharing GPUs to meet strict SLOs."
  ],
  "103": [
    "Such small switching overhead is critical for DL applications to satisfy strict SLO requirements.",
    "It exploits the characteristics of DL applications to achieve millisecond-scale task switch- ing overhead in order to satisfy SLO requirements."
  ],
  "228": [
    "To understand the problem, we rst perform a measure- ment study to prole the task switching overhead and analyze the overhead of each component.",
    "4.1 Proling Task Switching Overhead In order to understand the problem, we perform a measure- ment study to prole the task switching overhead."
  ],
  "8": [
    "We divide the switching overhead into four components, which are old task cleaning, new task initialization, GPU memory allocation, and model transmission via PCIe from CPU to GPU.",
    "Instance Type g4dn.2xlarge p3.2xlarge GPU Type NVIDIA T4 NVIDIA V100 Task Cleaning 155 ms 165 ms Task Initialization 5530 ms 7290 ms Memory Allocation 10 ms 13 ms Model Transmission 91 ms 81 ms Total Overhead 5787 ms 7551 ms Inference Time 105 ms 32 ms Table 1: Measurement results of task switching overhead and the breakdown of individual components."
  ],
  "297": [
    "Every component takes a considerable amount of time, varying from tens of mil- liseconds to seconds."
  ],
  "186": [
    "Such overhead is signicant, because an inference task itself only takes tens of milliseconds on a GPU and the latency SLOs are typically a small multiple of the inference time 3.",
    "One source of the overhead is the contentions both on the computation and memory of the GPU, as the training task do not stop when an inference task comes."
  ],
  "468": [
    "We take a holistic approach, and exploit the characteristics of DL applications to minimize the overhead of all the com- ponents."
  ],
  "61": [
    "Our design is based on a key observation that DNN models have a layered structure and a layer-by-layer compu- tation pattern.",
    "DNN models are usually deep, consisting of multiple layers stacking one on another.",
    "Furthermore, the computation of DNN models takes place layer by layer as well.",
    "Our key observation is that DNN models have a layered structure."
  ],
  "129": [
    "As such, there is no need to wait for the entire model to be transmitted to the GPU before starting computa- tion.",
    "In both cases, a task does not need to wait for the entire model to be transmitted to the GPU before beginning the computation."
  ],
  "97": [
    "Naive pipelining on per-layer granularity introduces high overhead on tensor transmission and synchronization.",
    "Pipelining on per-layer granularity requires synchronization for every layer."
  ],
  "100": [
    "We divide layers into groups, and design an optimal model-aware grouping algorithm to nd the best grouping strategy for a given model.",
    "Optimal model-aware grouping.",
    "Based on these two insights, we design an algo- rithm to nd the optimal grouping strategy for a given model."
  ],
  "43": [
    "The computation of a DL task is layer by layer, which has a simple, regular pattern for memory allocation.",
    "A DL task stores two important types of data in the GPU memory: the DNN model (including the model parameters), and the intermediate results."
  ],
  "229": [
    "The default general-purpose GPU memory management (e.g., CUDA uni- ed memory 4) is an overkill and incurs unnecessary over- head.",
    "NVIDIA also provides CUDA unied memory 4 to automatically handle memory movement between the host memory and the GPU memory for applications."
  ],
  "80": [
    "We design unied memory management with a dedi- cated memory daemon to minimize the overhead.",
    "We use unied memory management with the memory daemon to both achieve minimal memory footprint and eliminate extra memory copies."
  ],
  "17": [
    "The daemon pre-allocates the GPU memory, and re-allocates it to each task, without involving the expensive GPU memory manager.",
    "To be compatible with the existing system and incur minimal changes, instead of replacing the GPU memory manager, the memory daemon uses cudaMalloc to obtain the GPU mem- ory when the system starts, and then dynamically allocates the memory to the workers at runtime."
  ],
  "30": [
    "The DNN models are stored only once in the memory daemon, instead of in every worker, to minimize memory footprint.",
    "We exploit that the memory allocation for a DNN model is deterministic to eliminate extra memory copies between the daemon and the workers and reduce the IPC overhead .",
    "It is also important to note that no unied memory management requires each worker to keep a copy for each DNN model, which increases the memory footprint."
  ],
  "118": [
    "Each server contains an active worker and multiple standby workers.",
    "A server has one or more standby work- ers."
  ],
  "12": [
    "The active worker executes the current task on the GPU; the standby workers stay on the CPU and wait for the next task.",
    "The active worker is the worker that cur- rently executes a task in the GPU.",
    "Here a worker is a process that executes tasks on one GPU.",
    "After the active worker completes or stops the current task, the controller noties the memory daemon and the standby worker to load the task to GPU to execute with pipelined model transmission (4.2)."
  ],
  "0": [
    "Our mechanism parallelizes old task cleaning in the active worker and new task initialization in the standby worker to minimize worker switching overhead.",
    "As we have proled 506 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association No Task No Task Process- Cleaning Initialization Level Overhead Overhead Isolation Two Processes    One Process    Active-Standby    Table 2: Comparison of worker switching mechanisms.",
    "We design an active and standby worker switching mecha- nism that hides the overhead of both task cleaning and task initialization, and also ensures process-level isolation."
  ],
  "6": [
    "With separate worker processes, PipeSwitch enforces process-level isolation.",
    "Un- like pipelining for the same task, PipeSwitch requires us to address new technical challenges on memory management and worker switching across different processes.",
    "4.4 Active-Standby Worker Switching PipeSwitch aims to provide fast task switching and ensure process-level isolation.",
    "PipeSwitch has an active worker and multiple standby workers.",
    "6.4 Active-Standby Worker Switching To evaluate the effectiveness of active-standby worker switch- ing, we keep all other components of PipeSwitch the same, and compare the following mechanisms discussed in 4.4.",
    "PipeSwitch It is the active-standby worker switching mech- anism used by PipeSwitch.",
    "PipeSwitch uses an active-standby worker switching mechanism to parallelize old task cleaning and new task initialization, and incurs minimal overhead."
  ],
  "33": [
    "Pipelining is a canonical technique widely used in com- puter systems to improve system performance and maxi- mize resource utilization.",
    "Pipelining brings two sources of system overheads."
  ],
  "293": [
    "Prior work in DL systems such as PipeDream 8 and ByteScheduler 9 has applied pipelining to distributed training."
  ],
  "230": [
    "These solutions focus on inter-batch pipelining to overlap computation and gradient transmission of different batches for training workloads of the same DNN model.",
    "The key novelty of PipeSwitch is that it introduces intra-batch pipelining to overlap model transmission and com- putation to reduce the overhead of switching between different DNN models, which can be either inference or training."
  ],
  "436": [
    "We design new techniques to not only support training, but also inference that has strict SLOs."
  ],
  "259": [
    "In summary, we make the following contributions."
  ],
  "195": [
    "We introduce pipelined context switching, which exploits the characteristics of DL applications, and leverages pipelined model transmission, unied memory manage- ment, and active-standby worker switching to minimize switching overhead and enforce process-level isolation.",
    "Pipelined context switching includes three key techniques, which are pipelined model transmission, unied memory management and active- standby worker switching."
  ],
  "312": [
    "We implement a system prototype and integrate it with Py- Torch."
  ],
  "101": [
    "2 Motivation In this section, we identify the inefciencies in todays shared GPU clusters, and motivate running DL workloads on GPUs in the ne-grained time-sharing model.",
    "We propose to pack multiple DL applications onto the same GPU via ne-grained time-sharing abstraction to maximize GPU utilization.",
    "Such fast task switching enables more exible ne-grained scheduling to improve GPU utilization for dynamic workloads."
  ],
  "315": [
    "Such GPU clusters are usually specically designed with ded- icated physical forms and power supplies, along with high speed networks and specialized task schedulers."
  ],
  "36": [
    "500 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association Why build a shared cluster instead of a dedicated one for each user?",
    "USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 513 45 Kubernetes.",
    "514 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association"
  ],
  "489": [
    "The main reason is to bring down the cost."
  ],
  "329": [
    "The demand of training is not well predictableit would depend on the progress of different developers."
  ],
  "471": [
    "The demand of infer- ence is more predictable, e.g., an inference task for a partic- ular application usually has a daily periodical pattern based on the application usage."
  ],
  "393": [
    "Nevertheless, the patterns can still vary across different tasks."
  ],
  "353": [
    "Like traditional CPU workloads, a shared cluster by different tasks would increase the resource utilization via time-sharing."
  ],
  "341": [
    "No sharing between training and inference."
  ],
  "391": [
    "However, such shared clusters are not shared between training and infer- ence."
  ],
  "473": [
    "This brings several inefciencies."
  ],
  "488": [
    "Although inference clusters are not always run- ning at high utilization, they cannot be utilized by training."
  ],
  "443": [
    "Training clusters are equipped with powerful GPUs to run training tasks, which are often elastic and do not have strict deadlines."
  ],
  "447": [
    "One of the reasons for separately provisioning is that GPUs designed for inference tasks might be too wimpy for training tasks."
  ],
  "306": [
    "This, however, has started to change with the arrival of new GPU hardware, most notably NVIDIA T4."
  ],
  "441": [
    "Compared with NVIDIA V100 which has up to 32GB GPU memory and 15.7 TFLOPS (single-precision), NVIDIA T4 has compara- ble performance with 16GB GPU memory and 8.1 TFLOPS (single-precision)."
  ],
  "415": [
    "Also, new algorithms and systems for dis- tributed training 8,9,15,16 enable multiple GPUs to accel- erate training, if one GPU is not fast enough."
  ],
  "359": [
    "Our industry collaborator, a leading online service provider, conrms this observation."
  ],
  "148": [
    "This service provider currently runs more than 10K V100 GPUs for training, and at least 5 as many T4 GPUs for inference."
  ],
  "263": [
    "The computation power on both sides is within the same order of magnitude."
  ],
  "395": [
    "The inference workload uctuates in correlation with the number of active users, and shows clear peaks and valleys within each day the peak demand during daytime is  2 of the valley at midnight."
  ],
  "498": [
    "It would be a great match to utilize inference GPUs during less busy times for training models that require daily updates with latest data."
  ],
  "255": [
    "A good example is to ne-tune BERT using daily news."
  ],
  "279": [
    "This means great opportunity in improving GPU utilization by Borg-like 1 systems for GPUs."
  ],
  "381": [
    "It has the following advantages."
  ],
  "272": [
    "It would dramatically improve the resource utilization, es- pecially because inference and training workloads have complementary usage patterns."
  ],
  "403": [
    "Online inference services are often more idle during midnight, while many training developers would start a time-consuming job at night."
  ],
  "377": [
    "Be- sides, inference loads on different models have different patterns, which also benets from the time sharing."
  ],
  "439": [
    "It would greatly simplify the design of load balancers and schedulers as any server would be able to run any task with low overhead to switch between different applications."
  ],
  "351": [
    "A mod- ern server can be equipped with several TB of host memory, enabling it to load many applications."
  ],
  "446": [
    "However, task execu- tion on GPUs require GPU memory, which is very limited even on high-end GPUs, e.g., 16 GB for T4 and 32 GB for V100."
  ],
  "484": [
    "More importantly, GPU memory is purposed for task execution, not for storing the state of idle applications."
  ],
  "58": [
    "DL tasks, especially training, require a large amount, or even all of the memory on a GPU.",
    "First, DL applications have large models and generate large amounts of intermediate results, which require a lot of GPU memory."
  ],
  "300": [
    "Storing the models in the GPU like Salus 7 cannot support training tasks which are memory-intensive or even multiple inference tasks which have large models."
  ],
  "286": [
    "This is particularly important as state-of-the-art models are getting deeper and larger, and thus even idle applications can occupy large mem- ory space."
  ],
  "331": [
    "Ideally, the active application should be able to utilize the entire GPU memory for its purpose, and the number of applications that can be served by a GPU server should only be limited by its host memory size."
  ],
  "399": [
    "Consequently, switching a task would require heavy memory swapping."
  ],
  "44": [
    "Unfortunately, many online inference workloads require strict SLOs that naive memory swapping between the host memory and the GPU memory cannot meet.",
    "For inference tasks, strict SLOs require requests to be handled in small batches for low latency, so it is common to execute an inference task with a single GPU 18."
  ],
  "261": [
    "For example, we test the strawman scenario where we stop a training task and then start an inference task."
  ],
  "371": [
    "The rst inference batch would require several seconds to nish (4.1)."
  ],
  "114": [
    "Existing support such as NVIDIA MPS is not optimized for DL workloads, and incurs hundreds of milliseconds overhead (6).",
    "NVIDIA MPS has lower overhead compared to stop-and-start, but still incurs several hundred milliseconds overhead, which prevents MPS from meeting strict SLOs."
  ],
  "7": [
    "USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 501 Controller Active Worker GPU Memory Daemon Standby Worker Standby Worker New Task Figure 1: PipeSwitch architecture.",
    "USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 509 1s 2s 5s 10s 30s 0 100 200 300 400 Throughput (batchessec) Upper bound PipeSwitch MPS Stop-and-start (a) Throughput (eight p3.2xlarge instances).",
    "They use inter-batch pipelining for training of the same task, while PipeSwitch introduces intra-batch pipelining to fast start both USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 511 training and inference tasks and enables fast switching across tasks."
  ],
  "451": [
    "The opportunity: DL workloads have well-dened struc- tures."
  ],
  "445": [
    "Fortunately, the structure and computation pattern of DNN models allow us to highly optimize task switching and achieve millisecond-scale overhead."
  ],
  "307": [
    "In the following sections, we will show that such pipeline is indeed feasible and effective."
  ],
  "303": [
    "In addition, we will also need to resolve other challenges like memory management and worker switching."
  ],
  "320": [
    "It benets switching not only between inference and training, but also between inference on different models."
  ],
  "360": [
    "Here we provide an overview of the architecture and task execution."
  ],
  "444": [
    "System architecture."
  ],
  "31": [
    "Figure 1 shows the architecture of a PipeSwitch server.",
    "T0 T1 Tn-1 T2 Figure 2: PipeSwitch pipelines model transmission and task execution."
  ],
  "295": [
    "This server contains four types of compo- nents: a controller, a memory daemon, an active worker, and multiple standby workers."
  ],
  "386": [
    "Controller."
  ],
  "269": [
    "The controller is the central component."
  ],
  "367": [
    "It re- ceives tasks from clients, and controls the memory daemon and the workers to execute the tasks."
  ],
  "202": [
    "Memory daemon.",
    "Controller and memory daemon."
  ],
  "45": [
    "The memory daemon manages the GPU memory and the DNN models.",
    "The server stores the DNN models in the host memory."
  ],
  "47": [
    "It allocates the GPU memory to the active worker, and transfers the model from the host memory to the GPU memory.",
    "This eliminates the GPU environ- ment initialization overhead when a new task is assigned to a worker."
  ],
  "156": [
    "Active worker.",
    "Worker."
  ],
  "333": [
    "All components should be optimized to meet the SLOs."
  ],
  "469": [
    "Standby worker."
  ],
  "4": [
    "A standby worker is idle, is initializing a new task, or is cleaning its environment for the previous task.",
    "The standby worker becomes the new active worker to execute the new task, and the active worker becomes a standby worker and cleans the environment for the previous task (4.4).",
    "If a new task arrives before a standby worker nishes cleaning a previous task, the new task needs to wait, which increases its startup time."
  ],
  "32": [
    "Task execution.",
    "Task initialization."
  ],
  "199": [
    "The controller queues a set of tasks received from the clients."
  ],
  "137": [
    "It uses a scheduling policy to decide which task to execute next.",
    "The scheduling is preemptive, i.e., the controller can preempt the current task for the next one based on the scheduling policy."
  ],
  "323": [
    "It supports canonical scheduling policies such as rst come rst serve (FCFS) and earliest deadline rst (EDF), and can be easily extended to support new policies."
  ],
  "294": [
    "We focus on fast context switching, and the specic schedul- ing algorithm is orthogonal to this paper."
  ],
  "270": [
    "For example, if the current task is a training task, the controller can preempt it for an inference task that has a strict latency SLO."
  ],
  "475": [
    "To start a new task, the controller either waits for the current task to nish (e.g., if it is inference) or preempts it by notifying the active worker to stop (e.g., if it is training)."
  ],
  "23": [
    "At the same time, the controller noties an idle standby worker to initialize its environment for the new task.",
    "When the controller schedules a task, it determines whether to switch to another worker."
  ],
  "59": [
    "The memory daemon allocates the memory to the standby worker (4.3), and transmits the model used by the new task from the host memory to the GPU memory.",
    "Because the memory daemon also manages the GPU memory, it directly transmits the model from the host memory to the GPU memory for task startup, which eliminates the extra memory copy from the memory daemon to the worker.",
    "After the model is transmitted to the GPU, the memory daemon needs to notify the worker and export the relevant GPU memory handlers to the worker, so that the worker can access the model to execute its task.",
    "The memory daemon handles GPU memory allocation and model transmission, but creates and sends GPU memory handlers to workers."
  ],
  "493": [
    "The primary goal of this paper is to design a set of techniques based on the characteristics of DL applications to minimize the task switching overhead in this process."
  ],
  "54": [
    "4 PipeSwitch Design We rst perform a measurement study to prole the task switching overhead and break it down to individual compo- nents.",
    "6 Evaluation In this section, we rst use end-to-end experiments to demon- strate the benets of PipeSwitch, and then show the effective- ness of the design choices on each component."
  ],
  "491": [
    "Then we describe our design to systematically mini- mize the overhead of each component."
  ],
  "363": [
    "The mea- 502 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association surement considers a typical scenario that a server stops a training task running on the GPU, and then starts an in- ference task."
  ],
  "260": [
    "The DNN model used in the measurement is ResNet152 17."
  ],
  "301": [
    "The measurement covers two types of instances on Amazon AWS, which are g4dn.2xlarge with NVIDA T4 and p3.2xlarge with NVIDIA V100."
  ],
  "390": [
    "We assume the inference task has arrived at the server, and focus on mea- suring the time to start and execute it on the GPU."
  ],
  "401": [
    "We exclude the network time and the task queueing time."
  ],
  "402": [
    "Table 1 shows the results."
  ],
  "254": [
    "The total times to start the infer- ence task on the GPUs are 5787 ms and 7551 ms, respectively."
  ],
  "344": [
    "We break the overhead down into the four components."
  ],
  "128": [
    "Task cleaning.",
    "On one hand, task cleaning takes time."
  ],
  "425": [
    "The inference task creates and ini- tializes its environment (i.e., process launching, PyTorch CUDA runtime loading, and CUDA context initialization)."
  ],
  "490": [
    "Memory allocation."
  ],
  "173": [
    "The inference task allocates GPU memory for its neural network model.",
    "The inference task transmits the model from the host memory to the GPU memory."
  ],
  "34": [
    "Model transmission.",
    "Grouped transmission."
  ],
  "330": [
    "The inference time on V100 is lower than that on T4, and both of them are signicantly lower than the total overheads."
  ],
  "134": [
    "The reason for lower overhead on T4 is that task switching largely depends on CPU, and g4dn.2xlarge is equipped with better CPU than p3.2xlarge (Intel Platinum 8259CL vs. Intel Xeon E5-2686 v4)."
  ],
  "389": [
    "A strawman solution that simply stops the old task and starts the new task would easily violate SLOs."
  ],
  "397": [
    "Because all the components take considerable time com- pared to the inference time, we emphasize that all the com- ponents should be optimized to achieve minimal switching overhead and meet the SLOs."
  ],
  "334": [
    "The PCIe bandwidth is the physical limit on how fast an arbitrary task can be loaded to the GPU."
  ],
  "281": [
    "We exploit the characteristics of DL applications to circumvent this physical limit."
  ],
  "379": [
    "The computation is performed layer by layer."
  ],
  "396": [
    "An inference task only performs a forward pass from the rst layer to the nal layer to make a prediction; each iteration in a training task performs a forward pass and then a backward pass."
  ],
  "317": [
    "Instead, the task can start the com- putation of a layer as soon as the layer is loaded in the GPU and the input of the layer is ready (i.e., the previous layers have nished their computation), regardless of its following layers."
  ],
  "325": [
    "Figure 2 illustrates the advantage of pipelining over the strawman solution."
  ],
  "11": [
    "PipeSwitch requires the knowledge of models.",
    "It is the pipelining mechanism with optimal model-aware grouping in PipeSwitch.",
    "PipeSwitch uses model-aware grouping and achieves the best trade-off between pipeline overhead and efciency."
  ],
  "120": [
    "T0 E0 model transmission over PCIe task execution on GPU T1 Tn-1 E1 En-1 T2 E2 (a) Transmit model to GPU, and then execute task on GPU.",
    "PCIe GPU E0 E1 En-1 E2 (b) Pipeline model transmission and task execution."
  ],
  "358": [
    "The example shows an inference task that only has a forward pass in task execution."
  ],
  "369": [
    "Adding hooks can be automated, and PipeSwitch can be im- plemented as a part of the DNN framework, e.g., PyTorch, so it can gather the model structure information while remaining transparent to users and cluster managers."
  ],
  "197": [
    "The basic way for pipelin- ing is to pipeline on per-layer granularity, i.e., the system transmits the layers to the GPU memory one by one, and the computation for a layer is blocked before the layer is trans- mitted."
  ],
  "448": [
    "One is the overhead to invoke multiple calls to PCIe to trans- mit the data."
  ],
  "349": [
    "For a large amount of data (e.g., combining the entire model to a large tensor to transmit together), the trans- mission overhead is dominated by the data size."
  ],
  "398": [
    "But when we divide the model into many layers, invoking a PCIe call for each layer, especially given that some layers can be very small, would cause signicant extra overhead."
  ],
  "457": [
    "The other is the synchronization overhead between transmission and computa- tion, which is necessary for the computation to know when a layer is ready to compute."
  ],
  "435": [
    "We use grouping to minimize these two sources of over- head."
  ],
  "438": [
    "We combine multiple layers into a group, and the pipelining is performed on per-group granularity."
  ],
  "440": [
    "In this way, the pipelining overhead is paid once for each group, instead of each layer."
  ],
  "19": [
    "Grouping introduces a trade-off between pipelin- ing efciency and pipelining overhead.",
    "On one hand, using small groups (e.g., per-layer in the extreme case) enables more overlap between transmission and computation, which improves pipelining efciency, but it also pays more pipelin- ing overhead.",
    "On the other hand, using big groups (e.g., the entire model in one group in the extreme case) has minimal pipelining overhead, but reduces the chance for overlapping."
  ],
  "244": [
    "Grouping must be model-aware, because models have dif- ferent structures in terms of the number of layers and the size of each layer."
  ],
  "18": [
    "Naively, we can enumerate all possible com- binations to nd the optimal grouping strategy.",
    "In order to nd the optimal grouping strategy efciently, we introduce two pruning techniques based on two insights."
  ],
  "463": [
    "This is not amenable because large models can have hundreds of layers and the time complexity for enumeration is exponential."
  ],
  "432": [
    "USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 503 PCIe GPU lower bound of F(Group(0, i), i1) PCIe GPU Group(0, i) Group(i1, j) Group(0, i) Group(i1, j) j, j1, , n-1 j, j1, , n-1 (a) Prune this case if lower bound current optimal time."
  ],
  "62": [
    "(b) Prune the cases that group from i to j  j. batch at least from layer i1 to j Group(0, i) Group(i1, n-1) Group(0, i) Group(i1, n-1) Figure 3: Examples for two pruning techniques.",
    "Therefore, we can prune the cases that group from layer (i  1) to j  j and only search for j j. Algorithm."
  ],
  "313": [
    "Before we dive into the details, we rst formulate the problem."
  ],
  "89": [
    "Let the number of layers be n. Let F(B,i) be a function that returns the total time of the optimal grouping strategy from layer i to n-1 given that layer 0 to i-1 have formed groups rep- resented by B.",
    "Given the group from layer x to i is formed, the function recursively applies itself to nd the optimal groups from layer i1 to n-1 (line 21-23), and updates optgroups if the current strategy is better (line 24-26)."
  ],
  "405": [
    "Then we have the following recursive formula."
  ],
  "365": [
    "F(,0)  min i F(group(0,i),i1) (1) Specically, to nd the optimal grouping strategy for the en- tire model (i.e., F(, 0)), we divide all possible combinations into n cases based on how the rst group is formed, i.e., case i means the rst group contains layer 0 to i."
  ],
  "327": [
    "This formula can be applied recursively to compute F(group(0,i),i1)."
  ],
  "119": [
    "Our rst insight is that it is not necessary to examine all the n cases, because if the rst group contains too many layers, the computation of the rst group would be delayed too much to compensate the pipeline efciency.",
    "Our second insight is that other than the rst group, we can safely pack multiple layers in a group based on the progress of computation without affecting pipeline efciency."
  ],
  "164": [
    "Let T(i, j) and E(i, j) be the transmission and execution times for a group from layer i to j respectively, where T(i, j) is calculated based on the size of layer i to j and PCIe bandwidth, and E(i, j) is proled on the GPU."
  ],
  "311": [
    "Note that the overhead of invoking multiple calls is included in T(i, j)."
  ],
  "352": [
    "As illustrated by Figure 3(a), we compute a lower bound for the total time for each case in Equation 1."
  ],
  "387": [
    "F(group(0,i),i1)  min(T(0,i)T(i1,n1), T(0,i)E(0,i)E(i1,n1)) (2) The lower bound considers the best case that all the remaining layers are combined in one group for transmission and com- putation, and that the computation and communication can be perfectly overlapped, i.e., its computation can happen right after the computation of the rst group nishes."
  ],
  "407": [
    "If the lower bound of case i is already larger than the total time of the best grouping strategy found so far, then case i (i.e., the recursive computation for F(group(0,i),i1)) can be pruned."
  ],
  "345": [
    "Figure 3(b) shows an example for this insight."
  ],
  "437": [
    "Suppose that we have already xed the rst group to be from layer 0 to i, and we apply Equation 1 recursively to enumerate the cases for the PCIe GPU B.delay (group at least from layer x to j to fill) 0, 1, , x-1 0, 1, , x-1 Group(a, i) Group(i1, n-1) Group(x, i) Group(i1, n-1) lower bound of F(B  Group(a,i), i1) Figure 4: General case for the two pruning techniques."
  ],
  "257": [
    "second group."
  ],
  "487": [
    "We can hide the transmission of the second group into the computation of the rst group, as long as the transmission nishes no later than the computation of the rst group."
  ],
  "337": [
    "The least number of layers to group can be computed using the following equation."
  ],
  "410": [
    "j argmax j T(i1, j) E(0,i) (3) Group from layer (i1) to j  jis no better than grouping from (i  1) to jbecause it does not increase the pipeline efciency and has higher pipeline overhead."
  ],
  "433": [
    "We emphasize that this algorithm runs ofine to nd the strat- egy, and the resulting strategy is used online by PipeSwitch for context switching."
  ],
  "342": [
    "Algorithm 1 shows the pseudo code."
  ],
  "68": [
    "The function FindOptGrouping recursively nds the opti- mal grouping strategy based on Equation 1 (line 1-27).",
    "Algorithm 1 computes the recursive function FindOptGrouping(B,x)."
  ],
  "88": [
    "It takes two inputs: B represents the groups that have already formed, x is the rst layer that have not formed a group.",
    "For case i  k, the rst group contains all layers from x to n1."
  ],
  "290": [
    "It uses optgroups to store the best grouping strategy from layer x given B, which is initialized to none (line 2)."
  ],
  "108": [
    "The algorithm applies the second pruning insight to form the rst group from layer x (line 3-9).",
    "The algorithm divides the problem into k  1 cases, where case i (0 i k) forms the rst group from layer x to xi."
  ],
  "428": [
    "Equation 3 and Figure 3(b) illustrate this insight with a special example that B only contains one group from layer 0 to i."
  ],
  "268": [
    "In general, B can contain multiple groups formed by previous layers, and we use B.delay to de- note the time to which the group can be formed, as shown in Figure 4."
  ],
  "216": [
    "The algorithm nds jbased on B.delay (line 4-9), and the enumeration for i can skip the layers from x to j-1 (line 11)."
  ],
  "274": [
    "For case i, the algorithm applies the rst insight to compute the lower bound (line 12-17)."
  ],
  "470": [
    "Again, the example in Equation 2 and Figure 3(a) is a special case when x is 0."
  ],
  "271": [
    "For the general case, the computation from x has to wait for both its transmission (i.e., T(x,i)) and the computation of the previous groups (i.e., B.delay), as shown in Figure 4."
  ],
  "291": [
    "If the lower bound is already bigger than the current optimal time, then case i is pruned (line 18-19)."
  ],
  "479": [
    "Finally, it returns optgroups (line 27)."
  ],
  "361": [
    "In practice, we use a heuristic that bootstraps optgroups with a relative 504 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association Algorithm 1 Optimal Model-Aware Grouping 1: function FINDOPTGROUPING(B, x) 2: optgroups 0, optgroups.time  3:  nd rst group from layer i to j 4: jx 5: for layer i from x to n1 do 6: if T(x,i) B.delay then 7: ji 8: else 9: break 10:  recursively nd the optimal grouping 11: for layer i from jto n1 do 12: if optgroups  0 then 13:  compute lower bound 14: transtime T(x,i)T(i1,n1) 15: exectime max(T(x,i),B.delay) 16: E(x,i)E(i1,n1) 17: lowerbound min(transtime,exectime) 18: if lowerbound  optgroups.time then 19: continue 20:  recursively nd rest groups 21: firstgroup Group(x,i) 22: restgroups FindOptGrouping( 23: B firstgroup,i1) 24: curgroups firstgrouprestgroups 25: if curgroups.time  optgroups.time then 26: optgroups curgroups 27: return optgroups good strategy (e.g., group every ten layers)."
  ],
  "431": [
    "Given n layers, there are 2n1 different grouping strategies, so the time com- plexity of Algorithm 1 is O(2n), as in the worst case it needs to enumerate all strategies."
  ],
  "64": [
    "The two pruning techniques are able to prune most of the strategies, and can quickly nd the optimal one as we will show in 6.",
    "The algorithm uses two pruning techniques."
  ],
  "454": [
    "We have the following theorem for the algorithm."
  ],
  "314": [
    "Theorem 1."
  ],
  "198": [
    "Algorithm 1 nds the optimal grouping strategy that minimizes the total time for the pipeline."
  ],
  "280": [
    "Proof."
  ],
  "494": [
    "Let m  n x, which is the num- ber of layers the function considers."
  ],
  "221": [
    "We use induction on m to show that FindOptGrouping(B,x) outputs the optimal grouping strategy from layer x to n 1 given that previous layers have formed groups represented by B.",
    "Assume that for some k 1 and any m k, FindOptGrouping(B,x) outputs the optimal strategy.",
    "For case i where 0 i k 1, because FindOptGrouping(B  Group(x,x  i),x  i  1) only considers k i k layers, it outputs the optimal grouping strategy for case i based on the assumption."
  ],
  "368": [
    "Base case."
  ],
  "222": [
    "When m  1, the function only examines one layer."
  ],
  "412": [
    "Because there is only one strategy which is layer x itself is one group, this strategy is the optimal strategy."
  ],
  "375": [
    "Inductive step."
  ],
  "374": [
    "Con- sider m  k1, i.e., the algorithm now considers k1 layers."
  ],
  "296": [
    "The optimal strategy for this case is one group."
  ],
  "481": [
    "Because these cases are exclusive and cover the entire search space, by choosing the optimal grouping strategy from these cases, the algorithm outputs the optimal grouping strat- egy for m  k 1."
  ],
  "292": [
    "The rst tech- nique prunes the cases if their lower bounds are no better than the current found optimal."
  ],
  "147": [
    "It is obvious that this tech- nique does not affect the optimality."
  ],
  "464": [
    "The second technique prunes the case if their rst groups are from layer x to j  j."
  ],
  "350": [
    "Because these cases cannot advance the computation to an earlier point than grouping from x to at least j, pruning these cases also do not affect the optimality."
  ],
  "388": [
    "Generality."
  ],
  "267": [
    "Algorithm 1 achieves optimality for a given list of layers."
  ],
  "170": [
    "This, however, does not require the models to be linear."
  ],
  "474": [
    "In general, the layers or operators in a DNN model can be connected as an arbitrary computation graph, instead of a simple chain."
  ],
  "319": [
    "Models like ResNet and Inception are techni- cally non-linear directed acyclic graph (DAGs)."
  ],
  "133": [
    "Yet, there is an execution order that the layersoperators in the DAG are issued to the GPU one by one."
  ],
  "135": [
    "Algorithm 1 does not have any special assumptions on the execution order."
  ],
  "308": [
    "It is only interested in nding out how to group the layers given the execution order (and corresponding data dependencies) to achieve high pipelining efciency and low pipelining over- head."
  ],
  "316": [
    "It even applies for graphs with loops, in which the order is based on the rst time an operator is executed."
  ],
  "266": [
    "The order does not affect correctness, because an operator can be exe- cuted only when it is transmitted to the GPU and the input is ready."
  ],
  "86": [
    "Thus, our pipelined model transmission is applicable to the general case.",
    "6.2 Pipelined Model Transmission To evaluate the effectiveness of pipelined model transmission, we keep all other components of PipeSwitch the same, and compare the following mechanisms discussed in 4.2.",
    "Figure 7: Effectiveness of pipelined model transmission."
  ],
  "174": [
    "4.3 Unied Memory Management Task execution in a GPU requires GPU memory."
  ],
  "495": [
    "A GPU has its own memory management system, and provides a malloc function (e.g., cudaMalloc for NVIDIA GPUs) sim- ilar to CPUs for memory allocation."
  ],
  "92": [
    "A naive solution for GPU memory manage- ment is that each task uses the native cudaMallocManaged function for GPU memory allocation, and delegates model transmission to CUDA unied memory.",
    "To share GPU memory between the controller and the workers, we add functions for allocating GPU memory, sharing the GPU USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 507 memory to workers through CUDA IPC API, and getting the shared GPU memory.",
    "Each worker uses cudaMalloc to allocate GPU memory, and transmits the model to GPU by its own.",
    "Each worker allocates GPU mem- ory with cudaMallocManaged, and CUDA automatically transmits the model to GPU when needed."
  ],
  "430": [
    "This solution incurs high overhead for DL applications because of two reasons."
  ],
  "28": [
    "Second, the native cudaMalloc function and CUDA unied memory are designed for general-purpose applications, and may incur unnecessary overhead for DL applications.",
    "Finally, CUDA unied memory is not optimized for DL applications, and in- troduces more than one hundred milliseconds overhead than PipeSwitch."
  ],
  "63": [
    "We exploit two characteristics of DL applications to mini- mize GPU memory management overhead.",
    "The general-purpose GPU memory management does not consider these characteristics, and is too heavy-weight for DL applications that require fast task switching."
  ],
  "52": [
    "First, the amount of memory allocated to the DNN model is xed, and does not change during task execution.",
    "While a training task updates the model, it only updates the model parameters (i.e., the weights of the neural network), not the DNN structure, and the amount of memory needed to store them stays the same."
  ],
  "404": [
    "An USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 505 inference task only uses the model for inference, and does not change the model itself."
  ],
  "335": [
    "Second, the intermediate results change in a simple, regular pattern, which do not cause memory fragmentation."
  ],
  "324": [
    "For an inference task, the intermediate results are the outputs of each layer, which are used by the next layer."
  ],
  "411": [
    "After the next layer is computed, they are no longer needed and can be safely freed."
  ],
  "423": [
    "A training task differs in that the intermediate results gener- ated in the forward pass cannot be immediately freed, because they are also used by the backward pass to update the weights."
  ],
  "459": [
    "However, the backward pass consumes the intermediate re- sults in the reverse order as that the forward pass generates them, i.e., the intermediate results are rst-in-last-out."
  ],
  "73": [
    "The memory allocation and release can be handled by a simple stack-like mechanism, without causing memory fragmenta- tion."
  ],
  "110": [
    "Minimize memory allocation overhead.",
    "Minimize memory footprint and avoid extra memory copies."
  ],
  "429": [
    "Based on these two characteristics, we design a memory management mech- anism tailored for DL applications."
  ],
  "13": [
    "PipeSwitch uses a ded- icated memory daemon to manage the GPU memory.",
    "To compare, PipeSwitch simply sends an 64-bit integer offset for the shared GPU memory to workers.",
    "First, compared to no unied memory management, PipeSwitch saves 223 ms by eliminating the memory allocation over- head with the memory daemon."
  ],
  "231": [
    "This eliminates the overhead for each worker to use cudaMalloc to get a large amount of memory to store their models and intermediate results."
  ],
  "332": [
    "The memory daemon only needs to pass memory pointers to the workers, which is light-weight."
  ],
  "442": [
    "The daemon ensures that each time only one worker owns the GPU mem- ory to guarantee memory isolation between workers."
  ],
  "283": [
    "Each worker uses a memory pool to allocate the memory to store its model and intermediate results, and recycles the memory to the pool after the intermediate results are no longer needed."
  ],
  "60": [
    "The memory management of PipeSwitch extends that of Py- Torch.",
    "PipeSwitch inserts GPU memory blocks to PyTorch GPU memory pool, and PyTorch creates tensors on them."
  ],
  "478": [
    "It is designed and optimized for efcient GPU memory allocation between different tasks, while the memory man- agement in PyTorch handles memory allocation for a task itself."
  ],
  "419": [
    "Replicating the models in each worker incurs high memory footprint, and reduces the number of models a server can store, and consequently the types of tasks the server can execute."
  ],
  "357": [
    "On the other hand, storing the models in a dedicate process has minimal memory footprint as each model is only stored once, but it incurs an extra memory copy from this process to a worker to start a task, which hurts the task switching time."
  ],
  "424": [
    "PipeSwitch stores the models in the memory daemon so that the server only needs to keep one copy of each model in the host memory."
  ],
  "39": [
    "Minimize IPC overhead.",
    "We leverage a property of DL applications to minimize the IPC overhead."
  ],
  "318": [
    "This can be implemented by IPC APIs provided by GPUs, e.g., cudaIpcOpenMemHandle for NVIDIA GPUs."
  ],
  "376": [
    "We have measured the performance of these IPC APIs and found that they incur high overhead (6)."
  ],
  "483": [
    "The overhead is exacerbated by the pipeline because the pipeline needs to invoke the IPCs frequently to synchronize model transmission and task exe- cution for every pipeline group, instead of invoking the IPC only once for the entire model transmission."
  ],
  "211": [
    "The property is that the memory allocation process for a neural network model is deterministic."
  ],
  "220": [
    "Specif- ically, given the same GPU memory region and the same model, as long as the memory daemon and the worker uses the same order to allocate memory for the model parameters, the memory pointers for the parameters would be the same."
  ],
  "193": [
    "It is easy to keep the same order for the memory daemon and the worker because the neural network model is known and given, and the memory daemon only needs to use the same order to transmit the model as the worker would."
  ],
  "16": [
    "As a result, the memory daemon can minimize the usage of expensive GPU IPCs.",
    "Without IPC opti- mization, the latency is even higher than no unied memory management."
  ],
  "204": [
    "It only uses the GPU IPC once to initialize the worker, and then uses cheap CPU IPCs to notify the worker which pipeline group has been transmitted."
  ],
  "203": [
    "Pin memory.",
    "No pin memory."
  ],
  "258": [
    "The OS would swap a memory page to disk if the page is inactive for a certain amount of time."
  ],
  "250": [
    "GPUs require a page in the host memory to be pinned (or page- locked) in order to transmit the data in the page to the GPU memory."
  ],
  "476": [
    "Otherwise, a temporary pinned page is created for the transmission."
  ],
  "236": [
    "We pin the pages of the memory daemon to the host memory, to eliminate this overhead."
  ],
  "42": [
    "Process-level isolation is desirable because it ensures that one task cannot read the memory of another task, and that the crashing of one task, e.g., because of a bug, does not affect other tasks or the entire system.",
    "Similar to the naive solution, we use separate processes to achieve process-level isolation."
  ],
  "338": [
    "A naive solution is to use separate processes and start the new task after the current task is stopped."
  ],
  "326": [
    "in Table 1, such sequential execution incurs long delay due to old task cleaning and new task initialization."
  ],
  "21": [
    "Another possible solution is to let the current and new tasks share the same process with a warm CUDA context, so that the new task can reuse the GPU environment of the current task.",
    "The process of the old task cleans the GPU environment, and then another process is created and ini- tialized for the new task.",
    "The process cleans the GPU environment for the old task, and reuses the environment for the new task."
  ],
  "373": [
    "This avoids the new task initialization, but it still has the overhead for the current task to clean its status."
  ],
  "233": [
    "In addition, it does not provide process-level isolation between tasks."
  ],
  "394": [
    "Each worker is a separate process, and initializes its own GPU environment (i.e., CUDA context) when it is rst created."
  ],
  "380": [
    "When a current task is stopped, a major job is to clear asynchronous CUDA functions queued on the GPU."
  ],
  "466": [
    "We in- sert synchronization points into training tasks, so the number of queued functions are limited and can be quickly cleared."
  ],
  "243": [
    "Synchronization points are not needed for inference tasks as they are short and not preempted."
  ],
  "497": [
    "Another job is to free its GPU memory."
  ],
  "9": [
    "An important property of the cleaning proce- dure is that it does not modify the content of the memory, but only cleans the metadata, i.e., GPU memory pointers.",
    "As the GPU memory is managed by PipeSwitch, the cleaning pro- cedure deletes the pointers pointing to the tensor data rather than freeing the actual data."
  ],
  "500": [
    "Therefore, it is safe for the new task to transmit its model to the GPU memory at the same time."
  ],
  "305": [
    "In other words, we can parallelize the task cleaning of the current task and the pipelined model transmission of the new task, to hide the task cleaning overhead."
  ],
  "66": [
    "This choice is optimized for performance, and is not a problem for a trusted environment."
  ],
  "223": [
    "It is possible that a latter process can read the memory data of a previous process."
  ],
  "145": [
    "If this is a concern, an additional zero-out operation can be added."
  ],
  "392": [
    "GPU has high memory bandwidth (e.g., 900GBs for V100)."
  ],
  "187": [
    "It would incur sub-millisecond overhead for zeroing-out most models like ResNet-152 (around 240MB)."
  ],
  "499": [
    "On the other hand, for a trusted environment, it is unnecessary to release all allocated memory for the preempted process if the new process does not require entire GPU memory, and this could be achieved by some sim- ple coordination."
  ],
  "246": [
    "Table 2 summarizes the differences between these three solutions."
  ],
  "50": [
    "In summary, to switch workers, the controller signals the current active worker to stop, deletes the GPU memory allo- cated to it, and allocates the GPU memory to the new active worker.",
    "If a new model should be loaded to the GPU, the controller will notify the current active worker to stop, and transfers the parameters of the new model to the GPU after receiving the current ac- tive workers reply."
  ],
  "302": [
    "The controller ensures only one active worker to guar- antee exclusive occupation of the GPU."
  ],
  "15": [
    "There is a trade-off between the number of standby work- ers and their GPU memory consumption.",
    "However, every standby worker needs to maintain its own CUDA context, which consumes a few hundred MB GPU memory."
  ],
  "51": [
    "On the other hand, it is possible to have many standby workers so that there is always at least one idle standby worker.",
    "Our expe- rience is that two standby workers are sufcient to ensure at least one idle worker, which eliminates the waiting time and has moderate GPU memory consumption."
  ],
  "232": [
    "A transaction here means a model is switched in or out on all of its GPUs to enable or disable inference on this model."
  ],
  "252": [
    "However, it does not work out of the box with synchronous multi-GPU training."
  ],
  "251": [
    "We have analyzed a production GPU training trace from Microsoft 19,20."
  ],
  "178": [
    "Among 111,883 tasks in this trace, 96,662 tasks (or 86 of all the tasks) are single- GPU training tasks."
  ],
  "287": [
    "However, these jobs only account for 18 of total GPU hours and we expect the share of multi-GPU jobs to increase in the future."
  ],
  "256": [
    "Unfortunately, current training frameworks do not have mature support of elastic training."
  ],
  "102": [
    "This remains an active research topic and is orthogonal to PipeSwitch.",
    "PipeSwitch.",
    "PipeSwitch performs the best and is close to the lower bound.",
    "PipeSwitch.",
    "PipeSwitch.",
    "These solutions are complementary to PipeSwitch."
  ],
  "75": [
    "PyTorch Plugins.",
    "https:pytorch.org."
  ],
  "29": [
    "We add C and Python functions to the GPU memory management module of PyTorch.",
    "We also add functions which insert the received GPU memory into PyTorch GPU memory pool for a specic CUDA stream or clear the GPU memory from the pool.",
    "Note that the shared GPU memory can be inserted into the PyTorch GPU memory pool for multiple times for different CUDA streams, and the controller guarantees that only one of these CUDA streams is active."
  ],
  "214": [
    "The controller process consists of a TCP thread and a scheduler thread."
  ],
  "144": [
    "For better performance, the scheduler and the memory daemon are im- plemented together."
  ],
  "304": [
    "The TCP thread accepts task through TCP from clients, and sends the task to the scheduler thread."
  ],
  "492": [
    "The scheduler thread allocates and shares the GPU memory with workers, activates or deactivates workers, sends the task to a worker, and transfers parameters for the corresponding model to the GPU memory."
  ],
  "422": [
    "Before starting a task, the user should register the model in the scheduler to notify the controller to load the model from the disk to the CPU memory."
  ],
  "372": [
    "Parameters are transmitted to the GPU memory in groups in a pipeline."
  ],
  "196": [
    "After each group is trans- ferred, the controller noties the worker to start computing the corresponding layers."
  ],
  "288": [
    "The worker process consists of two threads."
  ],
  "265": [
    "The ter- mination thread waits for the termination signal from the con- troller, and noties the main thread."
  ],
  "406": [
    "The main thread manages the DNN models and performs the computation for inference or training."
  ],
  "158": [
    "Similar to the controller, the worker also requires the user to register the model before starting a task, so the worker can load the models and add the hooks to wait for parameter transmission or terminate on notication."
  ],
  "408": [
    "Note that the worker only loads the model structures, which is small, not the model parameters."
  ],
  "420": [
    "The parameters are only stored once in the memory daemon for minimal memory footprint."
  ],
  "37": [
    "When the models are loaded, they are attached to different CUDA streams, and their parameters are assigned to locations in the shared GPU memory.",
    "Different models might use the same GPU memory location, but the value is not valid until the controller transfers the corresponding parameters to these locations."
  ],
  "370": [
    "After loading the models, the worker waits for the scheduler to transfer required parameters for DNN models, and performs inference or training."
  ],
  "299": [
    "Setup."
  ],
  "153": [
    "All experiments are conducted on AWS."
  ],
  "215": [
    "We use two EC2 instance types."
  ],
  "206": [
    "One is p3.2xlarge, which is congured with 8 vCPUs (Intel Xeon E5-2686 v4), 1 GPU (NVIDIA V100 with 16 GB GPU memory), PCIe 3.0 16, and 61 GB memory.",
    "The other is g4dn.2xlarge, which is congured with 8 vCPUs (Intel Platinum 8259CL), 1 GPU (NVIDIA T4 with 16 GB GPU memory), PCIe 3.0 8, and 32 GB memory."
  ],
  "482": [
    "The software environment includes PyTorch-1.3.0, torchvision- 0.4.2, scipy-1.3.2, and CUDA-10.1."
  ],
  "248": [
    "We use PyTorch with our plugins for all mechanisms in comparison for consistency, which provides better results for stop-and-start than native PyTorch from Python-PyPI used in Table 1."
  ],
  "149": [
    "Workloads."
  ],
  "496": [
    "The models include ResNet152 17, Incep- tionv3 22 and Bertbase 23, which are standard bench- marks for evaluating DL systems."
  ],
  "277": [
    "We use representative con- gurations for each model."
  ],
  "273": [
    "The experiments cover both train- ing and inference."
  ],
  "200": [
    "We use single-GPU inference and training tasks as discussed in 4.5."
  ],
  "218": [
    "Training tasks periodically check- point their models to the host memory, and restart from the latest checkpoint after preemption."
  ],
  "328": [
    "The checkpointing fre- quency of training tasks is set according to the scheduling cycle to minimize checkpointing overhead."
  ],
  "421": [
    "The default batch size for training is 32, and that for inference is 8."
  ],
  "247": [
    "Metrics."
  ],
  "35": [
    "We use throughput and latency as evaluation metrics.",
    "We measure the the end-to-end latency experienced by the client.",
    "Figure 5: Total latency experienced by the client for different mechanisms.",
    "Figure 5 shows the latency experienced by the client, and Table 3 shows the total overhead."
  ],
  "143": [
    "Each number is reported with the average of 100 runs."
  ],
  "460": [
    "For Figure 6(b), we additionally report the minimum and maxi- mum latencies using the error bar, because the latency of the rst batch and those of later batches in one scheduling cycle can differ signicantly due to switching overhead."
  ],
  "189": [
    "6.1 End-to-End Experiments Minimizing end-to-end overhead."
  ],
  "472": [
    "In this experiment, a client sends an inference task to a GPU server, and the GPU server preempts the training task to execute the inference task and sends a reply back to the client."
  ],
  "485": [
    "We compare the following mechanisms."
  ],
  "146": [
    "Ready model."
  ],
  "155": [
    "There is no training task."
  ],
  "427": [
    "The process with the required model is already loaded in the GPU."
  ],
  "285": [
    "This solu- tion provides the lower bound, which is the lowest latency we can achieve for an inference task."
  ],
  "159": [
    "Stop-and-start."
  ],
  "262": [
    "This solution is used by ex- isting systems like Gandiva 24 for task switching, which reported similar second-scale overhead."
  ],
  "382": [
    "NVIDIA MPS."
  ],
  "339": [
    "We initialize separate pro- cesses in advance."
  ],
  "48": [
    "CUDA unied memory is used for memory swapping.",
    "CUDA unied memory.",
    "4 CUDA Unied Memory.",
    "https: devblogs.nvidia.comunified-memory-cuda- beginners."
  ],
  "142": [
    "This is the proposed system."
  ],
  "152": [
    "The properties are described in 4."
  ],
  "3": [
    "508 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association 7500 10000 Latency (ms) Ready model PipeSwitch MPS Stop-and-start ResNet152 Inceptionv3 Bertbase 0 200 400 (a) p3.2xlarge (NVIDIA V100, PCIe 3.0 16).",
    "5000 10000 Latency (ms) Ready model PipeSwitch MPS Stop-and-start ResNet152 Inceptionv3 Bertbase 0 200 400 600 (b) g4dn.2xlarge (NVIDIA T4, PCIe 3.0 8).",
    "p3.2xlarge (NVIDIA V100, PCIe 3.0 16) g4dn.2xlarge (NVIDIA T4, PCIe 3.0 8) ResNet152 Inceptionv3 Bertbase ResNet152 Inceptionv3 Bertbase Stop-and-start 6475.40 ms 7536.07 ms 6371.32 ms 5486.74 ms 6558.76 ms 5355.95 ms NVIDIA MPS 307.02 ms 232.25 ms 204.52 ms 259.20 ms 193.05 ms 338.25 ms PipeSwitch 6.01 ms 5.40 ms 10.27 ms 5.57 ms 7.66 ms 34.56 ms Table 3: Total overhead, i.e., the difference on total latency between different mechanisms and ready model.",
    "ResNet152 Inceptionv3 Bertbase 0 20 40 60 80 100 Latency (ms) PipeSwitch Per-layer pipeline Grouped transmission No optimization (a) p3.2xlarge (NVIDIA V100, PCIe 3.0 16).",
    "ResNet152 Inceptionv3 Bertbase 0 50 100 150 200 250 Latency (ms) PipeSwitch Per-layer pipeline Grouped transmission No optimization (b) g4dn.2xlarge (NVIDIA T4, PCIe 3.0 8).",
    "510 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association ResNet152 Inceptionv3 Bertbase 0 100 200 300 400 Latency (ms) PipeSwitch No memory management No IPC optimization No pin memory CUDA unified memory (a) p3.2xlarge (NVIDIA V100, PCIe 3.0 16).",
    "ResNet152 Inceptionv3 Bertbase 0 100 200 300 400 Latency (ms) PipeSwitch No memory management No IPC optimization No pin memory CUDA unified memory (b) g4dn.2xlarge (NVIDIA T4, PCIe 3.0 8).",
    "6000 8000 Latency (ms) PipeSwitch One process Two processes ResNet152 Inceptionv3 Bertbase 0 100 200 (a) p3.2xlarge (NVIDIA V100, PCIe 3.0 16).",
    "5000 7500 Latency (ms) PipeSwitch One process Two processes ResNet152 Inceptionv3 Bertbase 0 200 400 (b) g4dn.2xlarge (NVIDIA T4, PCIe 3.0 8)."
  ],
  "94": [
    "ResNet152 Inceptionv3 Bertbase p3.2xlarge 3.62 ms 4.82 ms 3.62 ms g4dn.2xlarge 2.53 ms 5.49 ms 6.57 ms Table 4: The startup overhead for PipeSwitch to start comput- ing the rst layer.",
    "We also show the task startup overhead for PipeSwitch in Table 4, which is the difference between the time for ResNet152 Inceptionv3 Bertbase  of Layers 464 189 139 Algorithm 1 1.33 s 0.18 s 0.34 s Only Pruning 1 2.09 s 0.30 s 0.88 s Only Pruning 2 3.44 h 5.07 s  24 h No Pruning  24 h  24 h  24 h Table 5: Effectiveness of two pruning techniques."
  ],
  "452": [
    "Salus 7 is not directly comparable because it requires the models to be preloaded to the GPU, and has several limitations described in 2.2."
  ],
  "150": [
    "Its performance is similar to the ready model when the model is preloaded, and is similar to NVIDIA MPS when the model is in the host memory."
  ],
  "162": [
    "The total overhead is the difference between the latency of a mechanism and that of the ready model."
  ],
  "417": [
    "It is obvious that stop-and-start performs the worst, which takes several seconds."
  ],
  "185": [
    "The main source of the overhead is CUDA context initialization and rst-time library loading operations in PyTorch."
  ],
  "163": [
    "Another source is GPU memory swapping."
  ],
  "194": [
    "The overhead of PipeSwitch for most congurations is up to 10ms, except for BERT on T4, which is due to the large model size and the smaller PCIe bandwidth on T4 than that on V100."
  ],
  "383": [
    "Since it also takes longer (120ms) to compute BERT on T4 even with the ready model, the relative overhead is acceptable."
  ],
  "98": [
    "PipeSwitch to start computing the rst layer and that for the ready model to start computing."
  ],
  "240": [
    "The startup overhead of PipeSwitch is only a few milliseconds.",
    "PipeSwitch incurs only a few milliseconds overhead for task switching, and achieves low latency close to the lower bound."
  ],
  "115": [
    "In this experi- ment, we compare throughput and end-to-end latency of dif- ferent mechanisms under different scheduling cycles."
  ],
  "190": [
    "We use ResNet152 for both training and inference on eight p3.2xlarge instances, and switch between these two tasks after each scheduling cycle."
  ],
  "480": [
    "Figure 6(a) shows the inference throughput."
  ],
  "78": [
    "The dashed line is the upper bound, which is the throughput of the ready model assuming no task switching.",
    "The dashed line is the lower bound, which is the average latency of the ready model assuming no task switching."
  ],
  "132": [
    "The throughput of stop-and-start is nearly zero for scheduling cycles smaller than 10 s, because it takes several seconds for task switching."
  ],
  "141": [
    "MPS keeps poor throughput around 100 batches per second."
  ],
  "165": [
    "We dene GPU utilization as the ratio to the upper bound."
  ],
  "366": [
    "Figure 6(b) shows the average latency of the inference tasks."
  ],
  "209": [
    "The error bar indicates the minimum and maximum latency."
  ],
  "219": [
    "Stop- and-start has poor latency because the rst batch has several seconds overhead."
  ],
  "416": [
    "MPS has about 80 ms average latency, and has several hundred milliseconds latency for the rst batch."
  ],
  "239": [
    "7500 10000 Latency (ms) PipeSwitch MPS Stop-and-start 1s 2s 5s 10s 30s 0 200 400 Lower bound (b) Latency."
  ],
  "310": [
    "Figure 6: Throughput and latency under different scheduling cycles for ResNet on p3.2xlarge."
  ],
  "384": [
    "No optimization.",
    "No optimization performs the worst in most cases."
  ],
  "249": [
    "It transmits the model layer by layer (with many PCIe calls), and then executes the task."
  ],
  "245": [
    "It groups the entire model in one transmission, and then executes the task."
  ],
  "122": [
    "Per-layer pipeline."
  ],
  "275": [
    "It transits model parameters layer by layer."
  ],
  "151": [
    "Computation starts, once parameters are transmitted."
  ],
  "22": [
    "Figure 7 shows the total time measured by the client for an in- ference task to preempt a training task and nish its inference.",
    "Figure 8 shows the total time measured by the client."
  ],
  "264": [
    "Grouped transmission improves no optimization by combining the lay- ers of the model into one big tensor and transmitting it in one group."
  ],
  "278": [
    "Per-layer pipeline overlaps transmission and com- putation at the granularity of layer."
  ],
  "362": [
    "But because it has PCIe overhead and synchronization overhead for every layer, for the models with many layers but relatively light computa- tion such as ResNet152 and Inception, it can perform worse than grouped transmission and sometimes even no pipeline."
  ],
  "56": [
    "It reduces the total time by up to 38.2 ms compared to other solutions.",
    "It reduces the latency by 116307 ms compared to one process, and 57 s compared to two processes."
  ],
  "213": [
    "Note that this reduction is signicant, especially consider- ing that it is evaluated when the optimizations on memory management and worker switching have already been applied."
  ],
  "456": [
    "We would like to emphasize that to meet strict SLOs, it is important to reduce all overheads for task switching, not only the most signicant one."
  ],
  "210": [
    "Table 5 shows the running time of Algorithm 1, as well as the effects of the two pruning techniques mentioned in  4.2."
  ],
  "175": [
    "Note that the number of layers includes both weighted and unweighted layers, as both contribute to the computation time."
  ],
  "123": [
    "We measure the parameter size and running time for each layer in advance."
  ],
  "461": [
    "Algorithm 1 takes only several seconds to compute an optimal grouping strategy, even for ResNet152 which has hundreds of layers."
  ],
  "184": [
    "On the contrary, no pruning does not nish for all three models after running for 24 hours."
  ],
  "90": [
    "6.3 Unied Memory Management To evaluate the effectiveness of unied memory management.",
    "No unied memory management.",
    "Figure 8: Effectiveness of unied memory management."
  ],
  "225": [
    "we keep all other components of PipeSwitch the same, and compare the following ve mechanisms discussed in 4.3."
  ],
  "450": [
    "No IPC optimization."
  ],
  "26": [
    "It has all optimizations on unied memory management except that the pages of the memory daemon are not pinned to the main memory.",
    "Overall, this experiment demonstrates that all the optimizations on memory management are effective."
  ],
  "71": [
    "It is the unied memory management mecha- nism used by PipeSwitch."
  ],
  "96": [
    "Second, IPC optimization is important, which reduces the latency by 1648 ms."
  ],
  "322": [
    "Third, pinning the pages to the host memory can reduce the latency with a few milliseconds."
  ],
  "69": [
    "Two processes.",
    "One process."
  ],
  "160": [
    "Figure 9: Effectiveness of active-standby switching."
  ],
  "282": [
    "Figure 9 shows the results."
  ],
  "235": [
    "Two processes perform the worst as it stops the training task and initializes a new process for the new task."
  ],
  "82": [
    "The new process needs to create a new CUDA environment, which dominates the total time.",
    "One process reuses the CUDA environment, but still pays the overhead to clean the environment."
  ],
  "79": [
    "7 Related Work Many frameworks have been developed for deep learning, such as TensorFlow 25, PyTorch 21 and MXNet 26."
  ],
  "336": [
    "Sev- eral algorithms and systems have been designed for executing and scheduling deep learning tasks on clusters, including both training and inference tasks 3, 10, 24, 2732."
  ],
  "426": [
    "These scheduling solutions are orthogonal and complementary to PipeSwitch."
  ],
  "95": [
    "They focus on what scheduling decisions to make, while PipeSwitch focuses on how to realize a schedul- ing decision.",
    "Importantly, PipeSwitch enables the scheduler to change the resource allocation more often with millisecond- scale task switching."
  ],
  "107": [
    "Many techniques and systems have been proposed to optimize communication and improve dis- tributed training 8,9,15,3342."
  ],
  "207": [
    "The most relevant ones are PipeDream 8, ByteScheduler 9 and Poseidon 40."
  ],
  "486": [
    "Other works like vDNN 43 and SwapAdvisor 44 also have GPU memory management module, but they focus on memory management for a single training task of large models, which are not directly comparable to PipeSwitch."
  ],
  "477": [
    "Cluster managers 4548 typically allocate GPUs to VMs or containers at device granularity."
  ],
  "183": [
    "Several solutions have been proposed to share a GPU at application granularity us- ing techniques like library interception 6,4953."
  ],
  "127": [
    "They are general-purpose and focus on sharing only a few kernels."
  ],
  "340": [
    "As such, they are not suitable for deep learning applications that typically require hundreds of kernels."
  ],
  "364": [
    "It is also not specially designed for deep learning and thus cannot meet strict SLOs of inference tasks as shown in 6."
  ],
  "182": [
    "There are many efforts on GPU optimization to im- prove the performance of running a single task, such as tensor fusion and kernel-level concurrency and scheduling 5458."
  ],
  "276": [
    "Acknowledgments."
  ],
  "449": [
    "We thank our shepherd Madan Musu- vathi and the anonymous reviewers for their valuable feed- back."
  ],
  "70": [
    "Zhihao Bai, Zhen Zhang and Xin Jin were supported in part by an AWS Machine Learning Research Award."
  ],
  "309": [
    "References 1 A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes, Large-scale cluster management at Google with Borg, in EuroSys, 2015."
  ],
  "53": [
    "2 J.",
    "J."
  ],
  "237": [
    "Dean and L. A. Barroso, The tail at scale, Commu- nications of the ACM, vol."
  ],
  "77": [
    "56, 2013.",
    "41, 2013."
  ],
  "104": [
    "3 H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Phili- pose, A. Krishnamurthy, and R. Sundaram, Nexus: A GPU cluster engine for accelerating DNN-based video analysis, in ACM SOSP, 2019."
  ],
  "169": [
    "5 A. Ousterhout, J."
  ],
  "179": [
    "Fried, J. Behrens, A. Belay, and H. Bal- akrishnan, Shenango: Achieving high CPU efciency for latency-sensitive datacenter workloads, in USENIX NSDI, 2019."
  ],
  "224": [
    "6 CUDA Multi-Process Service."
  ],
  "378": [
    "https: docs.nvidia.comdeploypdf CUDAMultiProcessServiceOverview.pdf."
  ],
  "385": [
    "7 P. Yu and M. Chowdhury, Salus: Fine-grained GPU sharing primitives for deep learning applications, in Conference on Machine Learning and Systems, 2020."
  ],
  "241": [
    "8 D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Za- haria, PipeDream: generalized pipeline parallelism for DNN training, in ACM SOSP, 2019."
  ],
  "138": [
    "9 Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo, A generic communication scheduler for distributed DNN training acceleration, in ACM SOSP, 2019."
  ],
  "168": [
    "10 J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo, Tiresias: A GPU clus- ter manager for distributed deep learning, in USENIX NSDI, 2019.",
    "40 H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie, and E. P. Xing, Poseidon: An ef- cient communication architecture for distributed deep learning on GPU clusters, in USENIX ATC, 2017."
  ],
  "84": [
    "12 Amazon Web Services."
  ],
  "136": [
    "https:aws.amazon.com."
  ],
  "234": [
    "13 Microsoft Azure."
  ],
  "157": [
    "https:azure.microsoft.com."
  ],
  "414": [
    "14 Google Cloud Platform."
  ],
  "130": [
    "https: cloud.google.com."
  ],
  "72": [
    "15 A. Sergeev and M. Del Balso, Horovod: fast and easy distributed deep learning in tensorow, arXiv preprint arXiv:1802.05799, 2018."
  ],
  "99": [
    "16 M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J."
  ],
  "201": [
    "Long, E. J. Shekita, and B.-Y."
  ],
  "188": [
    "Su, Scaling distributed machine learning with the parameter server, in USENIX OSDI, 2014."
  ],
  "226": [
    "17 K. He, X. Zhang, S. Ren, and J."
  ],
  "116": [
    "Sun, Deep residual learning for image recognition, in IEEE Conference on Computer Vision and Pattern Recognition, 2016."
  ],
  "57": [
    "18 Nvidia data center deep learning product perfor- mance."
  ],
  "458": [
    "https:developer.nvidia.comdeep- learning-performance-training-inference."
  ],
  "354": [
    "20 Philly traces."
  ],
  "180": [
    "https:github.commsr-fiddle philly-traces."
  ],
  "346": [
    "21 PyTorch."
  ],
  "111": [
    "22 C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wo- jna, Rethinking the inception architecture for computer vision, in IEEE Conference on Computer Vision and Pattern Recognition, 2016."
  ],
  "242": [
    "23 J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of the 2019 Conference of the North American Chapter of the Associ- ation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019."
  ],
  "93": [
    "24 W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P. Patel, X. Peng, H. Zhao, Q. Zhang, et al., Gandiva: Introspective cluster scheduling for deep learning, in USENIX OSDI, 2018."
  ],
  "191": [
    "25 TensorFlow.",
    "54 TensorFlow XLA."
  ],
  "109": [
    "https:www.tensorflow.org.",
    "https:www.tensorflow.org xla."
  ],
  "176": [
    "26 MXNet."
  ],
  "347": [
    "https:mxnet.apache.org."
  ],
  "172": [
    "27 H. Zhang, L. Stafman, A."
  ],
  "253": [
    "Or, and M. J. Freedman, Slaq: quality-driven scheduling for distributed machine learn- ing, in ACM Symposium on Cloud Computing, 2017."
  ],
  "121": [
    "28 Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo, Opti- mus: an efcient dynamic resource scheduler for deep learning clusters, in EuroSys, 2018."
  ],
  "67": [
    "29 K. Mahajan, A. Balasubramanian, A. Singhvi, S. Venkataraman, A. Akella, A. Phanishayee, and S. Chawla, Themis: Fair and efcient GPU cluster scheduling, in USENIX NSDI, 2020."
  ],
  "192": [
    "30 R. Liaw, R. Bhardwaj, L. Dunlap, Y. Zou, J. E. Gonza- lez, I. Stoica, and A. Tumanov, HyperSched: Dynamic resource reallocation for model development on a dead- line, in ACM Symposium on Cloud Computing, 2019."
  ],
  "139": [
    "31 R. Dathathri, O. Saarikivi, H. Chen, K. Laine, K. Lauter, S. Maleki, M. Musuvathi, and T. Mytkowicz, CHET: An optimizing compiler for fully-homomorphic neural- network inferencing, in ACM Conference on Program- ming Language Design and Implementation, 2019."
  ],
  "74": [
    "32 T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, et al., TVM: An automated end-to-end optimizing compiler for deep learning, in USENIX OSDI, 2018."
  ],
  "91": [
    "33 Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al., Gpipe: Efcient training of giant neural networks using pipeline parallelism, in Advances in Neural Informa- tion Processing Systems, 2019."
  ],
  "208": [
    "34 G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica, Blink: Fast and generic col- lectives for distributed ML, in Conference on Machine Learning and Systems, 2020."
  ],
  "131": [
    "35 NVIDIA Collective Communications Library (NCCL)."
  ],
  "348": [
    "https:developer.nvidia.comnccl."
  ],
  "87": [
    "36 J. Liu, J. Wu, and D. K. Panda, High performance RDMA-based MPI implementation over inniband, Int."
  ],
  "154": [
    "Parallel Program., vol."
  ],
  "205": [
    "32, 2004."
  ],
  "167": [
    "37 Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gib- bons, G. A. Gibson, G. Ganger, and E. P. Xing, More effective distributed ML via a stale synchronous parallel parameter server, in Advances in Neural Information Processing Systems, 2013."
  ],
  "125": [
    "38 A."
  ],
  "212": [
    "A. Awan, C.-H. Chu, H. Subramoni, and D. K. Panda, Optimized broadcast for deep learning workloads on dense-GPU inniband clusters: MPI or NCCL?, in Pro- ceedings of the 25th European MPI Users Group Meet- ing, 2018."
  ],
  "105": [
    "39 J."
  ],
  "81": [
    "Daily, A. Vishnu, C. Siegel, T. Warfel, and V. Am- atya, GossipGraD: Scalable deep learning using gossip communication based asynchronous gradient descent, CoRR, vol."
  ],
  "181": [
    "abs1803.05880, 2018."
  ],
  "166": [
    "41 Z. Zhang, C. Chang, H. Lin, Y. Wang, R. Arora, and X. Jin, Is network the bottleneck of distributed train- ing?, in ACM SIGCOMM Workshop on Network Meets AI  ML (NetAI), August 2020."
  ],
  "161": [
    "42 Y. Chen, Z. Liu, B. Ren, and X. Jin, On efcient con- structions of checkpoints, in International Conference on Machine Learning (ICML), July 2020."
  ],
  "126": [
    "43 M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler, vDNN: Virtualized deep neural net- works for scalable, memory-efcient neural network de- sign, in 2016 49th Annual IEEEACM International Symposium on Microarchitecture (MICRO), 2016."
  ],
  "112": [
    "44 C.-C. Huang, G. Jin, and J. Li, SwapAdvisor: Pushing deep learning beyond the GPU memory limit via smart swapping, in ACM ASPLOS, 2020."
  ],
  "177": [
    "https:kubernetes.io."
  ],
  "14": [
    "46 NVIDIA Container Runtime for Docker.",
    "https: github.comNVIDIAnvidia-docker."
  ],
  "217": [
    "47 B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H. Katz, S. Shenker, and I. Stoica, Mesos: A platform for ne-grained resource sharing in the data center., in USENIX NSDI, 2011."
  ],
  "106": [
    "48 V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al., Apache Hadoop YARN: Yet another resource negotiator, in ACM Symposium on Cloud Computing, 2013."
  ],
  "27": [
    "49 G. Giunta, R. Montella, G. Agrillo, and G. Coviello, A GPGPU transparent virtualization component for high performance computing clouds, in European Confer- ence on Parallel Processing, 2010.",
    "52 V. T. Ravi, M. Becchi, G. Agrawal, and S. Chakradhar, Supporting GPU sharing in cloud environments with a transparent runtime consolidation framework, in Pro- ceedings of the 20th international symposium on High performance distributed computing, 2011."
  ],
  "10": [
    "50 V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan, GViM: GPU- accelerated virtual machines, in Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing, 2009.",
    "51 J. Duato, A. J. Pena, F. Silla, R. Mayo, and E. S. Quintana-Ort, rCUDA: Reducing the number of GPU- based accelerators in high performance clusters, in 2010 International Conference on High Performance Computing  Simulation, 2010.",
    "Sun, and K. Li, vCUDA: GPU- accelerated high-performance computing in virtual ma- chines, IEEE Transactions on Computers, vol."
  ],
  "140": [
    "53 L. Shi, H. Chen, J."
  ],
  "124": [
    "61, 2011."
  ],
  "171": [
    "55 T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang, MXNet: A exible and efcient machine learning library for heterogeneous distributed systems, arXiv preprint arXiv:1512.01274, 2015."
  ],
  "40": [
    "56 C. Gregg, J. Dorn, K. Hazelwood, and K. Skadron, Fine- grained resource sharing for concurrent GPGPU kernels, in Presented as part of the 4th USENIX Workshop on Hot Topics in Parallelism, 2012.",
    "57 S. Pai, M. J. Thazhuthaveetil, and R. Govindarajan, Im- proving GPGPU concurrency with elastic kernels, ACM SIGARCH Computer Architecture News, vol."
  ],
  "413": [
    "58 Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia, and A. Aiken, TASO: optimizing deep learning compu- tation with automatic generation of graph substitutions, in ACM SOSP, 2019."
  ]
}