[
  {
    "subject": "THE DOMINANT PRACTICE TODAY",
    "predicate": "IS TO PROVISION",
    "object": "DEDICATED GPU CLUSTERS FOR TRAINING AND INFERENCE SEPARATELY"
  },
  {
    "subject": "GPU CLUSTERS",
    "predicate": "ARE",
    "object": "SHARED GPU CLUSTERS"
  },
  {
    "subject": "TRAINING AND INFERENCE",
    "predicate": "USE",
    "object": "GPUS"
  },
  {
    "subject": "CURRENT PRACTICE",
    "predicate": "IS TO BUILD",
    "object": "DEDICATED CLUSTERS FOR TRAINING AND INFERENCE SEPARATELY"
  },
  {
    "subject": "WE",
    "predicate": "ENVISION TO BUILD",
    "object": "FINE-GRAINED TIME-SHARING GPU CLUSTERS"
  },
  {
    "subject": "GPU CLUSTERS",
    "predicate": "CAN BE SHARED ACROSS",
    "object": "DIFFERENT APPLICATIONS"
  },
  {
    "subject": "DIFFERENT APPLICATIONS",
    "predicate": "INCLUDING",
    "object": "TRAINING AND INFERENCE"
  },
  {
    "subject": "GPU CLUSTERS",
    "predicate": "ARE OFTEN OVER-PROVISIONED BASED ON",
    "object": "THE PEAK LOAD"
  },
  {
    "subject": "GPU CLUSTERS",
    "predicate": "HAVE",
    "object": "LIMITED SHARING BETWEEN APPLICATIONS AND TASK TYPES"
  },
  {
    "subject": "SERVICE-LEVEL OBJECTIVES (SLOS)",
    "predicate": "NEED TO BE MET",
    "object": "STRICT"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IS",
    "object": "A SYSTEM"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENABLES",
    "object": "UNUSED CYCLES OF AN INFERENCE APPLICATION TO BE FILLED BY TRAINING OR OTHER INFERENCE APPLICATIONS"
  },
  {
    "subject": "GPU UTILIZATION",
    "predicate": "CAN BE IMPROVED",
    "object": "SIGNIFICANTLY WITH PIPESWITCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IMPROVES",
    "object": "GPU UTILIZATION"
  },
  {
    "subject": "IMPROVEMENT",
    "predicate": "DOES NOT SACRIFICE",
    "object": "SLOS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IS FOCUSED ON",
    "object": "SINGLE-GPU TASKS FOR TRAINING AND INFERENCE"
  },
  {
    "subject": "MULTI-GPU INFERENCE TASKS",
    "predicate": "CAN BE SUPPORTED BY",
    "object": "PERFORMING PIPESWITCH ON EACH GPU WITH TRANSACTIONS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "SUPPORTS",
    "object": "SINGLE-GPU TRAINING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "SUPPORTS",
    "object": "ASYNCHRONOUS MULTI-GPU TRAINING"
  },
  {
    "subject": "ASYNCHRONOUS MULTI-GPU TRAINING",
    "predicate": "IS FOR",
    "object": "DATA PARALLEL STRATEGIES"
  },
  {
    "subject": "PREEMPTING ONE GPU",
    "predicate": "DOES NOT AFFECT",
    "object": "OTHER GPUS"
  },
  {
    "subject": "A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS",
    "predicate": "USE",
    "object": "A SINGLE GPU"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IS APPLICABLE TO",
    "object": "A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IS APPLICABLE TO",
    "object": "THEM OUT OF THE BOX"
  },
  {
    "subject": "ONE WAY TO SEAMLESSLY USE PIPESWITCH FOR SYNCHRONOUS MULTI-GPU TRAINING",
    "predicate": "IS TO USE",
    "object": "ELASTIC SYNCHRONOUS TRAINING"
  },
  {
    "subject": "ELASTIC SYNCHRONOUS TRAINING",
    "predicate": "ALLOWS",
    "object": "THE DYNAMIC CHANGING OF THE NUMBER OF GPUS USED FOR TRAINING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "HAS",
    "object": "HIGH THROUGHPUT CLOSE TO THE UPPER BOUND"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ACHIEVES",
    "object": "NEAR 100 GPU UTILIZATION"
  },
  {
    "subject": "WE",
    "predicate": "DEMONSTRATE",
    "object": "THE PERFORMANCE OF PIPESWITCH"
  },
  {
    "subject": "EXPERIMENTS",
    "predicate": "ARE ON",
    "object": "A VARIETY OF DNN MODELS AND GPU CARDS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "CAN INCREASE",
    "object": "GPU UTILIZATION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "CAN IMPROVE",
    "object": "THE AGILITY OF DL APPLICATIONS"
  },
  {
    "subject": "WE",
    "predicate": "ACHIEVE",
    "object": "SO BY INTRODUCING PIPELINED CONTEXT SWITCHING"
  },
  {
    "subject": "THE KEY IDEA",
    "predicate": "IS TO LEVERAGE",
    "object": "THE LAYERED STRUCTURE OF NEURAL NETWORK MODELS AND THEIR LAYER-BY-LAYER COMPUTATION PATTERN"
  },
  {
    "subject": "THE KEY IDEA",
    "predicate": "IS TO PIPELINE",
    "object": "MODEL TRANSMISSION OVER THE PCIE AND TASK EXECUTION IN THE GPU WITH MODEL-AWARE GROUPING"
  },
  {
    "subject": "WE",
    "predicate": "DESIGN",
    "object": "A PIPELINED MODEL TRANSMISSION MECHANISM"
  },
  {
    "subject": "PIPELINED MODEL TRANSMISSION MECHANISM",
    "predicate": "PIPELINES",
    "object": "MODEL TRANSMISSION OVER THE PCIE"
  },
  {
    "subject": "PIPELINED MODEL TRANSMISSION MECHANISM",
    "predicate": "PIPELINES",
    "object": "MODEL COMPUTATION IN THE GPU"
  },
  {
    "subject": "TRANSMITTING A TASK FROM CPU TO GPU",
    "predicate": "IS BOUNDED BY",
    "object": "PCIE BANDWIDTH"
  },
  {
    "subject": "WE",
    "predicate": "DESIGN",
    "object": "UNIFIED MEMORY MANAGEMENT"
  },
  {
    "subject": "WE",
    "predicate": "DESIGN",
    "object": "ACTIVE-STANDBY WORKER SWITCHING MECHANISMS"
  },
  {
    "subject": "ACTIVE-STANDBY WORKER SWITCHING MECHANISMS",
    "predicate": "ACCOMPANY",
    "object": "THE PIPELINING"
  },
  {
    "subject": "ACTIVE-STANDBY WORKER SWITCHING MECHANISMS",
    "predicate": "ENSURE",
    "object": "PROCESS-LEVEL ISOLATION"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "AN ACTIVE-STANDBY MECHANISM"
  },
  {
    "subject": "AN ACTIVE-STANDBY MECHANISM",
    "predicate": "IS USED FOR",
    "object": "FAST WORKER SWITCHING"
  },
  {
    "subject": "AN ACTIVE-STANDBY MECHANISM",
    "predicate": "IS USED FOR",
    "object": "PROCESS-LEVEL ISOLATION"
  },
  {
    "subject": "WE",
    "predicate": "HAVE BUILT",
    "object": "A PIPESWITCH PROTOTYPE"
  },
  {
    "subject": "WE",
    "predicate": "HAVE INTEGRATED",
    "object": "IT WITH PYTORCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "DOES NOT MODIFY",
    "object": "THE MODEL STRUCTURE"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ADDS",
    "object": "HOOKS FOR PYTORCH TO WAIT FOR TRANSMISSION OR SYNCHRONIZE THE EXECUTION"
  },
  {
    "subject": "SYSTEM PROTOTYPE FOR PIPESWITCH",
    "predicate": "HAS",
    "object": "3600 LINES OF CODE"
  },
  {
    "subject": "3600 LINES OF CODE",
    "predicate": "ARE IN",
    "object": "C AND PYTHON"
  },
  {
    "subject": "SYSTEM PROTOTYPE FOR PIPESWITCH",
    "predicate": "IS INTEGRATED WITH",
    "object": "PYTORCH 21"
  },
  {
    "subject": "DEEP LEARNING (DL)",
    "predicate": "POWERS",
    "object": "AN EMERGING FAMILY OF INTELLIGENT APPLICATIONS"
  },
  {
    "subject": "AN EMERGING FAMILY OF INTELLIGENT APPLICATIONS",
    "predicate": "EXISTS IN",
    "object": "MANY DOMAINS"
  },
  {
    "subject": "MANY DOMAINS",
    "predicate": "INCLUDE",
    "object": "RETAIL"
  },
  {
    "subject": "MANY DOMAINS",
    "predicate": "INCLUDE",
    "object": "TRANSPORTATION"
  },
  {
    "subject": "MANY DOMAINS",
    "predicate": "INCLUDE",
    "object": "FINANCE"
  },
  {
    "subject": "MANY DOMAINS",
    "predicate": "INCLUDE",
    "object": "HEALTHCARE"
  },
  {
    "subject": "GPUS",
    "predicate": "ARE",
    "object": "ONE OF THE MOST WIDELY-USED CLASSES OF ACCELERATORS FOR DL"
  },
  {
    "subject": "DL WORKLOADS",
    "predicate": "INCLUDE",
    "object": "THROUGHPUT-INTENSIVE TRAINING TASKS"
  },
  {
    "subject": "DL WORKLOADS",
    "predicate": "INCLUDE",
    "object": "LATENCY-SENSITIVE INFERENCE TASKS"
  },
  {
    "subject": "INFERENCE TASKS",
    "predicate": "CANNOT BE SERVED WITH",
    "object": "TRAINING CLUSTERS UNDER ASH CROWDS"
  },
  {
    "subject": "TRAINING TASKS",
    "predicate": "CANNOT UTILIZE",
    "object": "INFERENCE CLUSTERS WHEN THE INFERENCE LOAD IS LOW"
  },
  {
    "subject": "ASH CROWD",
    "predicate": "IS",
    "object": "WHEN AN APPLICATION SUDDENLY BECOMES POPULAR AND THE DEMAND GROWS BEYOND THE OPERATORS EXPECTATION"
  },
  {
    "subject": "TRAINING CLUSTER",
    "predicate": "CANNOT PREEMPT",
    "object": "TRAINING TASKS FOR INFERENCE TASKS"
  },
  {
    "subject": "INFERENCE CLUSTERS",
    "predicate": "ARE OFTEN OVER-PROVISIONED FOR",
    "object": "THE PEAK LOAD"
  },
  {
    "subject": "INFERENCE CLUSTERS",
    "predicate": "ARE OVER-PROVISIONED IN ORDER TO MEET",
    "object": "STRICT SERVICE LEVEL OBJECTIVES (SLOS)"
  },
  {
    "subject": "INFERENCE CLUSTERS",
    "predicate": "ARE",
    "object": "OVER-PROVISIONED FOR THE PEAK LOAD"
  },
  {
    "subject": "INFERENCE CLUSTERS",
    "predicate": "SERVE",
    "object": "USER REQUESTS"
  },
  {
    "subject": "INFERENCE CLUSTERS",
    "predicate": "NEED TO MEET",
    "object": "STRICT SLOS"
  },
  {
    "subject": "PRODUCTION SYSTEMS",
    "predicate": "ARE PROVISIONED TO",
    "object": "EACH APPLICATION ON PER-GPU GRANULARITY"
  },
  {
    "subject": "PROVISIONING",
    "predicate": "LIMITS",
    "object": "THE INTERFERENCE BETWEEN APPLICATIONS"
  },
  {
    "subject": "PRODUCTION SYSTEMS",
    "predicate": "ALLOCATE",
    "object": "GPUS TO APPLICATIONS ON PER-GPU GRANULARITY"
  },
  {
    "subject": "GPUS",
    "predicate": "ARE BOUND TO",
    "object": "VMS, CONTAINERS OR PROCESSES OF AN APPLICATION"
  },
  {
    "subject": "ALLOCATING GPUS TO APPLICATIONS",
    "predicate": "LIMITS",
    "object": "THE INTERFERENCE BETWEEN DIFFERENT APPLICATIONS"
  },
  {
    "subject": "ALLOCATING GPUS TO APPLICATIONS",
    "predicate": "SATISFIES",
    "object": "THE SLO REQUIREMENTS"
  },
  {
    "subject": "MULTIPLE DL APPLICATIONS",
    "predicate": "SHOULD BE ABLE TO BE PACKED TO",
    "object": "THE SAME GPU SERVER"
  },
  {
    "subject": "PACKING MULTIPLE DL APPLICATIONS TO THE SAME GPU SERVER",
    "predicate": "MAXIMIZES",
    "object": "GPU UTILIZATION VIA TIME-SHARING"
  },
  {
    "subject": "OPERATING SYSTEMS",
    "predicate": "ACHIEVE",
    "object": "HIGH CPU UTILIZATION"
  },
  {
    "subject": "HIGH CPU UTILIZATION",
    "predicate": "ACHIEVE VIA",
    "object": "TASK SCHEDULING AND CONTEXT SWITCHING"
  },
  {
    "subject": "THE IDEA OF NE-GRAINED CPU TIME-SHARING",
    "predicate": "HAS BEEN FURTHER EXTENDED TO",
    "object": "CLUSTER SCHEDULING"
  },
  {
    "subject": "NE-GRAINED TIME-SHARING",
    "predicate": "CAN PROVIDE",
    "object": "BETTER UTILIZATION THAN PROVISIONING DEDICATED RESOURCES"
  },
  {
    "subject": "NE-GRAINED TIME-SHARING",
    "predicate": "PROVIDES",
    "object": "NECESSARY PROCESS-LEVEL ISOLATION"
  },
  {
    "subject": "CPU WORKLOADS",
    "predicate": "ARE SIMILAR TO",
    "object": "NE-GRAINED TIME-SHARING"
  },
  {
    "subject": "SCHEDULING CYCLES",
    "predicate": "ARE",
    "object": "ENABLED"
  },
  {
    "subject": "GOOGLE BORG 1",
    "predicate": "PACKS",
    "object": "ONLINE SERVICES AND BATCH JOBS"
  },
  {
    "subject": "GOOGLE BORG 1",
    "predicate": "SAVES",
    "object": "20-30 MACHINES"
  },
  {
    "subject": "20-30 MACHINES",
    "predicate": "ARE SAVED",
    "object": "COMPARED WITH NOT PACKING THEM"
  },
  {
    "subject": "GPU",
    "predicate": "HAS",
    "object": "HIGH OVERHEAD WHEN SWITCHING BETWEEN TASKS"
  },
  {
    "subject": "THE GAP",
    "predicate": "IS",
    "object": "THE PRECIOUS GPU MEMORY AND SLOW SWITCHING"
  },
  {
    "subject": "NAIVELY USING GPUS",
    "predicate": "WILL NOT SATISFY",
    "object": "THE REQUIREMENTS OF DL INFERENCE THAT HAVE STRICT SLOS IN THE RANGE OF TENS TO HUNDREDS OF MILLISECONDS"
  },
  {
    "subject": "GPU",
    "predicate": "SWITCHES TO",
    "object": "DNN MODEL (E.G., RESNET)"
  },
  {
    "subject": "DNN MODEL (E.G., RESNET)",
    "predicate": "HAS NOT BEEN PRELOADED ONTO",
    "object": "GPU"
  },
  {
    "subject": "STATE-OF-THE-ART TRICKS LIKE CUDA UNIFIED MEMORY 4 (6)",
    "predicate": "DO NOT PREVENT",
    "object": "MULTIPLE SECONDS DELAY"
  },
  {
    "subject": "CPU APPLICATIONS",
    "predicate": "CAN BE SWITCHED IN",
    "object": "MILLISECONDS OR EVEN MICROSECONDS"
  },
  {
    "subject": "THE EXISTING SOLUTION",
    "predicate": "IS TO",
    "object": "SPATIALLY SHARE THE GPU MEMORY"
  },
  {
    "subject": "THIS APPROACH",
    "predicate": "DOES NOT PROVIDE",
    "object": "STRONG GPU MEMORY ISOLATION BETWEEN APPLICATIONS"
  },
  {
    "subject": "NVIDIA MULTIPLE PROCESS SHARING (MPS) 6",
    "predicate": "ALLOW",
    "object": "MULTIPLE PROCESSES TO USE THE SAME GPU"
  },
  {
    "subject": "SALUS 7",
    "predicate": "ALLOW",
    "object": "MULTIPLE PROCESSES TO USE THE SAME GPU"
  },
  {
    "subject": "MULTI-PROCESS SUPPORT FROM NVIDIA",
    "predicate": "ALLOWS",
    "object": "INFERENCE PROCESS TO SHARE THE GPU WITH THE TRAINING PROCESS"
  },
  {
    "subject": "NVIDIA MPS 6",
    "predicate": "PROVIDES",
    "object": "OFFICIAL SUPPORT FOR SHARING A GPU BETWEEN MULTIPLE PROCESSES"
  },
  {
    "subject": "GPU MEMORY",
    "predicate": "IS",
    "object": "MUCH MORE LIMITED THAN HOST MEMORY"
  },
  {
    "subject": "GPU MEMORY",
    "predicate": "CANNOT",
    "object": "PRELOAD MANY APPLICATIONS"
  },
  {
    "subject": "ONE SINGLE MEMORY-INTENSIVE TRAINING TASK",
    "predicate": "MAY CONSUME",
    "object": "ALL THE GPU MEMORY"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "STOPS AND CLEANS",
    "object": "ITS GPU ENVIRONMENT"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "FREES",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "OCCUPIES",
    "object": "THE ENTIRE GPU MEMORY"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "DOES NOT STOP",
    "object": "WHEN INFERENCE TASKS COME"
  },
  {
    "subject": "MEMORY FOOTPRINTS OF INFERENCE TASKS",
    "predicate": "ARE",
    "object": "INCREASING"
  },
  {
    "subject": "MODELS",
    "predicate": "ARE",
    "object": "GETTING LARGER"
  },
  {
    "subject": "REQUEST BATCHING",
    "predicate": "IS",
    "object": "PREVALENTLY USED TO INCREASE THROUGHPUT"
  },
  {
    "subject": "REQUEST BATCHING",
    "predicate": "IS USED TO INCREASE",
    "object": "THROUGHPUT 3"
  },
  {
    "subject": "THROUGHPUT 3",
    "predicate": "INCREASES",
    "object": "GPU MEMORY REQUIREMENT OF INFERENCE APPLICATIONS"
  },
  {
    "subject": "A CONTEXT SWITCHING DESIGN",
    "predicate": "MINIMIZES",
    "object": "THE SWITCHING OVERHEAD"
  },
  {
    "subject": "A CONTEXT SWITCHING DESIGN",
    "predicate": "SWITCHES",
    "object": "THE CONTENTS ON GPU MEMORY QUICKLY"
  },
  {
    "subject": "A CONTEXT SWITCHING DESIGN",
    "predicate": "IS",
    "object": "A BETTER APPROACH FOR EFFICIENTLY TIME-SHARING GPUS"
  },
  {
    "subject": "NO EXISTING SOLUTION",
    "predicate": "OFFERS",
    "object": "SUCH CONTEXT SWITCHING ABSTRACTION FOR GPU"
  },
  {
    "subject": "WE",
    "predicate": "ACHIEVE",
    "object": "SO BY INTRODUCING A NEW TECHNOLOGY CALLED PIPELINED CONTEXT SWITCHING"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "EXPLOITS",
    "object": "THE CHARACTERISTICS OF DL APPLICATIONS"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "ACHIEVE",
    "object": "MILLISECOND-SCALE OVERHEAD FOR SWITCHING TASKS ON GPUS"
  },
  {
    "subject": "WE",
    "predicate": "FACE",
    "object": "A MAJOR CHALLENGE FAST GPU CONTEXT SWITCHING BETWEEN DIFFERENT PROCESSES"
  },
  {
    "subject": "APPLICATION",
    "predicate": "IS LOADED IN",
    "object": "GPU"
  },
  {
    "subject": "THERE",
    "predicate": "IS NO NEED FOR",
    "object": "CONTEXT SWITCHING"
  },
  {
    "subject": "WE",
    "predicate": "INTRODUCE",
    "object": "PIPELINED CONTEXT SWITCHING"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "MINIMIZE",
    "object": "TASK SWITCHING OVERHEAD ON GPUS FOR DL APPLICATIONS"
  },
  {
    "subject": "DNN MODELS",
    "predicate": "CAN BE HELD IN",
    "object": "HOST MEMORY"
  },
  {
    "subject": "HOST MEMORY",
    "predicate": "IS",
    "object": "MUCH LARGER AND CHEAPER THAN GPU MEMORY"
  },
  {
    "subject": "GPU",
    "predicate": "CAN QUICKLY CONTEXT-SWITCH BETWEEN",
    "object": "THE MODELS"
  },
  {
    "subject": "THE MODELS",
    "predicate": "ARE USED FOR",
    "object": "TRAINING OR INFERENCE"
  },
  {
    "subject": "ENTERPRISES",
    "predicate": "BUILD",
    "object": "GPU CLUSTERS"
  },
  {
    "subject": "GPU CLUSTERS",
    "predicate": "ARE",
    "object": "EITHER PRIVATELY OR PUBLICLY SHARED BY MULTIPLE USERS"
  },
  {
    "subject": "11 M. JEON, S. VENKATARAMAN, A. PHANISHAYEE, U. QIAN, W. XIAO, AND F. YANG",
    "predicate": "AUTHORED",
    "object": "ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS"
  },
  {
    "subject": "ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS",
    "predicate": "PUBLISHED_IN",
    "object": "USENIX ATC"
  },
  {
    "subject": "ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS",
    "predicate": "PUBLISHED_YEAR",
    "object": "2019"
  },
  {
    "subject": "512 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION",
    "predicate": "IS ORGANIZED BY",
    "object": "USENIX ASSOCIATION"
  },
  {
    "subject": "ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS",
    "predicate": "WAS PUBLISHED IN",
    "object": "USENIX ATC"
  },
  {
    "subject": "ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2019"
  },
  {
    "subject": "NUMBER OF APPLICATIONS THAT CAN BE MULTIPLEXED",
    "predicate": "IS NOT LIMITED BY",
    "object": "GPU MEMORY SIZE"
  },
  {
    "subject": "EACH APPLICATION",
    "predicate": "IS ABLE TO USE",
    "object": "ENTIRE GPU COMPUTE AND MEMORY RESOURCES DURING ITS TIME SLICE"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENABLES",
    "object": "GPU-EFFICIENT MULTIPLEXING OF MANY DL APPLICATIONS ON GPU SERVERS VIA FINE-GRAINED TIME-SHARING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ACHIEVES",
    "object": "MILLISECOND-SCALE LATENCIES AND HIGH THROUGHPUT AS DEDICATED SERVERS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENABLES",
    "object": "GPU-EFFICIENT FINE-GRAINED TIME-SHARING FOR MULTIPLE DL APPLICATIONS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ACHIEVES",
    "object": "MILLISECOND-SCALE CONTEXT SWITCHING LATENCIES"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ACHIEVES",
    "object": "HIGH THROUGHPUT"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "COMBINES",
    "object": "ALL THE IDEAS INTO OUR SYSTEM"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "CLOSES",
    "object": "THE GAP OF GPU MEMORY SHARING AND SWITCHING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENABLES",
    "object": "THE DESIGN OF AN EFFICIENT TIME-SHARING GPU CLUSTER FOR DL WORKLOADS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENABLES",
    "object": "GPU-EFFICIENT MULTIPLEXING OF MULTIPLE DL APPLICATIONS ON GPU SERVERS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENABLES",
    "object": "GPU-EFFICIENT FINE-GRAINED TIME-SHARING"
  },
  {
    "subject": "GPU-EFFICIENT FINE-GRAINED TIME-SHARING",
    "predicate": "IS FOR",
    "object": "MULTIPLE DL APPLICATIONS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IS ABLE TO ACHIEVE",
    "object": "MILLISECOND-SCALE TASK SWITCHING TIME"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENABLES",
    "object": "DL APPLICATIONS ON TIME-SHARING GPUS TO MEET STRICT SLOS"
  },
  {
    "subject": "SUCH SMALL SWITCHING OVERHEAD",
    "predicate": "IS CRITICAL FOR",
    "object": "DL APPLICATIONS TO SATISFY STRICT SLO REQUIREMENTS"
  },
  {
    "subject": "MILLISECOND-SCALE TASK SWITCHING OVERHEAD",
    "predicate": "SATISFY",
    "object": "SLO REQUIREMENTS"
  },
  {
    "subject": "WE",
    "predicate": "PERFORM",
    "object": "A MEASUREMENT STUDY"
  },
  {
    "subject": "THE MEASUREMENT STUDY",
    "predicate": "PROBE",
    "object": "THE TASK SWITCHING OVERHEAD"
  },
  {
    "subject": "WE",
    "predicate": "ANALYZE",
    "object": "THE OVERHEAD OF EACH COMPONENT"
  },
  {
    "subject": "THE MEASUREMENT STUDY",
    "predicate": "PROLE",
    "object": "THE TASK SWITCHING OVERHEAD"
  },
  {
    "subject": "WE",
    "predicate": "DIVIDE",
    "object": "THE SWITCHING OVERHEAD INTO FOUR COMPONENTS"
  },
  {
    "subject": "THE FOUR COMPONENTS",
    "predicate": "ARE",
    "object": "OLD TASK CLEANING"
  },
  {
    "subject": "THE FOUR COMPONENTS",
    "predicate": "ARE",
    "object": "NEW TASK INITIALIZATION"
  },
  {
    "subject": "THE FOUR COMPONENTS",
    "predicate": "ARE",
    "object": "GPU MEMORY ALLOCATION"
  },
  {
    "subject": "THE FOUR COMPONENTS",
    "predicate": "ARE",
    "object": "MODEL TRANSMISSION VIA PCIE FROM CPU TO GPU"
  },
  {
    "subject": "INSTANCE TYPE",
    "predicate": "INCLUDES",
    "object": "G4DN.2XLARGE"
  },
  {
    "subject": "INSTANCE TYPE",
    "predicate": "INCLUDES",
    "object": "P3.2XLARGE"
  },
  {
    "subject": "GPU TYPE OF G4DN.2XLARGE",
    "predicate": "IS",
    "object": "NVIDIA T4"
  },
  {
    "subject": "GPU TYPE OF P3.2XLARGE",
    "predicate": "IS",
    "object": "NVIDIA V100"
  },
  {
    "subject": "TASK CLEANING TIME FOR G4DN.2XLARGE",
    "predicate": "IS",
    "object": "155 MS"
  },
  {
    "subject": "TASK CLEANING TIME FOR P3.2XLARGE",
    "predicate": "IS",
    "object": "165 MS"
  },
  {
    "subject": "TASK INITIALIZATION TIME FOR G4DN.2XLARGE",
    "predicate": "IS",
    "object": "5530 MS"
  },
  {
    "subject": "TASK INITIALIZATION TIME FOR P3.2XLARGE",
    "predicate": "IS",
    "object": "7290 MS"
  },
  {
    "subject": "MEMORY ALLOCATION TIME FOR G4DN.2XLARGE",
    "predicate": "IS",
    "object": "10 MS"
  },
  {
    "subject": "MEMORY ALLOCATION TIME FOR P3.2XLARGE",
    "predicate": "IS",
    "object": "13 MS"
  },
  {
    "subject": "MODEL TRANSMISSION TIME FOR G4DN.2XLARGE",
    "predicate": "IS",
    "object": "91 MS"
  },
  {
    "subject": "MODEL TRANSMISSION TIME FOR P3.2XLARGE",
    "predicate": "IS",
    "object": "81 MS"
  },
  {
    "subject": "TOTAL OVERHEAD FOR G4DN.2XLARGE",
    "predicate": "IS",
    "object": "5787 MS"
  },
  {
    "subject": "TOTAL OVERHEAD FOR P3.2XLARGE",
    "predicate": "IS",
    "object": "7551 MS"
  },
  {
    "subject": "INFERENCE TIME FOR G4DN.2XLARGE",
    "predicate": "IS",
    "object": "105 MS"
  },
  {
    "subject": "INFERENCE TIME FOR P3.2XLARGE",
    "predicate": "IS",
    "object": "32 MS"
  },
  {
    "subject": "EVERY COMPONENT",
    "predicate": "TAKES",
    "object": "A CONSIDERABLE AMOUNT OF TIME"
  },
  {
    "subject": "TIME",
    "predicate": "VARIES FROM",
    "object": "TENS OF MILLISECONDS TO SECONDS"
  },
  {
    "subject": "ONE SOURCE OF THE OVERHEAD",
    "predicate": "IS",
    "object": "THE CONTENTIONS BOTH ON THE COMPUTATION AND MEMORY OF THE GPU"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "DO NOT STOP",
    "object": "WHEN AN INFERENCE TASK COMES"
  },
  {
    "subject": "WE",
    "predicate": "TAKE",
    "object": "A HOLISTIC APPROACH"
  },
  {
    "subject": "WE",
    "predicate": "EXPLOIT",
    "object": "THE CHARACTERISTICS OF DL APPLICATIONS"
  },
  {
    "subject": "WE",
    "predicate": "MINIMIZE",
    "object": "THE OVERHEAD OF ALL THE COMPONENTS"
  },
  {
    "subject": "OUR DESIGN",
    "predicate": "IS BASED ON",
    "object": "A KEY OBSERVATION"
  },
  {
    "subject": "DNN MODELS",
    "predicate": "HAVE",
    "object": "A LAYERED STRUCTURE"
  },
  {
    "subject": "DNN MODELS",
    "predicate": "HAVE",
    "object": "A LAYER-BY-LAYER COMPUTATION PATTERN"
  },
  {
    "subject": "DNN MODELS",
    "predicate": "ARE",
    "object": "USUALLY DEEP"
  },
  {
    "subject": "DNN MODELS",
    "predicate": "CONSIST OF",
    "object": "MULTIPLE LAYERS STACKING ONE ON ANOTHER"
  },
  {
    "subject": "COMPUTATION OF DNN MODELS",
    "predicate": "TAKES PLACE",
    "object": "LAYER BY LAYER"
  },
  {
    "subject": "THERE",
    "predicate": "IS",
    "object": "NO NEED TO WAIT FOR THE ENTIRE MODEL TO BE TRANSMITTED TO THE GPU BEFORE STARTING COMPUTATION"
  },
  {
    "subject": "A TASK",
    "predicate": "DOES NOT NEED TO WAIT FOR",
    "object": "THE ENTIRE MODEL TO BE TRANSMITTED TO THE GPU BEFORE BEGINNING THE COMPUTATION"
  },
  {
    "subject": "NAIVE PIPELINING ON PER-LAYER GRANULARITY",
    "predicate": "INTRODUCES",
    "object": "HIGH OVERHEAD ON TENSOR TRANSMISSION AND SYNCHRONIZATION"
  },
  {
    "subject": "PIPELINING ON PER-LAYER GRANULARITY",
    "predicate": "REQUIRES",
    "object": "SYNCHRONIZATION FOR EVERY LAYER"
  },
  {
    "subject": "WE",
    "predicate": "DIVIDE",
    "object": "LAYERS INTO GROUPS"
  },
  {
    "subject": "WE",
    "predicate": "DESIGN",
    "object": "AN OPTIMAL MODEL-AWARE GROUPING ALGORITHM"
  },
  {
    "subject": "AN OPTIMAL MODEL-AWARE GROUPING ALGORITHM",
    "predicate": "FIND",
    "object": "THE BEST GROUPING STRATEGY FOR A GIVEN MODEL"
  },
  {
    "subject": "WE",
    "predicate": "DESIGN",
    "object": "AN ALGORITHM TO FIND THE OPTIMAL GROUPING STRATEGY FOR A GIVEN MODEL"
  },
  {
    "subject": "THE COMPUTATION OF A DL TASK",
    "predicate": "IS",
    "object": "LAYER BY LAYER"
  },
  {
    "subject": "THE COMPUTATION OF A DL TASK",
    "predicate": "HAS",
    "object": "A SIMPLE, REGULAR PATTERN FOR MEMORY ALLOCATION"
  },
  {
    "subject": "A DL TASK",
    "predicate": "STORES",
    "object": "TWO IMPORTANT TYPES OF DATA IN THE GPU MEMORY"
  },
  {
    "subject": "TWO IMPORTANT TYPES OF DATA",
    "predicate": "INCLUDE",
    "object": "THE DNN MODEL"
  },
  {
    "subject": "THE DNN MODEL",
    "predicate": "INCLUDES",
    "object": "THE MODEL PARAMETERS"
  },
  {
    "subject": "TWO IMPORTANT TYPES OF DATA",
    "predicate": "INCLUDE",
    "object": "THE INTERMEDIATE RESULTS"
  },
  {
    "subject": "THE DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT (E.G., CUDA UNI-ED MEMORY 4)",
    "predicate": "IS",
    "object": "AN OVERKILL"
  },
  {
    "subject": "THE DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT (E.G., CUDA UNI-ED MEMORY 4)",
    "predicate": "INCURS",
    "object": "UNNECESSARY OVERHEAD"
  },
  {
    "subject": "NVIDIA",
    "predicate": "PROVIDES",
    "object": "CUDA UNIFIED MEMORY 4"
  },
  {
    "subject": "CUDA UNIFIED MEMORY 4",
    "predicate": "HANDLES",
    "object": "MEMORY MOVEMENT BETWEEN THE HOST MEMORY AND THE GPU MEMORY"
  },
  {
    "subject": "MEMORY MOVEMENT",
    "predicate": "IS FOR",
    "object": "APPLICATIONS"
  },
  {
    "subject": "UNIFIED MEMORY MANAGEMENT",
    "predicate": "HAS",
    "object": "DEDICATED MEMORY DAEMON"
  },
  {
    "subject": "DEDICATED MEMORY DAEMON",
    "predicate": "MINIMIZES",
    "object": "OVERHEAD"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "UNIFIED MEMORY MANAGEMENT WITH THE MEMORY DAEMON"
  },
  {
    "subject": "UNIFIED MEMORY MANAGEMENT WITH THE MEMORY DAEMON",
    "predicate": "ACHIEVE",
    "object": "MINIMAL MEMORY FOOTPRINT"
  },
  {
    "subject": "UNIFIED MEMORY MANAGEMENT WITH THE MEMORY DAEMON",
    "predicate": "ELIMINATE",
    "object": "EXTRA MEMORY COPIES"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "PRE-ALLOCATES",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "RE-ALLOCATES",
    "object": "IT TO EACH TASK"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "DOES NOT INVOLVE",
    "object": "THE EXPENSIVE GPU MEMORY MANAGER"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "USES",
    "object": "CUDAMALLOC"
  },
  {
    "subject": "CUDAMALLOC",
    "predicate": "OBTAINS",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "ALLOCATES",
    "object": "THE MEMORY TO THE WORKERS"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "ALLOCATES",
    "object": "THE MEMORY AT RUNTIME"
  },
  {
    "subject": "THE DNN MODELS",
    "predicate": "ARE STORED",
    "object": "ONLY ONCE IN THE MEMORY DAEMON"
  },
  {
    "subject": "THE DNN MODELS",
    "predicate": "ARE NOT STORED",
    "object": "IN EVERY WORKER"
  },
  {
    "subject": "STORING THE DNN MODELS ONLY ONCE IN THE MEMORY DAEMON",
    "predicate": "MINIMIZES",
    "object": "MEMORY FOOTPRINT"
  },
  {
    "subject": "WE",
    "predicate": "EXPLOIT",
    "object": "THE MEMORY ALLOCATION FOR A DNN MODEL IS DETERMINISTIC"
  },
  {
    "subject": "WE",
    "predicate": "ELIMINATE",
    "object": "EXTRA MEMORY COPIES BETWEEN THE DAEMON AND THE WORKERS"
  },
  {
    "subject": "WE",
    "predicate": "REDUCE",
    "object": "THE IPC OVERHEAD"
  },
  {
    "subject": "NO UNIFIED MEMORY MANAGEMENT",
    "predicate": "REQUIRES",
    "object": "EACH WORKER TO KEEP A COPY FOR EACH DNN MODEL"
  },
  {
    "subject": "KEEPING A COPY FOR EACH DNN MODEL",
    "predicate": "INCREASES",
    "object": "THE MEMORY FOOTPRINT"
  },
  {
    "subject": "EACH SERVER",
    "predicate": "CONTAINS",
    "object": "AN ACTIVE WORKER"
  },
  {
    "subject": "EACH SERVER",
    "predicate": "CONTAINS",
    "object": "MULTIPLE STANDBY WORKERS"
  },
  {
    "subject": "A SERVER",
    "predicate": "HAS",
    "object": "ONE OR MORE STANDBY WORKERS"
  },
  {
    "subject": "THE ACTIVE WORKER",
    "predicate": "EXECUTES",
    "object": "THE CURRENT TASK ON THE GPU"
  },
  {
    "subject": "THE STANDBY WORKERS",
    "predicate": "STAY ON",
    "object": "THE CPU"
  },
  {
    "subject": "THE STANDBY WORKERS",
    "predicate": "WAIT FOR",
    "object": "THE NEXT TASK"
  },
  {
    "subject": "THE ACTIVE WORKER",
    "predicate": "IS",
    "object": "THE WORKER THAT CURRENTLY EXECUTES A TASK IN THE GPU"
  },
  {
    "subject": "WORKER",
    "predicate": "IS",
    "object": "PROCESS"
  },
  {
    "subject": "PROCESS",
    "predicate": "EXECUTES",
    "object": "TASKS"
  },
  {
    "subject": "TASKS",
    "predicate": "EXECUTES ON",
    "object": "ONE GPU"
  },
  {
    "subject": "ACTIVE WORKER",
    "predicate": "COMPLETES OR STOPS",
    "object": "CURRENT TASK"
  },
  {
    "subject": "CONTROLLER",
    "predicate": "NOTIFIES",
    "object": "MEMORY DAEMON"
  },
  {
    "subject": "CONTROLLER",
    "predicate": "NOTIFIES",
    "object": "STANDBY WORKER"
  },
  {
    "subject": "STANDBY WORKER",
    "predicate": "LOADS",
    "object": "TASK TO GPU"
  },
  {
    "subject": "TASK",
    "predicate": "EXECUTES WITH",
    "object": "PIPELINED MODEL TRANSMISSION"
  },
  {
    "subject": "OUR MECHANISM",
    "predicate": "PARALLELIZES",
    "object": "OLD TASK CLEANING IN THE ACTIVE WORKER AND NEW TASK INITIALIZATION IN THE STANDBY WORKER"
  },
  {
    "subject": "OUR MECHANISM",
    "predicate": "MINIMIZES",
    "object": "WORKER SWITCHING OVERHEAD"
  },
  {
    "subject": "TABLE 2",
    "predicate": "IS",
    "object": "COMPARISON OF WORKER SWITCHING MECHANISMS"
  },
  {
    "subject": "WE",
    "predicate": "DESIGN",
    "object": "AN ACTIVE AND STANDBY WORKER SWITCHING MECHANISM"
  },
  {
    "subject": "THE ACTIVE AND STANDBY WORKER SWITCHING MECHANISM",
    "predicate": "HIDES",
    "object": "THE OVERHEAD OF BOTH TASK CLEANING AND TASK INITIALIZATION"
  },
  {
    "subject": "THE ACTIVE AND STANDBY WORKER SWITCHING MECHANISM",
    "predicate": "ENSURES",
    "object": "PROCESS-LEVEL ISOLATION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENFORCES",
    "object": "PROCESS-LEVEL ISOLATION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENFORCES WITH",
    "object": "SEPARATE WORKER PROCESSES"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "REQUIRES",
    "object": "US TO ADDRESS NEW TECHNICAL CHALLENGES ON MEMORY MANAGEMENT AND WORKER SWITCHING ACROSS DIFFERENT PROCESSES"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "AIMS TO PROVIDE",
    "object": "FAST TASK SWITCHING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "AIMS TO ENSURE",
    "object": "PROCESS-LEVEL ISOLATION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "HAS",
    "object": "AN ACTIVE WORKER"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "HAS",
    "object": "MULTIPLE STANDBY WORKERS"
  },
  {
    "subject": "WE",
    "predicate": "KEEP",
    "object": "ALL OTHER COMPONENTS OF PIPESWITCH THE SAME"
  },
  {
    "subject": "WE",
    "predicate": "COMPARE",
    "object": "THE FOLLOWING MECHANISMS DISCUSSED IN 4.4"
  },
  {
    "subject": "ACTIVE-STANDBY WORKER SWITCHING",
    "predicate": "IS USED TO",
    "object": "EVALUATE THE EFFECTIVENESS OF ACTIVE-STANDBY WORKER SWITCHING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IS",
    "object": "THE ACTIVE-STANDBY WORKER SWITCHING MECHANISM USED BY PIPESWITCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "USES",
    "object": "AN ACTIVE-STANDBY WORKER SWITCHING MECHANISM"
  },
  {
    "subject": "AN ACTIVE-STANDBY WORKER SWITCHING MECHANISM",
    "predicate": "TO",
    "object": "PARALLELIZE OLD TASK CLEANING AND NEW TASK INITIALIZATION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "INCURS",
    "object": "MINIMAL OVERHEAD"
  },
  {
    "subject": "PIPELINING",
    "predicate": "IS",
    "object": "A CANONICAL TECHNIQUE"
  },
  {
    "subject": "PIPELINING",
    "predicate": "IS WIDELY USED IN",
    "object": "COMPUTER SYSTEMS"
  },
  {
    "subject": "PIPELINING",
    "predicate": "IMPROVES",
    "object": "SYSTEM PERFORMANCE"
  },
  {
    "subject": "PIPELINING",
    "predicate": "MAXIMIZES",
    "object": "RESOURCE UTILIZATION"
  },
  {
    "subject": "PIPELINING",
    "predicate": "BRINGS",
    "object": "TWO SOURCES OF SYSTEM OVERHEADS"
  },
  {
    "subject": "PRIOR WORK IN DL SYSTEMS SUCH AS PIPEDREAM 8 AND BYTESCHEDULER 9",
    "predicate": "HAS APPLIED",
    "object": "PIPELINING TO DISTRIBUTED TRAINING"
  },
  {
    "subject": "THESE SOLUTIONS",
    "predicate": "FOCUS ON",
    "object": "INTER-BATCH PIPELINING"
  },
  {
    "subject": "INTER-BATCH PIPELINING",
    "predicate": "OVERLAPS",
    "object": "COMPUTATION AND GRADIENT TRANSMISSION OF DIFFERENT BATCHES"
  },
  {
    "subject": "COMPUTATION AND GRADIENT TRANSMISSION",
    "predicate": "ARE FOR",
    "object": "TRAINING WORKLOADS OF THE SAME DNN MODEL"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "INTRODUCES",
    "object": "INTRA-BATCH PIPELINING"
  },
  {
    "subject": "INTRA-BATCH PIPELINING",
    "predicate": "OVERLAPS",
    "object": "MODEL TRANSMISSION AND COMPUTATION"
  },
  {
    "subject": "INTRA-BATCH PIPELINING",
    "predicate": "REDUCES",
    "object": "OVERHEAD OF SWITCHING BETWEEN DIFFERENT DNN MODELS"
  },
  {
    "subject": "DNN MODELS",
    "predicate": "CAN BE",
    "object": "EITHER INFERENCE OR TRAINING"
  },
  {
    "subject": "WE",
    "predicate": "DESIGN",
    "object": "NEW TECHNIQUES"
  },
  {
    "subject": "NEW TECHNIQUES",
    "predicate": "SUPPORT",
    "object": "TRAINING"
  },
  {
    "subject": "NEW TECHNIQUES",
    "predicate": "SUPPORT",
    "object": "INFERENCE"
  },
  {
    "subject": "INFERENCE",
    "predicate": "HAS",
    "object": "STRICT SLOS"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "LEVERAGES",
    "object": "PIPELINED MODEL TRANSMISSION"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "LEVERAGES",
    "object": "UNIFIED MEMORY MANAGEMENT"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "LEVERAGES",
    "object": "ACTIVE-STANDBY WORKER SWITCHING"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "MINIMIZES",
    "object": "SWITCHING OVERHEAD"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "ENFORCES",
    "object": "PROCESS-LEVEL ISOLATION"
  },
  {
    "subject": "PIPELINED CONTEXT SWITCHING",
    "predicate": "INCLUDES",
    "object": "THREE KEY TECHNIQUES"
  },
  {
    "subject": "THREE KEY TECHNIQUES",
    "predicate": "ARE",
    "object": "PIPELINED MODEL TRANSMISSION"
  },
  {
    "subject": "THREE KEY TECHNIQUES",
    "predicate": "ARE",
    "object": "UNIFIED MEMORY MANAGEMENT"
  },
  {
    "subject": "THREE KEY TECHNIQUES",
    "predicate": "ARE",
    "object": "ACTIVE-STANDBY WORKER SWITCHING"
  },
  {
    "subject": "WE",
    "predicate": "IMPLEMENT",
    "object": "A SYSTEM PROTOTYPE"
  },
  {
    "subject": "WE",
    "predicate": "INTEGRATE",
    "object": "IT WITH PYTORCH"
  },
  {
    "subject": "WE",
    "predicate": "IDENTIFY",
    "object": "INEFFICIENCIES IN TODAYS SHARED GPU CLUSTERS"
  },
  {
    "subject": "WE",
    "predicate": "MOTIVATE",
    "object": "RUNNING DL WORKLOADS ON GPUS IN THE NE-GRAINED TIME-SHARING MODEL"
  },
  {
    "subject": "WE",
    "predicate": "PROPOSE TO PACK",
    "object": "MULTIPLE DL APPLICATIONS ONTO THE SAME GPU"
  },
  {
    "subject": "PACKING MULTIPLE DL APPLICATIONS ONTO THE SAME GPU",
    "predicate": "IS DONE VIA",
    "object": "NE-GRAINED TIME-SHARING ABSTRACTION"
  },
  {
    "subject": "NE-GRAINED TIME-SHARING ABSTRACTION",
    "predicate": "MAXIMIZES",
    "object": "GPU UTILIZATION"
  },
  {
    "subject": "FAST TASK SWITCHING",
    "predicate": "ENABLES",
    "object": "MORE FLEXIBLE FINE-GRAINED SCHEDULING"
  },
  {
    "subject": "MORE FLEXIBLE FINE-GRAINED SCHEDULING",
    "predicate": "IMPROVES",
    "object": "GPU UTILIZATION"
  },
  {
    "subject": "GPU UTILIZATION",
    "predicate": "IS FOR",
    "object": "DYNAMIC WORKLOADS"
  },
  {
    "subject": "GPU CLUSTERS",
    "predicate": "ARE DESIGNED WITH",
    "object": "DEDICATED PHYSICAL FORMS AND POWER SUPPLIES"
  },
  {
    "subject": "GPU CLUSTERS",
    "predicate": "ARE DESIGNED WITH",
    "object": "HIGH SPEED NETWORKS"
  },
  {
    "subject": "GPU CLUSTERS",
    "predicate": "ARE DESIGNED WITH",
    "object": "SPECIALIZED TASK SCHEDULERS"
  },
  {
    "subject": "500 14TH USENIX SYMPOSIUM",
    "predicate": "IS ON",
    "object": "OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"
  },
  {
    "subject": "USENIX SYMPOSIUM",
    "predicate": "IS ORGANIZED BY",
    "object": "USENIX ASSOCIATION"
  },
  {
    "subject": "QUESTION",
    "predicate": "IS",
    "object": "WHY BUILD A SHARED CLUSTER INSTEAD OF A DEDICATED ONE FOR EACH USER"
  },
  {
    "subject": "USENIX ASSOCIATION",
    "predicate": "ORGANIZES",
    "object": "14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"
  },
  {
    "subject": "14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION",
    "predicate": "INCLUDES",
    "object": "KUBERNETES"
  },
  {
    "subject": "14TH USENIX SYMPOSIUM",
    "predicate": "IS ON",
    "object": "OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"
  },
  {
    "subject": "14TH USENIX SYMPOSIUM",
    "predicate": "IS ORGANIZED BY",
    "object": "USENIX ASSOCIATION"
  },
  {
    "subject": "THE MAIN REASON",
    "predicate": "IS",
    "object": "TO BRING DOWN THE COST"
  },
  {
    "subject": "THE DEMAND OF TRAINING",
    "predicate": "IS NOT",
    "object": "WELL PREDICTABLE"
  },
  {
    "subject": "THE DEMAND OF TRAINING",
    "predicate": "WOULD DEPEND ON",
    "object": "THE PROGRESS OF DIFFERENT DEVELOPERS"
  },
  {
    "subject": "THE DEMAND OF INFERENCE",
    "predicate": "IS",
    "object": "MORE PREDICTABLE"
  },
  {
    "subject": "AN INFERENCE TASK FOR A PARTICULAR APPLICATION",
    "predicate": "HAS",
    "object": "A DAILY PERIODICAL PATTERN"
  },
  {
    "subject": "A DAILY PERIODICAL PATTERN",
    "predicate": "IS BASED ON",
    "object": "THE APPLICATION USAGE"
  },
  {
    "subject": "THE PATTERNS",
    "predicate": "CAN VARY",
    "object": "ACROSS DIFFERENT TASKS"
  },
  {
    "subject": "A SHARED CLUSTER",
    "predicate": "WOULD INCREASE",
    "object": "THE RESOURCE UTILIZATION"
  },
  {
    "subject": "RESOURCE UTILIZATION",
    "predicate": "INCREASE VIA",
    "object": "TIME-SHARING"
  },
  {
    "subject": "A SHARED CLUSTER",
    "predicate": "IS USED BY",
    "object": "DIFFERENT TASKS"
  },
  {
    "subject": "TRAINING",
    "predicate": "HAS NO SHARING WITH",
    "object": "INFERENCE"
  },
  {
    "subject": "SHARED CLUSTERS",
    "predicate": "ARE NOT SHARED BETWEEN",
    "object": "TRAINING AND INFERENCE"
  },
  {
    "subject": "INFERENCE CLUSTERS",
    "predicate": "ARE NOT ALWAYS RUNNING AT",
    "object": "HIGH UTILIZATION"
  },
  {
    "subject": "INFERENCE CLUSTERS",
    "predicate": "CANNOT BE UTILIZED BY",
    "object": "TRAINING"
  },
  {
    "subject": "TRAINING CLUSTERS",
    "predicate": "ARE EQUIPPED WITH",
    "object": "POWERFUL GPUS"
  },
  {
    "subject": "TRAINING CLUSTERS",
    "predicate": "ARE USED TO",
    "object": "RUN TRAINING TASKS"
  },
  {
    "subject": "TRAINING TASKS",
    "predicate": "ARE",
    "object": "OFTEN ELASTIC"
  },
  {
    "subject": "TRAINING TASKS",
    "predicate": "DO NOT HAVE",
    "object": "STRICT DEADLINES"
  },
  {
    "subject": "GPUS DESIGNED FOR INFERENCE TASKS",
    "predicate": "MIGHT BE TOO WIMPY FOR",
    "object": "TRAINING TASKS"
  },
  {
    "subject": "NEW GPU HARDWARE",
    "predicate": "INCLUDES",
    "object": "NVIDIA T4"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "HAS",
    "object": "UP TO 32GB GPU MEMORY"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "HAS",
    "object": "15.7 TFLOPS (SINGLE-PRECISION)"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "HAS",
    "object": "16GB GPU MEMORY"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "HAS",
    "object": "8.1 TFLOPS (SINGLE-PRECISION)"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "HAS",
    "object": "COMPARABLE PERFORMANCE WITH NVIDIA V100"
  },
  {
    "subject": "NEW ALGORITHMS AND SYSTEMS FOR DISTRIBUTED TRAINING",
    "predicate": "ENABLE",
    "object": "MULTIPLE GPUS TO ACCELERATE TRAINING"
  },
  {
    "subject": "OUR INDUSTRY COLLABORATOR",
    "predicate": "IS",
    "object": "A LEADING ONLINE SERVICE PROVIDER"
  },
  {
    "subject": "OUR INDUSTRY COLLABORATOR",
    "predicate": "CONFIRMS",
    "object": "THIS OBSERVATION"
  },
  {
    "subject": "THIS SERVICE PROVIDER",
    "predicate": "RUNS",
    "object": "MORE THAN 10K V100 GPUS FOR TRAINING"
  },
  {
    "subject": "THIS SERVICE PROVIDER",
    "predicate": "RUNS",
    "object": "AT LEAST 5 AS MANY T4 GPUS FOR INFERENCE"
  },
  {
    "subject": "THE COMPUTATION POWER ON BOTH SIDES",
    "predicate": "IS WITHIN",
    "object": "THE SAME ORDER OF MAGNITUDE"
  },
  {
    "subject": "INFERENCE GPUS",
    "predicate": "ARE UTILIZED",
    "object": "DURING LESS BUSY TIMES"
  },
  {
    "subject": "INFERENCE GPUS",
    "predicate": "ARE USED FOR",
    "object": "TRAINING MODELS"
  },
  {
    "subject": "MODELS",
    "predicate": "REQUIRE",
    "object": "DAILY UPDATES WITH LATEST DATA"
  },
  {
    "subject": "A GOOD EXAMPLE",
    "predicate": "IS TO",
    "object": "NE-TUNE BERT USING DAILY NEWS"
  },
  {
    "subject": "BORG-LIKE 1 SYSTEMS",
    "predicate": "IMPROVE",
    "object": "GPU UTILIZATION"
  },
  {
    "subject": "INFERENCE AND TRAINING WORKLOADS",
    "predicate": "HAVE",
    "object": "COMPLEMENTARY USAGE PATTERNS"
  },
  {
    "subject": "ONLINE INFERENCE SERVICES",
    "predicate": "ARE OFTEN",
    "object": "MORE IDLE DURING MIDNIGHT"
  },
  {
    "subject": "MANY TRAINING DEVELOPERS",
    "predicate": "WOULD START",
    "object": "A TIME-CONSUMING JOB AT NIGHT"
  },
  {
    "subject": "INFERENCE LOADS ON DIFFERENT MODELS",
    "predicate": "HAVE",
    "object": "DIFFERENT PATTERNS"
  },
  {
    "subject": "DIFFERENT PATTERNS",
    "predicate": "BENEFIT FROM",
    "object": "TIME SHARING"
  },
  {
    "subject": "ANY SERVER",
    "predicate": "WOULD BE ABLE TO RUN",
    "object": "ANY TASK"
  },
  {
    "subject": "ANY SERVER",
    "predicate": "WOULD HAVE",
    "object": "LOW OVERHEAD TO SWITCH BETWEEN DIFFERENT APPLICATIONS"
  },
  {
    "subject": "A MODERN SERVER",
    "predicate": "CAN BE EQUIPPED WITH",
    "object": "SEVERAL TB OF HOST MEMORY"
  },
  {
    "subject": "SEVERAL TB OF HOST MEMORY",
    "predicate": "ENABLES",
    "object": "IT TO LOAD MANY APPLICATIONS"
  },
  {
    "subject": "TASK EXECUTION ON GPUS",
    "predicate": "REQUIRES",
    "object": "GPU MEMORY"
  },
  {
    "subject": "GPU MEMORY",
    "predicate": "IS",
    "object": "VERY LIMITED"
  },
  {
    "subject": "HIGH-END GPUS",
    "predicate": "HAVE",
    "object": "16 GB FOR T4"
  },
  {
    "subject": "HIGH-END GPUS",
    "predicate": "HAVE",
    "object": "32 GB FOR V100"
  },
  {
    "subject": "GPU MEMORY",
    "predicate": "IS PURPOSED FOR",
    "object": "TASK EXECUTION"
  },
  {
    "subject": "GPU MEMORY",
    "predicate": "IS NOT FOR",
    "object": "STORING THE STATE OF IDLE APPLICATIONS"
  },
  {
    "subject": "DL TASKS",
    "predicate": "REQUIRE",
    "object": "A LARGE AMOUNT OF MEMORY ON A GPU"
  },
  {
    "subject": "DL TASKS",
    "predicate": "REQUIRE",
    "object": "ALL OF THE MEMORY ON A GPU"
  },
  {
    "subject": "TRAINING",
    "predicate": "IS A TYPE OF",
    "object": "DL TASKS"
  },
  {
    "subject": "DL APPLICATIONS",
    "predicate": "HAVE",
    "object": "LARGE MODELS"
  },
  {
    "subject": "DL APPLICATIONS",
    "predicate": "GENERATE",
    "object": "LARGE AMOUNTS OF INTERMEDIATE RESULTS"
  },
  {
    "subject": "INTERMEDIATE RESULTS",
    "predicate": "REQUIRE",
    "object": "A LOT OF GPU MEMORY"
  },
  {
    "subject": "SALUS 7",
    "predicate": "CANNOT SUPPORT",
    "object": "TRAINING TASKS WHICH ARE MEMORY-INTENSIVE"
  },
  {
    "subject": "SALUS 7",
    "predicate": "CANNOT SUPPORT",
    "object": "MULTIPLE INFERENCE TASKS WHICH HAVE LARGE MODELS"
  },
  {
    "subject": "STATE-OF-THE-ART MODELS",
    "predicate": "ARE GETTING",
    "object": "DEEPER AND LARGER"
  },
  {
    "subject": "IDLE APPLICATIONS",
    "predicate": "CAN OCCUPY",
    "object": "LARGE MEMORY SPACE"
  },
  {
    "subject": "THE ACTIVE APPLICATION",
    "predicate": "SHOULD BE ABLE TO UTILIZE",
    "object": "THE ENTIRE GPU MEMORY"
  },
  {
    "subject": "THE NUMBER OF APPLICATIONS THAT CAN BE SERVED BY A GPU SERVER",
    "predicate": "SHOULD ONLY BE LIMITED BY",
    "object": "ITS HOST MEMORY SIZE"
  },
  {
    "subject": "SWITCHING A TASK",
    "predicate": "WOULD REQUIRE",
    "object": "HEAVY MEMORY SWAPPING"
  },
  {
    "subject": "MANY ONLINE INFERENCE WORKLOADS",
    "predicate": "REQUIRE",
    "object": "STRICT SLOS"
  },
  {
    "subject": "NAIVE MEMORY SWAPPING BETWEEN THE HOST MEMORY AND THE GPU MEMORY",
    "predicate": "CANNOT MEET",
    "object": "STRICT SLOS"
  },
  {
    "subject": "STRICT SLOS",
    "predicate": "REQUIRE",
    "object": "REQUESTS TO BE HANDLED IN SMALL BATCHES FOR LOW LATENCY"
  },
  {
    "subject": "WE",
    "predicate": "TEST",
    "object": "THE STRAWMAN SCENARIO"
  },
  {
    "subject": "WE",
    "predicate": "STOP",
    "object": "A TRAINING TASK"
  },
  {
    "subject": "WE",
    "predicate": "START",
    "object": "AN INFERENCE TASK"
  },
  {
    "subject": "THE RST INFERENCE BATCH",
    "predicate": "WOULD REQUIRE",
    "object": "SEVERAL SECONDS TO FINISH"
  },
  {
    "subject": "EXISTING SUPPORT SUCH AS NVIDIA MPS",
    "predicate": "IS NOT OPTIMIZED FOR",
    "object": "DL WORKLOADS"
  },
  {
    "subject": "EXISTING SUPPORT SUCH AS NVIDIA MPS",
    "predicate": "INCURS",
    "object": "HUNDREDS OF MILLISECONDS OVERHEAD"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "HAS",
    "object": "LOWER OVERHEAD COMPARED TO STOP-AND-START"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "INCURS",
    "object": "SEVERAL HUNDRED MILLISECONDS OVERHEAD"
  },
  {
    "subject": "SEVERAL HUNDRED MILLISECONDS OVERHEAD",
    "predicate": "PREVENTS",
    "object": "MPS FROM MEETING STRICT SLOS"
  },
  {
    "subject": "USENIX ASSOCIATION",
    "predicate": "HOSTS",
    "object": "14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"
  },
  {
    "subject": "FIGURE 1",
    "predicate": "SHOWS",
    "object": "PIPESWITCH ARCHITECTURE"
  },
  {
    "subject": "PIPESWITCH ARCHITECTURE",
    "predicate": "INCLUDES",
    "object": "501 CONTROLLER"
  },
  {
    "subject": "PIPESWITCH ARCHITECTURE",
    "predicate": "INCLUDES",
    "object": "ACTIVE WORKER"
  },
  {
    "subject": "PIPESWITCH ARCHITECTURE",
    "predicate": "INCLUDES",
    "object": "GPU MEMORY DAEMON"
  },
  {
    "subject": "PIPESWITCH ARCHITECTURE",
    "predicate": "INCLUDES",
    "object": "STANDBY WORKER"
  },
  {
    "subject": "PIPESWITCH ARCHITECTURE",
    "predicate": "HANDLES",
    "object": "NEW TASK"
  },
  {
    "subject": "THROUGHPUT",
    "predicate": "MEASURED IN",
    "object": "BATCHES PER SECOND"
  },
  {
    "subject": "THROUGHPUT",
    "predicate": "HAS UPPER BOUND",
    "object": "PIPESWITCH MPS STOP-AND-START"
  },
  {
    "subject": "THROUGHPUT",
    "predicate": "MEASURED FOR",
    "object": "EIGHT P3.2XLARGE INSTANCES"
  },
  {
    "subject": "DL WORKLOADS",
    "predicate": "HAVE",
    "object": "WELL-DENED STRUCTURES"
  },
  {
    "subject": "THE STRUCTURE AND COMPUTATION PATTERN OF DNN MODELS",
    "predicate": "ALLOW",
    "object": "US TO HIGHLY OPTIMIZE TASK SWITCHING"
  },
  {
    "subject": "THE STRUCTURE AND COMPUTATION PATTERN OF DNN MODELS",
    "predicate": "ALLOW",
    "object": "US TO ACHIEVE MILLISECOND-SCALE OVERHEAD"
  },
  {
    "subject": "PIPELINE",
    "predicate": "IS",
    "object": "FEASIBLE"
  },
  {
    "subject": "PIPELINE",
    "predicate": "IS",
    "object": "EFFECTIVE"
  },
  {
    "subject": "WE",
    "predicate": "WILL NEED TO RESOLVE",
    "object": "OTHER CHALLENGES"
  },
  {
    "subject": "OTHER CHALLENGES",
    "predicate": "INCLUDE",
    "object": "MEMORY MANAGEMENT"
  },
  {
    "subject": "OTHER CHALLENGES",
    "predicate": "INCLUDE",
    "object": "WORKER SWITCHING"
  },
  {
    "subject": "WE",
    "predicate": "PROVIDE",
    "object": "AN OVERVIEW OF THE ARCHITECTURE AND TASK EXECUTION"
  },
  {
    "subject": "FIGURE 1",
    "predicate": "SHOWS",
    "object": "THE ARCHITECTURE OF A PIPESWITCH SERVER"
  },
  {
    "subject": "PIPESWITCH PIPELINES MODEL",
    "predicate": "MODEL",
    "object": "TRANSMISSION AND TASK EXECUTION"
  },
  {
    "subject": "THIS SERVER",
    "predicate": "CONTAINS",
    "object": "FOUR TYPES OF COMPONENTS"
  },
  {
    "subject": "FOUR TYPES OF COMPONENTS",
    "predicate": "INCLUDE",
    "object": "A CONTROLLER"
  },
  {
    "subject": "FOUR TYPES OF COMPONENTS",
    "predicate": "INCLUDE",
    "object": "A MEMORY DAEMON"
  },
  {
    "subject": "FOUR TYPES OF COMPONENTS",
    "predicate": "INCLUDE",
    "object": "AN ACTIVE WORKER"
  },
  {
    "subject": "FOUR TYPES OF COMPONENTS",
    "predicate": "INCLUDE",
    "object": "MULTIPLE STANDBY WORKERS"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "IS",
    "object": "THE CENTRAL COMPONENT"
  },
  {
    "subject": "THE WORKERS",
    "predicate": "EXECUTE",
    "object": "THE TASKS"
  },
  {
    "subject": "MEMORY",
    "predicate": "IS",
    "object": "DAEMON"
  },
  {
    "subject": "CONTROLLER",
    "predicate": "AND",
    "object": "MEMORY DAEMON"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "MANAGES",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "MANAGES",
    "object": "THE DNN MODELS"
  },
  {
    "subject": "THE SERVER",
    "predicate": "STORES",
    "object": "THE DNN MODELS IN THE HOST MEMORY"
  },
  {
    "subject": "ACTIVE WORKER",
    "predicate": "IS",
    "object": "ACTIVE WORKER"
  },
  {
    "subject": "ALL COMPONENTS",
    "predicate": "SHOULD BE OPTIMIZED TO MEET",
    "object": "THE SLOS"
  },
  {
    "subject": "STANDBY WORKER",
    "predicate": "IS",
    "object": "STANDBY WORKER"
  },
  {
    "subject": "A STANDBY WORKER",
    "predicate": "IS",
    "object": "IDLE"
  },
  {
    "subject": "A STANDBY WORKER",
    "predicate": "IS",
    "object": "INITIALIZING A NEW TASK"
  },
  {
    "subject": "A STANDBY WORKER",
    "predicate": "IS",
    "object": "CLEANING ITS ENVIRONMENT FOR THE PREVIOUS TASK"
  },
  {
    "subject": "THE STANDBY WORKER",
    "predicate": "BECOMES",
    "object": "THE NEW ACTIVE WORKER"
  },
  {
    "subject": "THE NEW ACTIVE WORKER",
    "predicate": "EXECUTES",
    "object": "THE NEW TASK"
  },
  {
    "subject": "THE ACTIVE WORKER",
    "predicate": "BECOMES",
    "object": "A STANDBY WORKER"
  },
  {
    "subject": "THE ACTIVE WORKER",
    "predicate": "CLEANS",
    "object": "THE ENVIRONMENT FOR THE PREVIOUS TASK"
  },
  {
    "subject": "NEW TASK",
    "predicate": "ARRIVES BEFORE",
    "object": "STANDBY WORKER FINISHES CLEANING PREVIOUS TASK"
  },
  {
    "subject": "NEW TASK",
    "predicate": "NEEDS TO",
    "object": "WAIT"
  },
  {
    "subject": "WAITING",
    "predicate": "INCREASES",
    "object": "NEW TASK STARTUP TIME"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "QUEUES",
    "object": "A SET OF TASKS RECEIVED FROM THE CLIENTS"
  },
  {
    "subject": "A SCHEDULING POLICY",
    "predicate": "DECIDES",
    "object": "WHICH TASK TO EXECUTE NEXT"
  },
  {
    "subject": "THE SCHEDULING",
    "predicate": "IS",
    "object": "PREEMPTIVE"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "CAN PREEMPT",
    "object": "THE CURRENT TASK FOR THE NEXT ONE"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "PREEMPTS BASED ON",
    "object": "THE SCHEDULING POLICY"
  },
  {
    "subject": "CANONICAL SCHEDULING POLICIES",
    "predicate": "INCLUDE",
    "object": "RST COME RST SERVE (FCFS)"
  },
  {
    "subject": "CANONICAL SCHEDULING POLICIES",
    "predicate": "INCLUDE",
    "object": "EARLIEST DEADLINE RST (EDF)"
  },
  {
    "subject": "WE",
    "predicate": "FOCUS ON",
    "object": "FAST CONTEXT SWITCHING"
  },
  {
    "subject": "THE SPECIFIC SCHEDULING ALGORITHM",
    "predicate": "IS",
    "object": "ORTHOGONAL TO THIS PAPER"
  },
  {
    "subject": "CONTROLLER",
    "predicate": "CAN PREEMPT",
    "object": "CURRENT TASK"
  },
  {
    "subject": "CURRENT TASK",
    "predicate": "IS",
    "object": "TRAINING TASK"
  },
  {
    "subject": "INFERENCE TASK",
    "predicate": "HAS",
    "object": "STRICT LATENCY SLO"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "WAITS FOR",
    "object": "THE CURRENT TASK TO FINISH"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "PREEMPTS",
    "object": "THE CURRENT TASK"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "NOTIFIES",
    "object": "THE ACTIVE WORKER TO STOP"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "NOTIFIES",
    "object": "AN IDLE STANDBY WORKER"
  },
  {
    "subject": "AN IDLE STANDBY WORKER",
    "predicate": "INITIALIZES",
    "object": "ITS ENVIRONMENT FOR THE NEW TASK"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "SCHEDULES",
    "object": "A TASK"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "ALLOCATES",
    "object": "THE MEMORY TO THE STANDBY WORKER (4.3)"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "TRANSMITS",
    "object": "THE MODEL USED BY THE NEW TASK FROM THE HOST MEMORY TO THE GPU MEMORY"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "TRANSMITS",
    "object": "THE MODEL FROM THE HOST MEMORY TO THE GPU MEMORY"
  },
  {
    "subject": "THE TRANSMISSION",
    "predicate": "ELIMINATES",
    "object": "THE EXTRA MEMORY COPY FROM THE MEMORY DAEMON TO THE WORKER"
  },
  {
    "subject": "MODEL",
    "predicate": "IS TRANSMITTED TO",
    "object": "GPU"
  },
  {
    "subject": "MEMORY DAEMON",
    "predicate": "NEEDS TO NOTIFY",
    "object": "WORKER"
  },
  {
    "subject": "MEMORY DAEMON",
    "predicate": "NEEDS TO EXPORT",
    "object": "RELEVANT GPU MEMORY HANDLERS"
  },
  {
    "subject": "RELEVANT GPU MEMORY HANDLERS",
    "predicate": "ARE EXPORTED TO",
    "object": "WORKER"
  },
  {
    "subject": "WORKER",
    "predicate": "CAN ACCESS",
    "object": "MODEL"
  },
  {
    "subject": "WORKER",
    "predicate": "CAN EXECUTE",
    "object": "ITS TASK"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "HANDLES",
    "object": "GPU MEMORY ALLOCATION"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "HANDLES",
    "object": "MODEL TRANSMISSION"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "CREATES",
    "object": "GPU MEMORY HANDLERS"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "SENDS",
    "object": "GPU MEMORY HANDLERS TO WORKERS"
  },
  {
    "subject": "THE PRIMARY GOAL OF THIS PAPER",
    "predicate": "IS TO DESIGN",
    "object": "A SET OF TECHNIQUES BASED ON THE CHARACTERISTICS OF DL APPLICATIONS"
  },
  {
    "subject": "THE SET OF TECHNIQUES",
    "predicate": "IS TO MINIMIZE",
    "object": "THE TASK SWITCHING OVERHEAD IN THIS PROCESS"
  },
  {
    "subject": "THE TASK SWITCHING OVERHEAD",
    "predicate": "IS BROKEN DOWN TO",
    "object": "INDIVIDUAL COMPONENTS"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "END-TO-END EXPERIMENTS"
  },
  {
    "subject": "END-TO-END EXPERIMENTS",
    "predicate": "DEMONSTRATE",
    "object": "THE BENEFITS OF PIPESWITCH"
  },
  {
    "subject": "WE",
    "predicate": "SHOW",
    "object": "THE EFFECTIVENESS OF THE DESIGN CHOICES ON EACH COMPONENT"
  },
  {
    "subject": "WE",
    "predicate": "DESCRIBE",
    "object": "OUR DESIGN"
  },
  {
    "subject": "OUR DESIGN",
    "predicate": "MINIMIZES",
    "object": "THE OVERHEAD OF EACH COMPONENT"
  },
  {
    "subject": "THE MEA-502 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION",
    "predicate": "IS ORGANIZED BY",
    "object": "USENIX ASSOCIATION"
  },
  {
    "subject": "SUREMENT",
    "predicate": "CONSIDERS",
    "object": "A TYPICAL SCENARIO"
  },
  {
    "subject": "A SERVER",
    "predicate": "STOPS",
    "object": "A TRAINING TASK RUNNING ON THE GPU"
  },
  {
    "subject": "A SERVER",
    "predicate": "STARTS",
    "object": "AN INFERENCE TASK"
  },
  {
    "subject": "THE DNN MODEL",
    "predicate": "IS",
    "object": "RESNET152 17"
  },
  {
    "subject": "THE MEASUREMENT",
    "predicate": "COVERS",
    "object": "TWO TYPES OF INSTANCES ON AMAZON AWS"
  },
  {
    "subject": "TWO TYPES OF INSTANCES ON AMAZON AWS",
    "predicate": "ARE",
    "object": "G4DN.2XLARGE WITH NVIDIA T4"
  },
  {
    "subject": "TWO TYPES OF INSTANCES ON AMAZON AWS",
    "predicate": "ARE",
    "object": "P3.2XLARGE WITH NVIDIA V100"
  },
  {
    "subject": "INFERENCE TASK",
    "predicate": "HAS ARRIVED AT",
    "object": "THE SERVER"
  },
  {
    "subject": "WE",
    "predicate": "FOCUS ON",
    "object": "MEASURING THE TIME TO START AND EXECUTE IT ON THE GPU"
  },
  {
    "subject": "WE",
    "predicate": "EXCLUDE",
    "object": "THE NETWORK TIME"
  },
  {
    "subject": "WE",
    "predicate": "EXCLUDE",
    "object": "THE TASK QUEUEING TIME"
  },
  {
    "subject": "TABLE 1",
    "predicate": "SHOWS",
    "object": "THE RESULTS"
  },
  {
    "subject": "TOTAL TIMES TO START THE INFERENCE TASK ON THE GPUS",
    "predicate": "ARE",
    "object": "5787 MS AND 7551 MS"
  },
  {
    "subject": "WE",
    "predicate": "BREAK DOWN",
    "object": "THE OVERHEAD INTO THE FOUR COMPONENTS"
  },
  {
    "subject": "TASK",
    "predicate": "IS",
    "object": "CLEANING"
  },
  {
    "subject": "TASK CLEANING",
    "predicate": "TAKES",
    "object": "TIME"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "CREATES AND INITIALIZES",
    "object": "ITS ENVIRONMENT"
  },
  {
    "subject": "ITS ENVIRONMENT",
    "predicate": "INCLUDES",
    "object": "PROCESS LAUNCHING"
  },
  {
    "subject": "ITS ENVIRONMENT",
    "predicate": "INCLUDES",
    "object": "PYTORCH CUDA RUNTIME LOADING"
  },
  {
    "subject": "ITS ENVIRONMENT",
    "predicate": "INCLUDES",
    "object": "CUDA CONTEXT INITIALIZATION"
  },
  {
    "subject": "MEMORY",
    "predicate": "HAS",
    "object": "ALLOCATION"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "ALLOCATES",
    "object": "GPU MEMORY"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "ALLOCATES",
    "object": "ITS NEURAL NETWORK MODEL"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "TRANSMITS",
    "object": "THE MODEL"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "TRANSMITS FROM",
    "object": "THE HOST MEMORY"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "TRANSMITS TO",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "MODEL",
    "predicate": "IS",
    "object": "TRANSMISSION"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "IS",
    "object": "TEXT"
  },
  {
    "subject": "TASK SWITCHING",
    "predicate": "DEPENDS ON",
    "object": "CPU"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "IS EQUIPPED WITH",
    "object": "BETTER CPU THAN P3.2XLARGE"
  },
  {
    "subject": "BETTER CPU",
    "predicate": "IS",
    "object": "INTEL PLATINUM 8259CL"
  },
  {
    "subject": "CPU IN P3.2XLARGE",
    "predicate": "IS",
    "object": "INTEL XEON E5-2686 V4"
  },
  {
    "subject": "LOWER OVERHEAD ON T4",
    "predicate": "IS BECAUSE",
    "object": "TASK SWITCHING LARGELY DEPENDS ON CPU"
  },
  {
    "subject": "A STRAWMAN SOLUTION",
    "predicate": "STOPS",
    "object": "THE OLD TASK"
  },
  {
    "subject": "A STRAWMAN SOLUTION",
    "predicate": "STARTS",
    "object": "THE NEW TASK"
  },
  {
    "subject": "A STRAWMAN SOLUTION THAT SIMPLY STOPS THE OLD TASK AND STARTS THE NEW TASK",
    "predicate": "WOULD VIOLATE",
    "object": "SLOS"
  },
  {
    "subject": "ALL THE COMPONENTS",
    "predicate": "TAKE",
    "object": "CONSIDERABLE TIME COMPARED TO THE INFERENCE TIME"
  },
  {
    "subject": "WE",
    "predicate": "EMPHASIZE",
    "object": "ALL THE COMPONENTS SHOULD BE OPTIMIZED TO ACHIEVE MINIMAL SWITCHING OVERHEAD AND MEET THE SLOS"
  },
  {
    "subject": "THE PCIE BANDWIDTH",
    "predicate": "IS",
    "object": "THE PHYSICAL LIMIT ON HOW FAST AN ARBITRARY TASK CAN BE LOADED TO THE GPU"
  },
  {
    "subject": "WE",
    "predicate": "EXPLOIT",
    "object": "THE CHARACTERISTICS OF DL APPLICATIONS TO CIRCUMVENT THIS PHYSICAL LIMIT"
  },
  {
    "subject": "THE COMPUTATION",
    "predicate": "IS PERFORMED",
    "object": "LAYER BY LAYER"
  },
  {
    "subject": "AN INFERENCE TASK",
    "predicate": "PERFORMS",
    "object": "A FORWARD PASS FROM THE RST LAYER TO THE NAL LAYER"
  },
  {
    "subject": "AN INFERENCE TASK",
    "predicate": "PURPOSE",
    "object": "MAKE A PREDICTION"
  },
  {
    "subject": "EACH ITERATION IN A TRAINING TASK",
    "predicate": "PERFORMS",
    "object": "A FORWARD PASS"
  },
  {
    "subject": "EACH ITERATION IN A TRAINING TASK",
    "predicate": "PERFORMS",
    "object": "A BACKWARD PASS"
  },
  {
    "subject": "THE TASK",
    "predicate": "CAN START",
    "object": "THE COMPUTATION OF A LAYER"
  },
  {
    "subject": "THE COMPUTATION OF A LAYER",
    "predicate": "STARTS AS SOON AS",
    "object": "THE LAYER IS LOADED IN THE GPU"
  },
  {
    "subject": "THE COMPUTATION OF A LAYER",
    "predicate": "STARTS AS SOON AS",
    "object": "THE INPUT OF THE LAYER IS READY"
  },
  {
    "subject": "THE INPUT OF THE LAYER",
    "predicate": "IS READY WHEN",
    "object": "THE PREVIOUS LAYERS HAVE FINISHED THEIR COMPUTATION"
  },
  {
    "subject": "FIGURE 2",
    "predicate": "ILLUSTRATES",
    "object": "THE ADVANTAGE OF PIPELINING OVER THE STRAWMAN SOLUTION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "REQUIRES",
    "object": "THE KNOWLEDGE OF MODELS"
  },
  {
    "subject": "PIPELINING MECHANISM",
    "predicate": "HAS",
    "object": "OPTIMAL MODEL-AWARE GROUPING IN PIPESWITCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "USES",
    "object": "MODEL-AWARE GROUPING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ACHIEVES",
    "object": "THE BEST TRADE-OFF BETWEEN PIPELINE OVERHEAD AND EFFICIENCY"
  },
  {
    "subject": "MODEL",
    "predicate": "TRANSMITS OVER",
    "object": "PCIE"
  },
  {
    "subject": "TASK",
    "predicate": "EXECUTES ON",
    "object": "GPU"
  },
  {
    "subject": "MODEL",
    "predicate": "TRANSMITS TO",
    "object": "GPU"
  },
  {
    "subject": "PCIE GPU E0 E1 EN-1 E2 (B)",
    "predicate": "PIPELINES",
    "object": "MODEL TRANSMISSION AND TASK EXECUTION"
  },
  {
    "subject": "THE EXAMPLE",
    "predicate": "SHOWS",
    "object": "AN INFERENCE TASK"
  },
  {
    "subject": "AN INFERENCE TASK",
    "predicate": "HAS",
    "object": "ONLY A FORWARD PASS IN TASK EXECUTION"
  },
  {
    "subject": "ADDING HOOKS",
    "predicate": "CAN BE",
    "object": "AUTOMATED"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "CAN BE IMPLEMENTED AS",
    "object": "A PART OF THE DNN FRAMEWORK"
  },
  {
    "subject": "DNN FRAMEWORK",
    "predicate": "EXAMPLES",
    "object": "PYTORCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "CAN GATHER",
    "object": "MODEL STRUCTURE INFORMATION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "REMAINS",
    "object": "TRANSPARENT TO USERS AND CLUSTER MANAGERS"
  },
  {
    "subject": "THE BASIC WAY FOR PIPELINING",
    "predicate": "IS",
    "object": "TO PIPELINE ON PER-LAYER GRANULARITY"
  },
  {
    "subject": "THE SYSTEM",
    "predicate": "TRANSMITS",
    "object": "THE LAYERS TO THE GPU MEMORY ONE BY ONE"
  },
  {
    "subject": "THE COMPUTATION FOR A LAYER",
    "predicate": "IS BLOCKED BEFORE",
    "object": "THE LAYER IS TRANSMITTED"
  },
  {
    "subject": "ONE",
    "predicate": "IS",
    "object": "THE OVERHEAD TO INVOKE MULTIPLE CALLS TO PCIE TO TRANSMIT THE DATA"
  },
  {
    "subject": "TRANSMISSION OVERHEAD",
    "predicate": "IS DOMINATED BY",
    "object": "DATA SIZE"
  },
  {
    "subject": "DIVIDING THE MODEL INTO MANY LAYERS",
    "predicate": "CAUSES",
    "object": "SIGNIFICANT EXTRA OVERHEAD"
  },
  {
    "subject": "INVOKING A PCIE CALL FOR EACH LAYER",
    "predicate": "CAUSES",
    "object": "SIGNIFICANT EXTRA OVERHEAD"
  },
  {
    "subject": "SOME LAYERS",
    "predicate": "CAN BE",
    "object": "VERY SMALL"
  },
  {
    "subject": "SYNCHRONIZATION OVERHEAD",
    "predicate": "IS",
    "object": "THE OTHER"
  },
  {
    "subject": "SYNCHRONIZATION OVERHEAD",
    "predicate": "IS BETWEEN",
    "object": "TRANSMISSION AND COMPUTATION"
  },
  {
    "subject": "SYNCHRONIZATION OVERHEAD",
    "predicate": "IS NECESSARY FOR",
    "object": "THE COMPUTATION TO KNOW WHEN A LAYER IS READY TO COMPUTE"
  },
  {
    "subject": "WE",
    "predicate": "COMBINE",
    "object": "MULTIPLE LAYERS INTO A GROUP"
  },
  {
    "subject": "PIPELINING",
    "predicate": "IS PERFORMED ON",
    "object": "PER-GROUP GRANULARITY"
  },
  {
    "subject": "PIPELINING OVERHEAD",
    "predicate": "IS PAID",
    "object": "ONCE FOR EACH GROUP"
  },
  {
    "subject": "PIPELINING OVERHEAD",
    "predicate": "IS PAID",
    "object": "INSTEAD OF EACH LAYER"
  },
  {
    "subject": "GROUPING",
    "predicate": "INTRODUCES",
    "object": "A TRADE-OFF BETWEEN PIPELINING EFFICIENCY AND PIPELINING OVERHEAD"
  },
  {
    "subject": "USING SMALL GROUPS",
    "predicate": "ENABLES",
    "object": "MORE OVERLAP BETWEEN TRANSMISSION AND COMPUTATION"
  },
  {
    "subject": "MORE OVERLAP BETWEEN TRANSMISSION AND COMPUTATION",
    "predicate": "IMPROVES",
    "object": "PIPELINING EFFICIENCY"
  },
  {
    "subject": "USING SMALL GROUPS",
    "predicate": "PAYS",
    "object": "MORE PIPELINING OVERHEAD"
  },
  {
    "subject": "USING BIG GROUPS",
    "predicate": "HAS",
    "object": "MINIMAL PIPELINING OVERHEAD"
  },
  {
    "subject": "USING BIG GROUPS",
    "predicate": "REDUCES",
    "object": "THE CHANCE FOR OVERLAPPING"
  },
  {
    "subject": "GROUPING",
    "predicate": "MUST BE",
    "object": "MODEL-AWARE"
  },
  {
    "subject": "MODELS",
    "predicate": "HAVE",
    "object": "DIFFERENT STRUCTURES"
  },
  {
    "subject": "DIFFERENT STRUCTURES",
    "predicate": "ARE IN TERMS OF",
    "object": "THE NUMBER OF LAYERS"
  },
  {
    "subject": "DIFFERENT STRUCTURES",
    "predicate": "ARE IN TERMS OF",
    "object": "THE SIZE OF EACH LAYER"
  },
  {
    "subject": "WE",
    "predicate": "CAN ENUMERATE",
    "object": "ALL POSSIBLE COMBINATIONS"
  },
  {
    "subject": "ALL POSSIBLE COMBINATIONS",
    "predicate": "ARE USED TO FIND",
    "object": "THE OPTIMAL GROUPING STRATEGY"
  },
  {
    "subject": "WE",
    "predicate": "INTRODUCE",
    "object": "TWO PRUNING TECHNIQUES"
  },
  {
    "subject": "TWO PRUNING TECHNIQUES",
    "predicate": "ARE BASED ON",
    "object": "TWO INSIGHTS"
  },
  {
    "subject": "LARGE MODELS",
    "predicate": "CAN HAVE",
    "object": "HUNDREDS OF LAYERS"
  },
  {
    "subject": "TIME COMPLEXITY FOR ENUMERATION",
    "predicate": "IS",
    "object": "EXPONENTIAL"
  },
  {
    "subject": "PCIE GPU",
    "predicate": "HAS",
    "object": "LOWER BOUND OF F(GROUP(0, I), I1)"
  },
  {
    "subject": "GROUP(0, I)",
    "predicate": "IS RELATED TO",
    "object": "GROUP(I1, J)"
  },
  {
    "subject": "J, J1",
    "predicate": "RANGE",
    "object": "N-1"
  },
  {
    "subject": "CASE (A)",
    "predicate": "IS PRUNED IF",
    "object": "LOWER BOUND CURRENT OPTIMAL TIME"
  },
  {
    "subject": "PRUNE",
    "predicate": "THE CASES THAT GROUP",
    "object": "FROM I TO J"
  },
  {
    "subject": "BATCH",
    "predicate": "AT LEAST",
    "object": "FROM LAYER I1 TO J"
  },
  {
    "subject": "FIGURE 3",
    "predicate": "SHOWS",
    "object": "EXAMPLES FOR TWO PRUNING TECHNIQUES"
  },
  {
    "subject": "WE",
    "predicate": "CAN PRUNE",
    "object": "THE CASES THAT GROUP FROM LAYER (I 1) TO J J"
  },
  {
    "subject": "WE",
    "predicate": "ONLY SEARCH FOR",
    "object": "J J"
  },
  {
    "subject": "WE",
    "predicate": "FORMULATE",
    "object": "THE PROBLEM"
  },
  {
    "subject": "F(B,I)",
    "predicate": "IS",
    "object": "A FUNCTION"
  },
  {
    "subject": "F(B,I)",
    "predicate": "RETURNS",
    "object": "THE TOTAL TIME OF THE OPTIMAL GROUPING STRATEGY FROM LAYER I TO N-1"
  },
  {
    "subject": "N",
    "predicate": "IS",
    "object": "THE NUMBER OF LAYERS"
  },
  {
    "subject": "B",
    "predicate": "REPRESENTS",
    "object": "GROUPS FORMED BY LAYER 0 TO I-1"
  },
  {
    "subject": "THE FUNCTION",
    "predicate": "APPLIES ITSELF TO",
    "object": "ND THE OPTIMAL GROUPS FROM LAYER I1 TO N-1"
  },
  {
    "subject": "THE FUNCTION",
    "predicate": "UPDATES",
    "object": "OPTGROUPS"
  },
  {
    "subject": "THE FUNCTION",
    "predicate": "APPLIES ITSELF RECURSIVELY",
    "object": "ND THE OPTIMAL GROUPS FROM LAYER I1 TO N-1"
  },
  {
    "subject": "OPTGROUPS",
    "predicate": "IS UPDATED IF",
    "object": "THE CURRENT STRATEGY IS BETTER"
  },
  {
    "subject": "WE",
    "predicate": "DIVIDE",
    "object": "ALL POSSIBLE COMBINATIONS INTO N CASES"
  },
  {
    "subject": "CASE I",
    "predicate": "MEANS",
    "object": "THE FIRST GROUP CONTAINS LAYER 0 TO I"
  },
  {
    "subject": "THIS FORMULA",
    "predicate": "CAN BE APPLIED",
    "object": "RECURSIVELY"
  },
  {
    "subject": "THIS FORMULA",
    "predicate": "CAN BE APPLIED TO COMPUTE",
    "object": "F(GROUP(0,I),I1)"
  },
  {
    "subject": "OUR RST INSIGHT",
    "predicate": "IS",
    "object": "IT IS NOT NECESSARY TO EXAMINE ALL THE N CASES"
  },
  {
    "subject": "THE RST GROUP",
    "predicate": "CONTAINS",
    "object": "TOO MANY LAYERS"
  },
  {
    "subject": "THE COMPUTATION OF THE RST GROUP",
    "predicate": "WOULD BE",
    "object": "DELAYED TOO MUCH"
  },
  {
    "subject": "THE DELAY",
    "predicate": "WOULD",
    "object": "COMPENSATE THE PIPELINE EFFICIENCY"
  },
  {
    "subject": "WE",
    "predicate": "CAN PACK",
    "object": "MULTIPLE LAYERS IN A GROUP BASED ON THE PROGRESS OF COMPUTATION"
  },
  {
    "subject": "PACKING MULTIPLE LAYERS IN A GROUP",
    "predicate": "DOES NOT AFFECT",
    "object": "PIPELINE EFFICIENCY"
  },
  {
    "subject": "OTHER THAN THE RST GROUP",
    "predicate": "IS EXCLUDED FROM",
    "object": "SAFELY PACKING MULTIPLE LAYERS IN A GROUP"
  },
  {
    "subject": "T(I, J)",
    "predicate": "IS",
    "object": "TRANSMISSION TIME FOR A GROUP FROM LAYER I TO J"
  },
  {
    "subject": "E(I, J)",
    "predicate": "IS",
    "object": "EXECUTION TIME FOR A GROUP FROM LAYER I TO J"
  },
  {
    "subject": "T(I, J)",
    "predicate": "IS CALCULATED BASED ON",
    "object": "SIZE OF LAYER I TO J AND PCIE BANDWIDTH"
  },
  {
    "subject": "E(I, J)",
    "predicate": "IS PROFILED ON",
    "object": "GPU"
  },
  {
    "subject": "THE OVERHEAD OF INVOKING MULTIPLE CALLS",
    "predicate": "IS INCLUDED IN",
    "object": "T(I, J)"
  },
  {
    "subject": "WE",
    "predicate": "COMPUTE",
    "object": "A LOWER BOUND FOR THE TOTAL TIME FOR EACH CASE IN EQUATION 1"
  },
  {
    "subject": "THE LOWER BOUND",
    "predicate": "CONSIDERS",
    "object": "THE BEST CASE THAT ALL THE REMAINING LAYERS ARE COMBINED IN ONE GROUP FOR TRANSMISSION AND COMPUTATION"
  },
  {
    "subject": "THE COMPUTATION AND COMMUNICATION",
    "predicate": "CAN BE",
    "object": "PERFECTLY OVERLAPPED"
  },
  {
    "subject": "ITS COMPUTATION",
    "predicate": "CAN HAPPEN",
    "object": "RIGHT AFTER THE COMPUTATION OF THE FIRST GROUP FINISHES"
  },
  {
    "subject": "THE LOWER BOUND OF CASE I",
    "predicate": "IS LARGER THAN",
    "object": "THE TOTAL TIME OF THE BEST GROUPING STRATEGY FOUND SO FAR"
  },
  {
    "subject": "CASE I",
    "predicate": "CAN BE PRUNED",
    "object": "IF THE LOWER BOUND OF CASE I IS ALREADY LARGER THAN THE TOTAL TIME OF THE BEST GROUPING STRATEGY FOUND SO FAR"
  },
  {
    "subject": "FIGURE 3(B)",
    "predicate": "SHOWS",
    "object": "AN EXAMPLE FOR THIS INSIGHT"
  },
  {
    "subject": "WE",
    "predicate": "CAN HIDE",
    "object": "THE TRANSMISSION OF THE SECOND GROUP INTO THE COMPUTATION OF THE RST GROUP"
  },
  {
    "subject": "THE TRANSMISSION",
    "predicate": "NISHES NO LATER THAN",
    "object": "THE COMPUTATION OF THE RST GROUP"
  },
  {
    "subject": "THE LEAST NUMBER OF LAYERS TO GROUP",
    "predicate": "CAN BE COMPUTED USING",
    "object": "THE FOLLOWING EQUATION"
  },
  {
    "subject": "JIS",
    "predicate": "IS NO BETTER THAN",
    "object": "GROUPING FROM (I1) TO J"
  },
  {
    "subject": "GROUPING FROM (I1) TO J",
    "predicate": "DOES NOT INCREASE",
    "object": "PIPELINE EFFICIENCY"
  },
  {
    "subject": "JIS",
    "predicate": "HAS",
    "object": "HIGHER PIPELINE OVERHEAD"
  },
  {
    "subject": "ALGORITHM",
    "predicate": "RUNS",
    "object": "OFFLINE TO FIND THE STRATEGY"
  },
  {
    "subject": "RESULTING STRATEGY",
    "predicate": "IS USED BY",
    "object": "PIPESWITCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "USES",
    "object": "RESULTING STRATEGY FOR CONTEXT SWITCHING"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "SHOWS",
    "object": "THE PSEUDO CODE"
  },
  {
    "subject": "THE FUNCTION FINDOPTGROUPING",
    "predicate": "FINDS",
    "object": "THE OPTIMAL GROUPING STRATEGY"
  },
  {
    "subject": "THE FUNCTION FINDOPTGROUPING",
    "predicate": "FINDS",
    "object": "THE OPTIMAL GROUPING STRATEGY BASED ON EQUATION 1"
  },
  {
    "subject": "EQUATION 1",
    "predicate": "IS LOCATED AT",
    "object": "LINE 1-27"
  },
  {
    "subject": "B",
    "predicate": "REPRESENTS",
    "object": "THE GROUPS THAT HAVE ALREADY FORMED"
  },
  {
    "subject": "X",
    "predicate": "IS",
    "object": "THE RST LAYER THAT HAVE NOT FORMED A GROUP"
  },
  {
    "subject": "THE RST GROUP",
    "predicate": "CONTAINS",
    "object": "ALL LAYERS FROM X TO N1"
  },
  {
    "subject": "OPTGROUPS",
    "predicate": "STORE",
    "object": "THE BEST GROUPING STRATEGY FROM LAYER X GIVEN B"
  },
  {
    "subject": "B",
    "predicate": "IS INITIALIZED TO",
    "object": "NONE"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "APPLIES",
    "object": "THE SECOND PRUNING INSIGHT"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "FORMS",
    "object": "THE RST GROUP FROM LAYER X (LINE 3-9)"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "DIVIDES",
    "object": "THE PROBLEM INTO K 1 CASES"
  },
  {
    "subject": "CASE I",
    "predicate": "FORMS",
    "object": "THE RST GROUP FROM LAYER X TO XI"
  },
  {
    "subject": "EQUATION 3 AND FIGURE 3(B)",
    "predicate": "ILLUSTRATE",
    "object": "THIS INSIGHT"
  },
  {
    "subject": "B",
    "predicate": "CONTAINS",
    "object": "ONE GROUP FROM LAYER 0 TO I"
  },
  {
    "subject": "B",
    "predicate": "CAN CONTAIN",
    "object": "MULTIPLE GROUPS FORMED BY PREVIOUS LAYERS"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "B.DELAY TO DENOTE THE TIME TO WHICH THE GROUP CAN BE FORMED"
  },
  {
    "subject": "B.DELAY",
    "predicate": "DENOTES",
    "object": "THE TIME TO WHICH THE GROUP CAN BE FORMED"
  },
  {
    "subject": "THE ALGORITHM NDS",
    "predicate": "IS BASED ON",
    "object": "B.DELAY (LINE 4-9)"
  },
  {
    "subject": "THE ENUMERATION FOR I",
    "predicate": "CAN SKIP",
    "object": "THE LAYERS FROM X TO J-1 (LINE 11)"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "APPLIES",
    "object": "THE RST INSIGHT"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "COMPUTES",
    "object": "THE LOWER BOUND"
  },
  {
    "subject": "EXAMPLE IN EQUATION 2 AND FIGURE 3(A)",
    "predicate": "IS",
    "object": "A SPECIAL CASE"
  },
  {
    "subject": "X",
    "predicate": "IS",
    "object": "0"
  },
  {
    "subject": "COMPUTATION FROM X",
    "predicate": "HAS TO WAIT FOR",
    "object": "ITS TRANSMISSION (T(X,I))"
  },
  {
    "subject": "COMPUTATION FROM X",
    "predicate": "HAS TO WAIT FOR",
    "object": "COMPUTATION OF THE PREVIOUS GROUPS (B.DELAY)"
  },
  {
    "subject": "COMPUTATION FROM X",
    "predicate": "IS SHOWN IN",
    "object": "FIGURE 4"
  },
  {
    "subject": "LOWER BOUND",
    "predicate": "IS BIGGER THAN",
    "object": "CURRENT OPTIMAL TIME"
  },
  {
    "subject": "CASE I",
    "predicate": "IS PRUNED",
    "object": "LINE 18-19"
  },
  {
    "subject": "THE TWO PRUNING TECHNIQUES",
    "predicate": "ARE ABLE TO PRUNE",
    "object": "MOST OF THE STRATEGIES"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "USES",
    "object": "TWO PRUNING TECHNIQUES"
  },
  {
    "subject": "M N X",
    "predicate": "IS",
    "object": "THE NUMBER OF LAYERS THE FUNCTION CONSIDERS"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "INDUCTION ON M"
  },
  {
    "subject": "FINDOPTGROUPING(B,X)",
    "predicate": "OUTPUTS",
    "object": "THE OPTIMAL GROUPING STRATEGY FROM LAYER X TO N 1"
  },
  {
    "subject": "PREVIOUS LAYERS",
    "predicate": "HAVE FORMED",
    "object": "GROUPS REPRESENTED BY B"
  },
  {
    "subject": "K",
    "predicate": "IS",
    "object": "SOME K 1"
  },
  {
    "subject": "M",
    "predicate": "IS",
    "object": "ANY M K"
  },
  {
    "subject": "FINDOPTGROUPING(B,X)",
    "predicate": "OUTPUTS",
    "object": "THE OPTIMAL STRATEGY"
  },
  {
    "subject": "FINDOPTGROUPING(B GROUP(X,X I),X I 1)",
    "predicate": "ONLY CONSIDERS",
    "object": "K I K LAYERS"
  },
  {
    "subject": "FINDOPTGROUPING(B GROUP(X,X I),X I 1)",
    "predicate": "OUTPUTS",
    "object": "THE OPTIMAL GROUPING STRATEGY FOR CASE I"
  },
  {
    "subject": "THE OPTIMAL GROUPING STRATEGY FOR CASE I",
    "predicate": "IS BASED ON",
    "object": "THE ASSUMPTION"
  },
  {
    "subject": "M",
    "predicate": "EQUALS",
    "object": "1"
  },
  {
    "subject": "THE FUNCTION",
    "predicate": "ONLY EXAMINES",
    "object": "ONE LAYER"
  },
  {
    "subject": "LAYER X",
    "predicate": "IS",
    "object": "ONE GROUP"
  },
  {
    "subject": "THIS STRATEGY",
    "predicate": "IS",
    "object": "THE OPTIMAL STRATEGY"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "CONSIDERS",
    "object": "K1 LAYERS"
  },
  {
    "subject": "THE OPTIMAL STRATEGY FOR THIS CASE",
    "predicate": "IS",
    "object": "ONE GROUP"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "OUTPUTS",
    "object": "THE OPTIMAL GROUPING STRATEGY FOR M K 1"
  },
  {
    "subject": "THESE CASES",
    "predicate": "ARE",
    "object": "EXCLUSIVE"
  },
  {
    "subject": "THESE CASES",
    "predicate": "COVER",
    "object": "THE ENTIRE SEARCH SPACE"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "CHOOSES",
    "object": "THE OPTIMAL GROUPING STRATEGY FROM THESE CASES"
  },
  {
    "subject": "THE RST TECHNIQUE",
    "predicate": "PRUNES",
    "object": "THE CASES"
  },
  {
    "subject": "THE LOWER BOUNDS",
    "predicate": "ARE",
    "object": "NO BETTER THAN THE CURRENT FOUND OPTIMAL"
  },
  {
    "subject": "THIS TECHNIQUE",
    "predicate": "DOES NOT AFFECT",
    "object": "THE OPTIMALITY"
  },
  {
    "subject": "THE SECOND TECHNIQUE",
    "predicate": "PRUNES",
    "object": "THE CASE"
  },
  {
    "subject": "THEIR RST GROUPS",
    "predicate": "ARE FROM",
    "object": "LAYER X TO J J"
  },
  {
    "subject": "THESE CASES",
    "predicate": "CANNOT ADVANCE",
    "object": "THE COMPUTATION TO AN EARLIER POINT THAN GROUPING FROM X TO AT LEAST J"
  },
  {
    "subject": "PRUNING THESE CASES",
    "predicate": "DO NOT AFFECT",
    "object": "THE OPTIMALITY"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "ACHIEVES",
    "object": "OPTIMALITY"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "ACHIEVES OPTIMALITY FOR",
    "object": "A GIVEN LIST OF LAYERS"
  },
  {
    "subject": "LAYERS OR OPERATORS IN A DNN MODEL",
    "predicate": "CAN BE CONNECTED AS",
    "object": "AN ARBITRARY COMPUTATION GRAPH"
  },
  {
    "subject": "MODELS LIKE RESNET AND INCEPTION",
    "predicate": "ARE",
    "object": "TECHNICALLY NON-LINEAR DIRECTED ACYCLIC GRAPH (DAGS)"
  },
  {
    "subject": "EXECUTION ORDER",
    "predicate": "IS",
    "object": "ISSUED TO THE GPU ONE BY ONE"
  },
  {
    "subject": "LAYERSOPERATORS IN THE DAG",
    "predicate": "ARE ISSUED TO",
    "object": "THE GPU ONE BY ONE"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "DOES NOT HAVE",
    "object": "ANY SPECIAL ASSUMPTIONS ON THE EXECUTION ORDER"
  },
  {
    "subject": "GROUPING THE LAYERS",
    "predicate": "ACHIEVES",
    "object": "HIGH PIPELINING EFFICIENCY AND LOW PIPELINING OVERHEAD"
  },
  {
    "subject": "ORDER",
    "predicate": "IS BASED ON",
    "object": "THE RST TIME AN OPERATOR IS EXECUTED"
  },
  {
    "subject": "ORDER",
    "predicate": "DOES NOT AFFECT",
    "object": "CORRECTNESS"
  },
  {
    "subject": "OPERATOR",
    "predicate": "CAN BE EXECUTED",
    "object": "ONLY WHEN IT IS TRANSMITTED TO THE GPU AND THE INPUT IS READY"
  },
  {
    "subject": "OUR PIPELINED MODEL TRANSMISSION",
    "predicate": "IS APPLICABLE TO",
    "object": "THE GENERAL CASE"
  },
  {
    "subject": "PIPELINED MODEL TRANSMISSION",
    "predicate": "IS USED TO",
    "object": "EVALUATE THE EFFECTIVENESS OF PIPELINED MODEL TRANSMISSION"
  },
  {
    "subject": "FIGURE 7",
    "predicate": "DESCRIBES",
    "object": "EFFECTIVENESS OF PIPELINED MODEL TRANSMISSION"
  },
  {
    "subject": "UNIFIED MEMORY MANAGEMENT TASK EXECUTION IN A GPU",
    "predicate": "REQUIRES",
    "object": "GPU MEMORY"
  },
  {
    "subject": "A GPU",
    "predicate": "HAS",
    "object": "ITS OWN MEMORY MANAGEMENT SYSTEM"
  },
  {
    "subject": "A GPU",
    "predicate": "PROVIDES",
    "object": "A MALLOC FUNCTION"
  },
  {
    "subject": "MALLOC FUNCTION",
    "predicate": "IS SIMILAR TO",
    "object": "CPUS FOR MEMORY ALLOCATION"
  },
  {
    "subject": "MALLOC FUNCTION",
    "predicate": "INCLUDES",
    "object": "CUDAMALLOC FOR NVIDIA GPUS"
  },
  {
    "subject": "EACH TASK",
    "predicate": "USES",
    "object": "NATIVE CUDAMALLOCMANAGED FUNCTION FOR GPU MEMORY ALLOCATION"
  },
  {
    "subject": "EACH TASK",
    "predicate": "DELEGATES",
    "object": "MODEL TRANSMISSION TO CUDA UNIFIED MEMORY"
  },
  {
    "subject": "WE",
    "predicate": "ADD",
    "object": "FUNCTIONS FOR ALLOCATING GPU MEMORY"
  },
  {
    "subject": "WE",
    "predicate": "ADD",
    "object": "FUNCTIONS FOR SHARING THE GPU MEMORY TO WORKERS THROUGH CUDA IPC API"
  },
  {
    "subject": "WE",
    "predicate": "ADD",
    "object": "FUNCTIONS FOR GETTING THE SHARED GPU MEMORY"
  },
  {
    "subject": "EACH WORKER",
    "predicate": "USES",
    "object": "CUDAMALLOC TO ALLOCATE GPU MEMORY"
  },
  {
    "subject": "EACH WORKER",
    "predicate": "TRANSMITS",
    "object": "THE MODEL TO GPU BY ITS OWN"
  },
  {
    "subject": "EACH WORKER",
    "predicate": "ALLOCATES",
    "object": "GPU MEMORY WITH CUDAMALLOCMANAGED"
  },
  {
    "subject": "CUDA",
    "predicate": "AUTOMATICALLY TRANSMITS",
    "object": "THE MODEL TO GPU WHEN NEEDED"
  },
  {
    "subject": "THIS SOLUTION",
    "predicate": "INCURS",
    "object": "HIGH OVERHEAD FOR DL APPLICATIONS"
  },
  {
    "subject": "HIGH OVERHEAD",
    "predicate": "IS BECAUSE OF",
    "object": "TWO REASONS"
  },
  {
    "subject": "NATIVE CUDAMALLOC FUNCTION",
    "predicate": "IS DESIGNED FOR",
    "object": "GENERAL-PURPOSE APPLICATIONS"
  },
  {
    "subject": "CUDA UNIFIED MEMORY",
    "predicate": "IS DESIGNED FOR",
    "object": "GENERAL-PURPOSE APPLICATIONS"
  },
  {
    "subject": "NATIVE CUDAMALLOC FUNCTION AND CUDA UNIFIED MEMORY",
    "predicate": "MAY INCUR",
    "object": "UNNECESSARY OVERHEAD FOR DL APPLICATIONS"
  },
  {
    "subject": "CUDA UNIFIED MEMORY",
    "predicate": "IS NOT OPTIMIZED FOR",
    "object": "DL APPLICATIONS"
  },
  {
    "subject": "CUDA UNIFIED MEMORY",
    "predicate": "INTRODUCES",
    "object": "MORE THAN ONE HUNDRED MILLISECONDS OVERHEAD THAN PIPESWITCH"
  },
  {
    "subject": "WE",
    "predicate": "EXPLOIT",
    "object": "TWO CHARACTERISTICS OF DL APPLICATIONS"
  },
  {
    "subject": "TWO CHARACTERISTICS OF DL APPLICATIONS",
    "predicate": "MINIMIZE",
    "object": "GPU MEMORY MANAGEMENT OVERHEAD"
  },
  {
    "subject": "THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT",
    "predicate": "DOES NOT CONSIDER",
    "object": "THESE CHARACTERISTICS"
  },
  {
    "subject": "THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT",
    "predicate": "IS",
    "object": "TOO HEAVY-WEIGHT FOR DL APPLICATIONS THAT REQUIRE FAST TASK SWITCHING"
  },
  {
    "subject": "AMOUNT OF MEMORY ALLOCATED TO THE DNN MODEL",
    "predicate": "IS",
    "object": "FIXED"
  },
  {
    "subject": "AMOUNT OF MEMORY ALLOCATED TO THE DNN MODEL",
    "predicate": "DOES NOT CHANGE DURING",
    "object": "TASK EXECUTION"
  },
  {
    "subject": "A TRAINING TASK",
    "predicate": "UPDATES",
    "object": "THE MODEL PARAMETERS"
  },
  {
    "subject": "THE MODEL PARAMETERS",
    "predicate": "ARE",
    "object": "THE WEIGHTS OF THE NEURAL NETWORK"
  },
  {
    "subject": "A TRAINING TASK",
    "predicate": "DOES NOT UPDATE",
    "object": "THE DNN STRUCTURE"
  },
  {
    "subject": "THE AMOUNT OF MEMORY NEEDED TO STORE THEM",
    "predicate": "STAYS",
    "object": "THE SAME"
  },
  {
    "subject": "INFERENCE TASK",
    "predicate": "USES",
    "object": "THE MODEL FOR INFERENCE"
  },
  {
    "subject": "INFERENCE TASK",
    "predicate": "DOES NOT CHANGE",
    "object": "THE MODEL ITSELF"
  },
  {
    "subject": "INTERMEDIATE RESULTS",
    "predicate": "CHANGE IN",
    "object": "A SIMPLE, REGULAR PATTERN"
  },
  {
    "subject": "INTERMEDIATE RESULTS",
    "predicate": "DO NOT CAUSE",
    "object": "MEMORY FRAGMENTATION"
  },
  {
    "subject": "INTERMEDIATE RESULTS",
    "predicate": "ARE",
    "object": "OUTPUTS OF EACH LAYER"
  },
  {
    "subject": "INTERMEDIATE RESULTS",
    "predicate": "ARE USED BY",
    "object": "NEXT LAYER"
  },
  {
    "subject": "A TRAINING TASK",
    "predicate": "DIFFERS IN THAT",
    "object": "THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS CANNOT BE IMMEDIATELY FREED"
  },
  {
    "subject": "THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS",
    "predicate": "ARE USED BY",
    "object": "THE BACKWARD PASS"
  },
  {
    "subject": "THE BACKWARD PASS",
    "predicate": "UPDATES",
    "object": "THE WEIGHTS"
  },
  {
    "subject": "THE BACKWARD PASS",
    "predicate": "CONSUMES",
    "object": "THE INTERMEDIATE RESULTS"
  },
  {
    "subject": "THE BACKWARD PASS",
    "predicate": "CONSUMES IN ORDER",
    "object": "REVERSE ORDER"
  },
  {
    "subject": "THE FORWARD PASS",
    "predicate": "GENERATES",
    "object": "THE INTERMEDIATE RESULTS"
  },
  {
    "subject": "THE INTERMEDIATE RESULTS",
    "predicate": "ARE",
    "object": "RST-IN-LAST-OUT"
  },
  {
    "subject": "THE MEMORY ALLOCATION AND RELEASE",
    "predicate": "CAN BE HANDLED BY",
    "object": "A SIMPLE STACK-LIKE MECHANISM"
  },
  {
    "subject": "A SIMPLE STACK-LIKE MECHANISM",
    "predicate": "DOES NOT CAUSE",
    "object": "MEMORY FRAGMENTATION"
  },
  {
    "subject": "MEMORY ALLOCATION OVERHEAD",
    "predicate": "SHOULD BE",
    "object": "MINIMIZED"
  },
  {
    "subject": "MINIMIZE MEMORY FOOTPRINT",
    "predicate": "AND",
    "object": "AVOID EXTRA MEMORY COPIES"
  },
  {
    "subject": "WE",
    "predicate": "DESIGN",
    "object": "A MEMORY MANAGEMENT MECHANISM TAILORED FOR DL APPLICATIONS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "USES",
    "object": "DEDICATED MEMORY DAEMON"
  },
  {
    "subject": "DEDICATED MEMORY DAEMON",
    "predicate": "MANAGES",
    "object": "GPU MEMORY"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "SENDS",
    "object": "64-BIT INTEGER OFFSET FOR THE SHARED GPU MEMORY TO WORKERS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "SAVES",
    "object": "223 MS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ELIMINATES",
    "object": "THE MEMORY ALLOCATION OVERHEAD"
  },
  {
    "subject": "THE MEMORY ALLOCATION OVERHEAD",
    "predicate": "IS ELIMINATED WITH",
    "object": "THE MEMORY DAEMON"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "ONLY NEEDS TO PASS",
    "object": "MEMORY POINTERS TO THE WORKERS"
  },
  {
    "subject": "PASSING MEMORY POINTERS TO THE WORKERS",
    "predicate": "IS",
    "object": "LIGHT-WEIGHT"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "ENSURES",
    "object": "THAT EACH TIME ONLY ONE WORKER OWNS THE GPU MEMORY"
  },
  {
    "subject": "ONE WORKER",
    "predicate": "OWNS",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "GUARANTEES",
    "object": "MEMORY ISOLATION BETWEEN WORKERS"
  },
  {
    "subject": "EACH WORKER",
    "predicate": "USES",
    "object": "A MEMORY POOL"
  },
  {
    "subject": "EACH WORKER",
    "predicate": "ALLOCATES",
    "object": "THE MEMORY TO STORE ITS MODEL AND INTERMEDIATE RESULTS"
  },
  {
    "subject": "EACH WORKER",
    "predicate": "RECYCLES",
    "object": "THE MEMORY TO THE POOL"
  },
  {
    "subject": "THE MEMORY",
    "predicate": "IS RECYCLED TO",
    "object": "THE POOL AFTER THE INTERMEDIATE RESULTS ARE NO LONGER NEEDED"
  },
  {
    "subject": "THE MEMORY MANAGEMENT OF PIPESWITCH",
    "predicate": "EXTENDS",
    "object": "THAT OF PYTORCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "INSERTS",
    "object": "GPU MEMORY BLOCKS TO PYTORCH GPU MEMORY POOL"
  },
  {
    "subject": "PYTORCH",
    "predicate": "CREATES",
    "object": "TENSORS ON THEM"
  },
  {
    "subject": "MEMORY MANAGEMENT IN PYTORCH",
    "predicate": "HANDLES",
    "object": "MEMORY ALLOCATION FOR A TASK ITSELF"
  },
  {
    "subject": "REPLICATING THE MODELS IN EACH WORKER",
    "predicate": "INCURS",
    "object": "HIGH MEMORY FOOTPRINT"
  },
  {
    "subject": "REPLICATING THE MODELS IN EACH WORKER",
    "predicate": "REDUCES",
    "object": "THE NUMBER OF MODELS A SERVER CAN STORE"
  },
  {
    "subject": "REPLICATING THE MODELS IN EACH WORKER",
    "predicate": "REDUCES",
    "object": "THE TYPES OF TASKS THE SERVER CAN EXECUTE"
  },
  {
    "subject": "STORING THE MODELS IN A DEDICATE PROCESS",
    "predicate": "HAS",
    "object": "MINIMAL MEMORY FOOTPRINT"
  },
  {
    "subject": "EACH MODEL",
    "predicate": "IS STORED",
    "object": "ONLY ONCE"
  },
  {
    "subject": "STORING THE MODELS IN A DEDICATE PROCESS",
    "predicate": "INCURS",
    "object": "AN EXTRA MEMORY COPY FROM THIS PROCESS TO A WORKER TO START A TASK"
  },
  {
    "subject": "AN EXTRA MEMORY COPY",
    "predicate": "HURTS",
    "object": "THE TASK SWITCHING TIME"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "STORES",
    "object": "THE MODELS IN THE MEMORY DAEMON"
  },
  {
    "subject": "THE SERVER",
    "predicate": "ONLY NEEDS TO KEEP",
    "object": "ONE COPY OF EACH MODEL IN THE HOST MEMORY"
  },
  {
    "subject": "IPC",
    "predicate": "HAS",
    "object": "OVERHEAD"
  },
  {
    "subject": "OVERHEAD",
    "predicate": "SHOULD BE",
    "object": "MINIMIZED"
  },
  {
    "subject": "WE",
    "predicate": "LEVERAGE",
    "object": "A PROPERTY OF DL APPLICATIONS"
  },
  {
    "subject": "WE",
    "predicate": "MINIMIZE",
    "object": "THE IPC OVERHEAD"
  },
  {
    "subject": "IPC APIS",
    "predicate": "ARE PROVIDED BY",
    "object": "GPUS"
  },
  {
    "subject": "CUDAIPCOPENMEMHANDLE",
    "predicate": "IS FOR",
    "object": "NVIDIA GPUS"
  },
  {
    "subject": "WE",
    "predicate": "HAVE MEASURED",
    "object": "THE PERFORMANCE OF THESE IPC APIS"
  },
  {
    "subject": "THESE IPC APIS",
    "predicate": "INCUR",
    "object": "HIGH OVERHEAD"
  },
  {
    "subject": "THE OVERHEAD",
    "predicate": "IS EXACERBATED BY",
    "object": "THE PIPELINE"
  },
  {
    "subject": "THE PIPELINE",
    "predicate": "NEEDS TO INVOKE",
    "object": "THE IPCS FREQUENTLY"
  },
  {
    "subject": "THE IPCS",
    "predicate": "ARE INVOKED TO",
    "object": "SYNCHRONIZE MODEL TRANSMISSION AND TASK EXECUTION"
  },
  {
    "subject": "THE PIPELINE",
    "predicate": "INVOKES THE IPCS",
    "object": "FOR EVERY PIPELINE GROUP"
  },
  {
    "subject": "THE PIPELINE",
    "predicate": "INVOKES THE IPC",
    "object": "ONLY ONCE FOR THE ENTIRE MODEL TRANSMISSION"
  },
  {
    "subject": "MEMORY ALLOCATION PROCESS FOR A NEURAL NETWORK MODEL",
    "predicate": "IS",
    "object": "DETERMINISTIC"
  },
  {
    "subject": "MEMORY DAEMON AND THE WORKER",
    "predicate": "USE",
    "object": "THE SAME ORDER TO ALLOCATE MEMORY FOR THE MODEL PARAMETERS"
  },
  {
    "subject": "THE MEMORY POINTERS FOR THE PARAMETERS",
    "predicate": "WOULD BE",
    "object": "THE SAME"
  },
  {
    "subject": "THE NEURAL NETWORK MODEL",
    "predicate": "IS",
    "object": "KNOWN AND GIVEN"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "ONLY NEEDS TO USE",
    "object": "THE SAME ORDER TO TRANSMIT THE MODEL AS THE WORKER WOULD"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "CAN MINIMIZE",
    "object": "THE USAGE OF EXPENSIVE GPU IPCS"
  },
  {
    "subject": "LATENCY",
    "predicate": "IS HIGHER THAN",
    "object": "NO UNIFIED MEMORY MANAGEMENT"
  },
  {
    "subject": "CHEAP CPU IPCS",
    "predicate": "NOTIFY",
    "object": "THE WORKER"
  },
  {
    "subject": "THE WORKER",
    "predicate": "HAS BEEN TRANSMITTED",
    "object": "WHICH PIPELINE GROUP"
  },
  {
    "subject": "PIN",
    "predicate": "MEMORY",
    "object": ""
  },
  {
    "subject": "NO PIN",
    "predicate": "HAS",
    "object": "MEMORY"
  },
  {
    "subject": "THE OS",
    "predicate": "WOULD SWAP",
    "object": "A MEMORY PAGE TO DISK"
  },
  {
    "subject": "THE PAGE",
    "predicate": "IS",
    "object": "INACTIVE FOR A CERTAIN AMOUNT OF TIME"
  },
  {
    "subject": "GPUS",
    "predicate": "REQUIRE",
    "object": "A PAGE IN THE HOST MEMORY TO BE PINNED (OR PAGE-LOCKED)"
  },
  {
    "subject": "A PAGE IN THE HOST MEMORY",
    "predicate": "IS PINNED (OR PAGE-LOCKED)",
    "object": "IN ORDER TO TRANSMIT THE DATA IN THE PAGE TO THE GPU MEMORY"
  },
  {
    "subject": "A TEMPORARY PINNED PAGE",
    "predicate": "IS CREATED FOR",
    "object": "THE TRANSMISSION"
  },
  {
    "subject": "WE",
    "predicate": "PIN",
    "object": "THE PAGES OF THE MEMORY DAEMON TO THE HOST MEMORY"
  },
  {
    "subject": "PROCESS-LEVEL ISOLATION",
    "predicate": "IS",
    "object": "DESIRABLE"
  },
  {
    "subject": "PROCESS-LEVEL ISOLATION",
    "predicate": "ENSURES",
    "object": "ONE TASK CANNOT READ THE MEMORY OF ANOTHER TASK"
  },
  {
    "subject": "PROCESS-LEVEL ISOLATION",
    "predicate": "ENSURES",
    "object": "THE CRASHING OF ONE TASK DOES NOT AFFECT OTHER TASKS OR THE ENTIRE SYSTEM"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "SEPARATE PROCESSES"
  },
  {
    "subject": "SEPARATE PROCESSES",
    "predicate": "ACHIEVE",
    "object": "PROCESS-LEVEL ISOLATION"
  },
  {
    "subject": "SEPARATE PROCESSES",
    "predicate": "ARE SIMILAR TO",
    "object": "THE NAIVE SOLUTION"
  },
  {
    "subject": "A NAIVE SOLUTION",
    "predicate": "IS TO USE",
    "object": "SEPARATE PROCESSES"
  },
  {
    "subject": "A NAIVE SOLUTION",
    "predicate": "IS TO START",
    "object": "THE NEW TASK AFTER THE CURRENT TASK IS STOPPED"
  },
  {
    "subject": "SEQUENTIAL EXECUTION",
    "predicate": "INCURS",
    "object": "LONG DELAY"
  },
  {
    "subject": "LONG DELAY",
    "predicate": "IS DUE TO",
    "object": "OLD TASK CLEANING"
  },
  {
    "subject": "LONG DELAY",
    "predicate": "IS DUE TO",
    "object": "NEW TASK INITIALIZATION"
  },
  {
    "subject": "ANOTHER POSSIBLE SOLUTION",
    "predicate": "IS",
    "object": "TO LET THE CURRENT AND NEW TASKS SHARE THE SAME PROCESS WITH A WARM CUDA CONTEXT"
  },
  {
    "subject": "THE NEW TASK",
    "predicate": "CAN REUSE",
    "object": "THE GPU ENVIRONMENT OF THE CURRENT TASK"
  },
  {
    "subject": "THE PROCESS OF THE OLD TASK",
    "predicate": "CLEANS",
    "object": "THE GPU ENVIRONMENT"
  },
  {
    "subject": "ANOTHER PROCESS",
    "predicate": "IS CREATED FOR",
    "object": "THE NEW TASK"
  },
  {
    "subject": "ANOTHER PROCESS",
    "predicate": "IS INITIALIZED FOR",
    "object": "THE NEW TASK"
  },
  {
    "subject": "THE PROCESS",
    "predicate": "REUSES",
    "object": "THE ENVIRONMENT FOR THE NEW TASK"
  },
  {
    "subject": "EACH WORKER",
    "predicate": "IS",
    "object": "A SEPARATE PROCESS"
  },
  {
    "subject": "EACH WORKER",
    "predicate": "INITIALIZES",
    "object": "ITS OWN GPU ENVIRONMENT"
  },
  {
    "subject": "GPU ENVIRONMENT",
    "predicate": "IS",
    "object": "CUDA CONTEXT"
  },
  {
    "subject": "GPU ENVIRONMENT",
    "predicate": "IS INITIALIZED",
    "object": "WHEN IT IS RST CREATED"
  },
  {
    "subject": "A MAJOR JOB",
    "predicate": "IS TO CLEAR",
    "object": "ASYNCHRONOUS CUDA FUNCTIONS QUEUED ON THE GPU"
  },
  {
    "subject": "A CURRENT TASK",
    "predicate": "IS",
    "object": "STOPPED"
  },
  {
    "subject": "WE",
    "predicate": "INSERT",
    "object": "SYNCHRONIZATION POINTS INTO TRAINING TASKS"
  },
  {
    "subject": "NUMBER OF QUEUED FUNCTIONS",
    "predicate": "ARE",
    "object": "LIMITED"
  },
  {
    "subject": "NUMBER OF QUEUED FUNCTIONS",
    "predicate": "CAN BE",
    "object": "QUICKLY CLEARED"
  },
  {
    "subject": "SYNCHRONIZATION POINTS",
    "predicate": "ARE NOT NEEDED FOR",
    "object": "INFERENCE TASKS"
  },
  {
    "subject": "INFERENCE TASKS",
    "predicate": "ARE",
    "object": "SHORT"
  },
  {
    "subject": "INFERENCE TASKS",
    "predicate": "ARE NOT",
    "object": "PREEMPTED"
  },
  {
    "subject": "ANOTHER JOB",
    "predicate": "IS TO FREE",
    "object": "ITS GPU MEMORY"
  },
  {
    "subject": "AN IMPORTANT PROPERTY OF THE CLEANING PROCEDURE",
    "predicate": "IS",
    "object": "THAT IT DOES NOT MODIFY THE CONTENT OF THE MEMORY"
  },
  {
    "subject": "AN IMPORTANT PROPERTY OF THE CLEANING PROCEDURE",
    "predicate": "ONLY CLEANS",
    "object": "THE METADATA"
  },
  {
    "subject": "THE METADATA",
    "predicate": "IS",
    "object": "GPU MEMORY POINTERS"
  },
  {
    "subject": "GPU MEMORY",
    "predicate": "IS MANAGED BY",
    "object": "PIPESWITCH"
  },
  {
    "subject": "CLEANING PROCEDURE",
    "predicate": "DELETES",
    "object": "POINTERS POINTING TO THE TENSOR DATA"
  },
  {
    "subject": "CLEANING PROCEDURE",
    "predicate": "DOES NOT FREE",
    "object": "ACTUAL DATA"
  },
  {
    "subject": "WE",
    "predicate": "CAN PARALLELIZE",
    "object": "THE TASK CLEANING OF THE CURRENT TASK AND THE PIPELINED MODEL TRANSMISSION OF THE NEW TASK"
  },
  {
    "subject": "PARALLELIZING THE TASK CLEANING AND THE PIPELINED MODEL TRANSMISSION",
    "predicate": "HIDES",
    "object": "THE TASK CLEANING OVERHEAD"
  },
  {
    "subject": "THIS CHOICE",
    "predicate": "IS OPTIMIZED FOR",
    "object": "PERFORMANCE"
  },
  {
    "subject": "THIS CHOICE",
    "predicate": "IS NOT",
    "object": "A PROBLEM FOR A TRUSTED ENVIRONMENT"
  },
  {
    "subject": "A LATTER PROCESS",
    "predicate": "CAN READ",
    "object": "THE MEMORY DATA OF A PREVIOUS PROCESS"
  },
  {
    "subject": "AN ADDITIONAL ZERO-OUT OPERATION",
    "predicate": "CAN BE",
    "object": "ADDED"
  },
  {
    "subject": "GPU",
    "predicate": "HAS",
    "object": "HIGH MEMORY BANDWIDTH"
  },
  {
    "subject": "HIGH MEMORY BANDWIDTH",
    "predicate": "E.G.",
    "object": "900GBS FOR V100"
  },
  {
    "subject": "SUB-MILLISECOND OVERHEAD",
    "predicate": "IS FOR",
    "object": "ZEROING-OUT MOST MODELS LIKE RESNET-152"
  },
  {
    "subject": "RESNET-152",
    "predicate": "HAS SIZE",
    "object": "AROUND 240MB"
  },
  {
    "subject": "NEW PROCESS",
    "predicate": "DOES NOT REQUIRE",
    "object": "ENTIRE GPU MEMORY"
  },
  {
    "subject": "TABLE 2",
    "predicate": "SUMMARIZES",
    "object": "THE DIFFERENCES BETWEEN THESE THREE SOLUTIONS"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "SIGNALS",
    "object": "THE CURRENT ACTIVE WORKER TO STOP"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "DELETES",
    "object": "THE GPU MEMORY ALLOCATED TO THE CURRENT ACTIVE WORKER"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "ALLOCATES",
    "object": "THE GPU MEMORY TO THE NEW ACTIVE WORKER"
  },
  {
    "subject": "CONTROLLER",
    "predicate": "NOTIFIES",
    "object": "CURRENT ACTIVE WORKER TO STOP"
  },
  {
    "subject": "CONTROLLER",
    "predicate": "TRANSFERS",
    "object": "PARAMETERS OF THE NEW MODEL TO THE GPU"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "ENSURES",
    "object": "ONLY ONE ACTIVE WORKER"
  },
  {
    "subject": "ONLY ONE ACTIVE WORKER",
    "predicate": "GUARANTEES",
    "object": "EXCLUSIVE OCCUPATION OF THE GPU"
  },
  {
    "subject": "TRADE-OFF",
    "predicate": "EXISTS BETWEEN",
    "object": "NUMBER OF STANDBY WORKERS AND THEIR GPU MEMORY CONSUMPTION"
  },
  {
    "subject": "EVERY STANDBY WORKER",
    "predicate": "NEEDS TO MAINTAIN",
    "object": "ITS OWN CUDA CONTEXT"
  },
  {
    "subject": "CUDA CONTEXT",
    "predicate": "CONSUMES",
    "object": "A FEW HUNDRED MB GPU MEMORY"
  },
  {
    "subject": "THERE",
    "predicate": "IS",
    "object": "ALWAYS AT LEAST ONE IDLE STANDBY WORKER"
  },
  {
    "subject": "TWO STANDBY WORKERS",
    "predicate": "ARE SUFFICIENT TO ENSURE",
    "object": "AT LEAST ONE IDLE WORKER"
  },
  {
    "subject": "AT LEAST ONE IDLE WORKER",
    "predicate": "ELIMINATES",
    "object": "THE WAITING TIME"
  },
  {
    "subject": "TWO STANDBY WORKERS",
    "predicate": "HAS",
    "object": "MODERATE GPU MEMORY CONSUMPTION"
  },
  {
    "subject": "A TRANSACTION",
    "predicate": "MEANS",
    "object": "A MODEL IS SWITCHED IN OR OUT ON ALL OF ITS GPUS"
  },
  {
    "subject": "A TRANSACTION",
    "predicate": "ENABLES OR DISABLES",
    "object": "INFERENCE ON THIS MODEL"
  },
  {
    "subject": "WE",
    "predicate": "HAVE ANALYZED",
    "object": "A PRODUCTION GPU TRAINING TRACE FROM MICROSOFT 19,20"
  },
  {
    "subject": "TASKS IN THIS TRACE",
    "predicate": "NUMBER",
    "object": "111,883"
  },
  {
    "subject": "SINGLE-GPU TRAINING TASKS",
    "predicate": "NUMBER",
    "object": "96,662"
  },
  {
    "subject": "SINGLE-GPU TRAINING TASKS",
    "predicate": "PERCENTAGE OF ALL TASKS",
    "object": "86%"
  },
  {
    "subject": "THESE JOBS",
    "predicate": "ACCOUNT FOR",
    "object": "18 OF TOTAL GPU HOURS"
  },
  {
    "subject": "WE",
    "predicate": "EXPECT",
    "object": "THE SHARE OF MULTI-GPU JOBS TO INCREASE IN THE FUTURE"
  },
  {
    "subject": "CURRENT TRAINING FRAMEWORKS",
    "predicate": "DO NOT HAVE",
    "object": "MATURE SUPPORT OF ELASTIC TRAINING"
  },
  {
    "subject": "THESE SCHEDULING SOLUTIONS",
    "predicate": "ARE",
    "object": "ORTHOGONAL AND COMPLEMENTARY TO PIPESWITCH"
  },
  {
    "subject": "THESE SOLUTIONS",
    "predicate": "ARE COMPLEMENTARY TO",
    "object": "PIPESWITCH"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "PYTORCH.ORG"
  },
  {
    "subject": "WE",
    "predicate": "ADD",
    "object": "C AND PYTHON FUNCTIONS TO THE GPU MEMORY MANAGEMENT MODULE OF PYTORCH"
  },
  {
    "subject": "SHARED GPU MEMORY",
    "predicate": "CAN BE INSERTED INTO",
    "object": "PYTORCH GPU MEMORY POOL"
  },
  {
    "subject": "SHARED GPU MEMORY",
    "predicate": "CAN BE INSERTED MULTIPLE TIMES FOR",
    "object": "DIFFERENT CUDA STREAMS"
  },
  {
    "subject": "CONTROLLER",
    "predicate": "GUARANTEES",
    "object": "ONLY ONE OF THESE CUDA STREAMS IS ACTIVE"
  },
  {
    "subject": "THE CONTROLLER PROCESS",
    "predicate": "CONSISTS OF",
    "object": "A TCP THREAD"
  },
  {
    "subject": "THE CONTROLLER PROCESS",
    "predicate": "CONSISTS OF",
    "object": "A SCHEDULER THREAD"
  },
  {
    "subject": "THE SCHEDULER AND THE MEMORY DAEMON",
    "predicate": "ARE IMPLEMENTED TOGETHER",
    "object": "FOR BETTER PERFORMANCE"
  },
  {
    "subject": "THE TCP THREAD",
    "predicate": "ACCEPTS",
    "object": "TASK THROUGH TCP FROM CLIENTS"
  },
  {
    "subject": "THE TCP THREAD",
    "predicate": "SENDS",
    "object": "THE TASK TO THE SCHEDULER THREAD"
  },
  {
    "subject": "THE SCHEDULER THREAD",
    "predicate": "ALLOCATES AND SHARES",
    "object": "THE GPU MEMORY WITH WORKERS"
  },
  {
    "subject": "THE SCHEDULER THREAD",
    "predicate": "ACTIVATES OR DEACTIVATES",
    "object": "WORKERS"
  },
  {
    "subject": "THE SCHEDULER THREAD",
    "predicate": "SENDS",
    "object": "THE TASK TO A WORKER"
  },
  {
    "subject": "THE SCHEDULER THREAD",
    "predicate": "TRANSFERS",
    "object": "PARAMETERS FOR THE CORRESPONDING MODEL TO THE GPU MEMORY"
  },
  {
    "subject": "THE USER",
    "predicate": "SHOULD REGISTER",
    "object": "THE MODEL IN THE SCHEDULER"
  },
  {
    "subject": "THE SCHEDULER",
    "predicate": "NOTIFIES",
    "object": "THE CONTROLLER"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "LOADS",
    "object": "THE MODEL FROM THE DISK TO THE CPU MEMORY"
  },
  {
    "subject": "PARAMETERS",
    "predicate": "ARE TRANSMITTED TO",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "PARAMETERS",
    "predicate": "ARE TRANSMITTED IN",
    "object": "GROUPS"
  },
  {
    "subject": "PARAMETERS",
    "predicate": "ARE TRANSMITTED IN",
    "object": "A PIPELINE"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "NOTIFIES",
    "object": "THE WORKER TO START COMPUTING THE CORRESPONDING LAYERS"
  },
  {
    "subject": "THE WORKER PROCESS",
    "predicate": "CONSISTS OF",
    "object": "TWO THREADS"
  },
  {
    "subject": "THE TERMINATION THREAD",
    "predicate": "WAITS FOR",
    "object": "THE TERMINATION SIGNAL FROM THE CONTROLLER"
  },
  {
    "subject": "THE TERMINATION THREAD",
    "predicate": "NOTIFIES",
    "object": "THE MAIN THREAD"
  },
  {
    "subject": "THE MAIN THREAD",
    "predicate": "MANAGES",
    "object": "THE DNN MODELS"
  },
  {
    "subject": "THE MAIN THREAD",
    "predicate": "PERFORMS",
    "object": "THE COMPUTATION FOR INFERENCE OR TRAINING"
  },
  {
    "subject": "WORKER",
    "predicate": "REQUIRES",
    "object": "USER TO REGISTER THE MODEL BEFORE STARTING A TASK"
  },
  {
    "subject": "WORKER",
    "predicate": "CAN LOAD",
    "object": "THE MODELS"
  },
  {
    "subject": "WORKER",
    "predicate": "CAN ADD",
    "object": "THE HOOKS TO WAIT FOR PARAMETER TRANSMISSION OR TERMINATE ON NOTIFICATION"
  },
  {
    "subject": "THE WORKER",
    "predicate": "LOADS",
    "object": "THE MODEL STRUCTURES"
  },
  {
    "subject": "THE MODEL STRUCTURES",
    "predicate": "IS",
    "object": "SMALL"
  },
  {
    "subject": "THE WORKER",
    "predicate": "DOES NOT LOAD",
    "object": "THE MODEL PARAMETERS"
  },
  {
    "subject": "THE PARAMETERS",
    "predicate": "ARE STORED",
    "object": "ONCE IN THE MEMORY DAEMON"
  },
  {
    "subject": "THE PARAMETERS",
    "predicate": "ARE STORED FOR",
    "object": "MINIMAL MEMORY FOOTPRINT"
  },
  {
    "subject": "THE MODELS",
    "predicate": "ARE LOADED",
    "object": ""
  },
  {
    "subject": "THE MODELS",
    "predicate": "ARE ATTACHED TO",
    "object": "DIFFERENT CUDA STREAMS"
  },
  {
    "subject": "THEIR PARAMETERS",
    "predicate": "ARE ASSIGNED TO",
    "object": "LOCATIONS IN THE SHARED GPU MEMORY"
  },
  {
    "subject": "DIFFERENT MODELS",
    "predicate": "MIGHT USE",
    "object": "THE SAME GPU MEMORY LOCATION"
  },
  {
    "subject": "THE VALUE",
    "predicate": "IS NOT VALID UNTIL",
    "object": "THE CONTROLLER TRANSFERS THE CORRESPONDING PARAMETERS TO THESE LOCATIONS"
  },
  {
    "subject": "THE WORKER",
    "predicate": "WAITS FOR",
    "object": "THE SCHEDULER TO TRANSFER REQUIRED PARAMETERS FOR DNN MODELS"
  },
  {
    "subject": "THE WORKER",
    "predicate": "PERFORMS",
    "object": "INFERENCE OR TRAINING"
  },
  {
    "subject": "ALL EXPERIMENTS",
    "predicate": "ARE CONDUCTED ON",
    "object": "AWS"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "TWO EC2 INSTANCE TYPES"
  },
  {
    "subject": "ONE",
    "predicate": "IS",
    "object": "P3.2XLARGE"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "IS CONFIGURED WITH",
    "object": "8 VCPUS (INTEL XEON E5-2686 V4)"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "HAS",
    "object": "1 GPU (NVIDIA V100 WITH 16 GB GPU MEMORY)"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "HAS",
    "object": "PCIE 3.0 16"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "HAS",
    "object": "61 GB MEMORY"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "IS CONFIGURED WITH",
    "object": "8 VCPUS (INTEL PLATINUM 8259CL)"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "HAS",
    "object": "1 GPU (NVIDIA T4 WITH 16 GB GPU MEMORY)"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "HAS",
    "object": "PCIE 3.0 8"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "HAS",
    "object": "32 GB MEMORY"
  },
  {
    "subject": "THE SOFTWARE ENVIRONMENT",
    "predicate": "INCLUDES",
    "object": "PYTORCH-1.3.0"
  },
  {
    "subject": "THE SOFTWARE ENVIRONMENT",
    "predicate": "INCLUDES",
    "object": "TORCHVISION-0.4.2"
  },
  {
    "subject": "THE SOFTWARE ENVIRONMENT",
    "predicate": "INCLUDES",
    "object": "SCIPY-1.3.2"
  },
  {
    "subject": "THE SOFTWARE ENVIRONMENT",
    "predicate": "INCLUDES",
    "object": "CUDA-10.1"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "PYTORCH WITH OUR PLUGINS FOR ALL MECHANISMS"
  },
  {
    "subject": "PYTORCH WITH OUR PLUGINS",
    "predicate": "PROVIDES",
    "object": "BETTER RESULTS FOR STOP-AND-START"
  },
  {
    "subject": "BETTER RESULTS FOR STOP-AND-START",
    "predicate": "ARE BETTER THAN",
    "object": "NATIVE PYTORCH FROM PYTHON-PYPI USED IN TABLE 1"
  },
  {
    "subject": "THE MODELS",
    "predicate": "INCLUDE",
    "object": "RESNET152 17"
  },
  {
    "subject": "THE MODELS",
    "predicate": "INCLUDE",
    "object": "INCEPTIONV3 22"
  },
  {
    "subject": "THE MODELS",
    "predicate": "INCLUDE",
    "object": "BERTBASE 23"
  },
  {
    "subject": "RESNET152 17",
    "predicate": "ARE",
    "object": "STANDARD BENCHMARKS FOR EVALUATING DL SYSTEMS"
  },
  {
    "subject": "INCEPTIONV3 22",
    "predicate": "ARE",
    "object": "STANDARD BENCHMARKS FOR EVALUATING DL SYSTEMS"
  },
  {
    "subject": "BERTBASE 23",
    "predicate": "ARE",
    "object": "STANDARD BENCHMARKS FOR EVALUATING DL SYSTEMS"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "REPRESENTATIVE CONFIGURATIONS FOR EACH MODEL"
  },
  {
    "subject": "THE EXPERIMENTS",
    "predicate": "COVER",
    "object": "TRAINING AND INFERENCE"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "SINGLE-GPU INFERENCE AND TRAINING TASKS"
  },
  {
    "subject": "SINGLE-GPU INFERENCE AND TRAINING TASKS",
    "predicate": "ARE DISCUSSED IN",
    "object": "4.5"
  },
  {
    "subject": "TRAINING TASKS",
    "predicate": "CHECK-POINT",
    "object": "THEIR MODELS TO THE HOST MEMORY"
  },
  {
    "subject": "TRAINING TASKS",
    "predicate": "RESTART",
    "object": "FROM THE LATEST CHECKPOINT AFTER PREEMPTION"
  },
  {
    "subject": "THE CHECKPOINTING FREQUENCY OF TRAINING TASKS",
    "predicate": "IS SET ACCORDING TO",
    "object": "THE SCHEDULING CYCLE"
  },
  {
    "subject": "THE CHECKPOINTING FREQUENCY OF TRAINING TASKS",
    "predicate": "IS SET TO",
    "object": "MINIMIZE CHECKPOINTING OVERHEAD"
  },
  {
    "subject": "DEFAULT BATCH SIZE FOR TRAINING",
    "predicate": "IS",
    "object": "32"
  },
  {
    "subject": "DEFAULT BATCH SIZE FOR INFERENCE",
    "predicate": "IS",
    "object": "8"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "THROUGHPUT AND LATENCY AS EVALUATION METRICS"
  },
  {
    "subject": "WE",
    "predicate": "MEASURE",
    "object": "THE END-TO-END LATENCY EXPERIENCED BY THE CLIENT"
  },
  {
    "subject": "FIGURE 5",
    "predicate": "DESCRIBES",
    "object": "TOTAL LATENCY EXPERIENCED BY THE CLIENT FOR DIFFERENT MECHANISMS"
  },
  {
    "subject": "FIGURE 5",
    "predicate": "SHOWS",
    "object": "THE LATENCY EXPERIENCED BY THE CLIENT"
  },
  {
    "subject": "TABLE 3",
    "predicate": "SHOWS",
    "object": "THE TOTAL OVERHEAD"
  },
  {
    "subject": "EACH NUMBER",
    "predicate": "IS REPORTED WITH",
    "object": "THE AVERAGE OF 100 RUNS"
  },
  {
    "subject": "FIGURE 6(B)",
    "predicate": "REPORTS",
    "object": "MINIMUM AND MAXIMUM LATENCIES USING THE ERROR BAR"
  },
  {
    "subject": "LATENCY OF THE RST BATCH AND THOSE OF LATER BATCHES IN ONE SCHEDULING CYCLE",
    "predicate": "CAN DIFFER",
    "object": "SIGNIFICANTLY"
  },
  {
    "subject": "LATENCY DIFFERENCE",
    "predicate": "IS DUE TO",
    "object": "SWITCHING OVERHEAD"
  },
  {
    "subject": "6.1 END-TO-END EXPERIMENTS",
    "predicate": "MINIMIZE",
    "object": "END-TO-END OVERHEAD"
  },
  {
    "subject": "A CLIENT",
    "predicate": "SENDS",
    "object": "AN INFERENCE TASK TO A GPU SERVER"
  },
  {
    "subject": "THE GPU SERVER",
    "predicate": "PREEMPTS",
    "object": "THE TRAINING TASK"
  },
  {
    "subject": "THE GPU SERVER",
    "predicate": "EXECUTES",
    "object": "THE INFERENCE TASK"
  },
  {
    "subject": "THE GPU SERVER",
    "predicate": "SENDS",
    "object": "A REPLY BACK TO THE CLIENT"
  },
  {
    "subject": "MODEL",
    "predicate": "IS",
    "object": "READY"
  },
  {
    "subject": "THERE",
    "predicate": "IS",
    "object": "NO TRAINING TASK"
  },
  {
    "subject": "THE PROCESS WITH THE REQUIRED MODEL",
    "predicate": "IS LOADED IN",
    "object": "THE GPU"
  },
  {
    "subject": "THIS SOLUTION",
    "predicate": "PROVIDES",
    "object": "THE LOWER BOUND"
  },
  {
    "subject": "THE LOWER BOUND",
    "predicate": "IS",
    "object": "THE LOWEST LATENCY WE CAN ACHIEVE FOR AN INFERENCE TASK"
  },
  {
    "subject": "THIS SOLUTION",
    "predicate": "IS USED BY",
    "object": "EXISTING SYSTEMS LIKE GANDIVA 24"
  },
  {
    "subject": "GANDIVA 24",
    "predicate": "IS USED FOR",
    "object": "TASK SWITCHING"
  },
  {
    "subject": "TASK SWITCHING",
    "predicate": "REPORTED",
    "object": "SIMILAR SECOND-SCALE OVERHEAD"
  },
  {
    "subject": "NVIDIA",
    "predicate": "HAS",
    "object": "MPS"
  },
  {
    "subject": "WE",
    "predicate": "INITIALIZE",
    "object": "SEPARATE PROCESSES IN ADVANCE"
  },
  {
    "subject": "CUDA UNIFIED MEMORY",
    "predicate": "IS USED FOR",
    "object": "MEMORY SWAPPING"
  },
  {
    "subject": "CUDA",
    "predicate": "HAS",
    "object": "UNIFIED MEMORY"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS PART OF",
    "object": "DEVBLOGS.NVIDIA.COM/UNIFIED-MEMORY-CUDA-BEGINNERS"
  },
  {
    "subject": "THE PROPERTIES",
    "predicate": "ARE DESCRIBED IN",
    "object": "4"
  },
  {
    "subject": "USENIX SYMPOSIUM",
    "predicate": "IS",
    "object": "14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"
  },
  {
    "subject": "LATENCY",
    "predicate": "IS MEASURED IN",
    "object": "MILLISECONDS"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "USES",
    "object": "NVIDIA V100"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "HAS INTERFACE",
    "object": "PCIE 3.0 16"
  },
  {
    "subject": "LATENCY",
    "predicate": "RANGES",
    "object": "5000 TO 10000 MS"
  },
  {
    "subject": "MODEL",
    "predicate": "INCLUDES",
    "object": "PIPESWITCH"
  },
  {
    "subject": "MODEL",
    "predicate": "INCLUDES",
    "object": "MPS"
  },
  {
    "subject": "MODEL",
    "predicate": "INCLUDES",
    "object": "STOP-AND-START"
  },
  {
    "subject": "MODEL",
    "predicate": "INCLUDES",
    "object": "RESNET152"
  },
  {
    "subject": "MODEL",
    "predicate": "INCLUDES",
    "object": "INCEPTIONV3"
  },
  {
    "subject": "MODEL",
    "predicate": "INCLUDES",
    "object": "BERTBASE"
  },
  {
    "subject": "INSTANCE TYPE",
    "predicate": "IS",
    "object": "G4DN.2XLARGE"
  },
  {
    "subject": "GPU",
    "predicate": "IS",
    "object": "NVIDIA T4"
  },
  {
    "subject": "GPU",
    "predicate": "USES",
    "object": "PCIE 3.0 8"
  },
  {
    "subject": "RESNET152",
    "predicate": "HAS LATENCY",
    "object": "0 20 40 60 80 100 MS"
  },
  {
    "subject": "INCEPTIONV3",
    "predicate": "HAS LATENCY",
    "object": "0 20 40 60 80 100 MS"
  },
  {
    "subject": "BERTBASE",
    "predicate": "HAS LATENCY",
    "object": "0 20 40 60 80 100 MS"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "CONNECTS VIA",
    "object": "PCIE 3.0 16"
  },
  {
    "subject": "PIPELINE",
    "predicate": "HAS TYPE",
    "object": "PER-LAYER"
  },
  {
    "subject": "PIPELINE",
    "predicate": "HAS TYPE",
    "object": "GROUPED TRANSMISSION"
  },
  {
    "subject": "OPTIMIZATION",
    "predicate": "STATUS",
    "object": "NO OPTIMIZATION"
  },
  {
    "subject": "RESNET152",
    "predicate": "IS MENTIONED IN",
    "object": "TEXT"
  },
  {
    "subject": "INCEPTIONV3",
    "predicate": "IS MENTIONED IN",
    "object": "TEXT"
  },
  {
    "subject": "BERTBASE",
    "predicate": "IS MENTIONED IN",
    "object": "TEXT"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IS A",
    "object": "TECHNIQUE"
  },
  {
    "subject": "PER-LAYER PIPELINE",
    "predicate": "IS A",
    "object": "TECHNIQUE"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "IS A",
    "object": "TECHNIQUE"
  },
  {
    "subject": "NO OPTIMIZATION",
    "predicate": "IS A",
    "object": "CONDITION"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "CONTAINS",
    "object": "NVIDIA T4"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "USES",
    "object": "PCIE 3.0 8"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "HAS",
    "object": "NVIDIA V100"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "USES",
    "object": "PCIE 3.0 16"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "HAS",
    "object": "NO MEMORY MANAGEMENT"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "HAS",
    "object": "NO IPC OPTIMIZATION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "HAS",
    "object": "NO PIN MEMORY"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "USES",
    "object": "CUDA UNIFIED MEMORY"
  },
  {
    "subject": "RESNET152",
    "predicate": "HAS LATENCY",
    "object": "0 100 200 300 400 MS"
  },
  {
    "subject": "INCEPTIONV3",
    "predicate": "HAS LATENCY",
    "object": "0 100 200 300 400 MS"
  },
  {
    "subject": "BERTBASE",
    "predicate": "HAS LATENCY",
    "object": "0 100 200 300 400 MS"
  },
  {
    "subject": "CUDA",
    "predicate": "USES",
    "object": "UNIFIED MEMORY"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "HAS GPU",
    "object": "NVIDIA T4"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "SUPPORTS",
    "object": "PCIE 3.0 8"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "HAS LATENCY RANGE",
    "object": "6000 TO 8000 MS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "SUPPORTS",
    "object": "ONE PROCESS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "SUPPORTS",
    "object": "TWO PROCESSES"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "RUNS",
    "object": "RESNET152"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "RUNS",
    "object": "INCEPTIONV3"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "RUNS",
    "object": "BERTBASE"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "USES INTERFACE",
    "object": "PCIE 3.0 16"
  },
  {
    "subject": "LATENCY",
    "predicate": "RANGES FROM",
    "object": "5000 TO 7500 MS"
  },
  {
    "subject": "MODELS",
    "predicate": "INCLUDE",
    "object": "RESNET152"
  },
  {
    "subject": "MODELS",
    "predicate": "INCLUDE",
    "object": "INCEPTIONV3"
  },
  {
    "subject": "MODELS",
    "predicate": "INCLUDE",
    "object": "BERTBASE"
  },
  {
    "subject": "INSTANCE",
    "predicate": "IS",
    "object": "G4DN.2XLARGE"
  },
  {
    "subject": "INSTANCE",
    "predicate": "HAS GPU",
    "object": "NVIDIA T4"
  },
  {
    "subject": "RESNET152",
    "predicate": "HAS STARTUP OVERHEAD",
    "object": "3.62 MS"
  },
  {
    "subject": "INCEPTIONV3",
    "predicate": "HAS STARTUP OVERHEAD",
    "object": "4.82 MS"
  },
  {
    "subject": "BERTBASE",
    "predicate": "HAS STARTUP OVERHEAD",
    "object": "3.62 MS"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "HAS STARTUP OVERHEAD",
    "object": "3.62 MS"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "HAS STARTUP OVERHEAD",
    "object": "2.53 MS"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "HAS STARTUP OVERHEAD",
    "object": "5.49 MS"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "HAS STARTUP OVERHEAD",
    "object": "6.57 MS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "STARTS",
    "object": "COMPUTING THE RST LAYER"
  },
  {
    "subject": "SALUS 7",
    "predicate": "IS NOT DIRECTLY COMPARABLE",
    "object": ""
  },
  {
    "subject": "SALUS 7",
    "predicate": "REQUIRES",
    "object": "THE MODELS TO BE PRELOADED TO THE GPU"
  },
  {
    "subject": "SALUS 7",
    "predicate": "HAS",
    "object": "SEVERAL LIMITATIONS DESCRIBED IN 2.2"
  },
  {
    "subject": "ITS PERFORMANCE",
    "predicate": "IS SIMILAR TO",
    "object": "THE READY MODEL WHEN THE MODEL IS PRELOADED"
  },
  {
    "subject": "ITS PERFORMANCE",
    "predicate": "IS SIMILAR TO",
    "object": "NVIDIA MPS WHEN THE MODEL IS IN THE HOST MEMORY"
  },
  {
    "subject": "THE TOTAL OVERHEAD",
    "predicate": "IS",
    "object": "THE DIFFERENCE BETWEEN THE LATENCY OF A MECHANISM AND THAT OF THE READY MODEL"
  },
  {
    "subject": "STOP-AND-START",
    "predicate": "PERFORMS",
    "object": "THE WORST"
  },
  {
    "subject": "STOP-AND-START",
    "predicate": "TAKES",
    "object": "SEVERAL SECONDS"
  },
  {
    "subject": "THE MAIN SOURCE OF THE OVERHEAD",
    "predicate": "IS",
    "object": "CUDA CONTEXT INITIALIZATION AND RST-TIME LIBRARY LOADING OPERATIONS IN PYTORCH"
  },
  {
    "subject": "ANOTHER SOURCE",
    "predicate": "IS",
    "object": "GPU MEMORY SWAPPING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "PERFORMS",
    "object": "THE BEST"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "IS CLOSE TO",
    "object": "THE LOWER BOUND"
  },
  {
    "subject": "THE OVERHEAD OF PIPESWITCH FOR MOST CONFIGURATIONS",
    "predicate": "IS",
    "object": "UP TO 10MS"
  },
  {
    "subject": "BERT ON T4",
    "predicate": "HAS OVERHEAD DUE TO",
    "object": "THE LARGE MODEL SIZE"
  },
  {
    "subject": "BERT ON T4",
    "predicate": "HAS OVERHEAD DUE TO",
    "object": "THE SMALLER PCIE BANDWIDTH ON T4 THAN THAT ON V100"
  },
  {
    "subject": "TIME TO COMPUTE BERT ON T4",
    "predicate": "IS",
    "object": "120MS"
  },
  {
    "subject": "THE RELATIVE OVERHEAD",
    "predicate": "IS",
    "object": "ACCEPTABLE"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "TO START",
    "object": "COMPUTING THE RST LAYER"
  },
  {
    "subject": "READY MODEL",
    "predicate": "TO START",
    "object": "COMPUTING"
  },
  {
    "subject": "THE STARTUP OVERHEAD OF PIPESWITCH",
    "predicate": "IS",
    "object": "ONLY A FEW MILLISECONDS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "INCURS",
    "object": "ONLY A FEW MILLISECONDS OVERHEAD FOR TASK SWITCHING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ACHIEVES",
    "object": "LOW LATENCY CLOSE TO THE LOWER BOUND"
  },
  {
    "subject": "WE",
    "predicate": "COMPARE",
    "object": "THROUGHPUT AND END-TO-END LATENCY OF DIFFERENT MECHANISMS UNDER DIFFERENT SCHEDULING CYCLES"
  },
  {
    "subject": "WE",
    "predicate": "USE",
    "object": "RESNET152"
  },
  {
    "subject": "RESNET152",
    "predicate": "USED FOR",
    "object": "TRAINING"
  },
  {
    "subject": "RESNET152",
    "predicate": "USED FOR",
    "object": "INFERENCE"
  },
  {
    "subject": "TRAINING AND INFERENCE",
    "predicate": "PERFORMED ON",
    "object": "EIGHT P3.2XLARGE INSTANCES"
  },
  {
    "subject": "WE",
    "predicate": "SWITCH BETWEEN",
    "object": "TRAINING AND INFERENCE"
  },
  {
    "subject": "SWITCHING",
    "predicate": "OCCURS AFTER",
    "object": "EACH SCHEDULING CYCLE"
  },
  {
    "subject": "FIGURE 6(A)",
    "predicate": "SHOWS",
    "object": "INFERENCE THROUGHPUT"
  },
  {
    "subject": "THE DASHED LINE",
    "predicate": "IS",
    "object": "THE UPPER BOUND"
  },
  {
    "subject": "THE UPPER BOUND",
    "predicate": "IS",
    "object": "THE THROUGHPUT OF THE READY MODEL ASSUMING NO TASK SWITCHING"
  },
  {
    "subject": "THE DASHED LINE",
    "predicate": "IS",
    "object": "THE LOWER BOUND"
  },
  {
    "subject": "THE LOWER BOUND",
    "predicate": "IS",
    "object": "THE AVERAGE LATENCY OF THE READY MODEL"
  },
  {
    "subject": "THE AVERAGE LATENCY OF THE READY MODEL",
    "predicate": "ASSUMES",
    "object": "NO TASK SWITCHING"
  },
  {
    "subject": "THROUGHPUT OF STOP-AND-START",
    "predicate": "IS",
    "object": "NEARLY ZERO FOR SCHEDULING CYCLES SMALLER THAN 10 S"
  },
  {
    "subject": "TASK SWITCHING",
    "predicate": "TAKES",
    "object": "SEVERAL SECONDS"
  },
  {
    "subject": "MPS",
    "predicate": "KEEPS",
    "object": "POOR THROUGHPUT AROUND 100 BATCHES PER SECOND"
  },
  {
    "subject": "GPU UTILIZATION",
    "predicate": "IS DEFINED AS",
    "object": "THE RATIO TO THE UPPER BOUND"
  },
  {
    "subject": "FIGURE 6(B)",
    "predicate": "SHOWS",
    "object": "THE AVERAGE LATENCY OF THE INFERENCE TASKS"
  },
  {
    "subject": "THE ERROR BAR",
    "predicate": "INDICATES",
    "object": "THE MINIMUM AND MAXIMUM LATENCY"
  },
  {
    "subject": "STOP- AND-START",
    "predicate": "HAS",
    "object": "POOR LATENCY"
  },
  {
    "subject": "RST BATCH",
    "predicate": "HAS",
    "object": "SEVERAL SECONDS OVERHEAD"
  },
  {
    "subject": "STOP- AND-START",
    "predicate": "HAS POOR LATENCY BECAUSE",
    "object": "RST BATCH HAS SEVERAL SECONDS OVERHEAD"
  },
  {
    "subject": "MPS",
    "predicate": "HAS",
    "object": "ABOUT 80 MS AVERAGE LATENCY"
  },
  {
    "subject": "MPS",
    "predicate": "HAS",
    "object": "SEVERAL HUNDRED MILLISECONDS LATENCY FOR THE RST BATCH"
  },
  {
    "subject": "LATENCY",
    "predicate": "RANGES FROM",
    "object": "7500 TO 10000 MS"
  },
  {
    "subject": "PIPESWITCH MPS",
    "predicate": "HAS LATENCY LOWER BOUND",
    "object": "B"
  },
  {
    "subject": "THROUGHPUT",
    "predicate": "IS MEASURED UNDER",
    "object": "DIFFERENT SCHEDULING CYCLES"
  },
  {
    "subject": "LATENCY",
    "predicate": "IS MEASURED UNDER",
    "object": "DIFFERENT SCHEDULING CYCLES"
  },
  {
    "subject": "THROUGHPUT AND LATENCY",
    "predicate": "ARE FOR",
    "object": "RESNET"
  },
  {
    "subject": "RESNET",
    "predicate": "RUNS ON",
    "object": "P3.2XLARGE"
  },
  {
    "subject": "PER-LAYER",
    "predicate": "IS",
    "object": "PIPELINE"
  },
  {
    "subject": "COMPUTATION",
    "predicate": "STARTS",
    "object": "ONCE PARAMETERS ARE TRANSMITTED"
  },
  {
    "subject": "FIGURE 7",
    "predicate": "SHOWS",
    "object": "THE TOTAL TIME MEASURED BY THE CLIENT FOR AN INFERENCE TASK TO PREEMPT A TRAINING TASK AND FINISH ITS INFERENCE"
  },
  {
    "subject": "FIGURE 8",
    "predicate": "SHOWS",
    "object": "THE TOTAL TIME MEASURED BY THE CLIENT"
  },
  {
    "subject": "NO OPTIMIZATION",
    "predicate": "PERFORMS",
    "object": "THE WORST IN MOST CASES"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "IMPROVES",
    "object": "NO OPTIMIZATION"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "COMBINES",
    "object": "LAYERS OF THE MODEL INTO ONE BIG TENSOR"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "TRANSMITS",
    "object": "ONE BIG TENSOR IN ONE GROUP"
  },
  {
    "subject": "PER-LAYER PIPELINE",
    "predicate": "OVERLAPS",
    "object": "TRANSMISSION AND COMPUTATION AT THE GRANULARITY OF LAYER"
  },
  {
    "subject": "MODELS WITH MANY LAYERS BUT RELATIVELY LIGHT COMPUTATION SUCH AS RESNET152 AND INCEPTION",
    "predicate": "CAN PERFORM",
    "object": "WORSE THAN GROUPED TRANSMISSION"
  },
  {
    "subject": "MODELS WITH MANY LAYERS BUT RELATIVELY LIGHT COMPUTATION SUCH AS RESNET152 AND INCEPTION",
    "predicate": "CAN PERFORM",
    "object": "SOMETIMES EVEN WORSE THAN NO PIPELINE"
  },
  {
    "subject": "THIS REDUCTION",
    "predicate": "IS",
    "object": "SIGNIFICANT"
  },
  {
    "subject": "THIS REDUCTION",
    "predicate": "IS EVALUATED WHEN",
    "object": "THE OPTIMIZATIONS ON MEMORY MANAGEMENT AND WORKER SWITCHING HAVE ALREADY BEEN APPLIED"
  },
  {
    "subject": "WE",
    "predicate": "WOULD LIKE TO EMPHASIZE",
    "object": "THAT TO MEET STRICT SLOS, IT IS IMPORTANT TO REDUCE ALL OVERHEADS FOR TASK SWITCHING"
  },
  {
    "subject": "ALL OVERHEADS FOR TASK SWITCHING",
    "predicate": "INCLUDE",
    "object": "NOT ONLY THE MOST SIGNIFICANT ONE"
  },
  {
    "subject": "NUMBER OF LAYERS",
    "predicate": "INCLUDES",
    "object": "BOTH WEIGHTED AND UNWEIGHTED LAYERS"
  },
  {
    "subject": "BOTH WEIGHTED AND UNWEIGHTED LAYERS",
    "predicate": "CONTRIBUTE TO",
    "object": "COMPUTATION TIME"
  },
  {
    "subject": "WE",
    "predicate": "MEASURE",
    "object": "THE PARAMETER SIZE"
  },
  {
    "subject": "WE",
    "predicate": "MEASURE",
    "object": "RUNNING TIME"
  },
  {
    "subject": "THE PARAMETER SIZE",
    "predicate": "IS MEASURED FOR",
    "object": "EACH LAYER"
  },
  {
    "subject": "RUNNING TIME",
    "predicate": "IS MEASURED FOR",
    "object": "EACH LAYER"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "TAKES",
    "object": "ONLY SEVERAL SECONDS TO COMPUTE AN OPTIMAL GROUPING STRATEGY"
  },
  {
    "subject": "RESNET152",
    "predicate": "HAS",
    "object": "HUNDREDS OF LAYERS"
  },
  {
    "subject": "NO PRUNING",
    "predicate": "DOES NOT FINISH",
    "object": "FOR ALL THREE MODELS AFTER RUNNING FOR 24 HOURS"
  },
  {
    "subject": "UNIED MEMORY MANAGEMENT",
    "predicate": "IS USED TO EVALUATE",
    "object": "THE EFFECTIVENESS OF UNIED MEMORY MANAGEMENT"
  },
  {
    "subject": "NO UNIED MEMORY MANAGEMENT",
    "predicate": "IS",
    "object": "STATED"
  },
  {
    "subject": "FIGURE 8",
    "predicate": "DESCRIBES",
    "object": "EFFECTIVENESS OF UNIFIED MEMORY MANAGEMENT"
  },
  {
    "subject": "WE",
    "predicate": "COMPARE",
    "object": "THE FOLLOWING VE MECHANISMS DISCUSSED IN 4.3"
  },
  {
    "subject": "IPC",
    "predicate": "HAS",
    "object": "NO OPTIMIZATION"
  },
  {
    "subject": "THE PAGES OF THE MEMORY DAEMON",
    "predicate": "ARE NOT PINNED TO",
    "object": "THE MAIN MEMORY"
  },
  {
    "subject": "THIS EXPERIMENT",
    "predicate": "DEMONSTRATES",
    "object": "ALL THE OPTIMIZATIONS ON MEMORY MANAGEMENT ARE EFFECTIVE"
  },
  {
    "subject": "UNIED MEMORY MANAGEMENT MECHANISM",
    "predicate": "IS USED BY",
    "object": "PIPESWITCH"
  },
  {
    "subject": "IPC OPTIMIZATION",
    "predicate": "IS",
    "object": "IMPORTANT"
  },
  {
    "subject": "IPC OPTIMIZATION",
    "predicate": "REDUCES",
    "object": "LATENCY BY 1648 MS"
  },
  {
    "subject": "PINNING THE PAGES TO THE HOST MEMORY",
    "predicate": "CAN REDUCE",
    "object": "THE LATENCY WITH A FEW MILLISECONDS"
  },
  {
    "subject": "ONE",
    "predicate": "IS",
    "object": "PROCESS"
  },
  {
    "subject": "FIGURE 9",
    "predicate": "DESCRIBES",
    "object": "EFFECTIVENESS OF ACTIVE-STANDBY SWITCHING"
  },
  {
    "subject": "FIGURE 9",
    "predicate": "SHOWS",
    "object": "THE RESULTS"
  },
  {
    "subject": "TWO PROCESSES",
    "predicate": "PERFORM",
    "object": "THE WORST"
  },
  {
    "subject": "THE WORST",
    "predicate": "STOPS",
    "object": "THE TRAINING TASK"
  },
  {
    "subject": "THE WORST",
    "predicate": "INITIALIZES",
    "object": "A NEW PROCESS FOR THE NEW TASK"
  },
  {
    "subject": "THE NEW PROCESS",
    "predicate": "NEEDS TO CREATE",
    "object": "A NEW CUDA ENVIRONMENT"
  },
  {
    "subject": "A NEW CUDA ENVIRONMENT",
    "predicate": "DOMINATES",
    "object": "THE TOTAL TIME"
  },
  {
    "subject": "ONE PROCESS",
    "predicate": "REUSES",
    "object": "THE CUDA ENVIRONMENT"
  },
  {
    "subject": "ONE PROCESS",
    "predicate": "PAYS",
    "object": "THE OVERHEAD TO CLEAN THE ENVIRONMENT"
  },
  {
    "subject": "MANY FRAMEWORKS",
    "predicate": "HAVE BEEN DEVELOPED FOR",
    "object": "DEEP LEARNING"
  },
  {
    "subject": "FRAMEWORKS",
    "predicate": "INCLUDE",
    "object": "TENSORFLOW"
  },
  {
    "subject": "FRAMEWORKS",
    "predicate": "INCLUDE",
    "object": "PYTORCH"
  },
  {
    "subject": "FRAMEWORKS",
    "predicate": "INCLUDE",
    "object": "MXNET"
  },
  {
    "subject": "SEVERAL ALGORITHMS AND SYSTEMS",
    "predicate": "HAVE BEEN DESIGNED FOR",
    "object": "EXECUTING AND SCHEDULING DEEP LEARNING TASKS ON CLUSTERS"
  },
  {
    "subject": "SEVERAL ALGORITHMS AND SYSTEMS",
    "predicate": "INCLUDE",
    "object": "BOTH TRAINING AND INFERENCE TASKS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "FOCUSES ON",
    "object": "HOW TO REALIZE A SCHEDULING DECISION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "ENABLES",
    "object": "THE SCHEDULER TO CHANGE THE RESOURCE ALLOCATION MORE OFTEN WITH MILLISECOND-SCALE TASK SWITCHING"
  },
  {
    "subject": "MANY TECHNIQUES AND SYSTEMS",
    "predicate": "HAVE BEEN PROPOSED TO",
    "object": "OPTIMIZE COMMUNICATION"
  },
  {
    "subject": "MANY TECHNIQUES AND SYSTEMS",
    "predicate": "HAVE BEEN PROPOSED TO",
    "object": "IMPROVE DISTRIBUTED TRAINING"
  },
  {
    "subject": "THE MOST RELEVANT ONES",
    "predicate": "ARE",
    "object": "PIPEDREAM 8"
  },
  {
    "subject": "THE MOST RELEVANT ONES",
    "predicate": "ARE",
    "object": "BYTESCHEDULER 9"
  },
  {
    "subject": "THE MOST RELEVANT ONES",
    "predicate": "ARE",
    "object": "POSEIDON 40"
  },
  {
    "subject": "OTHER WORKS LIKE VDNN 43 AND SWAPADVISOR 44",
    "predicate": "HAVE",
    "object": "GPU MEMORY MANAGEMENT MODULE"
  },
  {
    "subject": "CLUSTER MANAGERS 4548",
    "predicate": "ALLOCATE",
    "object": "GPUS TO VMS OR CONTAINERS AT DEVICE GRANULARITY"
  },
  {
    "subject": "SEVERAL SOLUTIONS",
    "predicate": "HAVE BEEN PROPOSED TO",
    "object": "SHARE A GPU AT APPLICATION GRANULARITY"
  },
  {
    "subject": "TECHNIQUES",
    "predicate": "LIKE",
    "object": "LIBRARY INTERCEPTION 6,4953"
  },
  {
    "subject": "DEEP LEARNING APPLICATIONS",
    "predicate": "TYPICALLY REQUIRE",
    "object": "HUNDREDS OF KERNELS"
  },
  {
    "subject": "EFFORTS",
    "predicate": "ARE ON",
    "object": "GPU OPTIMIZATION"
  },
  {
    "subject": "GPU OPTIMIZATION",
    "predicate": "IMPROVES",
    "object": "PERFORMANCE OF RUNNING A SINGLE TASK"
  },
  {
    "subject": "EFFORTS",
    "predicate": "INCLUDE",
    "object": "TENSOR FUSION"
  },
  {
    "subject": "EFFORTS",
    "predicate": "INCLUDE",
    "object": "KERNEL-LEVEL CONCURRENCY"
  },
  {
    "subject": "EFFORTS",
    "predicate": "INCLUDE",
    "object": "SCHEDULING"
  },
  {
    "subject": "WE",
    "predicate": "THANK",
    "object": "OUR SHEPHERD MADAN MUSU-VATHI"
  },
  {
    "subject": "WE",
    "predicate": "THANK",
    "object": "THE ANONYMOUS REVIEWERS"
  },
  {
    "subject": "THE ANONYMOUS REVIEWERS",
    "predicate": "HAVE",
    "object": "VALUABLE FEEDBACK"
  },
  {
    "subject": "ZHIHAO BAI",
    "predicate": "WAS SUPPORTED BY",
    "object": "AN AWS MACHINE LEARNING RESEARCH AWARD"
  },
  {
    "subject": "ZHEN ZHANG",
    "predicate": "WAS SUPPORTED BY",
    "object": "AN AWS MACHINE LEARNING RESEARCH AWARD"
  },
  {
    "subject": "XIN JIN",
    "predicate": "WAS SUPPORTED BY",
    "object": "AN AWS MACHINE LEARNING RESEARCH AWARD"
  },
  {
    "subject": "A. VERMA, L. PEDROSA, M. KORUPOLU, D. OPPENHEIMER, E. TUNE, AND J. WILKES",
    "predicate": "AUTHORED",
    "object": "LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG"
  },
  {
    "subject": "LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG",
    "predicate": "PUBLISHED_IN",
    "object": "EUROSYS"
  },
  {
    "subject": "LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG",
    "predicate": "PUBLISHED_IN_YEAR",
    "object": "2015"
  },
  {
    "subject": "DEAN AND L. A. BARROSO",
    "predicate": "WROTE",
    "object": "THE TAIL AT SCALE"
  },
  {
    "subject": "THE TAIL AT SCALE",
    "predicate": "APPEARED IN",
    "object": "COMMUNICATIONS OF THE ACM"
  },
  {
    "subject": "COMMUNICATIONS OF THE ACM",
    "predicate": "HAS VOLUME",
    "object": "VOL."
  },
  {
    "subject": "NEXUS",
    "predicate": "IS",
    "object": "A GPU CLUSTER ENGINE"
  },
  {
    "subject": "NEXUS",
    "predicate": "PURPOSE",
    "object": "ACCELERATING DNN-BASED VIDEO ANALYSIS"
  },
  {
    "subject": "NEXUS",
    "predicate": "PUBLISHED_IN",
    "object": "ACM SOSP"
  },
  {
    "subject": "NEXUS",
    "predicate": "PUBLISHED_YEAR",
    "object": "2019"
  },
  {
    "subject": "H. SHEN",
    "predicate": "IS_AUTHOR_OF",
    "object": "NEXUS"
  },
  {
    "subject": "L. CHEN",
    "predicate": "IS_AUTHOR_OF",
    "object": "NEXUS"
  },
  {
    "subject": "Y. JIN",
    "predicate": "IS_AUTHOR_OF",
    "object": "NEXUS"
  },
  {
    "subject": "L. ZHAO",
    "predicate": "IS_AUTHOR_OF",
    "object": "NEXUS"
  },
  {
    "subject": "B. KONG",
    "predicate": "IS_AUTHOR_OF",
    "object": "NEXUS"
  },
  {
    "subject": "M. PHILIPOSE",
    "predicate": "IS_AUTHOR_OF",
    "object": "NEXUS"
  },
  {
    "subject": "A. KRISHNAMURTHY",
    "predicate": "IS_AUTHOR_OF",
    "object": "NEXUS"
  },
  {
    "subject": "R. SUNDARAM",
    "predicate": "IS_AUTHOR_OF",
    "object": "NEXUS"
  },
  {
    "subject": "FRIED, J. BEHRENS, A. BELAY, AND H. BAL-AKRISHNAN",
    "predicate": "AUTHORED",
    "object": "SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS"
  },
  {
    "subject": "SHENANGO",
    "predicate": "PUBLISHED_IN",
    "object": "USENIX NSDI"
  },
  {
    "subject": "SHENANGO",
    "predicate": "PUBLISHED_YEAR",
    "object": "2019"
  },
  {
    "subject": "CUDA MULTI-PROCESS SERVICE",
    "predicate": "IS",
    "object": "6"
  },
  {
    "subject": "CUDAMULTIPROCESSSERVICEOVERVIEW.PDF",
    "predicate": "IS LOCATED AT",
    "object": "HTTPS://DOCS.NVIDIA.COM/DEPLOY.PDF"
  },
  {
    "subject": "7 P. YU AND M. CHOWDHURY",
    "predicate": "AUTHORED",
    "object": "SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS"
  },
  {
    "subject": "SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS",
    "predicate": "PUBLISHED_IN",
    "object": "CONFERENCE ON MACHINE LEARNING AND SYSTEMS"
  },
  {
    "subject": "SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS",
    "predicate": "PUBLISHED_YEAR",
    "object": "2020"
  },
  {
    "subject": "PIPEDREAM",
    "predicate": "IS",
    "object": "GENERALIZED PIPELINE PARALLELISM FOR DNN TRAINING"
  },
  {
    "subject": "PIPEDREAM",
    "predicate": "WAS PUBLISHED IN",
    "object": "ACM SOSP"
  },
  {
    "subject": "PIPEDREAM",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2019"
  },
  {
    "subject": "D. NARAYANAN",
    "predicate": "IS AUTHOR OF",
    "object": "PIPEDREAM"
  },
  {
    "subject": "A. HARLAP",
    "predicate": "IS AUTHOR OF",
    "object": "PIPEDREAM"
  },
  {
    "subject": "A. PHANISHAYEE",
    "predicate": "IS AUTHOR OF",
    "object": "PIPEDREAM"
  },
  {
    "subject": "V. SESHADRI",
    "predicate": "IS AUTHOR OF",
    "object": "PIPEDREAM"
  },
  {
    "subject": "N. R. DEVANUR",
    "predicate": "IS AUTHOR OF",
    "object": "PIPEDREAM"
  },
  {
    "subject": "G. R. GANGER",
    "predicate": "IS AUTHOR OF",
    "object": "PIPEDREAM"
  },
  {
    "subject": "P. B. GIBBONS",
    "predicate": "IS AUTHOR OF",
    "object": "PIPEDREAM"
  },
  {
    "subject": "M. ZAHARIA",
    "predicate": "IS AUTHOR OF",
    "object": "PIPEDREAM"
  },
  {
    "subject": "9 Y. PENG, Y. ZHU, Y. CHEN, Y. BAO, B. YI, C. LAN, C. WU, AND C. GUO",
    "predicate": "AUTHORED",
    "object": "A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION"
  },
  {
    "subject": "A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION",
    "predicate": "PUBLISHED_IN",
    "object": "ACM SOSP"
  },
  {
    "subject": "A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION",
    "predicate": "PUBLISHED_IN_YEAR",
    "object": "2019"
  },
  {
    "subject": "TIRESIAS",
    "predicate": "IS",
    "object": "A GPU CLUSTER MANAGER FOR DISTRIBUTED DEEP LEARNING"
  },
  {
    "subject": "TIRESIAS",
    "predicate": "WAS PUBLISHED IN",
    "object": "USENIX NSDI"
  },
  {
    "subject": "TIRESIAS",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2019"
  },
  {
    "subject": "J. GU, M. CHOWDHURY, K. G. SHIN, Y. ZHU, M. JEON, J. QIAN, H. LIU, AND C. GUO",
    "predicate": "ARE AUTHORS OF",
    "object": "TIRESIAS"
  },
  {
    "subject": "POSEIDON",
    "predicate": "IS",
    "object": "AN EFFICIENT COMMUNICATION ARCHITECTURE"
  },
  {
    "subject": "POSEIDON",
    "predicate": "IS FOR",
    "object": "DISTRIBUTED DEEP LEARNING ON GPU CLUSTERS"
  },
  {
    "subject": "POSEIDON",
    "predicate": "WAS PRESENTED IN",
    "object": "USENIX ATC"
  },
  {
    "subject": "POSEIDON",
    "predicate": "WAS PRESENTED IN YEAR",
    "object": "2017"
  },
  {
    "subject": "H. ZHANG, Z. ZHENG, S. XU, W. DAI, Q. HO, X. LIANG, Z. HU, J. WEI, P. XIE, AND E. P. XING",
    "predicate": "ARE AUTHORS OF",
    "object": "POSEIDON"
  },
  {
    "subject": "AMAZON WEB SERVICES",
    "predicate": "IS",
    "object": "12"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "AWS.AMAZON.COM"
  },
  {
    "subject": "MICROSOFT",
    "predicate": "OFFERS",
    "object": "AZURE"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "AZURE.MICROSOFT.COM"
  },
  {
    "subject": "GOOGLE CLOUD PLATFORM",
    "predicate": "IS",
    "object": "14"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "CLOUD.GOOGLE.COM"
  },
  {
    "subject": "HOROVOD",
    "predicate": "IS",
    "object": "FAST AND EASY DISTRIBUTED DEEP LEARNING FRAMEWORK"
  },
  {
    "subject": "HOROVOD",
    "predicate": "IS USED IN",
    "object": "TENSORFLOW"
  },
  {
    "subject": "15 A. SERGEEV AND M. DEL BALSO",
    "predicate": "ARE AUTHORS OF",
    "object": "HOROVOD"
  },
  {
    "subject": "HOROVOD PAPER",
    "predicate": "IS PUBLISHED IN",
    "object": "ARXIV PREPRINT ARXIV:1802.05799"
  },
  {
    "subject": "HOROVOD PAPER",
    "predicate": "IS PUBLISHED IN YEAR",
    "object": "2018"
  },
  {
    "subject": "SU",
    "predicate": "PUBLISHED",
    "object": "SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER"
  },
  {
    "subject": "SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER",
    "predicate": "PUBLISHED_IN",
    "object": "USENIX OSDI"
  },
  {
    "subject": "SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER",
    "predicate": "PUBLISHED_YEAR",
    "object": "2014"
  },
  {
    "subject": "DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION",
    "predicate": "AUTHORED BY",
    "object": "SUN"
  },
  {
    "subject": "DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION",
    "predicate": "PUBLISHED IN",
    "object": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"
  },
  {
    "subject": "DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION",
    "predicate": "PUBLISHED IN YEAR",
    "object": "2016"
  },
  {
    "subject": "NVIDIA DATA CENTER DEEP LEARNING PRODUCT",
    "predicate": "HAS",
    "object": "PERFORMANCE"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "DEVELOPER.NVIDIA.COMDEEP-LEARNING-PERFORMANCE-TRAINING-INFERENCE"
  },
  {
    "subject": "PHILLY",
    "predicate": "HAS",
    "object": "20 TRACES"
  },
  {
    "subject": "HTTPS:GITHUB.COM/MSR-FIDDLE",
    "predicate": "RELATED TO",
    "object": "PHILLY-TRACES"
  },
  {
    "subject": "21",
    "predicate": "IS",
    "object": "PYTORCH"
  },
  {
    "subject": "22 C. SZEGEDY, V. VANHOUCKE, S. IOFFE, J. SHLENS, AND Z. WOJNA",
    "predicate": "AUTHORED",
    "object": "RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION"
  },
  {
    "subject": "RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION",
    "predicate": "PUBLISHED_IN",
    "object": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"
  },
  {
    "subject": "RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION",
    "predicate": "PUBLISHED_YEAR",
    "object": "2016"
  },
  {
    "subject": "J. DEVLIN, M.-W. CHANG, K. LEE, AND K. TOUTANOVA",
    "predicate": "AUTHORED",
    "object": "BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING"
  },
  {
    "subject": "BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING",
    "predicate": "WAS PUBLISHED IN",
    "object": "PROCEEDINGS OF THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, VOLUME 1 (LONG AND SHORT PAPERS)"
  },
  {
    "subject": "PROCEEDINGS OF THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, VOLUME 1 (LONG AND SHORT PAPERS)",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2019"
  },
  {
    "subject": "GANDIVA",
    "predicate": "IS",
    "object": "INTROSPECTIVE CLUSTER SCHEDULING FOR DEEP LEARNING"
  },
  {
    "subject": "GANDIVA",
    "predicate": "WAS PUBLISHED IN",
    "object": "USENIX OSDI"
  },
  {
    "subject": "GANDIVA",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2018"
  },
  {
    "subject": "24 W. XIAO, R. BHARDWAJ, R. RAMJEE, M. SIVATHANU, N. KWATRA, Z. HAN, P. PATEL, X. PENG, H. ZHAO, Q. ZHANG, ET AL.",
    "predicate": "ARE AUTHORS OF",
    "object": "GANDIVA"
  },
  {
    "subject": "25",
    "predicate": "IS",
    "object": "TENSORFLOW"
  },
  {
    "subject": "TENSORFLOW XLA",
    "predicate": "HAS NUMBER",
    "object": "54"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "WWW.TENSORFLOW.ORG"
  },
  {
    "subject": "HTTPS://WWW.TENSORFLOW.ORG",
    "predicate": "HAS",
    "object": "XLA"
  },
  {
    "subject": "26",
    "predicate": "IS",
    "object": "MXNET"
  },
  {
    "subject": "MXNET.APACHE.ORG",
    "predicate": "USES",
    "object": "HTTPS"
  },
  {
    "subject": "SLAQ",
    "predicate": "IS",
    "object": "QUALITY-DRIVEN SCHEDULING FOR DISTRIBUTED MACHINE LEARNING"
  },
  {
    "subject": "SLAQ",
    "predicate": "AUTHORED BY",
    "object": "M. J. FREEDMAN"
  },
  {
    "subject": "SLAQ",
    "predicate": "PUBLISHED IN",
    "object": "ACM SYMPOSIUM ON CLOUD COMPUTING"
  },
  {
    "subject": "SLAQ",
    "predicate": "PUBLISHED IN YEAR",
    "object": "2017"
  },
  {
    "subject": "OPTIMUS",
    "predicate": "IS",
    "object": "AN EFFICIENT DYNAMIC RESOURCE SCHEDULER FOR DEEP LEARNING CLUSTERS"
  },
  {
    "subject": "OPTIMUS",
    "predicate": "WAS PUBLISHED IN",
    "object": "EUROSYS"
  },
  {
    "subject": "OPTIMUS",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2018"
  },
  {
    "subject": "Y. PENG, Y. BAO, Y. CHEN, C. WU, AND C. GUO",
    "predicate": "ARE AUTHORS OF",
    "object": "OPTIMUS"
  },
  {
    "subject": "THEMIS",
    "predicate": "IS",
    "object": "FAIR AND EFFICIENT GPU CLUSTER SCHEDULING"
  },
  {
    "subject": "THEMIS",
    "predicate": "WAS PUBLISHED IN",
    "object": "USENIX NSDI"
  },
  {
    "subject": "THEMIS",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2020"
  },
  {
    "subject": "K. MAHAJAN, A. BALASUBRAMANIAN, A. SINGHVI, S. VENKATARAMAN, A. AKELLA, A. PHANISHAYEE, AND S. CHAWLA",
    "predicate": "ARE AUTHORS OF",
    "object": "THEMIS"
  },
  {
    "subject": "HYPERSCHED",
    "predicate": "IS",
    "object": "DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE"
  },
  {
    "subject": "HYPERSCHED",
    "predicate": "WAS PRESENTED IN",
    "object": "ACM SYMPOSIUM ON CLOUD COMPUTING"
  },
  {
    "subject": "HYPERSCHED",
    "predicate": "WAS PUBLISHED IN",
    "object": "2019"
  },
  {
    "subject": "R. LIAW",
    "predicate": "IS AN AUTHOR OF",
    "object": "HYPERSCHED"
  },
  {
    "subject": "R. BHARDWAJ",
    "predicate": "IS AN AUTHOR OF",
    "object": "HYPERSCHED"
  },
  {
    "subject": "L. DUNLAP",
    "predicate": "IS AN AUTHOR OF",
    "object": "HYPERSCHED"
  },
  {
    "subject": "Y. ZOU",
    "predicate": "IS AN AUTHOR OF",
    "object": "HYPERSCHED"
  },
  {
    "subject": "J. E. GONZALEZ",
    "predicate": "IS AN AUTHOR OF",
    "object": "HYPERSCHED"
  },
  {
    "subject": "I. STOICA",
    "predicate": "IS AN AUTHOR OF",
    "object": "HYPERSCHED"
  },
  {
    "subject": "A. TUMANOV",
    "predicate": "IS AN AUTHOR OF",
    "object": "HYPERSCHED"
  },
  {
    "subject": "CHET",
    "predicate": "IS",
    "object": "AN OPTIMIZING COMPILER FOR FULLY-HOMOMORPHIC NEURAL-NETWORK INFERENCING"
  },
  {
    "subject": "CHET",
    "predicate": "WAS PRESENTED IN",
    "object": "ACM CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION"
  },
  {
    "subject": "CHET",
    "predicate": "WAS PRESENTED IN YEAR",
    "object": "2019"
  },
  {
    "subject": "R. DATHATHRI",
    "predicate": "IS AUTHOR OF",
    "object": "CHET"
  },
  {
    "subject": "O. SAARIKIVI",
    "predicate": "IS AUTHOR OF",
    "object": "CHET"
  },
  {
    "subject": "H. CHEN",
    "predicate": "IS AUTHOR OF",
    "object": "CHET"
  },
  {
    "subject": "K. LAINE",
    "predicate": "IS AUTHOR OF",
    "object": "CHET"
  },
  {
    "subject": "K. LAUTER",
    "predicate": "IS AUTHOR OF",
    "object": "CHET"
  },
  {
    "subject": "S. MALEKI",
    "predicate": "IS AUTHOR OF",
    "object": "CHET"
  },
  {
    "subject": "M. MUSUVATHI",
    "predicate": "IS AUTHOR OF",
    "object": "CHET"
  },
  {
    "subject": "T. MYTKOWICZ",
    "predicate": "IS AUTHOR OF",
    "object": "CHET"
  },
  {
    "subject": "TVM",
    "predicate": "IS",
    "object": "AN AUTOMATED END-TO-END OPTIMIZING COMPILER FOR DEEP LEARNING"
  },
  {
    "subject": "TVM",
    "predicate": "WAS PUBLISHED IN",
    "object": "USENIX OSDI"
  },
  {
    "subject": "TVM",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2018"
  },
  {
    "subject": "T. CHEN, T. MOREAU, Z. JIANG, L. ZHENG, E. YAN, H. SHEN, M. COWAN, L. WANG, Y. HU, L. CEZE, ET AL.",
    "predicate": "ARE AUTHORS OF",
    "object": "TVM"
  },
  {
    "subject": "GPIPE",
    "predicate": "IS",
    "object": "EFFICIENT TRAINING OF GIANT NEURAL NETWORKS USING PIPELINE PARALLELISM"
  },
  {
    "subject": "GPIPE",
    "predicate": "PUBLISHED_IN",
    "object": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS"
  },
  {
    "subject": "GPIPE",
    "predicate": "PUBLISHED_YEAR",
    "object": "2019"
  },
  {
    "subject": "Y. HUANG ET AL.",
    "predicate": "AUTHORED",
    "object": "GPIPE"
  },
  {
    "subject": "BLINK",
    "predicate": "IS",
    "object": "FAST AND GENERIC COLLECTIVES FOR DISTRIBUTED ML"
  },
  {
    "subject": "BLINK",
    "predicate": "IS PRESENTED IN",
    "object": "CONFERENCE ON MACHINE LEARNING AND SYSTEMS"
  },
  {
    "subject": "BLINK",
    "predicate": "IS PUBLISHED IN",
    "object": "2020"
  },
  {
    "subject": "G. WANG, S. VENKATARAMAN, A. PHANISHAYEE, J. THELIN, N. DEVANUR, AND I. STOICA",
    "predicate": "ARE AUTHORS OF",
    "object": "BLINK"
  },
  {
    "subject": "NVIDIA COLLECTIVE COMMUNICATIONS LIBRARY",
    "predicate": "ABBREVIATED AS",
    "object": "NCCL"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "DEVELOPER.NVIDIA.COM/NCCL"
  },
  {
    "subject": "36 J. LIU, J. WU, AND D. K. PANDA",
    "predicate": "AUTHORED",
    "object": "HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION OVER INNIBAND"
  },
  {
    "subject": "HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION",
    "predicate": "IS",
    "object": "OVER INNIBAND"
  },
  {
    "subject": "37 Q. HO, J. CIPAR, H. CUI, S. LEE, J. K. KIM, P. B. GIBBONS, G. A. GIBSON, G. GANGER, AND E. P. XING",
    "predicate": "AUTHORED",
    "object": "MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER"
  },
  {
    "subject": "MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER",
    "predicate": "PUBLISHED_IN",
    "object": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS"
  },
  {
    "subject": "MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER",
    "predicate": "PUBLISHED_YEAR",
    "object": "2013"
  },
  {
    "subject": "A. AWAN, C.-H. CHU, H. SUBRAMONI, AND D. K. PANDA",
    "predicate": "AUTHORED",
    "object": "OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?"
  },
  {
    "subject": "OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?",
    "predicate": "PUBLISHED_IN",
    "object": "PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING"
  },
  {
    "subject": "PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING",
    "predicate": "YEAR",
    "object": "2018"
  },
  {
    "subject": "DAILY, A. VISHNU, C. SIEGEL, T. WARFEL, AND V. AMATYA",
    "predicate": "AUTHORED",
    "object": "GOSSIPGRAD: SCALABLE DEEP LEARNING USING GOSSIP COMMUNICATION BASED ASYNCHRONOUS GRADIENT DESCENT"
  },
  {
    "subject": "GOSSIPGRAD",
    "predicate": "IS",
    "object": "SCALABLE DEEP LEARNING USING GOSSIP COMMUNICATION BASED ASYNCHRONOUS GRADIENT DESCENT"
  },
  {
    "subject": "GOSSIPGRAD",
    "predicate": "PUBLISHED IN",
    "object": "CORR, VOL."
  },
  {
    "subject": "41 Z. ZHANG, C. CHANG, H. LIN, Y. WANG, R. ARORA, AND X. JIN",
    "predicate": "AUTHORED",
    "object": "IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?"
  },
  {
    "subject": "IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?",
    "predicate": "PUBLISHED_IN",
    "object": "ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)"
  },
  {
    "subject": "IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?",
    "predicate": "PUBLISHED_DATE",
    "object": "AUGUST 2020"
  },
  {
    "subject": "42 Y. CHEN, Z. LIU, B. REN, AND X. JIN",
    "predicate": "WROTE",
    "object": "ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS"
  },
  {
    "subject": "ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS",
    "predicate": "PUBLISHED IN",
    "object": "INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)"
  },
  {
    "subject": "ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS",
    "predicate": "PUBLISHED IN",
    "object": "JULY 2020"
  },
  {
    "subject": "VDNN",
    "predicate": "IS",
    "object": "VIRTUALIZED DEEP NEURAL NETWORKS FOR SCALABLE, MEMORY-EFFICIENT NEURAL NETWORK DESIGN"
  },
  {
    "subject": "VDNN",
    "predicate": "WAS PRESENTED IN",
    "object": "2016 49TH ANNUAL IEEEACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO)"
  },
  {
    "subject": "PAPER",
    "predicate": "WAS WRITTEN BY",
    "object": "43 M. RHU, N. GIMELSHEIN, J. CLEMONS, A. ZULQAR, AND S. W. KECKLER"
  },
  {
    "subject": "SYMPOSIUM",
    "predicate": "OCCURRED IN",
    "object": "2016"
  },
  {
    "subject": "SWAPADVISOR",
    "predicate": "AUTHORED_BY",
    "object": "44 C.-C. HUANG, G. JIN, AND J. LI"
  },
  {
    "subject": "SWAPADVISOR",
    "predicate": "PUBLISHED_IN",
    "object": "ACM ASPLOS"
  },
  {
    "subject": "SWAPADVISOR",
    "predicate": "PUBLISHED_YEAR",
    "object": "2020"
  },
  {
    "subject": "SWAPADVISOR",
    "predicate": "DESCRIPTION",
    "object": "PUSHING DEEP LEARNING BEYOND THE GPU MEMORY LIMIT VIA SMART SWAPPING"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "KUBERNETES.IO"
  },
  {
    "subject": "NVIDIA CONTAINER RUNTIME",
    "predicate": "IS FOR",
    "object": "DOCKER"
  },
  {
    "subject": "HTTPS",
    "predicate": "IS",
    "object": "GITHUB.COM/NVIDIA/NVIDIA-DOCKER"
  },
  {
    "subject": "MESOS",
    "predicate": "IS",
    "object": "A PLATFORM FOR NE-GRAINED RESOURCE SHARING IN THE DATA CENTER"
  },
  {
    "subject": "47 B. HINDMAN, A. KONWINSKI, M. ZAHARIA, A. GHODSI, A. D. JOSEPH, R. H. KATZ, S. SHENKER, AND I. STOICA",
    "predicate": "AUTHORED",
    "object": "MESOS: A PLATFORM FOR NE-GRAINED RESOURCE SHARING IN THE DATA CENTER"
  },
  {
    "subject": "MESOS",
    "predicate": "WAS PRESENTED IN",
    "object": "USENIX NSDI"
  },
  {
    "subject": "USENIX NSDI",
    "predicate": "OCCURRED IN",
    "object": "2011"
  },
  {
    "subject": "APACHE HADOOP YARN",
    "predicate": "IS",
    "object": "YET ANOTHER RESOURCE NEGOTIATOR"
  },
  {
    "subject": "V. K. VAVILAPALLI, A. C. MURTHY, C. DOUGLAS, S. AGARWAL, M. KONAR, R. EVANS, T. GRAVES, J. LOWE, H. SHAH, S. SETH, ET AL.",
    "predicate": "AUTHORED",
    "object": "APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR"
  },
  {
    "subject": "APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR",
    "predicate": "WAS PUBLISHED IN",
    "object": "ACM SYMPOSIUM ON CLOUD COMPUTING"
  },
  {
    "subject": "APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2013"
  },
  {
    "subject": "G. GIUNTA, R. MONTELLA, G. AGRILLO, AND G. COVIELLO",
    "predicate": "AUTHORED",
    "object": "A GPGPU TRANSPARENT VIRTUALIZATION COMPONENT FOR HIGH PERFORMANCE COMPUTING CLOUDS"
  },
  {
    "subject": "A GPGPU TRANSPARENT VIRTUALIZATION COMPONENT FOR HIGH PERFORMANCE COMPUTING CLOUDS",
    "predicate": "PRESENTED IN",
    "object": "EUROPEAN CONFERENCE ON PARALLEL PROCESSING"
  },
  {
    "subject": "EUROPEAN CONFERENCE ON PARALLEL PROCESSING",
    "predicate": "YEAR",
    "object": "2010"
  },
  {
    "subject": "V. T. RAVI, M. BECCHI, G. AGRAWAL, AND S. CHAKRADHAR",
    "predicate": "AUTHORED",
    "object": "SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK"
  },
  {
    "subject": "SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK",
    "predicate": "PUBLISHED_IN",
    "object": "PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING"
  },
  {
    "subject": "PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING",
    "predicate": "YEAR",
    "object": "2011"
  },
  {
    "subject": "GVIM",
    "predicate": "IS",
    "object": "GPU-ACCELERATED VIRTUAL MACHINES"
  },
  {
    "subject": "GVIM",
    "predicate": "IS PRESENTED IN",
    "object": "PROCEEDINGS OF THE 3RD ACM WORKSHOP ON SYSTEM-LEVEL VIRTUALIZATION FOR HIGH PERFORMANCE COMPUTING"
  },
  {
    "subject": "PROCEEDINGS OF THE 3RD ACM WORKSHOP ON SYSTEM-LEVEL VIRTUALIZATION FOR HIGH PERFORMANCE COMPUTING",
    "predicate": "OCCURRED IN",
    "object": "2009"
  },
  {
    "subject": "50 V. GUPTA, A. GAVRILOVSKA, K. SCHWAN, H. KHARCHE, N. TOLIA, V. TALWAR, AND P. RANGANATHAN",
    "predicate": "ARE AUTHORS OF",
    "object": "GVIM"
  },
  {
    "subject": "RCUDA",
    "predicate": "REDUCES",
    "object": "THE NUMBER OF GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS"
  },
  {
    "subject": "51 J. DUATO, A. J. PENA, F. SILLA, R. MAYO, AND E. S. QUINTANA-ORT",
    "predicate": "AUTHORED",
    "object": "RCUDA"
  },
  {
    "subject": "RCUDA",
    "predicate": "WAS PRESENTED IN",
    "object": "2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION"
  },
  {
    "subject": "2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION",
    "predicate": "OCCURRED IN",
    "object": "2010"
  },
  {
    "subject": "VCUDA",
    "predicate": "IS",
    "object": "GPU-ACCELERATED HIGH-PERFORMANCE COMPUTING IN VIRTUAL MACHINES"
  },
  {
    "subject": "SUN AND K. LI",
    "predicate": "ARE AUTHORS OF",
    "object": "VCUDA"
  },
  {
    "subject": "VCUDA",
    "predicate": "IS PUBLISHED IN",
    "object": "IEEE TRANSACTIONS ON COMPUTERS"
  },
  {
    "subject": "MXNET",
    "predicate": "IS",
    "object": "A FLEXIBLE AND EFFICIENT MACHINE LEARNING LIBRARY"
  },
  {
    "subject": "MXNET",
    "predicate": "IS FOR",
    "object": "HETEROGENEOUS DISTRIBUTED SYSTEMS"
  },
  {
    "subject": "55 T. CHEN, M. LI, Y. LI, M. LIN, N. WANG, M. WANG, T. XIAO, B. XU, C. ZHANG, AND Z. ZHANG",
    "predicate": "AUTHORED",
    "object": "MXNET"
  },
  {
    "subject": "MXNET",
    "predicate": "WAS PUBLISHED IN",
    "object": "ARXIV PREPRINT ARXIV:1512.01274"
  },
  {
    "subject": "ARXIV PREPRINT ARXIV:1512.01274",
    "predicate": "WAS PUBLISHED IN YEAR",
    "object": "2015"
  },
  {
    "subject": "56 C. GREGG, J. DORN, K. HAZELWOOD, AND K. SKADRON",
    "predicate": "PRESENTED",
    "object": "FINE-GRAINED RESOURCE SHARING FOR CONCURRENT GPGPU KERNELS"
  },
  {
    "subject": "FINE-GRAINED RESOURCE SHARING FOR CONCURRENT GPGPU KERNELS",
    "predicate": "PRESENTED AS PART OF",
    "object": "4TH USENIX WORKSHOP ON HOT TOPICS IN PARALLELISM"
  },
  {
    "subject": "4TH USENIX WORKSHOP ON HOT TOPICS IN PARALLELISM",
    "predicate": "OCCURRED IN",
    "object": "2012"
  },
  {
    "subject": "57 S. PAI, M. J. THAZHUTHAVEETIL, AND R. GOVINDARAJAN",
    "predicate": "ARE AUTHORS OF",
    "object": "IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS"
  },
  {
    "subject": "IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS",
    "predicate": "IS PUBLISHED IN",
    "object": "ACM SIGARCH COMPUTER ARCHITECTURE NEWS"
  },
  {
    "subject": "TASO",
    "predicate": "OPTIMIZES",
    "object": "DEEP LEARNING COMPUTATION"
  },
  {
    "subject": "TASO",
    "predicate": "USES",
    "object": "AUTOMATIC GENERATION OF GRAPH SUBSTITUTIONS"
  },
  {
    "subject": "TASO",
    "predicate": "PUBLISHED IN",
    "object": "ACM SOSP"
  },
  {
    "subject": "TASO",
    "predicate": "PUBLISHED IN YEAR",
    "object": "2019"
  },
  {
    "subject": "Z. JIA, O. PADON, J. THOMAS, T. WARSZAWSKI, M. ZAHARIA, AND A. AIKEN",
    "predicate": "AUTHORED",
    "object": "TASO"
  }
]