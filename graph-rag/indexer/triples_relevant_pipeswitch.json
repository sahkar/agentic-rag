[
  {
    "subject": "This paper",
    "predicate": "is included in",
    "object": "the proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "The proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "are sponsored by",
    "object": "USENIX"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is",
    "object": "fast pipelined context switching for deep learning applications"
  },
  {
    "subject": "Zhihao Bai",
    "predicate": "is affiliated with",
    "object": "Johns Hopkins University"
  },
  {
    "subject": "Zhen Zhang",
    "predicate": "is affiliated with",
    "object": "Johns Hopkins University"
  },
  {
    "subject": "Yibo Zhu",
    "predicate": "is affiliated with",
    "object": "ByteDance Inc."
  },
  {
    "subject": "Xin Jin",
    "predicate": "is affiliated with",
    "object": "Johns Hopkins University"
  },
  {
    "subject": "Deep learning workloads",
    "predicate": "include",
    "object": "throughput-intensive training tasks"
  },
  {
    "subject": "Deep learning workloads",
    "predicate": "include",
    "object": "latency-sensitive inference tasks"
  },
  {
    "subject": "The dominant practice today",
    "predicate": "is",
    "object": "to provision dedicated GPU clusters for training and inference separately"
  },
  {
    "subject": "training and inference",
    "predicate": "use",
    "object": "GPUs"
  },
  {
    "subject": "the current practice",
    "predicate": "is to build",
    "object": "dedicated clusters for training and inference separately"
  },
  {
    "subject": "We",
    "predicate": "envision to build",
    "object": "fine-grained time-sharing GPU clusters"
  },
  {
    "subject": "GPU clusters",
    "predicate": "can be shared across",
    "object": "different applications including training and inference"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are often over-provisioned",
    "object": "based on the peak load"
  },
  {
    "subject": "GPU clusters",
    "predicate": "have limited sharing",
    "object": "between applications and task types"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are over-provisioned due to",
    "object": "the need to meet strict service-level objectives (SLOs)"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "is",
    "object": "a system"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "enables",
    "object": "unused cycles of an inference application to be filled by training or other inference applications"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "can improve",
    "object": "GPU utilization"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "does not sacrifice",
    "object": "SLOs"
  },
  {
    "subject": "Experiments on a variety of DL models and GPU cards",
    "predicate": "show",
    "object": "PipeSwitch only incurs a task startup overhead of 3.66.6 ms"
  },
  {
    "subject": "Experiments on a variety of DL models and GPU cards",
    "predicate": "show",
    "object": "PipeSwitch incurs a total overhead of 5.434.6 ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has overhead",
    "object": "1050 better than NVIDIA MPS"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "near 100 GPU utilization"
  },
  {
    "subject": "Experiments on a variety of DL models and GPU cards",
    "predicate": "show",
    "object": "PipeSwitch has a total overhead of 5.434.6 ms"
  },
  {
    "subject": "PipeSwitch total overhead",
    "predicate": "is",
    "object": "1050 better than NVIDIA MPS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "is focused on",
    "object": "single-GPU tasks for training and inference"
  },
  {
    "subject": "MULTI-GPU INFERENCE TASKS",
    "predicate": "can be supported by",
    "object": "performing PIPESWITCH on each GPU with transactions"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "supports",
    "object": "single-GPU training for training tasks"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "supports",
    "object": "asynchronous multi-GPU training for data parallel strategies"
  },
  {
    "subject": "preempting one GPU",
    "predicate": "does not affect",
    "object": "other GPUs"
  },
  {
    "subject": "Elastic synchronous training",
    "predicate": "allows",
    "object": "the dynamic changing of the number of GPUs used for training"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "has",
    "object": "high throughput close to the upper bound"
  },
  {
    "subject": "We",
    "predicate": "demonstrate",
    "object": "the performance of PipeSwitch with experiments on a variety of DNN models and GPU cards"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "can significantly increase",
    "object": "GPU utilization"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "can improve",
    "object": "the agility of DL applications"
  },
  {
    "subject": "We",
    "predicate": "achieve",
    "object": "introducing pipelined context switching"
  },
  {
    "subject": "the key idea",
    "predicate": "is to leverage",
    "object": "the layered structure of neural network models and their layer-by-layer computation pattern"
  },
  {
    "subject": "the key idea",
    "predicate": "is to pipeline",
    "object": "model transmission over the PCIe and task execution in the GPU with model-aware grouping"
  },
  {
    "subject": "WE",
    "predicate": "design",
    "object": "a pipelined model transmission mechanism"
  },
  {
    "subject": "a pipelined model transmission mechanism",
    "predicate": "pipelines",
    "object": "model transmission over the PCIe"
  },
  {
    "subject": "a pipelined model transmission mechanism",
    "predicate": "pipelines",
    "object": "model computation in the GPU"
  },
  {
    "subject": "transmitting a task from CPU to GPU",
    "predicate": "is bounded by",
    "object": "the PCIe bandwidth"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "unied memory management and active-standby worker switching mechanisms"
  },
  {
    "subject": "unied memory management and active-standby worker switching mechanisms",
    "predicate": "accompany",
    "object": "the pipelining"
  },
  {
    "subject": "unied memory management and active-standby worker switching mechanisms",
    "predicate": "ensure",
    "object": "process-level isolation"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "an active-standby mechanism"
  },
  {
    "subject": "active-standby mechanism",
    "predicate": "is used for",
    "object": "fast worker switching"
  },
  {
    "subject": "active-standby mechanism",
    "predicate": "is used for",
    "object": "process-level isolation"
  },
  {
    "subject": "We",
    "predicate": "have built",
    "object": "a PipeSwitch prototype"
  },
  {
    "subject": "We",
    "predicate": "have integrated",
    "object": "it with PyTorch"
  },
  {
    "subject": "We",
    "predicate": "have implemented",
    "object": "a system prototype for PipeSwitch"
  },
  {
    "subject": "The system prototype for PipeSwitch",
    "predicate": "has",
    "object": "3600 lines of code"
  },
  {
    "subject": "The 3600 lines of code",
    "predicate": "are written in",
    "object": "C and Python"
  },
  {
    "subject": "We",
    "predicate": "have integrated it with",
    "object": "PyTorch 21"
  },
  {
    "subject": "Deep Learning (DL)",
    "predicate": "powers",
    "object": "an emerging family of intelligent applications"
  },
  {
    "subject": "intelligent applications",
    "predicate": "are in",
    "object": "many domains"
  },
  {
    "subject": "many domains",
    "predicate": "include",
    "object": "retail"
  },
  {
    "subject": "many domains",
    "predicate": "include",
    "object": "transportation"
  },
  {
    "subject": "many domains",
    "predicate": "include",
    "object": "finance"
  },
  {
    "subject": "many domains",
    "predicate": "include",
    "object": "healthcare"
  },
  {
    "subject": "GPUS",
    "predicate": "are",
    "object": "one of the most widely-used classes of accelerators for DL"
  },
  {
    "subject": "DL workloads",
    "predicate": "include",
    "object": "throughput-intensive training tasks"
  },
  {
    "subject": "DL workloads",
    "predicate": "include",
    "object": "latency-sensitive inference tasks"
  },
  {
    "subject": "Inference tasks",
    "predicate": "cannot be served with",
    "object": "training clusters under ash crowds"
  },
  {
    "subject": "Training tasks",
    "predicate": "cannot utilize",
    "object": "inference clusters when the inference load is low"
  },
  {
    "subject": "training cluster",
    "predicate": "cannot preempt",
    "object": "training tasks for inference tasks"
  },
  {
    "subject": "Inference clusters",
    "predicate": "are often over-provisioned for",
    "object": "the peak load"
  },
  {
    "subject": "Inference clusters",
    "predicate": "are over-provisioned in order to meet",
    "object": "strict service level objectives (SLOs)"
  },
  {
    "subject": "production systems",
    "predicate": "are provisioned to",
    "object": "each application on per-GPU granularity"
  },
  {
    "subject": "provisioning on per-GPU granularity",
    "predicate": "limits",
    "object": "the interference between applications"
  },
  {
    "subject": "production systems",
    "predicate": "allocate",
    "object": "GPUs to applications on per-GPU granularity"
  },
  {
    "subject": "binding GPUs",
    "predicate": "is done to",
    "object": "the VMs, containers or processes of an application"
  },
  {
    "subject": "production systems",
    "predicate": "allocate GPUs",
    "object": "in order to limit the interference between different applications"
  },
  {
    "subject": "production systems",
    "predicate": "allocate GPUs",
    "object": "in order to satisfy the SLO requirements"
  },
  {
    "subject": "multiple DL applications",
    "predicate": "should be able to be packed to",
    "object": "the same GPU server"
  },
  {
    "subject": "packing multiple DL applications to the same GPU server",
    "predicate": "maximizes",
    "object": "GPU utilization via time-sharing"
  },
  {
    "subject": "Operating systems",
    "predicate": "achieve",
    "object": "high CPU utilization via task scheduling and context switching"
  },
  {
    "subject": "THE IDEA OF NE-GRAINED CPU TIME-SHARING",
    "predicate": "has been further extended to",
    "object": "CLUSTER SCHEDULING"
  },
  {
    "subject": "NE-GRAINED TIME-SHARING",
    "predicate": "can provide",
    "object": "better utilization than provisioning dedicated resources"
  },
  {
    "subject": "NE-GRAINED TIME-SHARING",
    "predicate": "provides",
    "object": "necessary process-level isolation"
  },
  {
    "subject": "NE-GRAINED TIME-SHARING",
    "predicate": "is similar to",
    "object": "CPU workloads"
  },
  {
    "subject": "NE-GRAINED SCHEDULING CYCLES",
    "predicate": "are",
    "object": "enabled"
  },
  {
    "subject": "Google Borg",
    "predicate": "packs",
    "object": "online services and batch jobs"
  },
  {
    "subject": "Google Borg",
    "predicate": "saves",
    "object": "20-30 machines"
  },
  {
    "subject": "20-30 machines",
    "predicate": "are saved compared with",
    "object": "not packing them"
  },
  {
    "subject": "GPU",
    "predicate": "has",
    "object": "high overhead when switching between tasks"
  },
  {
    "subject": "THE GAP",
    "predicate": "is about",
    "object": "THE PRECIOUS GPU MEMORY AND SLOW SWITCHING"
  },
  {
    "subject": "Naively using GPUs in the same way as CPUs",
    "predicate": "will not satisfy",
    "object": "the requirements of DL inference that have strict SLOs in the range of tens to hundreds of milliseconds"
  },
  {
    "subject": "A GPU",
    "predicate": "switches to",
    "object": "a DNN model (e.g., ResNet)"
  },
  {
    "subject": "The DNN model (e.g., ResNet)",
    "predicate": "has not been preloaded onto",
    "object": "the GPU"
  },
  {
    "subject": "State-of-the-art tricks like CUDA unified memory",
    "predicate": "do not prevent",
    "object": "multiple seconds delay"
  },
  {
    "subject": "CPU applications",
    "predicate": "can be switched in",
    "object": "milliseconds or even microseconds"
  },
  {
    "subject": "the existing solution",
    "predicate": "is to",
    "object": "spatially share the GPU memory"
  },
  {
    "subject": "this approach",
    "predicate": "does not provide",
    "object": "strong GPU memory isolation between applications"
  },
  {
    "subject": "NVIDIA Multiple Process Sharing (MPS) 6",
    "predicate": "allow",
    "object": "multiple processes to use the same GPU"
  },
  {
    "subject": "Salus 7",
    "predicate": "allow",
    "object": "multiple processes to use the same GPU"
  },
  {
    "subject": "NVIDIA Multiple Process Sharing (MPS) 6",
    "predicate": "require",
    "object": "all processes data (e.g., DNN models) to be preloaded into the GPU memory"
  },
  {
    "subject": "Salus 7",
    "predicate": "require",
    "object": "all processes data (e.g., DNN models) to be preloaded into the GPU memory"
  },
  {
    "subject": "multi-process support from NVIDIA",
    "predicate": "allows",
    "object": "the inference process to share the GPU with the training process"
  },
  {
    "subject": "NVIDIA MPS 6",
    "predicate": "provides",
    "object": "official support for sharing a GPU between multiple processes"
  },
  {
    "subject": "GPU memory",
    "predicate": "is",
    "object": "much more limited than host memory"
  },
  {
    "subject": "GPU memory",
    "predicate": "cannot",
    "object": "preload many applications"
  },
  {
    "subject": "one single memory-intensive training task",
    "predicate": "may consume",
    "object": "all the GPU memory"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "stops and cleans",
    "object": "ITS GPU ENVIRONMENT"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "frees",
    "object": "the GPU MEMORY"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "occupies",
    "object": "THE ENTIRE GPU MEMORY"
  },
  {
    "subject": "THE TRAINING TASK",
    "predicate": "does not stop",
    "object": "WHEN INFERENCE TASKS COME"
  },
  {
    "subject": "memory footprints of inference tasks",
    "predicate": "are increasing",
    "object": ""
  },
  {
    "subject": "models",
    "predicate": "are getting",
    "object": "larger"
  },
  {
    "subject": "request batching",
    "predicate": "is used",
    "object": "to increase throughput"
  },
  {
    "subject": "Request batching",
    "predicate": "increases",
    "object": "the GPU memory requirement of inference applications"
  },
  {
    "subject": "A context switching design",
    "predicate": "minimizes",
    "object": "the switching overhead"
  },
  {
    "subject": "A context switching design",
    "predicate": "quickly switches",
    "object": "the contents on GPU memory"
  },
  {
    "subject": "A context switching design",
    "predicate": "is",
    "object": "a better approach for efficiently time-sharing GPUs"
  },
  {
    "subject": "no existing solution",
    "predicate": "offers",
    "object": "such context switching abstraction for GPU"
  },
  {
    "subject": "we",
    "predicate": "introduce",
    "object": "a new technology called pipelined context switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "exploits",
    "object": "the characteristics of DL applications"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "achieves",
    "object": "millisecond-scale overhead for switching tasks on GPUs"
  },
  {
    "subject": "we",
    "predicate": "face",
    "object": "a major challenge fast GPU context switching between different processes"
  },
  {
    "subject": "pipeline",
    "predicate": "overlaps",
    "object": "computation and GPU memory swapping"
  },
  {
    "subject": "pipeline",
    "predicate": "enables",
    "object": "fast context switching"
  },
  {
    "subject": "context switching",
    "predicate": "is not needed",
    "object": "if the application is already loaded in the GPU"
  },
  {
    "subject": "We",
    "predicate": "introduce",
    "object": "pipelined context switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "minimizes",
    "object": "task switching overhead on GPUs for DL applications"
  },
  {
    "subject": "DNN models",
    "predicate": "can be held in",
    "object": "host memory"
  },
  {
    "subject": "host memory",
    "predicate": "is",
    "object": "much larger and cheaper than GPU memory"
  },
  {
    "subject": "GPU",
    "predicate": "can quickly context-switch between",
    "object": "the models either for training or inference"
  },
  {
    "subject": "Enterprises",
    "predicate": "build",
    "object": "GPU clusters"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are",
    "object": "privately shared by multiple users"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are",
    "object": "publicly shared by multiple users"
  },
  {
    "subject": "M. Jeon, S. Venkataraman, A. Phanishayee, J. Qian, W. Xiao, and F. Yang",
    "predicate": "authored",
    "object": "Analysis of large-scale multi-tenant GPU clusters for DNN training workloads"
  },
  {
    "subject": "Analysis of large-scale multi-tenant GPU clusters for DNN training workloads",
    "predicate": "was published in",
    "object": "USENIX ATC, 2019"
  },
  {
    "subject": "512 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "is organized by",
    "object": "USENIX Association"
  },
  {
    "subject": "the number of applications that can be multiplexed",
    "predicate": "is not limited by",
    "object": "the GPU memory size"
  },
  {
    "subject": "each application",
    "predicate": "is able to use",
    "object": "the entire GPU compute and memory resources during its time slice"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "enables",
    "object": "GPU-efficient multiplexing of many DL applications on GPU servers via fine-grained time-sharing"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "achieves",
    "object": "millisecond-scale latencies and high throughput as dedicated servers"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "enables",
    "object": "GPU-efficient fine-grained time-sharing for multiple DL applications"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "achieves",
    "object": "millisecond-scale context switching latencies"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "achieves",
    "object": "high throughput"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "combines",
    "object": "all the ideas into our system"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "closes",
    "object": "the gap of GPU memory sharing and switching"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "enables",
    "object": "the design of an efficient time-sharing GPU cluster for DL workloads"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "enables",
    "object": "GPU-efficient multiplexing of multiple DL applications on GPU servers"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "is able to achieve",
    "object": "millisecond-scale task switching time"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "enables",
    "object": "DL applications on time-sharing GPUs to meet strict SLOs"
  },
  {
    "subject": "small switching overhead",
    "predicate": "is critical for",
    "object": "DL applications to satisfy strict SLO requirements"
  },
  {
    "subject": "millisecond-scale task switching overhead",
    "predicate": "is to satisfy",
    "object": "SLO requirements"
  },
  {
    "subject": "we",
    "predicate": "perform",
    "object": "a measurement study"
  },
  {
    "subject": "measurement study",
    "predicate": "probes",
    "object": "the task switching overhead"
  },
  {
    "subject": "we",
    "predicate": "analyze",
    "object": "the overhead of each component"
  },
  {
    "subject": "the measurement study",
    "predicate": "aims to",
    "object": "probe the task switching overhead"
  },
  {
    "subject": "switching overhead",
    "predicate": "is divided into",
    "object": "four components"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "old task cleaning"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "new task initialization"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "GPU memory allocation"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "model transmission via PCIe from CPU to GPU"
  },
  {
    "subject": "INSTANCE TYPE",
    "predicate": "includes",
    "object": "G4DN.2XLARGE"
  },
  {
    "subject": "INSTANCE TYPE",
    "predicate": "includes",
    "object": "P3.2XLARGE"
  },
  {
    "subject": "GPU TYPE of G4DN.2XLARGE",
    "predicate": "is",
    "object": "NVIDIA T4"
  },
  {
    "subject": "GPU TYPE of P3.2XLARGE",
    "predicate": "is",
    "object": "NVIDIA V100"
  },
  {
    "subject": "TASK CLEANING time on G4DN.2XLARGE",
    "predicate": "is",
    "object": "155 MS"
  },
  {
    "subject": "TASK CLEANING time on P3.2XLARGE",
    "predicate": "is",
    "object": "165 MS"
  },
  {
    "subject": "TASK INITIALIZATION time on G4DN.2XLARGE",
    "predicate": "is",
    "object": "5530 MS"
  },
  {
    "subject": "TASK INITIALIZATION time on P3.2XLARGE",
    "predicate": "is",
    "object": "7290 MS"
  },
  {
    "subject": "MEMORY ALLOCATION time on G4DN.2XLARGE",
    "predicate": "is",
    "object": "10 MS"
  },
  {
    "subject": "MEMORY ALLOCATION time on P3.2XLARGE",
    "predicate": "is",
    "object": "13 MS"
  },
  {
    "subject": "MODEL TRANSMISSION time on G4DN.2XLARGE",
    "predicate": "is",
    "object": "91 MS"
  },
  {
    "subject": "MODEL TRANSMISSION time on P3.2XLARGE",
    "predicate": "is",
    "object": "81 MS"
  },
  {
    "subject": "TOTAL OVERHEAD on G4DN.2XLARGE",
    "predicate": "is",
    "object": "5787 MS"
  },
  {
    "subject": "TOTAL OVERHEAD on P3.2XLARGE",
    "predicate": "is",
    "object": "7551 MS"
  },
  {
    "subject": "INFERENCE TIME on G4DN.2XLARGE",
    "predicate": "is",
    "object": "105 MS"
  },
  {
    "subject": "INFERENCE TIME on P3.2XLARGE",
    "predicate": "is",
    "object": "32 MS"
  },
  {
    "subject": "every component",
    "predicate": "takes",
    "object": "a considerable amount of time"
  },
  {
    "subject": "time",
    "predicate": "varies from",
    "object": "tens of milliseconds to seconds"
  },
  {
    "subject": "inference task",
    "predicate": "takes",
    "object": "tens of milliseconds on a GPU"
  },
  {
    "subject": "latency SLOs",
    "predicate": "are",
    "object": "a small multiple of the inference time"
  },
  {
    "subject": "one source of the overhead",
    "predicate": "is",
    "object": "the contentions both on the computation and memory of the GPU"
  },
  {
    "subject": "the training task",
    "predicate": "do not stop",
    "object": "when an inference task comes"
  },
  {
    "subject": "we",
    "predicate": "take",
    "object": "a holistic approach"
  },
  {
    "subject": "we",
    "predicate": "exploit",
    "object": "the characteristics of DL applications"
  },
  {
    "subject": "we",
    "predicate": "minimize",
    "object": "the overhead of all the components"
  },
  {
    "subject": "DNN models",
    "predicate": "have",
    "object": "a layered structure"
  },
  {
    "subject": "DNN models",
    "predicate": "have",
    "object": "a layer-by-layer computation pattern"
  },
  {
    "subject": "Our design",
    "predicate": "is based on",
    "object": "a key observation"
  },
  {
    "subject": "DNN models",
    "predicate": "are",
    "object": "usually deep"
  },
  {
    "subject": "DNN models",
    "predicate": "consist of",
    "object": "multiple layers stacking one on another"
  },
  {
    "subject": "computation of DNN models",
    "predicate": "takes place",
    "object": "layer by layer"
  },
  {
    "subject": "there",
    "predicate": "is no need to wait for",
    "object": "the entire model to be transmitted to the GPU before starting computation"
  },
  {
    "subject": "A task",
    "predicate": "does not need to wait for",
    "object": "the entire model to be transmitted to the GPU before beginning the computation"
  },
  {
    "subject": "Naive pipelining on per-layer granularity",
    "predicate": "introduces",
    "object": "high overhead on tensor transmission and synchronization"
  },
  {
    "subject": "Pipelining on per-layer granularity",
    "predicate": "requires",
    "object": "synchronization for every layer"
  },
  {
    "subject": "We",
    "predicate": "divide",
    "object": "layers into groups"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "an optimal model-aware grouping algorithm"
  },
  {
    "subject": "algorithm",
    "predicate": "finds",
    "object": "the best grouping strategy for a given model"
  },
  {
    "subject": "we",
    "predicate": "design",
    "object": "an algorithm to find the optimal grouping strategy for a given model"
  },
  {
    "subject": "The computation of a DL task",
    "predicate": "is",
    "object": "layer by layer"
  },
  {
    "subject": "The computation of a DL task",
    "predicate": "has",
    "object": "a simple, regular pattern for memory allocation"
  },
  {
    "subject": "A DL task",
    "predicate": "stores",
    "object": "two important types of data in the GPU memory"
  },
  {
    "subject": "two important types of data",
    "predicate": "are",
    "object": "the DNN model (including the model parameters) and the intermediate results"
  },
  {
    "subject": "The default general-purpose GPU memory management (e.g., CUDA Unified Memory 4)",
    "predicate": "is",
    "object": "an overkill"
  },
  {
    "subject": "The default general-purpose GPU memory management (e.g., CUDA Unified Memory 4)",
    "predicate": "incurs",
    "object": "unnecessary overhead"
  },
  {
    "subject": "NVIDIA",
    "predicate": "provides",
    "object": "CUDA Unified Memory 4"
  },
  {
    "subject": "CUDA Unified Memory 4",
    "predicate": "handles",
    "object": "memory movement between the host memory and the GPU memory"
  },
  {
    "subject": "memory movement",
    "predicate": "is handled automatically for",
    "object": "applications"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "unified memory management with a dedicated memory daemon"
  },
  {
    "subject": "dedicated memory daemon",
    "predicate": "minimizes",
    "object": "the overhead"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "unified memory management with the memory daemon"
  },
  {
    "subject": "unified memory management with the memory daemon",
    "predicate": "achieve",
    "object": "minimal memory footprint"
  },
  {
    "subject": "unified memory management with the memory daemon",
    "predicate": "eliminate",
    "object": "extra memory copies"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "pre-allocates",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "re-allocates",
    "object": "IT TO EACH TASK"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "does not involve",
    "object": "THE EXPENSIVE GPU MEMORY MANAGER"
  },
  {
    "subject": "the memory daemon",
    "predicate": "uses",
    "object": "cudamalloc to obtain the GPU memory when the system starts"
  },
  {
    "subject": "the memory daemon",
    "predicate": "allocates",
    "object": "the memory to the workers at runtime"
  },
  {
    "subject": "the memory daemon",
    "predicate": "does not replace",
    "object": "the GPU memory manager"
  },
  {
    "subject": "the memory daemon",
    "predicate": "is compatible with",
    "object": "the existing system"
  },
  {
    "subject": "the memory daemon",
    "predicate": "incurs",
    "object": "minimal changes"
  },
  {
    "subject": "DNN models",
    "predicate": "are stored",
    "object": "only once in the memory daemon"
  },
  {
    "subject": "DNN models",
    "predicate": "are not stored",
    "object": "in every worker"
  },
  {
    "subject": "storing DNN models only once in the memory daemon",
    "predicate": "minimizes",
    "object": "memory footprint"
  },
  {
    "subject": "memory allocation for a DNN model",
    "predicate": "is",
    "object": "deterministic"
  },
  {
    "subject": "we",
    "predicate": "exploit",
    "object": "that the memory allocation for a DNN model is deterministic"
  },
  {
    "subject": "deterministic memory allocation",
    "predicate": "eliminates",
    "object": "extra memory copies between the daemon and the workers"
  },
  {
    "subject": "deterministic memory allocation",
    "predicate": "reduces",
    "object": "the IPC overhead"
  },
  {
    "subject": "No unified memory management",
    "predicate": "requires",
    "object": "each worker to keep a copy for each DNN model"
  },
  {
    "subject": "Keeping a copy for each DNN model",
    "predicate": "increases",
    "object": "the memory footprint"
  },
  {
    "subject": "each server",
    "predicate": "contains",
    "object": "an active worker"
  },
  {
    "subject": "each server",
    "predicate": "contains",
    "object": "multiple standby workers"
  },
  {
    "subject": "A SERVER",
    "predicate": "HAS",
    "object": "ONE OR MORE STANDBY WORKERS"
  },
  {
    "subject": "THE ACTIVE WORKER",
    "predicate": "executes",
    "object": "THE CURRENT TASK ON THE GPU"
  },
  {
    "subject": "THE STANDBY WORKERS",
    "predicate": "stay",
    "object": "ON THE CPU"
  },
  {
    "subject": "THE STANDBY WORKERS",
    "predicate": "wait for",
    "object": "THE NEXT TASK"
  },
  {
    "subject": "THE ACTIVE WORKER",
    "predicate": "is",
    "object": "THE WORKER THAT CURRENTLY EXECUTES A TASK IN THE GPU"
  },
  {
    "subject": "worker",
    "predicate": "is",
    "object": "a process that executes tasks on one GPU"
  },
  {
    "subject": "the active worker",
    "predicate": "completes or stops",
    "object": "the current task"
  },
  {
    "subject": "the controller",
    "predicate": "notifies",
    "object": "the memory daemon"
  },
  {
    "subject": "the controller",
    "predicate": "notifies",
    "object": "the standby worker"
  },
  {
    "subject": "the memory daemon and the standby worker",
    "predicate": "load",
    "object": "the task to GPU"
  },
  {
    "subject": "the task",
    "predicate": "is executed with",
    "object": "pipelined model transmission (4.2)"
  },
  {
    "subject": "OUR MECHANISM",
    "predicate": "parallelizes",
    "object": "OLD TASK CLEANING IN THE ACTIVE WORKER AND NEW TASK INITIALIZATION IN THE STANDBY WORKER"
  },
  {
    "subject": "OUR MECHANISM",
    "predicate": "aims to minimize",
    "object": "WORKER SWITCHING OVERHEAD"
  },
  {
    "subject": "Table 2",
    "predicate": "compares",
    "object": "worker switching mechanisms"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "an active and standby worker switching mechanism"
  },
  {
    "subject": "active and standby worker switching mechanism",
    "predicate": "hides",
    "object": "the overhead of both task cleaning and task initialization"
  },
  {
    "subject": "active and standby worker switching mechanism",
    "predicate": "ensures",
    "object": "process-level isolation"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "enforces",
    "object": "process-level isolation"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "requires",
    "object": "us to address new technical challenges on memory management and worker switching across different processes"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "aims to provide",
    "object": "fast task switching"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "aims to ensure",
    "object": "process-level isolation"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "has",
    "object": "an active worker"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "has",
    "object": "multiple standby workers"
  },
  {
    "subject": "we",
    "predicate": "keep",
    "object": "all other components of PipeSwitch the same"
  },
  {
    "subject": "we",
    "predicate": "compare",
    "object": "the following mechanisms discussed in 4.4"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "is",
    "object": "the active-standby worker switching mechanism used by PIPESWITCH"
  },
  {
    "subject": "Pipelining",
    "predicate": "is",
    "object": "a canonical technique"
  },
  {
    "subject": "Pipelining",
    "predicate": "is widely used in",
    "object": "computer systems"
  },
  {
    "subject": "Pipelining",
    "predicate": "improves",
    "object": "system performance"
  },
  {
    "subject": "Pipelining",
    "predicate": "maximizes",
    "object": "resource utilization"
  },
  {
    "subject": "PIPELINING",
    "predicate": "BRINGS",
    "object": "TWO SOURCES OF SYSTEM OVERHEADS"
  },
  {
    "subject": "Prior work in DL systems such as Pipedream 8 and Bytescheduler 9",
    "predicate": "has applied",
    "object": "pipelining to distributed training"
  },
  {
    "subject": "These solutions",
    "predicate": "focus on",
    "object": "inter-batch pipelining"
  },
  {
    "subject": "Inter-batch pipelining",
    "predicate": "overlaps",
    "object": "computation and gradient transmission of different batches"
  },
  {
    "subject": "Computation and gradient transmission",
    "predicate": "are for",
    "object": "training workloads of the same DNN model"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "introduces",
    "object": "intra-batch pipelining"
  },
  {
    "subject": "intra-batch pipelining",
    "predicate": "overlaps",
    "object": "model transmission and computation"
  },
  {
    "subject": "overlapping model transmission and computation",
    "predicate": "reduces",
    "object": "the overhead of switching between different DNN models"
  },
  {
    "subject": "different DNN models",
    "predicate": "can be",
    "object": "either inference or training"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "new techniques"
  },
  {
    "subject": "new techniques",
    "predicate": "support",
    "object": "training"
  },
  {
    "subject": "new techniques",
    "predicate": "support",
    "object": "inference that has strict SLOs"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "pipelined model transmission"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "unified memory management"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "active-standby worker switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "minimizes",
    "object": "switching overhead"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "enforces",
    "object": "process-level isolation"
  },
  {
    "subject": "Pipelined context switching",
    "predicate": "includes",
    "object": "three key techniques"
  },
  {
    "subject": "three key techniques",
    "predicate": "are",
    "object": "pipelined model transmission"
  },
  {
    "subject": "three key techniques",
    "predicate": "are",
    "object": "unified memory management"
  },
  {
    "subject": "three key techniques",
    "predicate": "are",
    "object": "active-standby worker switching"
  },
  {
    "subject": "we",
    "predicate": "implement",
    "object": "a system prototype"
  },
  {
    "subject": "we",
    "predicate": "integrate",
    "object": "it with PyTorch"
  },
  {
    "subject": "We",
    "predicate": "identify",
    "object": "the inefficiencies in today's shared GPU clusters"
  },
  {
    "subject": "We",
    "predicate": "motivate",
    "object": "running DL workloads on GPUs in the fine-grained time-sharing model"
  },
  {
    "subject": "We",
    "predicate": "propose",
    "object": "to pack multiple DL applications onto the same GPU via NE-grained time-sharing abstraction to maximize GPU utilization"
  },
  {
    "subject": "fast task switching",
    "predicate": "enables",
    "object": "more flexible fine-grained scheduling"
  },
  {
    "subject": "more flexible fine-grained scheduling",
    "predicate": "improves",
    "object": "GPU utilization for dynamic workloads"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "dedicated physical forms and power supplies"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "high speed networks"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "specialized task schedulers"
  },
  {
    "subject": "500 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION",
    "predicate": "is organized by",
    "object": "USENIX ASSOCIATION"
  },
  {
    "subject": "USENIX ASSOCIATION",
    "predicate": "hosts",
    "object": "14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"
  },
  {
    "subject": "shared cluster",
    "predicate": "is compared to",
    "object": "dedicated cluster for each user"
  },
  {
    "subject": "14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "includes topic",
    "object": "Kubernetes"
  },
  {
    "subject": "514",
    "predicate": "is page number of",
    "object": "14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"
  },
  {
    "subject": "14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION",
    "predicate": "is organized by",
    "object": "USENIX ASSOCIATION"
  },
  {
    "subject": "the main reason",
    "predicate": "is",
    "object": "to bring down the cost"
  },
  {
    "subject": "The demand of training",
    "predicate": "is not",
    "object": "well predictable"
  },
  {
    "subject": "The demand of training",
    "predicate": "would depend on",
    "object": "the progress of different developers"
  },
  {
    "subject": "an inference task for a particular application",
    "predicate": "has",
    "object": "a daily periodical pattern based on the application usage"
  },
  {
    "subject": "patterns",
    "predicate": "can vary",
    "object": "across different tasks"
  },
  {
    "subject": "A shared cluster",
    "predicate": "would increase",
    "object": "the resource utilization via time-sharing"
  },
  {
    "subject": "training",
    "predicate": "has no sharing with",
    "object": "inference"
  },
  {
    "subject": "shared clusters",
    "predicate": "are not shared between",
    "object": "training and inference"
  },
  {
    "subject": "Inference clusters",
    "predicate": "are not always running at",
    "object": "high utilization"
  },
  {
    "subject": "Inference clusters",
    "predicate": "cannot be utilized by",
    "object": "training"
  },
  {
    "subject": "GPUs designed for inference tasks",
    "predicate": "might be",
    "object": "too wimpy for training tasks"
  },
  {
    "subject": "The arrival of new GPU hardware",
    "predicate": "has started to change",
    "object": "this"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "has",
    "object": "up to 32GB GPU memory"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "has",
    "object": "15.7 TFLOPS (single-precision)"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "has",
    "object": "comparable performance with NVIDIA V100"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "has",
    "object": "16GB GPU memory"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "has",
    "object": "8.1 TFLOPS (single-precision)"
  },
  {
    "subject": "new algorithms and systems for distributed training",
    "predicate": "enable",
    "object": "multiple GPUs to accelerate training"
  },
  {
    "subject": "OUR INDUSTRY COLLABORATOR",
    "predicate": "is",
    "object": "A LEADING ONLINE SERVICE PROVIDER"
  },
  {
    "subject": "OUR INDUSTRY COLLABORATOR",
    "predicate": "confirms",
    "object": "THIS OBSERVATION"
  },
  {
    "subject": "THIS SERVICE PROVIDER",
    "predicate": "runs",
    "object": "more than 10K V100 GPUs for training"
  },
  {
    "subject": "THIS SERVICE PROVIDER",
    "predicate": "runs",
    "object": "at least 5 as many T4 GPUs for inference"
  },
  {
    "subject": "computation power on both sides",
    "predicate": "is within",
    "object": "the same order of magnitude"
  },
  {
    "subject": "inference workload",
    "predicate": "fluctuates in correlation with",
    "object": "number of active users"
  },
  {
    "subject": "inference workload",
    "predicate": "shows",
    "object": "clear peaks and valleys within each day"
  },
  {
    "subject": "peak demand during daytime",
    "predicate": "is",
    "object": "2 times the valley at midnight"
  },
  {
    "subject": "inference GPUs",
    "predicate": "would be utilized",
    "object": "during less busy times"
  },
  {
    "subject": "inference GPUs",
    "predicate": "would be utilized for",
    "object": "training models"
  },
  {
    "subject": "training models",
    "predicate": "require",
    "object": "daily updates with latest data"
  },
  {
    "subject": "A good example",
    "predicate": "is to",
    "object": "ne-tune BERT using daily news"
  },
  {
    "subject": "BORG-LIKE 1 SYSTEMS FOR GPUS",
    "predicate": "means",
    "object": "great opportunity in improving GPU utilization"
  },
  {
    "subject": "Inference and training workloads",
    "predicate": "have",
    "object": "complementary usage patterns"
  },
  {
    "subject": "Inference loads on different models",
    "predicate": "have",
    "object": "different patterns"
  },
  {
    "subject": "Different patterns",
    "predicate": "benefit from",
    "object": "time sharing"
  },
  {
    "subject": "any server",
    "predicate": "would be able to run",
    "object": "any task"
  },
  {
    "subject": "any server",
    "predicate": "would have",
    "object": "low overhead to switch between different applications"
  },
  {
    "subject": "A modern server",
    "predicate": "can be equipped with",
    "object": "several TB of host memory"
  },
  {
    "subject": "Several TB of host memory",
    "predicate": "enables",
    "object": "it to load many applications"
  },
  {
    "subject": "Task execution on GPUs",
    "predicate": "requires",
    "object": "GPU memory"
  },
  {
    "subject": "GPU memory",
    "predicate": "is",
    "object": "very limited even on high-end GPUs"
  },
  {
    "subject": "T4 GPU",
    "predicate": "has",
    "object": "16 GB GPU memory"
  },
  {
    "subject": "V100 GPU",
    "predicate": "has",
    "object": "32 GB GPU memory"
  },
  {
    "subject": "GPU memory",
    "predicate": "is purposed for",
    "object": "task execution"
  },
  {
    "subject": "GPU memory",
    "predicate": "is not purposed for",
    "object": "storing the state of idle applications"
  },
  {
    "subject": "DL tasks, especially training",
    "predicate": "require",
    "object": "a large amount, or even all of the memory on a GPU"
  },
  {
    "subject": "DL applications",
    "predicate": "have",
    "object": "large models"
  },
  {
    "subject": "DL applications",
    "predicate": "generate",
    "object": "large amounts of intermediate results"
  },
  {
    "subject": "large amounts of intermediate results",
    "predicate": "require",
    "object": "a lot of GPU memory"
  },
  {
    "subject": "SALUS 7",
    "predicate": "cannot support",
    "object": "training tasks which are memory-intensive"
  },
  {
    "subject": "SALUS 7",
    "predicate": "cannot support",
    "object": "multiple inference tasks which have large models"
  },
  {
    "subject": "state-of-the-art models",
    "predicate": "are getting",
    "object": "deeper and larger"
  },
  {
    "subject": "idle applications",
    "predicate": "can occupy",
    "object": "large memory space"
  },
  {
    "subject": "the active application",
    "predicate": "should be able to utilize",
    "object": "the entire GPU memory for its purpose"
  },
  {
    "subject": "the number of applications that can be served by a GPU server",
    "predicate": "should be limited by",
    "object": "its host memory size"
  },
  {
    "subject": "switching a task",
    "predicate": "would require",
    "object": "heavy memory swapping"
  },
  {
    "subject": "many online inference workloads",
    "predicate": "require",
    "object": "strict SLOs"
  },
  {
    "subject": "naive memory swapping between the host memory and the GPU memory",
    "predicate": "cannot meet",
    "object": "strict SLOs"
  },
  {
    "subject": "we",
    "predicate": "test",
    "object": "the strawman scenario"
  },
  {
    "subject": "we",
    "predicate": "stop",
    "object": "a training task"
  },
  {
    "subject": "we",
    "predicate": "start",
    "object": "an inference task"
  },
  {
    "subject": "THE RST INFERENCE BATCH",
    "predicate": "would require",
    "object": "several seconds to finish"
  },
  {
    "subject": "Existing support such as NVIDIA MPS",
    "predicate": "is not optimized for",
    "object": "DL workloads"
  },
  {
    "subject": "Existing support such as NVIDIA MPS",
    "predicate": "incurs",
    "object": "hundreds of milliseconds overhead"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "has lower overhead compared to",
    "object": "stop-and-start"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "incurs",
    "object": "several hundred milliseconds overhead"
  },
  {
    "subject": "several hundred milliseconds overhead",
    "predicate": "prevents",
    "object": "MPS from meeting strict SLOs"
  },
  {
    "subject": "Figure 1",
    "predicate": "depicts",
    "object": "PipeSwitch architecture"
  },
  {
    "subject": "14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "includes",
    "object": "throughput measurements"
  },
  {
    "subject": "throughput measurements",
    "predicate": "measured in",
    "object": "batches per second"
  },
  {
    "subject": "throughput",
    "predicate": "has upper bound",
    "object": "PIPESWITCH MPS STOP-AND-START"
  },
  {
    "subject": "throughput",
    "predicate": "measured on",
    "object": "eight P3.2xlarge instances"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "fast starts",
    "object": "training and inference tasks"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "fast switching across tasks"
  },
  {
    "subject": "DL workloads",
    "predicate": "have",
    "object": "well-defined structures"
  },
  {
    "subject": "the structure and computation pattern of DNN models",
    "predicate": "allow",
    "object": "us to highly optimize task switching"
  },
  {
    "subject": "the structure and computation pattern of DNN models",
    "predicate": "allow",
    "object": "us to achieve millisecond-scale overhead"
  },
  {
    "subject": "we",
    "predicate": "need to resolve",
    "object": "other challenges like memory management and worker switching"
  },
  {
    "subject": "FIGURE 1",
    "predicate": "shows",
    "object": "the architecture of a PipeSwitch server"
  },
  {
    "subject": "PIPESWITCH PIPELINES",
    "predicate": "model",
    "object": "transmission and task execution"
  },
  {
    "subject": "THIS SERVER",
    "predicate": "contains",
    "object": "four types of components"
  },
  {
    "subject": "four types of components",
    "predicate": "are",
    "object": "a controller"
  },
  {
    "subject": "four types of components",
    "predicate": "are",
    "object": "a memory daemon"
  },
  {
    "subject": "four types of components",
    "predicate": "are",
    "object": "an active worker"
  },
  {
    "subject": "four types of components",
    "predicate": "are",
    "object": "multiple standby workers"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "is",
    "object": "THE CENTRAL COMPONENT"
  },
  {
    "subject": "the memory daemon and the workers",
    "predicate": "execute",
    "object": "the tasks"
  },
  {
    "subject": "MEMORY DAEMON",
    "predicate": "is",
    "object": "a daemon"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "MANAGES",
    "object": "THE GPU MEMORY"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "MANAGES",
    "object": "THE DNN MODELS"
  },
  {
    "subject": "THE SERVER",
    "predicate": "STORES",
    "object": "THE DNN MODELS IN THE HOST MEMORY"
  },
  {
    "subject": "all components",
    "predicate": "should be optimized to meet",
    "object": "the SLOs"
  },
  {
    "subject": "A STANDBY WORKER",
    "predicate": "is",
    "object": "idle"
  },
  {
    "subject": "A STANDBY WORKER",
    "predicate": "is",
    "object": "initializing a new task"
  },
  {
    "subject": "A STANDBY WORKER",
    "predicate": "is",
    "object": "cleaning its environment for the previous task"
  },
  {
    "subject": "The standby worker",
    "predicate": "becomes",
    "object": "the new active worker"
  },
  {
    "subject": "The new active worker",
    "predicate": "executes",
    "object": "the new task"
  },
  {
    "subject": "The active worker",
    "predicate": "becomes",
    "object": "a standby worker"
  },
  {
    "subject": "The active worker",
    "predicate": "cleans",
    "object": "the environment for the previous task"
  },
  {
    "subject": "a new task",
    "predicate": "arrives before",
    "object": "a standby worker finishes cleaning a previous task"
  },
  {
    "subject": "the new task",
    "predicate": "needs to",
    "object": "wait"
  },
  {
    "subject": "waiting",
    "predicate": "increases",
    "object": "its startup time"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "QUEUES",
    "object": "A SET OF TASKS RECEIVED FROM THE CLIENTS"
  },
  {
    "subject": "a scheduling policy",
    "predicate": "to decide",
    "object": "which task to execute next"
  },
  {
    "subject": "The scheduling",
    "predicate": "is",
    "object": "preemptive"
  },
  {
    "subject": "The controller",
    "predicate": "can preempt",
    "object": "the current task for the next one"
  },
  {
    "subject": "The controller",
    "predicate": "preempts based on",
    "object": "the scheduling policy"
  },
  {
    "subject": "canonical scheduling policies",
    "predicate": "include",
    "object": "RST COME RST SERVE (FCFS)"
  },
  {
    "subject": "canonical scheduling policies",
    "predicate": "include",
    "object": "earliest deadline RST (EDF)"
  },
  {
    "subject": "We",
    "predicate": "focus on",
    "object": "fast context switching"
  },
  {
    "subject": "The specific scheduling algorithm",
    "predicate": "is",
    "object": "orthogonal to this paper"
  },
  {
    "subject": "the controller",
    "predicate": "can preempt",
    "object": "the current task"
  },
  {
    "subject": "the current task",
    "predicate": "is",
    "object": "a training task"
  },
  {
    "subject": "an inference task",
    "predicate": "has",
    "object": "a strict latency SLO"
  },
  {
    "subject": "the controller",
    "predicate": "waits for",
    "object": "the current task to finish if it is inference"
  },
  {
    "subject": "the controller",
    "predicate": "preempts",
    "object": "the current task by notifying the active worker to stop if it is training"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "NOTICES",
    "object": "AN IDLE STANDBY WORKER"
  },
  {
    "subject": "AN IDLE STANDBY WORKER",
    "predicate": "INITIALIZES",
    "object": "ITS ENVIRONMENT FOR THE NEW TASK"
  },
  {
    "subject": "the controller",
    "predicate": "schedules",
    "object": "a task"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "ALLOCATES",
    "object": "THE MEMORY TO THE STANDBY WORKER (4.3)"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "TRANSMITS",
    "object": "THE MODEL USED BY THE NEW TASK FROM THE HOST MEMORY TO THE GPU MEMORY"
  },
  {
    "subject": "The memory daemon",
    "predicate": "transmits",
    "object": "the model from the host memory to the GPU memory"
  },
  {
    "subject": "Transmitting the model from the host memory to the GPU memory",
    "predicate": "eliminates",
    "object": "the extra memory copy from the memory daemon to the worker"
  },
  {
    "subject": "the memory daemon",
    "predicate": "needs to notify",
    "object": "the worker"
  },
  {
    "subject": "the memory daemon",
    "predicate": "needs to export",
    "object": "the relevant GPU memory handlers to the worker"
  },
  {
    "subject": "the worker",
    "predicate": "can access",
    "object": "the model"
  },
  {
    "subject": "the worker",
    "predicate": "can execute",
    "object": "its task"
  },
  {
    "subject": "the model",
    "predicate": "is transmitted to",
    "object": "the GPU"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "handles",
    "object": "GPU MEMORY ALLOCATION"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "handles",
    "object": "MODEL TRANSMISSION"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "creates",
    "object": "GPU MEMORY HANDLERS"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "sends",
    "object": "GPU MEMORY HANDLERS TO WORKERS"
  },
  {
    "subject": "THE PRIMARY GOAL OF THIS PAPER",
    "predicate": "is to design",
    "object": "a set of techniques based on the characteristics of DL applications to minimize the task switching overhead in this process"
  },
  {
    "subject": "task switching overhead",
    "predicate": "is broken down to",
    "object": "individual components"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "end-to-end experiments"
  },
  {
    "subject": "end-to-end experiments",
    "predicate": "demonstrate",
    "object": "the benefits of PipeSwitch"
  },
  {
    "subject": "we",
    "predicate": "show",
    "object": "the effectiveness of the design choices on each component"
  },
  {
    "subject": "we",
    "predicate": "describe",
    "object": "our design"
  },
  {
    "subject": "our design",
    "predicate": "minimize",
    "object": "the overhead of each component"
  },
  {
    "subject": "A server",
    "predicate": "stops",
    "object": "a training task running on the GPU"
  },
  {
    "subject": "A server",
    "predicate": "starts",
    "object": "an inference task"
  },
  {
    "subject": "The DNN model used in the measurement",
    "predicate": "is",
    "object": "ResNet152 17"
  },
  {
    "subject": "THE MEASUREMENT",
    "predicate": "covers",
    "object": "two types of instances on Amazon AWS"
  },
  {
    "subject": "two types of instances on Amazon AWS",
    "predicate": "are",
    "object": "G4dn.2xlarge with NVIDIA T4"
  },
  {
    "subject": "two types of instances on Amazon AWS",
    "predicate": "are",
    "object": "P3.2xlarge with NVIDIA V100"
  },
  {
    "subject": "the inference task",
    "predicate": "has arrived at",
    "object": "the server"
  },
  {
    "subject": "we",
    "predicate": "focus on measuring",
    "object": "the time to start and execute it on the GPU"
  },
  {
    "subject": "We",
    "predicate": "exclude",
    "object": "the network time"
  },
  {
    "subject": "We",
    "predicate": "exclude",
    "object": "the task queueing time"
  },
  {
    "subject": "total times to start the inference task on the GPUs",
    "predicate": "are",
    "object": "5787 ms and 7551 ms, respectively"
  },
  {
    "subject": "we",
    "predicate": "break down",
    "object": "the overhead into the four components"
  },
  {
    "subject": "TASK CLEANING",
    "predicate": "takes",
    "object": "time"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "creates and initializes",
    "object": "its environment"
  },
  {
    "subject": "its environment",
    "predicate": "includes",
    "object": "process launching"
  },
  {
    "subject": "its environment",
    "predicate": "includes",
    "object": "PyTorch CUDA runtime loading"
  },
  {
    "subject": "its environment",
    "predicate": "includes",
    "object": "CUDA context initialization"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "ALLOCATES",
    "object": "GPU MEMORY FOR ITS NEURAL NETWORK MODEL"
  },
  {
    "subject": "THE INFERENCE TASK",
    "predicate": "TRANSMITS",
    "object": "THE MODEL FROM THE HOST MEMORY TO THE GPU MEMORY"
  },
  {
    "subject": "inference time on V100",
    "predicate": "is lower than",
    "object": "inference time on T4"
  },
  {
    "subject": "inference time on V100 and inference time on T4",
    "predicate": "are significantly lower than",
    "object": "total overheads"
  },
  {
    "subject": "Lower overhead on T4",
    "predicate": "is because",
    "object": "task switching largely depends on CPU"
  },
  {
    "subject": "G4dn.2xlarge",
    "predicate": "is equipped with",
    "object": "better CPU than P3.2xlarge"
  },
  {
    "subject": "Better CPU on G4dn.2xlarge",
    "predicate": "is",
    "object": "Intel Platinum 8259CL"
  },
  {
    "subject": "CPU on P3.2xlarge",
    "predicate": "is",
    "object": "Intel Xeon E5-2686 v4"
  },
  {
    "subject": "A strawman solution",
    "predicate": "stops",
    "object": "the old task"
  },
  {
    "subject": "A strawman solution",
    "predicate": "starts",
    "object": "the new task"
  },
  {
    "subject": "A strawman solution that simply stops the old task and starts the new task",
    "predicate": "would violate",
    "object": "SLOs"
  },
  {
    "subject": "all the components",
    "predicate": "take",
    "object": "considerable time compared to the inference time"
  },
  {
    "subject": "we",
    "predicate": "emphasize",
    "object": "all the components should be optimized to achieve minimal switching overhead and meet the SLOs"
  },
  {
    "subject": "PCIE bandwidth",
    "predicate": "is",
    "object": "the physical limit on how fast an arbitrary task can be loaded to the GPU"
  },
  {
    "subject": "We",
    "predicate": "exploit the characteristics of DL applications",
    "object": "to circumvent this physical limit"
  },
  {
    "subject": "The computation",
    "predicate": "is performed",
    "object": "layer by layer"
  },
  {
    "subject": "an inference task",
    "predicate": "performs",
    "object": "a forward pass from the RST layer to the NAL layer"
  },
  {
    "subject": "an inference task",
    "predicate": "performs",
    "object": "a forward pass to make a prediction"
  },
  {
    "subject": "each iteration in a training task",
    "predicate": "performs",
    "object": "a forward pass"
  },
  {
    "subject": "each iteration in a training task",
    "predicate": "performs",
    "object": "a backward pass"
  },
  {
    "subject": "the task",
    "predicate": "can start",
    "object": "the computation of a layer as soon as the layer is loaded in the GPU and the input of the layer is ready"
  },
  {
    "subject": "the input of the layer",
    "predicate": "is ready",
    "object": "the previous layers have finished their computation"
  },
  {
    "subject": "the task",
    "predicate": "can start computation",
    "object": "regardless of its following layers"
  },
  {
    "subject": "FIGURE 2",
    "predicate": "illustrates",
    "object": "the advantage of pipelining over the strawman solution"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "requires",
    "object": "the knowledge of models"
  },
  {
    "subject": "pipelining mechanism",
    "predicate": "has",
    "object": "optimal model-aware grouping in PipeSwitch"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "uses",
    "object": "model-aware grouping"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "achieves",
    "object": "the best trade-off between pipeline overhead and efficiency"
  },
  {
    "subject": "MODEL",
    "predicate": "transmit over",
    "object": "PCIE"
  },
  {
    "subject": "TASK",
    "predicate": "execute on",
    "object": "GPU"
  },
  {
    "subject": "MODEL",
    "predicate": "transmit to",
    "object": "GPU"
  },
  {
    "subject": "PCIE GPU E0 E1 EN-1 E2 (B)",
    "predicate": "pipeline",
    "object": "model transmission and task execution"
  },
  {
    "subject": "THE EXAMPLE",
    "predicate": "shows",
    "object": "an inference task"
  },
  {
    "subject": "an inference task",
    "predicate": "has",
    "object": "only a forward pass in task execution"
  },
  {
    "subject": "ADDING HOOKS",
    "predicate": "can be",
    "object": "automated"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "can be implemented as",
    "object": "a part of the DNN framework"
  },
  {
    "subject": "DNN framework",
    "predicate": "example",
    "object": "PYTORCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "can gather",
    "object": "the model structure information"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "remains",
    "object": "transparent to users and cluster managers"
  },
  {
    "subject": "the basic way for pipelining",
    "predicate": "is",
    "object": "to pipeline on per-layer granularity"
  },
  {
    "subject": "the system",
    "predicate": "transmits",
    "object": "the layers to the GPU memory one by one"
  },
  {
    "subject": "the computation for a layer",
    "predicate": "is blocked",
    "object": "before the layer is transmitted"
  },
  {
    "subject": "ONE",
    "predicate": "is",
    "object": "the overhead to invoke multiple calls to PCIe to transmit the data"
  },
  {
    "subject": "transmission overhead",
    "predicate": "is dominated by",
    "object": "data size"
  },
  {
    "subject": "dividing the model into many layers and invoking a PCIe call for each layer",
    "predicate": "would cause",
    "object": "significant extra overhead"
  },
  {
    "subject": "the other",
    "predicate": "is",
    "object": "the synchronization overhead between transmission and computation"
  },
  {
    "subject": "the synchronization overhead",
    "predicate": "is necessary for",
    "object": "the computation to know when a layer is ready to compute"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "grouping"
  },
  {
    "subject": "grouping",
    "predicate": "minimize",
    "object": "these two sources of overhead"
  },
  {
    "subject": "we",
    "predicate": "combine",
    "object": "multiple layers into a group"
  },
  {
    "subject": "the pipelining",
    "predicate": "is performed on",
    "object": "per-group granularity"
  },
  {
    "subject": "pipelining overhead",
    "predicate": "is paid",
    "object": "once for each group"
  },
  {
    "subject": "pipelining overhead",
    "predicate": "is paid",
    "object": "instead of each layer"
  },
  {
    "subject": "GROUPING",
    "predicate": "introduces",
    "object": "a trade-off between pipelining efficiency and pipelining overhead"
  },
  {
    "subject": "using small groups",
    "predicate": "enables",
    "object": "more overlap between transmission and computation"
  },
  {
    "subject": "more overlap between transmission and computation",
    "predicate": "improves",
    "object": "pipelining efficiency"
  },
  {
    "subject": "using small groups",
    "predicate": "pays",
    "object": "more pipelining overhead"
  },
  {
    "subject": "using big groups",
    "predicate": "has",
    "object": "minimal pipelining overhead"
  },
  {
    "subject": "using big groups",
    "predicate": "reduces",
    "object": "the chance for overlapping"
  },
  {
    "subject": "grouping",
    "predicate": "must be",
    "object": "model-aware"
  },
  {
    "subject": "models",
    "predicate": "have",
    "object": "different structures"
  },
  {
    "subject": "different structures",
    "predicate": "are in terms of",
    "object": "the number of layers"
  },
  {
    "subject": "different structures",
    "predicate": "are in terms of",
    "object": "the size of each layer"
  },
  {
    "subject": "we",
    "predicate": "can enumerate",
    "object": "all possible combinations"
  },
  {
    "subject": "we",
    "predicate": "can find",
    "object": "the optimal grouping strategy"
  },
  {
    "subject": "we",
    "predicate": "introduce",
    "object": "two pruning techniques"
  },
  {
    "subject": "two pruning techniques",
    "predicate": "are based on",
    "object": "two insights"
  },
  {
    "subject": "large models",
    "predicate": "can have",
    "object": "hundreds of layers"
  },
  {
    "subject": "time complexity for enumeration",
    "predicate": "is",
    "object": "exponential"
  },
  {
    "subject": "PCIE GPU",
    "predicate": "has",
    "object": "LOWER BOUND OF F(GROUP(0, I), I1)"
  },
  {
    "subject": "PCIE GPU",
    "predicate": "contains",
    "object": "GROUP(0, I)"
  },
  {
    "subject": "PCIE GPU",
    "predicate": "contains",
    "object": "GROUP(I1, J)"
  },
  {
    "subject": "GROUP(0, I)",
    "predicate": "is related to",
    "object": "GROUP(I1, J)"
  },
  {
    "subject": "J, J1",
    "predicate": "range",
    "object": "N-1"
  },
  {
    "subject": "Case (A)",
    "predicate": "should be pruned if",
    "object": "LOWER BOUND CURRENT OPTIMAL TIME"
  },
  {
    "subject": "cases",
    "predicate": "group from",
    "object": "I to J"
  },
  {
    "subject": "batch",
    "predicate": "at least from layer",
    "object": "I1 to J"
  },
  {
    "subject": "GROUP(0, I)",
    "predicate": "is an example in",
    "object": "Figure 3"
  },
  {
    "subject": "GROUP(I1, N-1)",
    "predicate": "is an example in",
    "object": "Figure 3"
  },
  {
    "subject": "Figure 3",
    "predicate": "shows",
    "object": "two pruning techniques"
  },
  {
    "subject": "we",
    "predicate": "can prune",
    "object": "the cases that group from layer (I 1) to J J"
  },
  {
    "subject": "we",
    "predicate": "only search for",
    "object": "J J algorithm"
  },
  {
    "subject": "N",
    "predicate": "be",
    "object": "the number of layers"
  },
  {
    "subject": "F(B,I)",
    "predicate": "be",
    "object": "a function"
  },
  {
    "subject": "F(B,I)",
    "predicate": "returns",
    "object": "the total time of the optimal grouping strategy from layer I to N-1"
  },
  {
    "subject": "layers 0 to I-1",
    "predicate": "have formed",
    "object": "groups represented by B"
  },
  {
    "subject": "the function",
    "predicate": "applies itself to",
    "object": "the optimal groups from layer I1 to N-1"
  },
  {
    "subject": "the function",
    "predicate": "applies itself recursively",
    "object": "the optimal groups from layer I1 to N-1"
  },
  {
    "subject": "the function",
    "predicate": "updates",
    "object": "optgroups"
  },
  {
    "subject": "optgroups",
    "predicate": "is updated if",
    "object": "the current strategy is better"
  },
  {
    "subject": "the group",
    "predicate": "is formed",
    "object": "from layer X to I"
  },
  {
    "subject": "We",
    "predicate": "divide",
    "object": "all possible combinations into N cases"
  },
  {
    "subject": "The first group",
    "predicate": "contains",
    "object": "layer 0 to I"
  },
  {
    "subject": "Case I",
    "predicate": "means",
    "object": "the first group contains layer 0 to I"
  },
  {
    "subject": "The optimal grouping strategy",
    "predicate": "is for",
    "object": "the entire model"
  },
  {
    "subject": "THIS FORMULA",
    "predicate": "can be applied recursively to compute",
    "object": "F(GROUP(0,I),I1)"
  },
  {
    "subject": "The RST group",
    "predicate": "contains",
    "object": "too many layers"
  },
  {
    "subject": "The computation of the RST group",
    "predicate": "would be delayed",
    "object": "too much"
  },
  {
    "subject": "The delay",
    "predicate": "would",
    "object": "compensate the pipeline efficiency"
  },
  {
    "subject": "we",
    "predicate": "can safely pack",
    "object": "multiple layers in a group based on the progress of computation"
  },
  {
    "subject": "packing multiple layers in a group based on the progress of computation",
    "predicate": "does not affect",
    "object": "pipeline efficiency"
  },
  {
    "subject": "other than the RST group",
    "predicate": "is exception to",
    "object": "packing multiple layers in a group based on the progress of computation"
  },
  {
    "subject": "T(I, J)",
    "predicate": "is",
    "object": "the transmission time for a group from layer I to J"
  },
  {
    "subject": "E(I, J)",
    "predicate": "is",
    "object": "the execution time for a group from layer I to J"
  },
  {
    "subject": "T(I, J)",
    "predicate": "is calculated based on",
    "object": "the size of layer I to J and PCIe bandwidth"
  },
  {
    "subject": "E(I, J)",
    "predicate": "is profiled on",
    "object": "the GPU"
  },
  {
    "subject": "the overhead of invoking multiple calls",
    "predicate": "is included in",
    "object": "T(I, J)"
  },
  {
    "subject": "we",
    "predicate": "compute",
    "object": "a lower bound for the total time for each case in Equation 1"
  },
  {
    "subject": "the lower bound",
    "predicate": "considers",
    "object": "the best case that all the remaining layers are combined in one group for transmission and computation"
  },
  {
    "subject": "the computation and communication",
    "predicate": "can be",
    "object": "perfectly overlapped"
  },
  {
    "subject": "its computation",
    "predicate": "can happen",
    "object": "right after the computation of the rst group finishes"
  },
  {
    "subject": "RST group",
    "predicate": "is from layer",
    "object": "0 to I"
  },
  {
    "subject": "Equation 1",
    "predicate": "is applied recursively to",
    "object": "enumerate the cases for the PCIE GPU B.delay"
  },
  {
    "subject": "Group",
    "predicate": "is at least from layer",
    "object": "X to J"
  },
  {
    "subject": "Group",
    "predicate": "includes",
    "object": "0, 1, ..., X-1"
  },
  {
    "subject": "Lower bound",
    "predicate": "is of",
    "object": "F(B Group(A,I), I1)"
  },
  {
    "subject": "Figure 4",
    "predicate": "illustrates",
    "object": "general case for the two pruning techniques"
  },
  {
    "subject": "we",
    "predicate": "can hide",
    "object": "the transmission of the second group into the computation of the RST group"
  },
  {
    "subject": "the transmission",
    "predicate": "finishes no later than",
    "object": "the computation of the RST group"
  },
  {
    "subject": "the least number of layers to group",
    "predicate": "can be computed using",
    "object": "the following equation"
  },
  {
    "subject": "JIS",
    "predicate": "is no better than",
    "object": "grouping from (I1) to J"
  },
  {
    "subject": "grouping from (I1) to J",
    "predicate": "does not increase",
    "object": "pipeline efficiency"
  },
  {
    "subject": "JIS",
    "predicate": "has",
    "object": "higher pipeline overhead"
  },
  {
    "subject": "this algorithm",
    "predicate": "runs",
    "object": "offline to find the strategy"
  },
  {
    "subject": "the resulting strategy",
    "predicate": "is used by",
    "object": "PipeSwitch for context switching"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "shows",
    "object": "the pseudo code"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "computes",
    "object": "the recursive function FINDOPTGROUPING(B,X)"
  },
  {
    "subject": "B",
    "predicate": "represents",
    "object": "the groups that have already formed"
  },
  {
    "subject": "X",
    "predicate": "is",
    "object": "the rst layer that have not formed a group"
  },
  {
    "subject": "RST group",
    "predicate": "contains",
    "object": "all layers from X to N1"
  },
  {
    "subject": "optgroups",
    "predicate": "store",
    "object": "the best grouping strategy from layer X given B"
  },
  {
    "subject": "the best grouping strategy from layer X given B",
    "predicate": "is initialized to",
    "object": "none (line 2)"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "applies",
    "object": "THE SECOND PRUNING INSIGHT"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "forms",
    "object": "THE RST GROUP FROM LAYER X (LINE 3-9)"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "DIVIDES",
    "object": "THE PROBLEM INTO K 1 CASES"
  },
  {
    "subject": "CASE I",
    "predicate": "FORMS",
    "object": "THE RST GROUP FROM LAYER X TO XI"
  },
  {
    "subject": "I",
    "predicate": "RANGES FROM",
    "object": "0 TO K"
  },
  {
    "subject": "EQUATION 3 AND FIGURE 3(B)",
    "predicate": "illustrate",
    "object": "this insight with a special example"
  },
  {
    "subject": "B",
    "predicate": "contains",
    "object": "one group from layer 0 to I"
  },
  {
    "subject": "B",
    "predicate": "can contain",
    "object": "multiple groups formed by previous layers"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "B.DELAY to denote the time to which the group can be formed"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "is based on",
    "object": "B.DELAY (LINE 4-9)"
  },
  {
    "subject": "THE ENUMERATION FOR I",
    "predicate": "can skip",
    "object": "the layers from X to J-1 (LINE 11)"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "applies",
    "object": "THE RST INSIGHT"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "computes",
    "object": "THE LOWER BOUND"
  },
  {
    "subject": "The example in Equation 2 and Figure 3(A)",
    "predicate": "is",
    "object": "a special case when X is 0"
  },
  {
    "subject": "the computation from X",
    "predicate": "has to wait for",
    "object": "both its transmission (i.e., T(X,I)) and the computation of the previous groups (i.e., B.DELAY)"
  },
  {
    "subject": "the lower bound",
    "predicate": "is already bigger than",
    "object": "the current optimal time"
  },
  {
    "subject": "case I",
    "predicate": "is pruned",
    "object": "line 18-19"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "a heuristic that bootstraps optgroups"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "is",
    "object": "Optimal Model-Aware Grouping"
  },
  {
    "subject": "Function FINDOPTGROUPING",
    "predicate": "takes parameters",
    "object": "B, X"
  },
  {
    "subject": "OPTGROUPS",
    "predicate": "is initialized to",
    "object": "0"
  },
  {
    "subject": "OPTGROUPS.TIME",
    "predicate": "is initialized to",
    "object": "0"
  },
  {
    "subject": "Layer I",
    "predicate": "ranges from",
    "object": "X to N1"
  },
  {
    "subject": "If T(X,I)",
    "predicate": "is less than or equal to",
    "object": "B.DELAY"
  },
  {
    "subject": "J",
    "predicate": "is assigned",
    "object": "I"
  },
  {
    "subject": "Else",
    "predicate": "action",
    "object": "Break"
  },
  {
    "subject": "Lower bound",
    "predicate": "is computed by",
    "object": "min(TRANSTIME, EXECTIME)"
  },
  {
    "subject": "TRANSTIME",
    "predicate": "is calculated as",
    "object": "T(X,I) + T(I+1,N1)"
  },
  {
    "subject": "EXECTIME",
    "predicate": "is calculated as",
    "object": "max(T(X,I), B.DELAY)"
  },
  {
    "subject": "If LOWERBOUND",
    "predicate": "is greater than or equal to",
    "object": "OPTGROUPS.TIME"
  },
  {
    "subject": "FirstGroup",
    "predicate": "is assigned",
    "object": "GROUP(X,I)"
  },
  {
    "subject": "RestGroups",
    "predicate": "is assigned",
    "object": "FINDOPTGROUPING(B, FirstGroup, I+1)"
  },
  {
    "subject": "CurGroups",
    "predicate": "is assigned",
    "object": "FirstGroup + RestGroups"
  },
  {
    "subject": "If CURGROUPS.TIME",
    "predicate": "is less than",
    "object": "OPTGROUPS.TIME"
  },
  {
    "subject": "OPTGROUPS",
    "predicate": "is assigned",
    "object": "CURGROUPS"
  },
  {
    "subject": "Function FINDOPTGROUPING",
    "predicate": "returns",
    "object": "OPTGROUPS"
  },
  {
    "subject": "Good strategy",
    "predicate": "is",
    "object": "group every ten layers"
  },
  {
    "subject": "The two pruning techniques",
    "predicate": "are able to prune",
    "object": "most of the strategies"
  },
  {
    "subject": "THE ALGORITHM",
    "predicate": "USES",
    "object": "TWO PRUNING TECHNIQUES"
  },
  {
    "subject": "ALGORITHM 1 NDS",
    "predicate": "is",
    "object": "the optimal grouping strategy"
  },
  {
    "subject": "the optimal grouping strategy",
    "predicate": "minimizes",
    "object": "the total time for the pipeline"
  },
  {
    "subject": "M N X",
    "predicate": "is",
    "object": "the number of layers the function considers"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "induction on M"
  },
  {
    "subject": "FINDOPTGROUPING(B,X)",
    "predicate": "outputs",
    "object": "the optimal grouping strategy from layer X to N 1"
  },
  {
    "subject": "previous layers",
    "predicate": "have formed",
    "object": "groups represented by B"
  },
  {
    "subject": "FINDOPTGROUPING(B,X)",
    "predicate": "outputs",
    "object": "the optimal strategy"
  },
  {
    "subject": "FINDOPTGROUPING(B GROUP(X,X I),X I 1)",
    "predicate": "only considers",
    "object": "K I K layers"
  },
  {
    "subject": "FINDOPTGROUPING(B GROUP(X,X I),X I 1)",
    "predicate": "outputs",
    "object": "the optimal grouping strategy for case I"
  },
  {
    "subject": "the optimal grouping strategy for case I",
    "predicate": "is based on",
    "object": "the assumption"
  },
  {
    "subject": "THE FUNCTION",
    "predicate": "examines",
    "object": "ONE LAYER"
  },
  {
    "subject": "M",
    "predicate": "is",
    "object": "1"
  },
  {
    "subject": "Layer X",
    "predicate": "is",
    "object": "one group"
  },
  {
    "subject": "This strategy",
    "predicate": "is",
    "object": "the optimal strategy"
  },
  {
    "subject": "the algorithm",
    "predicate": "considers",
    "object": "K1 layers"
  },
  {
    "subject": "the optimal strategy for this case",
    "predicate": "is",
    "object": "one group"
  },
  {
    "subject": "these cases",
    "predicate": "are",
    "object": "exclusive"
  },
  {
    "subject": "these cases",
    "predicate": "cover",
    "object": "the entire search space"
  },
  {
    "subject": "the algorithm",
    "predicate": "outputs",
    "object": "the optimal grouping strategy for M K 1"
  },
  {
    "subject": "the algorithm",
    "predicate": "chooses",
    "object": "the optimal grouping strategy from these cases"
  },
  {
    "subject": "THE RST TECHNIQUE",
    "predicate": "prunes",
    "object": "the cases"
  },
  {
    "subject": "their lower bounds",
    "predicate": "are no better than",
    "object": "the current found optimal"
  },
  {
    "subject": "this technique",
    "predicate": "does not affect",
    "object": "the optimality"
  },
  {
    "subject": "THE SECOND TECHNIQUE",
    "predicate": "prunes",
    "object": "THE CASE"
  },
  {
    "subject": "THEIR RST GROUPS",
    "predicate": "are from",
    "object": "LAYER X TO J J"
  },
  {
    "subject": "these cases",
    "predicate": "cannot advance",
    "object": "the computation to an earlier point than grouping from X to at least J"
  },
  {
    "subject": "pruning these cases",
    "predicate": "do not affect",
    "object": "the optimality"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "achieves",
    "object": "optimality for a given list of layers"
  },
  {
    "subject": "layers or operators in a DNN model",
    "predicate": "can be connected as",
    "object": "an arbitrary computation graph"
  },
  {
    "subject": "Models like ResNet and Inception",
    "predicate": "are",
    "object": "technically non-linear directed acyclic graphs (DAGs)"
  },
  {
    "subject": "execution order",
    "predicate": "is",
    "object": "that the layers/operators in the DAG are issued to the GPU one by one"
  },
  {
    "subject": "grouping the layers",
    "predicate": "aims to achieve",
    "object": "high pipelining efficiency and low pipelining overhead"
  },
  {
    "subject": "the order",
    "predicate": "is based on",
    "object": "the rst time an operator is executed"
  },
  {
    "subject": "an operator",
    "predicate": "can be executed only when",
    "object": "it is transmitted to the GPU and the input is ready"
  },
  {
    "subject": "OUR PIPELINED MODEL TRANSMISSION",
    "predicate": "is applicable to",
    "object": "THE GENERAL CASE"
  },
  {
    "subject": "pipelined model transmission",
    "predicate": "is evaluated by",
    "object": "keeping all other components of PipeSwitch the same and comparing mechanisms discussed in 4.2"
  },
  {
    "subject": "UNIFIED MEMORY MANAGEMENT TASK EXECUTION IN A GPU",
    "predicate": "requires",
    "object": "GPU MEMORY"
  },
  {
    "subject": "A GPU",
    "predicate": "has",
    "object": "its own memory management system"
  },
  {
    "subject": "A GPU",
    "predicate": "provides",
    "object": "a malloc function"
  },
  {
    "subject": "malloc function",
    "predicate": "is similar to",
    "object": "CPUs for memory allocation"
  },
  {
    "subject": "malloc function",
    "predicate": "example",
    "object": "cudaMalloc for NVIDIA GPUs"
  },
  {
    "subject": "each task",
    "predicate": "uses",
    "object": "the native cudaMallocManaged function for GPU memory allocation"
  },
  {
    "subject": "each task",
    "predicate": "delegates",
    "object": "model transmission to CUDA unified memory"
  },
  {
    "subject": "we",
    "predicate": "add",
    "object": "functions for allocating GPU memory"
  },
  {
    "subject": "we",
    "predicate": "add",
    "object": "functions for sharing the GPU memory to workers through CUDA IPC API"
  },
  {
    "subject": "we",
    "predicate": "add",
    "object": "functions for getting the shared GPU memory"
  },
  {
    "subject": "each worker",
    "predicate": "uses",
    "object": "cudaMalloc to allocate GPU memory"
  },
  {
    "subject": "each worker",
    "predicate": "transmits",
    "object": "the model to GPU by its own"
  },
  {
    "subject": "Each worker",
    "predicate": "allocates",
    "object": "GPU memory with cudamallocmanaged"
  },
  {
    "subject": "CUDA",
    "predicate": "automatically transmits",
    "object": "the model to GPU when needed"
  },
  {
    "subject": "THIS SOLUTION",
    "predicate": "incurs",
    "object": "HIGH OVERHEAD FOR DL APPLICATIONS"
  },
  {
    "subject": "THE NATIVE CUDAMALLOC FUNCTION AND CUDA UNIFIED MEMORY",
    "predicate": "are designed for",
    "object": "GENERAL-PURPOSE APPLICATIONS"
  },
  {
    "subject": "THE NATIVE CUDAMALLOC FUNCTION AND CUDA UNIFIED MEMORY",
    "predicate": "may incur",
    "object": "UNNECESSARY OVERHEAD FOR DL APPLICATIONS"
  },
  {
    "subject": "CUDA Unified Memory",
    "predicate": "is not optimized for",
    "object": "DL applications"
  },
  {
    "subject": "CUDA Unified Memory",
    "predicate": "introduces",
    "object": "more than one hundred milliseconds overhead than PipeSwitch"
  },
  {
    "subject": "We",
    "predicate": "exploit",
    "object": "two characteristics of DL applications"
  },
  {
    "subject": "exploiting two characteristics of DL applications",
    "predicate": "minimizes",
    "object": "GPU memory management overhead"
  },
  {
    "subject": "THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT",
    "predicate": "does not consider",
    "object": "THESE CHARACTERISTICS"
  },
  {
    "subject": "THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT",
    "predicate": "is",
    "object": "too heavy-weight for DL applications that require fast task switching"
  },
  {
    "subject": "the amount of memory allocated to the DNN model",
    "predicate": "is",
    "object": "fixed"
  },
  {
    "subject": "the amount of memory allocated to the DNN model",
    "predicate": "does not change",
    "object": "during task execution"
  },
  {
    "subject": "a training task",
    "predicate": "updates",
    "object": "the model parameters"
  },
  {
    "subject": "the model parameters",
    "predicate": "are",
    "object": "the weights of the neural network"
  },
  {
    "subject": "a training task",
    "predicate": "does not update",
    "object": "the DNN structure"
  },
  {
    "subject": "the amount of memory needed to store the model parameters",
    "predicate": "stays",
    "object": "the same"
  },
  {
    "subject": "INFERENCE TASK",
    "predicate": "only uses",
    "object": "the model for inference"
  },
  {
    "subject": "INFERENCE TASK",
    "predicate": "does not change",
    "object": "the model itself"
  },
  {
    "subject": "THE INTERMEDIATE RESULTS",
    "predicate": "CHANGE IN",
    "object": "A SIMPLE, REGULAR PATTERN"
  },
  {
    "subject": "THE INTERMEDIATE RESULTS",
    "predicate": "DO NOT CAUSE",
    "object": "MEMORY FRAGMENTATION"
  },
  {
    "subject": "the intermediate results",
    "predicate": "are",
    "object": "the outputs of each layer"
  },
  {
    "subject": "the intermediate results",
    "predicate": "are used by",
    "object": "the next layer"
  },
  {
    "subject": "the backward pass",
    "predicate": "consumes",
    "object": "the intermediate results"
  },
  {
    "subject": "the backward pass",
    "predicate": "consumes in order",
    "object": "reverse order"
  },
  {
    "subject": "the forward pass",
    "predicate": "generates",
    "object": "the intermediate results"
  },
  {
    "subject": "the intermediate results",
    "predicate": "are",
    "object": "RST-IN-LAST-OUT"
  },
  {
    "subject": "memory allocation and release",
    "predicate": "can be handled by",
    "object": "a simple stack-like mechanism"
  },
  {
    "subject": "a simple stack-like mechanism",
    "predicate": "does not cause",
    "object": "memory fragmentation"
  },
  {
    "subject": "Memory allocation overhead",
    "predicate": "should be minimized",
    "object": ""
  },
  {
    "subject": "You",
    "predicate": "should minimize",
    "object": "memory footprint"
  },
  {
    "subject": "You",
    "predicate": "should avoid",
    "object": "extra memory copies"
  },
  {
    "subject": "we",
    "predicate": "design",
    "object": "a memory management mechanism tailored for DL applications"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "uses",
    "object": "a dedicated memory daemon"
  },
  {
    "subject": "a dedicated memory daemon",
    "predicate": "manages",
    "object": "the GPU memory"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "sends",
    "object": "a 64-bit integer offset for the shared GPU memory to workers"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "saves",
    "object": "223 ms"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "saves time by",
    "object": "eliminating the memory allocation overhead with the memory daemon"
  },
  {
    "subject": "compared to",
    "predicate": "no unified memory management",
    "object": "PIPESWITCH"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "only needs to pass",
    "object": "memory pointers to the workers"
  },
  {
    "subject": "passing memory pointers to the workers",
    "predicate": "is",
    "object": "light-weight"
  },
  {
    "subject": "THE DAEMON",
    "predicate": "ensures",
    "object": "that each time only one worker owns the GPU memory"
  },
  {
    "subject": "one worker",
    "predicate": "owns",
    "object": "the GPU memory"
  },
  {
    "subject": "memory isolation",
    "predicate": "is guaranteed",
    "object": "between workers"
  },
  {
    "subject": "each worker",
    "predicate": "uses",
    "object": "a memory pool to allocate the memory to store its model and intermediate results"
  },
  {
    "subject": "each worker",
    "predicate": "recycles",
    "object": "the memory to the pool after the intermediate results are no longer needed"
  },
  {
    "subject": "THE MEMORY MANAGEMENT OF PIPESWITCH",
    "predicate": "extends",
    "object": "THAT OF PYTORCH"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "inserts",
    "object": "GPU MEMORY BLOCKS TO PYTORCH GPU MEMORY POOL"
  },
  {
    "subject": "PYTORCH",
    "predicate": "creates",
    "object": "TENSORS ON THEM"
  },
  {
    "subject": "The memory management in PyTorch",
    "predicate": "handles",
    "object": "memory allocation for a task itself"
  },
  {
    "subject": "replicating the models in each worker",
    "predicate": "incurs",
    "object": "high memory footprint"
  },
  {
    "subject": "replicating the models in each worker",
    "predicate": "reduces",
    "object": "the number of models a server can store"
  },
  {
    "subject": "reducing the number of models a server can store",
    "predicate": "consequently reduces",
    "object": "the types of tasks the server can execute"
  },
  {
    "subject": "storing the models in a dedicate process",
    "predicate": "has",
    "object": "minimal memory footprint"
  },
  {
    "subject": "each model",
    "predicate": "is stored",
    "object": "only once"
  },
  {
    "subject": "storing the models in a dedicate process",
    "predicate": "incurs",
    "object": "an extra memory copy from this process to a worker to start a task"
  },
  {
    "subject": "an extra memory copy from this process to a worker to start a task",
    "predicate": "hurts",
    "object": "the task switching time"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "stores",
    "object": "the models in the memory daemon"
  },
  {
    "subject": "the server",
    "predicate": "needs to keep",
    "object": "one copy of each model in the host memory"
  },
  {
    "subject": "IPC overhead",
    "predicate": "should be",
    "object": "minimized"
  },
  {
    "subject": "WE",
    "predicate": "leverage",
    "object": "a property of DL applications to minimize the IPC overhead"
  },
  {
    "subject": "IPC APIs",
    "predicate": "are provided by",
    "object": "GPUs"
  },
  {
    "subject": "CUDAIPCOPENMEMHANDLE",
    "predicate": "is an example of",
    "object": "IPC APIs"
  },
  {
    "subject": "CUDAIPCOPENMEMHANDLE",
    "predicate": "is for",
    "object": "NVIDIA GPUs"
  },
  {
    "subject": "IPC APIs",
    "predicate": "have measured performance of",
    "object": "these IPC APIs"
  },
  {
    "subject": "these IPC APIs",
    "predicate": "incur",
    "object": "high overhead"
  },
  {
    "subject": "The overhead",
    "predicate": "is exacerbated by",
    "object": "the pipeline"
  },
  {
    "subject": "The pipeline",
    "predicate": "needs to invoke",
    "object": "the IPCs frequently"
  },
  {
    "subject": "The frequent invocation of the IPCs",
    "predicate": "is for",
    "object": "synchronize model transmission and task execution for every pipeline group"
  },
  {
    "subject": "The pipeline",
    "predicate": "does not invoke",
    "object": "the IPC only once for the entire model transmission"
  },
  {
    "subject": "memory allocation process for a neural network model",
    "predicate": "is",
    "object": "deterministic"
  },
  {
    "subject": "memory daemon and the worker",
    "predicate": "use",
    "object": "the same order to allocate memory for the model parameters"
  },
  {
    "subject": "memory pointers for the parameters",
    "predicate": "would be",
    "object": "the same"
  },
  {
    "subject": "the memory daemon",
    "predicate": "needs to use",
    "object": "the same order to transmit the model"
  },
  {
    "subject": "the neural network model",
    "predicate": "is",
    "object": "known and given"
  },
  {
    "subject": "the memory daemon and the worker",
    "predicate": "have",
    "object": "the same order"
  },
  {
    "subject": "THE MEMORY DAEMON",
    "predicate": "can minimize",
    "object": "THE USAGE OF EXPENSIVE GPU IPCS"
  },
  {
    "subject": "latency",
    "predicate": "is higher than",
    "object": "no unified memory management without IPC optimization"
  },
  {
    "subject": "NO PIN",
    "predicate": "is",
    "object": "MEMORY"
  },
  {
    "subject": "THE OS",
    "predicate": "would swap",
    "object": "a memory page to disk"
  },
  {
    "subject": "the page",
    "predicate": "is",
    "object": "inactive for a certain amount of time"
  },
  {
    "subject": "GPUS",
    "predicate": "require",
    "object": "a page in the host memory to be pinned (or page-locked)"
  },
  {
    "subject": "a page in the host memory",
    "predicate": "is pinned",
    "object": "in order to transmit the data in the page to the GPU memory"
  },
  {
    "subject": "A temporary pinned page",
    "predicate": "is created for",
    "object": "the transmission"
  },
  {
    "subject": "WE",
    "predicate": "pin",
    "object": "the pages of the memory daemon to the host memory"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "is desirable",
    "object": "because it ensures that one task cannot read the memory of another task"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "ensures",
    "object": "one task cannot read the memory of another task"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "ensures",
    "object": "the crashing of one task does not affect other tasks or the entire system"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "separate processes"
  },
  {
    "subject": "separate processes",
    "predicate": "achieve",
    "object": "process-level isolation"
  },
  {
    "subject": "A naive solution",
    "predicate": "is to use",
    "object": "separate processes"
  },
  {
    "subject": "A naive solution",
    "predicate": "is to start",
    "object": "the new task after the current task is stopped"
  },
  {
    "subject": "SEQUENTIAL EXECUTION",
    "predicate": "incurs",
    "object": "LONG DELAY"
  },
  {
    "subject": "LONG DELAY",
    "predicate": "is due to",
    "object": "OLD TASK CLEANING AND NEW TASK INITIALIZATION"
  },
  {
    "subject": "another possible solution",
    "predicate": "is to let",
    "object": "the current and new tasks share the same process with a warm CUDA context"
  },
  {
    "subject": "the new task",
    "predicate": "can reuse",
    "object": "the GPU environment of the current task"
  },
  {
    "subject": "THE PROCESS OF THE OLD TASK",
    "predicate": "cleans",
    "object": "THE GPU ENVIRONMENT"
  },
  {
    "subject": "ANOTHER PROCESS",
    "predicate": "is created",
    "object": "for THE NEW TASK"
  },
  {
    "subject": "ANOTHER PROCESS",
    "predicate": "is initialized",
    "object": "for THE NEW TASK"
  },
  {
    "subject": "THE PROCESS",
    "predicate": "REUSES",
    "object": "THE ENVIRONMENT FOR THE NEW TASK"
  },
  {
    "subject": "Each worker",
    "predicate": "is",
    "object": "a separate process"
  },
  {
    "subject": "Each worker",
    "predicate": "initializes",
    "object": "its own GPU environment (i.e., CUDA context)"
  },
  {
    "subject": "GPU environment (i.e., CUDA context)",
    "predicate": "is initialized",
    "object": "when it is rst created"
  },
  {
    "subject": "a major job",
    "predicate": "is to clear",
    "object": "asynchronous CUDA functions queued on the GPU"
  },
  {
    "subject": "we",
    "predicate": "insert",
    "object": "synchronization points into training tasks"
  },
  {
    "subject": "the number of queued functions",
    "predicate": "are",
    "object": "limited"
  },
  {
    "subject": "the number of queued functions",
    "predicate": "can be",
    "object": "quickly cleared"
  },
  {
    "subject": "Synchronization points",
    "predicate": "are not needed for",
    "object": "inference tasks"
  },
  {
    "subject": "Inference tasks",
    "predicate": "are",
    "object": "short"
  },
  {
    "subject": "Inference tasks",
    "predicate": "are not",
    "object": "preempted"
  },
  {
    "subject": "another job",
    "predicate": "is to",
    "object": "free its GPU memory"
  },
  {
    "subject": "the cleaning procedure",
    "predicate": "does not modify",
    "object": "the content of the memory"
  },
  {
    "subject": "the cleaning procedure",
    "predicate": "cleans",
    "object": "the metadata"
  },
  {
    "subject": "the metadata",
    "predicate": "is",
    "object": "GPU memory pointers"
  },
  {
    "subject": "GPU memory",
    "predicate": "is managed by",
    "object": "PipeSwitch"
  },
  {
    "subject": "cleaning procedure",
    "predicate": "deletes",
    "object": "pointers pointing to the tensor data"
  },
  {
    "subject": "cleaning procedure",
    "predicate": "does not free",
    "object": "the actual data"
  },
  {
    "subject": "the new task",
    "predicate": "can transmit",
    "object": "its model to the GPU memory at the same time"
  },
  {
    "subject": "we",
    "predicate": "can parallelize",
    "object": "the task cleaning of the current task and the pipelined model transmission of the new task"
  },
  {
    "subject": "parallelizing the task cleaning and the pipelined model transmission",
    "predicate": "serves to",
    "object": "hide the task cleaning overhead"
  },
  {
    "subject": "THIS CHOICE",
    "predicate": "is optimized for",
    "object": "performance"
  },
  {
    "subject": "THIS CHOICE",
    "predicate": "is not a problem for",
    "object": "a trusted environment"
  },
  {
    "subject": "a latter process",
    "predicate": "can read",
    "object": "the memory data of a previous process"
  },
  {
    "subject": "an additional zero-out operation",
    "predicate": "can be added",
    "object": "if this is a concern"
  },
  {
    "subject": "GPU",
    "predicate": "has",
    "object": "high memory bandwidth"
  },
  {
    "subject": "high memory bandwidth",
    "predicate": "example",
    "object": "900GBS for V100"
  },
  {
    "subject": "zeroing-out most models like ResNet-152 (around 240MB)",
    "predicate": "would incur",
    "object": "sub-millisecond overhead"
  },
  {
    "subject": "the controller",
    "predicate": "signals",
    "object": "the current active worker to stop"
  },
  {
    "subject": "the controller",
    "predicate": "deletes",
    "object": "the GPU memory allocated to the current active worker"
  },
  {
    "subject": "the controller",
    "predicate": "allocates",
    "object": "the GPU memory to the new active worker"
  },
  {
    "subject": "the controller",
    "predicate": "will notify",
    "object": "the current active worker to stop"
  },
  {
    "subject": "the controller",
    "predicate": "transfers",
    "object": "the parameters of the new model to the GPU"
  },
  {
    "subject": "the controller",
    "predicate": "transfers the parameters after",
    "object": "receiving the current active worker's reply"
  },
  {
    "subject": "THE CONTROLLER",
    "predicate": "ensures",
    "object": "only one active worker"
  },
  {
    "subject": "only one active worker",
    "predicate": "guarantees",
    "object": "exclusive occupation of the GPU"
  },
  {
    "subject": "number of standby workers",
    "predicate": "has trade-off with",
    "object": "their GPU memory consumption"
  },
  {
    "subject": "every standby worker",
    "predicate": "needs to maintain",
    "object": "its own CUDA context"
  },
  {
    "subject": "CUDA context",
    "predicate": "consumes",
    "object": "a few hundred MB GPU memory"
  },
  {
    "subject": "many standby workers",
    "predicate": "can have",
    "object": "at least one idle standby worker"
  },
  {
    "subject": "two standby workers",
    "predicate": "are sufficient to ensure",
    "object": "at least one idle worker"
  },
  {
    "subject": "one idle worker",
    "predicate": "eliminates",
    "object": "the waiting time"
  },
  {
    "subject": "two standby workers",
    "predicate": "have",
    "object": "moderate GPU memory consumption"
  },
  {
    "subject": "A transaction here",
    "predicate": "means",
    "object": "a model is switched in or out on all of its GPUs to enable or disable inference on this model"
  },
  {
    "subject": "We",
    "predicate": "have analyzed",
    "object": "a production GPU training trace from Microsoft"
  },
  {
    "subject": "these jobs",
    "predicate": "account for",
    "object": "18 of total GPU hours"
  },
  {
    "subject": "we",
    "predicate": "expect",
    "object": "the share of multi-GPU jobs to increase in the future"
  },
  {
    "subject": "current training frameworks",
    "predicate": "do not have",
    "object": "mature support of elastic training"
  },
  {
    "subject": "These scheduling solutions",
    "predicate": "are",
    "object": "orthogonal and complementary to PipeSwitch"
  },
  {
    "subject": "These solutions",
    "predicate": "are complementary to",
    "object": "PipeSwitch"
  },
  {
    "subject": "we",
    "predicate": "add",
    "object": "C and Python functions to the GPU memory management module of PyTorch"
  },
  {
    "subject": "we",
    "predicate": "add",
    "object": "functions"
  },
  {
    "subject": "functions",
    "predicate": "insert",
    "object": "the received GPU memory into PyTorch GPU memory pool for a specific CUDA stream"
  },
  {
    "subject": "functions",
    "predicate": "clear",
    "object": "the GPU memory from the pool"
  },
  {
    "subject": "shared GPU memory",
    "predicate": "can be inserted into",
    "object": "PyTorch GPU memory pool"
  },
  {
    "subject": "shared GPU memory",
    "predicate": "can be inserted multiple times for",
    "object": "different CUDA streams"
  },
  {
    "subject": "the controller",
    "predicate": "guarantees",
    "object": "only one of these CUDA streams is active"
  },
  {
    "subject": "THE CONTROLLER PROCESS",
    "predicate": "consists of",
    "object": "A TCP THREAD"
  },
  {
    "subject": "THE CONTROLLER PROCESS",
    "predicate": "consists of",
    "object": "A SCHEDULER THREAD"
  },
  {
    "subject": "the scheduler and the memory daemon",
    "predicate": "are implemented",
    "object": "together"
  },
  {
    "subject": "THE TCP THREAD",
    "predicate": "accepts",
    "object": "TASK THROUGH TCP FROM CLIENTS"
  },
  {
    "subject": "THE TCP THREAD",
    "predicate": "sends",
    "object": "THE TASK TO THE SCHEDULER THREAD"
  },
  {
    "subject": "THE SCHEDULER THREAD",
    "predicate": "ALLOCATES AND SHARES",
    "object": "THE GPU MEMORY WITH WORKERS"
  },
  {
    "subject": "THE SCHEDULER THREAD",
    "predicate": "ACTIVATES OR DEACTIVATES",
    "object": "WORKERS"
  },
  {
    "subject": "THE SCHEDULER THREAD",
    "predicate": "SENDS",
    "object": "THE TASK TO A WORKER"
  },
  {
    "subject": "THE SCHEDULER THREAD",
    "predicate": "TRANSFERS",
    "object": "PARAMETERS FOR THE CORRESPONDING MODEL TO THE GPU MEMORY"
  },
  {
    "subject": "the user",
    "predicate": "should register",
    "object": "the model in the scheduler"
  },
  {
    "subject": "the scheduler",
    "predicate": "notifies",
    "object": "the controller"
  },
  {
    "subject": "the controller",
    "predicate": "loads",
    "object": "the model from the disk to the CPU memory"
  },
  {
    "subject": "parameters",
    "predicate": "are transmitted to",
    "object": "the GPU memory"
  },
  {
    "subject": "parameters",
    "predicate": "are transmitted in",
    "object": "groups"
  },
  {
    "subject": "parameters",
    "predicate": "are transmitted in",
    "object": "a pipeline"
  },
  {
    "subject": "the controller",
    "predicate": "notifies",
    "object": "the worker to start computing the corresponding layers"
  },
  {
    "subject": "THE WORKER PROCESS",
    "predicate": "consists of",
    "object": "TWO THREADS"
  },
  {
    "subject": "THE TERMINATION THREAD",
    "predicate": "waits for",
    "object": "THE TERMINATION SIGNAL FROM THE CONTROLLER"
  },
  {
    "subject": "THE TERMINATION THREAD",
    "predicate": "notifies",
    "object": "THE MAIN THREAD"
  },
  {
    "subject": "the worker",
    "predicate": "requires",
    "object": "the user to register the model before starting a task"
  },
  {
    "subject": "the worker",
    "predicate": "can load",
    "object": "the models"
  },
  {
    "subject": "the worker",
    "predicate": "can add",
    "object": "the hooks to wait for parameter transmission or terminate on notification"
  },
  {
    "subject": "the worker",
    "predicate": "is similar to",
    "object": "the controller"
  },
  {
    "subject": "the worker",
    "predicate": "loads",
    "object": "the model structures"
  },
  {
    "subject": "the model structures",
    "predicate": "is",
    "object": "small"
  },
  {
    "subject": "the worker",
    "predicate": "does not load",
    "object": "the model parameters"
  },
  {
    "subject": "the parameters",
    "predicate": "are stored",
    "object": "only once in the memory daemon"
  },
  {
    "subject": "storing the parameters in the memory daemon",
    "predicate": "results in",
    "object": "minimal memory footprint"
  },
  {
    "subject": "different models",
    "predicate": "might use",
    "object": "the same GPU memory location"
  },
  {
    "subject": "the value",
    "predicate": "is not valid until",
    "object": "the controller transfers the corresponding parameters to these locations"
  },
  {
    "subject": "the worker",
    "predicate": "waits for",
    "object": "the scheduler to transfer required parameters for DNN models"
  },
  {
    "subject": "the worker",
    "predicate": "performs",
    "object": "inference or training"
  },
  {
    "subject": "ALL EXPERIMENTS",
    "predicate": "ARE CONDUCTED ON",
    "object": "AWS"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "two EC2 instance types"
  },
  {
    "subject": "ONE",
    "predicate": "is",
    "object": "P3.2XLARGE"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "is configured with",
    "object": "8 vCPUs (Intel Xeon E5-2686 v4)"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "has",
    "object": "1 GPU (NVIDIA V100 with 16 GB GPU memory)"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "has",
    "object": "PCIE 3.0 16"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "has",
    "object": "61 GB memory"
  },
  {
    "subject": "G4dn.2xlarge",
    "predicate": "is configured with",
    "object": "8 vCPUs (Intel Platinum 8259CL)"
  },
  {
    "subject": "G4dn.2xlarge",
    "predicate": "has",
    "object": "1 GPU (NVIDIA T4 with 16 GB GPU memory)"
  },
  {
    "subject": "G4dn.2xlarge",
    "predicate": "has",
    "object": "PCIe 3.0 8"
  },
  {
    "subject": "G4dn.2xlarge",
    "predicate": "has",
    "object": "32 GB memory"
  },
  {
    "subject": "The software environment",
    "predicate": "includes",
    "object": "PyTorch-1.3.0"
  },
  {
    "subject": "The software environment",
    "predicate": "includes",
    "object": "TorchVision-0.4.2"
  },
  {
    "subject": "The software environment",
    "predicate": "includes",
    "object": "SciPy-1.3.2"
  },
  {
    "subject": "The software environment",
    "predicate": "includes",
    "object": "CUDA-10.1"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "PyTorch with our plugins for all mechanisms in comparison for consistency"
  },
  {
    "subject": "PyTorch with our plugins",
    "predicate": "provides",
    "object": "better results for stop-and-start than native PyTorch from Python-PyPI used in Table 1"
  },
  {
    "subject": "RESNET152 17",
    "predicate": "is a",
    "object": "standard benchmark for evaluating DL systems"
  },
  {
    "subject": "INCEPTIONV3 22",
    "predicate": "is a",
    "object": "standard benchmark for evaluating DL systems"
  },
  {
    "subject": "BERTBASE 23",
    "predicate": "is a",
    "object": "standard benchmark for evaluating DL systems"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "representative configurations for each model"
  },
  {
    "subject": "THE EXPERIMENTS",
    "predicate": "cover",
    "object": "BOTH TRAINING AND INFERENCE"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "single-GPU inference and training tasks"
  },
  {
    "subject": "Training tasks",
    "predicate": "periodically checkpoint",
    "object": "their models to the host memory"
  },
  {
    "subject": "Training tasks",
    "predicate": "restart from",
    "object": "the latest checkpoint after preemption"
  },
  {
    "subject": "checkpointing frequency of training tasks",
    "predicate": "is set according to",
    "object": "scheduling cycle"
  },
  {
    "subject": "checkpointing frequency of training tasks",
    "predicate": "is set to minimize",
    "object": "checkpointing overhead"
  },
  {
    "subject": "default batch size for training",
    "predicate": "is",
    "object": "32"
  },
  {
    "subject": "default batch size for inference",
    "predicate": "is",
    "object": "8"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "throughput and latency as evaluation metrics"
  },
  {
    "subject": "we",
    "predicate": "measure",
    "object": "the end-to-end latency experienced by the client"
  },
  {
    "subject": "FIGURE 5",
    "predicate": "shows",
    "object": "the latency experienced by the client"
  },
  {
    "subject": "TABLE 3",
    "predicate": "shows",
    "object": "the total overhead"
  },
  {
    "subject": "each number",
    "predicate": "is reported with",
    "object": "the average of 100 runs"
  },
  {
    "subject": "Figure 6(B)",
    "predicate": "reports",
    "object": "minimum and maximum latencies using the error bar"
  },
  {
    "subject": "latency of the RST batch and those of later batches in one scheduling cycle",
    "predicate": "can differ",
    "object": "significantly"
  },
  {
    "subject": "difference in latency",
    "predicate": "is due to",
    "object": "switching overhead"
  },
  {
    "subject": "a client",
    "predicate": "sends",
    "object": "an inference task to a GPU server"
  },
  {
    "subject": "the GPU server",
    "predicate": "preempts",
    "object": "the training task"
  },
  {
    "subject": "the GPU server",
    "predicate": "executes",
    "object": "the inference task"
  },
  {
    "subject": "the GPU server",
    "predicate": "sends",
    "object": "a reply back to the client"
  },
  {
    "subject": "the process with the required model",
    "predicate": "is loaded in",
    "object": "the GPU"
  },
  {
    "subject": "THIS SOLUTION",
    "predicate": "provides",
    "object": "THE LOWER BOUND"
  },
  {
    "subject": "THE LOWER BOUND",
    "predicate": "is",
    "object": "THE LOWEST LATENCY WE CAN ACHIEVE FOR AN INFERENCE TASK"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "is",
    "object": "NVIDIA MPS"
  },
  {
    "subject": "We",
    "predicate": "initialize",
    "object": "separate processes in advance"
  },
  {
    "subject": "CUDA UNIED MEMORY",
    "predicate": "is used for",
    "object": "MEMORY SWAPPING"
  },
  {
    "subject": "CUDA",
    "predicate": "has feature",
    "object": "Unified Memory"
  },
  {
    "subject": "URL",
    "predicate": "is",
    "object": "https://devblogs.nvidia.com/unified-memory-cuda-beginners"
  },
  {
    "subject": "THE PROPERTIES",
    "predicate": "are described in",
    "object": "4"
  },
  {
    "subject": "LATENCY (MS)",
    "predicate": "measured for",
    "object": "READY MODEL PIPESWITCH MPS STOP-AND-START RESNET152 INCEPTIONV3 BERTBASE"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "has hardware",
    "object": "NVIDIA V100, PCIE 3.0 16"
  },
  {
    "subject": "LATENCY (MS)",
    "predicate": "range",
    "object": "5000 to 10000"
  },
  {
    "subject": "MODEL",
    "predicate": "includes",
    "object": "PIPESWITCH, MPS, STOP-AND-START, RESNET152, INCEPTIONV3, BERTBASE"
  },
  {
    "subject": "INSTANCE TYPE",
    "predicate": "is",
    "object": "G4DN.2XLARGE"
  },
  {
    "subject": "GPU",
    "predicate": "is",
    "object": "NVIDIA T4"
  },
  {
    "subject": "PCIe VERSION",
    "predicate": "is",
    "object": "3.0"
  },
  {
    "subject": "GPU COUNT",
    "predicate": "is",
    "object": "8"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "has GPU",
    "object": "NVIDIA V100"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "has PCIE version",
    "object": "3.0 16"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "has GPU",
    "object": "NVIDIA T4"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "has PCIE version",
    "object": "3.0 8"
  },
  {
    "subject": "STOP-AND-START on P3.2XLARGE with RESNET152",
    "predicate": "latency",
    "object": "6475.40 MS"
  },
  {
    "subject": "STOP-AND-START on P3.2XLARGE with INCEPTIONV3",
    "predicate": "latency",
    "object": "7536.07 MS"
  },
  {
    "subject": "STOP-AND-START on P3.2XLARGE with BERTBASE",
    "predicate": "latency",
    "object": "6371.32 MS"
  },
  {
    "subject": "STOP-AND-START on G4DN.2XLARGE with RESNET152",
    "predicate": "latency",
    "object": "5486.74 MS"
  },
  {
    "subject": "STOP-AND-START on G4DN.2XLARGE with INCEPTIONV3",
    "predicate": "latency",
    "object": "6558.76 MS"
  },
  {
    "subject": "STOP-AND-START on G4DN.2XLARGE with BERTBASE",
    "predicate": "latency",
    "object": "5355.95 MS"
  },
  {
    "subject": "NVIDIA MPS on P3.2XLARGE with RESNET152",
    "predicate": "latency",
    "object": "307.02 MS"
  },
  {
    "subject": "NVIDIA MPS on P3.2XLARGE with INCEPTIONV3",
    "predicate": "latency",
    "object": "232.25 MS"
  },
  {
    "subject": "NVIDIA MPS on P3.2XLARGE with BERTBASE",
    "predicate": "latency",
    "object": "204.52 MS"
  },
  {
    "subject": "NVIDIA MPS on G4DN.2XLARGE with RESNET152",
    "predicate": "latency",
    "object": "259.20 MS"
  },
  {
    "subject": "NVIDIA MPS on G4DN.2XLARGE with INCEPTIONV3",
    "predicate": "latency",
    "object": "193.05 MS"
  },
  {
    "subject": "NVIDIA MPS on G4DN.2XLARGE with BERTBASE",
    "predicate": "latency",
    "object": "338.25 MS"
  },
  {
    "subject": "PIPESWITCH on P3.2XLARGE with RESNET152",
    "predicate": "latency",
    "object": "6.01 MS"
  },
  {
    "subject": "PIPESWITCH on P3.2XLARGE with INCEPTIONV3",
    "predicate": "latency",
    "object": "5.40 MS"
  },
  {
    "subject": "PIPESWITCH on P3.2XLARGE with BERTBASE",
    "predicate": "latency",
    "object": "10.27 MS"
  },
  {
    "subject": "PIPESWITCH on G4DN.2XLARGE with RESNET152",
    "predicate": "latency",
    "object": "5.57 MS"
  },
  {
    "subject": "PIPESWITCH on G4DN.2XLARGE with INCEPTIONV3",
    "predicate": "latency",
    "object": "7.66 MS"
  },
  {
    "subject": "PIPESWITCH on G4DN.2XLARGE with BERTBASE",
    "predicate": "latency",
    "object": "34.56 MS"
  },
  {
    "subject": "RESNET152",
    "predicate": "is mentioned in",
    "object": "the text"
  },
  {
    "subject": "INCEPTIONV3",
    "predicate": "is mentioned in",
    "object": "the text"
  },
  {
    "subject": "BERTBASE",
    "predicate": "is mentioned in",
    "object": "the text"
  },
  {
    "subject": "Latency",
    "predicate": "is measured in",
    "object": "milliseconds (MS)"
  },
  {
    "subject": "P3.2xlarge",
    "predicate": "uses",
    "object": "NVIDIA V100"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "connects via",
    "object": "PCIE 3.0 16"
  },
  {
    "subject": "Optimization",
    "predicate": "types include",
    "object": "Pipeswitch per-layer pipeline grouped transmission no optimization"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "is a type of",
    "object": "pipeline"
  },
  {
    "subject": "PER-LAYER PIPELINE",
    "predicate": "is a type of",
    "object": "pipeline"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "is a type of",
    "object": "pipeline"
  },
  {
    "subject": "NO OPTIMIZATION",
    "predicate": "is a condition of",
    "object": "pipeline"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "uses interface",
    "object": "PCIE 3.0 x8"
  },
  {
    "subject": "510 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION",
    "predicate": "organized by",
    "object": "USENIX ASSOCIATION"
  },
  {
    "subject": "RESNET152",
    "predicate": "measured by",
    "object": "LATENCY (MS)"
  },
  {
    "subject": "INCEPTIONV3",
    "predicate": "measured by",
    "object": "LATENCY (MS)"
  },
  {
    "subject": "BERTBASE",
    "predicate": "measured by",
    "object": "LATENCY (MS)"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "has",
    "object": "NO MEMORY MANAGEMENT"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "has",
    "object": "NO IPC OPTIMIZATION"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "has",
    "object": "NO PIN MEMORY"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "uses",
    "object": "CUDA UNIFIED MEMORY"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "equipped with",
    "object": "NVIDIA V100"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "uses",
    "object": "PCIE 3.0 16"
  },
  {
    "subject": "Latency",
    "predicate": "is measured in",
    "object": "milliseconds"
  },
  {
    "subject": "CUDA",
    "predicate": "uses",
    "object": "unified memory"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "has PCI version",
    "object": "3.0"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "has PCIe lanes",
    "object": "8"
  },
  {
    "subject": "P3.2XLARGE",
    "predicate": "has GPU",
    "object": "NVIDIA V100, PCIE 3.0 16"
  },
  {
    "subject": "LATENCY (MS)",
    "predicate": "range",
    "object": "5000 to 7500"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "has",
    "object": "ONE PROCESS"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "has",
    "object": "TWO PROCESSES"
  },
  {
    "subject": "Models",
    "predicate": "include",
    "object": "RESNET152, INCEPTIONV3, BERTBASE"
  },
  {
    "subject": "G4DN.2XLARGE",
    "predicate": "contains",
    "object": "NVIDIA T4, PCIE 3.0 x8"
  },
  {
    "subject": "RESNET152 on P3.2XLARGE",
    "predicate": "has startup overhead",
    "object": "3.62 MS"
  },
  {
    "subject": "INCEPTIONV3 on P3.2XLARGE",
    "predicate": "has startup overhead",
    "object": "4.82 MS"
  },
  {
    "subject": "BERTBASE on P3.2XLARGE",
    "predicate": "has startup overhead",
    "object": "3.62 MS"
  },
  {
    "subject": "RESNET152 on G4DN.2XLARGE",
    "predicate": "has startup overhead",
    "object": "2.53 MS"
  },
  {
    "subject": "INCEPTIONV3 on G4DN.2XLARGE",
    "predicate": "has startup overhead",
    "object": "5.49 MS"
  },
  {
    "subject": "BERTBASE on G4DN.2XLARGE",
    "predicate": "has startup overhead",
    "object": "6.57 MS"
  },
  {
    "subject": "Table 4",
    "predicate": "describes",
    "object": "the startup overhead for PipeSwitch to start computing the RST layer"
  },
  {
    "subject": "Table 4",
    "predicate": "shows",
    "object": "task startup overhead for PipeSwitch"
  },
  {
    "subject": "task startup overhead for PipeSwitch",
    "predicate": "is",
    "object": "the difference between the time for ResNet152, InceptionV3, BERTBase"
  },
  {
    "subject": "ResNet152",
    "predicate": "has",
    "object": "464 layers"
  },
  {
    "subject": "InceptionV3",
    "predicate": "has",
    "object": "189 layers"
  },
  {
    "subject": "BERTBase",
    "predicate": "has",
    "object": "139 layers"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "has task startup overhead",
    "object": "1.33 s, 0.18 s, 0.34 s"
  },
  {
    "subject": "Only Pruning 1",
    "predicate": "has task startup overhead",
    "object": "2.09 s, 0.30 s, 0.88 s"
  },
  {
    "subject": "Only Pruning 2",
    "predicate": "has task startup overhead",
    "object": "3.44 h, 5.07 s, 24 h"
  },
  {
    "subject": "No Pruning",
    "predicate": "has task startup overhead",
    "object": "24 h, 24 h, 24 h"
  },
  {
    "subject": "Table 5",
    "predicate": "shows",
    "object": "effectiveness of two pruning techniques"
  },
  {
    "subject": "SALUS 7",
    "predicate": "is not directly comparable",
    "object": "because it requires the models to be preloaded to the GPU"
  },
  {
    "subject": "SALUS 7",
    "predicate": "has",
    "object": "several limitations described in 2.2"
  },
  {
    "subject": "ITS PERFORMANCE",
    "predicate": "is similar to",
    "object": "THE READY MODEL"
  },
  {
    "subject": "THE READY MODEL",
    "predicate": "is",
    "object": "preloaded"
  },
  {
    "subject": "ITS PERFORMANCE",
    "predicate": "is similar to",
    "object": "NVIDIA MPS"
  },
  {
    "subject": "THE MODEL",
    "predicate": "is in",
    "object": "THE HOST MEMORY"
  },
  {
    "subject": "THE TOTAL OVERHEAD",
    "predicate": "is",
    "object": "the difference between the latency of a mechanism and that of the ready model"
  },
  {
    "subject": "STOP-AND-START",
    "predicate": "performs",
    "object": "the worst"
  },
  {
    "subject": "the worst",
    "predicate": "takes",
    "object": "several seconds"
  },
  {
    "subject": "The main source of the overhead",
    "predicate": "is",
    "object": "CUDA context initialization and rst-time library loading operations in PyTorch"
  },
  {
    "subject": "ANOTHER SOURCE",
    "predicate": "is",
    "object": "GPU MEMORY SWAPPING"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "performs",
    "object": "the best"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "is close to",
    "object": "the lower bound"
  },
  {
    "subject": "computing BERT on T4",
    "predicate": "takes",
    "object": "120ms"
  },
  {
    "subject": "relative overhead",
    "predicate": "is",
    "object": "acceptable"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "starts",
    "object": "computing the RST layer"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "starts",
    "object": "computing for the ready model"
  },
  {
    "subject": "The startup overhead of PipeSwitch",
    "predicate": "is",
    "object": "only a few milliseconds"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "incurs",
    "object": "only a few milliseconds overhead for task switching"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "achieves",
    "object": "low latency close to the lower bound"
  },
  {
    "subject": "we",
    "predicate": "compare",
    "object": "throughput and end-to-end latency of different mechanisms under different scheduling cycles"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "ResNet152 for both training and inference"
  },
  {
    "subject": "ResNet152",
    "predicate": "runs on",
    "object": "eight p3.2xlarge instances"
  },
  {
    "subject": "We",
    "predicate": "switch between",
    "object": "training and inference after each scheduling cycle"
  },
  {
    "subject": "FIGURE 6(A)",
    "predicate": "shows",
    "object": "the inference throughput"
  },
  {
    "subject": "THE DASHED LINE",
    "predicate": "is",
    "object": "THE UPPER BOUND"
  },
  {
    "subject": "THE UPPER BOUND",
    "predicate": "is",
    "object": "THE THROUGHPUT OF THE READY MODEL ASSUMING NO TASK SWITCHING"
  },
  {
    "subject": "THE DASHED LINE",
    "predicate": "is",
    "object": "THE LOWER BOUND"
  },
  {
    "subject": "THE LOWER BOUND",
    "predicate": "is",
    "object": "THE AVERAGE LATENCY OF THE READY MODEL"
  },
  {
    "subject": "THE AVERAGE LATENCY OF THE READY MODEL",
    "predicate": "assumes",
    "object": "NO TASK SWITCHING"
  },
  {
    "subject": "throughput of stop-and-start",
    "predicate": "is nearly",
    "object": "zero for scheduling cycles smaller than 10 s"
  },
  {
    "subject": "stop-and-start",
    "predicate": "takes",
    "object": "several seconds for task switching"
  },
  {
    "subject": "MPS",
    "predicate": "keeps throughput",
    "object": "around 100 batches per second"
  },
  {
    "subject": "GPU utilization",
    "predicate": "is defined as",
    "object": "the ratio to the upper bound"
  },
  {
    "subject": "FIGURE 6(B)",
    "predicate": "shows",
    "object": "the average latency of the inference tasks"
  },
  {
    "subject": "THE ERROR BAR",
    "predicate": "indicates",
    "object": "THE MINIMUM AND MAXIMUM LATENCY"
  },
  {
    "subject": "STOP- AND-START",
    "predicate": "has",
    "object": "poor latency"
  },
  {
    "subject": "RST BATCH",
    "predicate": "has",
    "object": "several seconds overhead"
  },
  {
    "subject": "poor latency",
    "predicate": "is because of",
    "object": "several seconds overhead of RST BATCH"
  },
  {
    "subject": "MPS",
    "predicate": "has",
    "object": "about 80 ms average latency"
  },
  {
    "subject": "MPS",
    "predicate": "has",
    "object": "several hundred milliseconds latency for the RST batch"
  },
  {
    "subject": "LATENCY (MS)",
    "predicate": "range",
    "object": "7500 to 10000"
  },
  {
    "subject": "PIPESWITCH MPS",
    "predicate": "has latency measurements",
    "object": "1S 2S 5S 10S 30S"
  },
  {
    "subject": "LOWER BOUND",
    "predicate": "is measured in",
    "object": "B"
  },
  {
    "subject": "LATENCY",
    "predicate": "has lower bound",
    "object": "0 200 400"
  },
  {
    "subject": "Throughput and latency",
    "predicate": "are measured under",
    "object": "different scheduling cycles"
  },
  {
    "subject": "Throughput and latency",
    "predicate": "are for",
    "object": "ResNet"
  },
  {
    "subject": "ResNet",
    "predicate": "runs on",
    "object": "p3.2xlarge"
  },
  {
    "subject": "Computation",
    "predicate": "starts",
    "object": "once parameters are transmitted"
  },
  {
    "subject": "FIGURE 8",
    "predicate": "shows",
    "object": "the total time measured by the client"
  },
  {
    "subject": "NO OPTIMIZATION",
    "predicate": "performs",
    "object": "the worst in most cases"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "improves",
    "object": "NO OPTIMIZATION"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "combines",
    "object": "THE LAYERS OF THE MODEL INTO ONE BIG TENSOR"
  },
  {
    "subject": "GROUPED TRANSMISSION",
    "predicate": "transmits",
    "object": "ONE BIG TENSOR IN ONE GROUP"
  },
  {
    "subject": "PER-LAYER PIPELINE",
    "predicate": "overlaps",
    "object": "transmission and computation"
  },
  {
    "subject": "overlaps",
    "predicate": "occur at the granularity of",
    "object": "layer"
  },
  {
    "subject": "PCIE overhead and synchronization overhead",
    "predicate": "occur",
    "object": "for every layer"
  },
  {
    "subject": "models with many layers but relatively light computation such as ResNet152 and Inception",
    "predicate": "can perform",
    "object": "worse than grouped transmission"
  },
  {
    "subject": "models with many layers but relatively light computation such as ResNet152 and Inception",
    "predicate": "can perform",
    "object": "sometimes even worse than no pipeline"
  },
  {
    "subject": "this reduction",
    "predicate": "is",
    "object": "significant"
  },
  {
    "subject": "this reduction",
    "predicate": "is evaluated when",
    "object": "the optimizations on memory management and worker switching have already been applied"
  },
  {
    "subject": "we",
    "predicate": "would like to emphasize",
    "object": "to meet strict SLOs it is important to reduce all overheads for task switching, not only the most significant one"
  },
  {
    "subject": "TABLE 5",
    "predicate": "shows",
    "object": "the running time of Algorithm 1"
  },
  {
    "subject": "TABLE 5",
    "predicate": "shows",
    "object": "the effects of the two pruning techniques mentioned in 4.2"
  },
  {
    "subject": "the number of layers",
    "predicate": "includes",
    "object": "both weighted and unweighted layers"
  },
  {
    "subject": "both weighted and unweighted layers",
    "predicate": "contribute to",
    "object": "the computation time"
  },
  {
    "subject": "we",
    "predicate": "measure",
    "object": "the parameter size and running time for each layer in advance"
  },
  {
    "subject": "ALGORITHM 1",
    "predicate": "takes",
    "object": "only several seconds to compute an optimal grouping strategy"
  },
  {
    "subject": "RESNET152",
    "predicate": "has",
    "object": "hundreds of layers"
  },
  {
    "subject": "no pruning",
    "predicate": "does not finish",
    "object": "for all three models after running for 24 hours"
  },
  {
    "subject": "UNIED MEMORY MANAGEMENT",
    "predicate": "is used to evaluate",
    "object": "the effectiveness of UNIED MEMORY MANAGEMENT"
  },
  {
    "subject": "Memory management",
    "predicate": "is",
    "object": "not unified"
  },
  {
    "subject": "We",
    "predicate": "compare",
    "object": "the following VE mechanisms discussed in 4.3"
  },
  {
    "subject": "IPC",
    "predicate": "has",
    "object": "no optimization"
  },
  {
    "subject": "the pages of the memory daemon",
    "predicate": "are not pinned to",
    "object": "the main memory"
  },
  {
    "subject": "this experiment",
    "predicate": "demonstrates",
    "object": "all the optimizations on memory management are effective"
  },
  {
    "subject": "Unified Memory Management Mechanism",
    "predicate": "is used by",
    "object": "PipeSwitch"
  },
  {
    "subject": "IPC optimization",
    "predicate": "is",
    "object": "important"
  },
  {
    "subject": "IPC optimization",
    "predicate": "reduces",
    "object": "latency by 1648 ms"
  },
  {
    "subject": "Pinning the pages to the host memory",
    "predicate": "can reduce",
    "object": "the latency with a few milliseconds"
  },
  {
    "subject": "two processes",
    "predicate": "perform",
    "object": "the worst"
  },
  {
    "subject": "the worst",
    "predicate": "stop",
    "object": "the training task"
  },
  {
    "subject": "the worst",
    "predicate": "initialize",
    "object": "a new process for the new task"
  },
  {
    "subject": "THE NEW PROCESS",
    "predicate": "needs to create",
    "object": "A NEW CUDA ENVIRONMENT"
  },
  {
    "subject": "A NEW CUDA ENVIRONMENT",
    "predicate": "dominates",
    "object": "THE TOTAL TIME"
  },
  {
    "subject": "ONE PROCESS",
    "predicate": "reuses",
    "object": "THE CUDA ENVIRONMENT"
  },
  {
    "subject": "ONE PROCESS",
    "predicate": "pays",
    "object": "THE OVERHEAD TO CLEAN THE ENVIRONMENT"
  },
  {
    "subject": "Several algorithms and systems",
    "predicate": "have been designed for",
    "object": "executing and scheduling deep learning tasks on clusters"
  },
  {
    "subject": "Deep learning tasks on clusters",
    "predicate": "include",
    "object": "both training and inference tasks"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "focuses on",
    "object": "how to realize a scheduling decision"
  },
  {
    "subject": "PIPESWITCH",
    "predicate": "enables",
    "object": "the scheduler to change the resource allocation more often with millisecond-scale task switching"
  },
  {
    "subject": "many techniques and systems",
    "predicate": "have been proposed to",
    "object": "optimize communication and improve distributed training"
  },
  {
    "subject": "THE MOST RELEVANT ONES",
    "predicate": "are",
    "object": "PIPEDREAM 8"
  },
  {
    "subject": "THE MOST RELEVANT ONES",
    "predicate": "are",
    "object": "BYTESCHEDULER 9"
  },
  {
    "subject": "THE MOST RELEVANT ONES",
    "predicate": "are",
    "object": "POSEIDON 40"
  },
  {
    "subject": "VDNN 43",
    "predicate": "have",
    "object": "GPU memory management module"
  },
  {
    "subject": "SWAPADVISOR 44",
    "predicate": "have",
    "object": "GPU memory management module"
  },
  {
    "subject": "VDNN 43 and SWAPADVISOR 44",
    "predicate": "focus on",
    "object": "memory management for a single training task of large models"
  },
  {
    "subject": "memory management for a single training task of large models",
    "predicate": "are not directly comparable to",
    "object": "PIPESWITCH"
  },
  {
    "subject": "CLUSTER MANAGERS 4548",
    "predicate": "allocate",
    "object": "GPUS to VMS or CONTAINERS at device granularity"
  },
  {
    "subject": "Several solutions",
    "predicate": "have been proposed",
    "object": "to share a GPU at application granularity using techniques like library interception"
  },
  {
    "subject": "Efforts on GPU optimization",
    "predicate": "aim to improve",
    "object": "the performance of running a single task"
  },
  {
    "subject": "Efforts on GPU optimization",
    "predicate": "include",
    "object": "tensor fusion"
  },
  {
    "subject": "Efforts on GPU optimization",
    "predicate": "include",
    "object": "kernel-level concurrency and scheduling"
  },
  {
    "subject": "We",
    "predicate": "thank",
    "object": "our shepherd Madan Musu-Vathi"
  },
  {
    "subject": "We",
    "predicate": "thank",
    "object": "the anonymous reviewers"
  },
  {
    "subject": "Madan Musu-Vathi and the anonymous reviewers",
    "predicate": "provide",
    "object": "valuable feedback"
  },
  {
    "subject": "ZHIHAO BAI, ZHEN ZHANG AND XIN JIN",
    "predicate": "were supported in part by",
    "object": "an AWS Machine Learning Research Award"
  },
  {
    "subject": "A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes",
    "predicate": "authored",
    "object": "Large-scale cluster management at Google with Borg"
  },
  {
    "subject": "Large-scale cluster management at Google with Borg",
    "predicate": "published in",
    "object": "EuroSys"
  },
  {
    "subject": "Large-scale cluster management at Google with Borg",
    "predicate": "published in year",
    "object": "2015"
  },
  {
    "subject": "NEXUS",
    "predicate": "is",
    "object": "a GPU cluster engine"
  },
  {
    "subject": "NEXUS",
    "predicate": "purpose",
    "object": "accelerating DNN-based video analysis"
  },
  {
    "subject": "3 H. SHEN, L. CHEN, Y. JIN, L. ZHAO, B. KONG, M. PHILIPOSE, A. KRISHNAMURTHY, and R. SUNDARAM",
    "predicate": "authored",
    "object": "NEXUS: A GPU cluster engine for accelerating DNN-based video analysis"
  },
  {
    "subject": "NEXUS paper",
    "predicate": "published in",
    "object": "ACM SOSP"
  },
  {
    "subject": "NEXUS paper",
    "predicate": "published in year",
    "object": "2019"
  },
  {
    "subject": "FRIED, J. BEHRENS, A. BELAY, AND H. BALAKRISHNAN",
    "predicate": "authored",
    "object": "SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS"
  },
  {
    "subject": "SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS",
    "predicate": "published in",
    "object": "USENIX NSDI"
  },
  {
    "subject": "SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS",
    "predicate": "published in year",
    "object": "2019"
  },
  {
    "subject": "7 P. YU AND M. CHOWDHURY",
    "predicate": "authored",
    "object": "SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS"
  },
  {
    "subject": "SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS",
    "predicate": "published in",
    "object": "CONFERENCE ON MACHINE LEARNING AND SYSTEMS"
  },
  {
    "subject": "SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS",
    "predicate": "published in year",
    "object": "2020"
  },
  {
    "subject": "D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia",
    "predicate": "authored",
    "object": "Pipedream: Generalized Pipeline Parallelism for DNN Training"
  },
  {
    "subject": "Pipedream: Generalized Pipeline Parallelism for DNN Training",
    "predicate": "published_in",
    "object": "ACM SOSP"
  },
  {
    "subject": "Pipedream: Generalized Pipeline Parallelism for DNN Training",
    "predicate": "published_year",
    "object": "2019"
  },
  {
    "subject": "Y. PENG, Y. ZHU, Y. CHEN, Y. BAO, B. YI, C. LAN, C. WU, AND C. GUO",
    "predicate": "authored",
    "object": "A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION"
  },
  {
    "subject": "A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION",
    "predicate": "was published in",
    "object": "ACM SOSP"
  },
  {
    "subject": "A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION",
    "predicate": "was published in year",
    "object": "2019"
  },
  {
    "subject": "TIRESIAS",
    "predicate": "is",
    "object": "a GPU cluster manager for distributed deep learning"
  },
  {
    "subject": "TIRESIAS",
    "predicate": "was presented in",
    "object": "USENIX NSDI"
  },
  {
    "subject": "TIRESIAS",
    "predicate": "was published in year",
    "object": "2019"
  },
  {
    "subject": "TIRESIAS",
    "predicate": "was authored by",
    "object": "J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo"
  },
  {
    "subject": "POSEIDON",
    "predicate": "is",
    "object": "an efficient communication architecture for distributed deep learning on GPU clusters"
  },
  {
    "subject": "POSEIDON",
    "predicate": "was presented in",
    "object": "USENIX ATC, 2017"
  },
  {
    "subject": "H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie, and E. P. Xing",
    "predicate": "are authors of",
    "object": "POSEIDON"
  },
  {
    "subject": "AMAZON WEB SERVICES",
    "predicate": "is",
    "object": "12"
  },
  {
    "subject": "HTTPS",
    "predicate": "is associated with",
    "object": "AWS.AMAZON.COM"
  },
  {
    "subject": "MICROSOFT AZURE",
    "predicate": "is",
    "object": "a cloud computing service"
  },
  {
    "subject": "GOOGLE CLOUD PLATFORM",
    "predicate": "is",
    "object": "14"
  },
  {
    "subject": "HTTPS",
    "predicate": "is associated with",
    "object": "CLOUD.GOOGLE.COM"
  },
  {
    "subject": "HOROVOD",
    "predicate": "is",
    "object": "fast and easy distributed deep learning in TensorFlow"
  },
  {
    "subject": "15 A. Sergeev and M. Del Balso",
    "predicate": "are authors of",
    "object": "HOROVOD: Fast and Easy Distributed Deep Learning in TensorFlow"
  },
  {
    "subject": "HOROVOD paper",
    "predicate": "was published in",
    "object": "arXiv preprint arXiv:1802.05799"
  },
  {
    "subject": "HOROVOD paper",
    "predicate": "was published in year",
    "object": "2018"
  },
  {
    "subject": "SU",
    "predicate": "authored",
    "object": "SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER"
  },
  {
    "subject": "SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER",
    "predicate": "published_in",
    "object": "USENIX OSDI"
  },
  {
    "subject": "SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER",
    "predicate": "published_year",
    "object": "2014"
  },
  {
    "subject": "DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION",
    "predicate": "author",
    "object": "SUN"
  },
  {
    "subject": "DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION",
    "predicate": "published in",
    "object": "IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"
  },
  {
    "subject": "DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION",
    "predicate": "publication year",
    "object": "2016"
  },
  {
    "subject": "PHILLY",
    "predicate": "has quantity",
    "object": "20 TRACES"
  },
  {
    "subject": "21",
    "predicate": "is",
    "object": "PYTORCH"
  },
  {
    "subject": "C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna",
    "predicate": "authored",
    "object": "Rethinking the Inception Architecture for Computer Vision"
  },
  {
    "subject": "Rethinking the Inception Architecture for Computer Vision",
    "predicate": "was presented in",
    "object": "IEEE Conference on Computer Vision and Pattern Recognition"
  },
  {
    "subject": "Rethinking the Inception Architecture for Computer Vision",
    "predicate": "was published in year",
    "object": "2016"
  },
  {
    "subject": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
    "predicate": "authored",
    "object": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
  },
  {
    "subject": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "predicate": "was published in",
    "object": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"
  },
  {
    "subject": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "predicate": "was published in year",
    "object": "2019"
  },
  {
    "subject": "GANDIVA",
    "predicate": "is",
    "object": "introspective cluster scheduling for deep learning"
  },
  {
    "subject": "GANDIVA",
    "predicate": "was presented in",
    "object": "USENIX OSDI, 2018"
  },
  {
    "subject": "24 W. XIAO, R. BHARDWAJ, R. RAMJEE, M. SIVATHANU, N. KWATRA, Z. HAN, P. PATEL, X. PENG, H. ZHAO, Q. ZHANG, ET AL.",
    "predicate": "are authors of",
    "object": "GANDIVA"
  },
  {
    "subject": "25",
    "predicate": "is related to",
    "object": "TENSORFLOW"
  },
  {
    "subject": "54",
    "predicate": "is related to",
    "object": "TENSORFLOW XLA"
  },
  {
    "subject": "XLA",
    "predicate": "is associated with",
    "object": "https://www.tensorflow.org"
  },
  {
    "subject": "26",
    "predicate": "is",
    "object": "MXNET"
  },
  {
    "subject": "M. J. Freedman",
    "predicate": "authored",
    "object": "SLAQ: Quality-Driven Scheduling for Distributed Machine Learning"
  },
  {
    "subject": "SLAQ: Quality-Driven Scheduling for Distributed Machine Learning",
    "predicate": "published in",
    "object": "ACM Symposium on Cloud Computing"
  },
  {
    "subject": "SLAQ: Quality-Driven Scheduling for Distributed Machine Learning",
    "predicate": "published in year",
    "object": "2017"
  },
  {
    "subject": "OPTIMUS",
    "predicate": "is",
    "object": "an efficient dynamic resource scheduler for deep learning clusters"
  },
  {
    "subject": "OPTIMUS",
    "predicate": "was presented in",
    "object": "EuroSys"
  },
  {
    "subject": "OPTIMUS",
    "predicate": "was published in year",
    "object": "2018"
  },
  {
    "subject": "Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo",
    "predicate": "are authors of",
    "object": "OPTIMUS"
  },
  {
    "subject": "THEMIS",
    "predicate": "is",
    "object": "fair and efficient GPU cluster scheduling"
  },
  {
    "subject": "THEMIS",
    "predicate": "was presented in",
    "object": "USENIX NSDI"
  },
  {
    "subject": "THEMIS",
    "predicate": "was presented in year",
    "object": "2020"
  },
  {
    "subject": "THEMIS",
    "predicate": "was authored by",
    "object": "K. Mahajan, A. Balasubramanian, A. Singhvi, S. Venkataraman, A. Akella, A. Phanishayee, and S. Chawla"
  },
  {
    "subject": "R. LIAW, R. BHARDWAJ, L. DUNLAP, Y. ZOU, J. E. GONZALEZ, I. STOICA, AND A. TUMANOV",
    "predicate": "authored",
    "object": "HYPERSCHED: DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE"
  },
  {
    "subject": "HYPERSCHED: DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE",
    "predicate": "published in",
    "object": "ACM SYMPOSIUM ON CLOUD COMPUTING"
  },
  {
    "subject": "HYPERSCHED: DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE",
    "predicate": "published in year",
    "object": "2019"
  },
  {
    "subject": "CHET",
    "predicate": "is",
    "object": "an optimizing compiler for fully-homomorphic neural-network inferencing"
  },
  {
    "subject": "CHET",
    "predicate": "was presented in",
    "object": "ACM Conference on Programming Language Design and Implementation"
  },
  {
    "subject": "CHET",
    "predicate": "was presented in year",
    "object": "2019"
  },
  {
    "subject": "R. Dathathri, O. Saarikivi, H. Chen, K. Laine, K. Lauter, S. Maleki, M. Musuvathi, and T. Mytkowicz",
    "predicate": "are authors of",
    "object": "CHET"
  },
  {
    "subject": "TVM",
    "predicate": "is",
    "object": "an automated end-to-end optimizing compiler for deep learning"
  },
  {
    "subject": "TVM",
    "predicate": "was presented in",
    "object": "USENIX OSDI"
  },
  {
    "subject": "TVM",
    "predicate": "was presented in year",
    "object": "2018"
  },
  {
    "subject": "T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, et al.",
    "predicate": "are authors of",
    "object": "TVM: An automated end-to-end optimizing compiler for deep learning"
  },
  {
    "subject": "GPIPE",
    "predicate": "is described in",
    "object": "33 Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al."
  },
  {
    "subject": "GPIPE",
    "predicate": "is about",
    "object": "efficient training of giant neural networks using pipeline parallelism"
  },
  {
    "subject": "GPIPE",
    "predicate": "was presented in",
    "object": "Advances in Neural Information Processing Systems"
  },
  {
    "subject": "GPIPE",
    "predicate": "was published in",
    "object": "2019"
  },
  {
    "subject": "BLINK",
    "predicate": "is described as",
    "object": "fast and generic collectives for distributed ML"
  },
  {
    "subject": "BLINK",
    "predicate": "was presented in",
    "object": "Conference on Machine Learning and Systems"
  },
  {
    "subject": "BLINK",
    "predicate": "was presented in year",
    "object": "2020"
  },
  {
    "subject": "Authors",
    "predicate": "include",
    "object": "G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica"
  },
  {
    "subject": "NCCL",
    "predicate": "is",
    "object": "NVIDIA Collective Communications Library"
  },
  {
    "subject": "NVIDIA",
    "predicate": "has website",
    "object": "developer.nvidia.com/nccl"
  },
  {
    "subject": "36 J. LIU, J. WU, AND D. K. PANDA",
    "predicate": "authored",
    "object": "HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION OVER INNIBAND"
  },
  {
    "subject": "Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing",
    "predicate": "authored",
    "object": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server"
  },
  {
    "subject": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server",
    "predicate": "published in",
    "object": "Advances in Neural Information Processing Systems"
  },
  {
    "subject": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server",
    "predicate": "published in year",
    "object": "2013"
  },
  {
    "subject": "A. AWAN, C.-H. CHU, H. SUBRAMONI, AND D. K. PANDA",
    "predicate": "authored",
    "object": "OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?"
  },
  {
    "subject": "OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?",
    "predicate": "published_in",
    "object": "PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING"
  },
  {
    "subject": "OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?",
    "predicate": "published_year",
    "object": "2018"
  },
  {
    "subject": "A. Vishnu, C. Siegel, T. Warfel, and V. Amatya",
    "predicate": "authored",
    "object": "GossipGrad: Scalable Deep Learning Using Gossip Communication Based Asynchronous Gradient Descent"
  },
  {
    "subject": "41 Z. ZHANG, C. CHANG, H. LIN, Y. WANG, R. ARORA, AND X. JIN",
    "predicate": "authored",
    "object": "IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?"
  },
  {
    "subject": "IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?",
    "predicate": "published in",
    "object": "ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)"
  },
  {
    "subject": "IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?",
    "predicate": "published in month and year",
    "object": "AUGUST 2020"
  },
  {
    "subject": "42 Y. CHEN, Z. LIU, B. REN, AND X. JIN",
    "predicate": "authored",
    "object": "ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS"
  },
  {
    "subject": "ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS",
    "predicate": "published in",
    "object": "INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)"
  },
  {
    "subject": "INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)",
    "predicate": "date",
    "object": "JULY 2020"
  },
  {
    "subject": "VDNN",
    "predicate": "is",
    "object": "Virtualized Deep Neural Networks for scalable, memory-efficient neural network design"
  },
  {
    "subject": "VDNN",
    "predicate": "was presented in",
    "object": "2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)"
  },
  {
    "subject": "VDNN",
    "predicate": "was published in year",
    "object": "2016"
  },
  {
    "subject": "Authors",
    "predicate": "include",
    "object": "M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler"
  },
  {
    "subject": "SWAPADVISOR",
    "predicate": "is authored by",
    "object": "C.-C. Huang, G. Jin, and J. Li"
  },
  {
    "subject": "SWAPADVISOR",
    "predicate": "was presented in",
    "object": "ACM ASPLOS"
  },
  {
    "subject": "SWAPADVISOR",
    "predicate": "was published in",
    "object": "2020"
  },
  {
    "subject": "SWAPADVISOR",
    "predicate": "pushes deep learning beyond",
    "object": "GPU memory limit via smart swapping"
  },
  {
    "subject": "HTTPS",
    "predicate": "is",
    "object": "KUBERNETES.IO"
  },
  {
    "subject": "HTTPS",
    "predicate": "refers to",
    "object": "GITHUB.COMNVIDIANVIDIA-DOCKER"
  },
  {
    "subject": "MESOS",
    "predicate": "is",
    "object": "a platform for fine-grained resource sharing in the data center"
  },
  {
    "subject": "47 B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H. Katz, S. Shenker, and I. Stoica",
    "predicate": "authored",
    "object": "MESOS: A platform for fine-grained resource sharing in the data center"
  },
  {
    "subject": "MESOS paper",
    "predicate": "was published in",
    "object": "USENIX NSDI, 2011"
  },
  {
    "subject": "V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al.",
    "predicate": "authored",
    "object": "Apache Hadoop YARN: Yet Another Resource Negotiator"
  },
  {
    "subject": "Apache Hadoop YARN: Yet Another Resource Negotiator",
    "predicate": "was published in",
    "object": "ACM Symposium on Cloud Computing"
  },
  {
    "subject": "Apache Hadoop YARN: Yet Another Resource Negotiator",
    "predicate": "was published in year",
    "object": "2013"
  },
  {
    "subject": "G. GIUNTA, R. MONTELLA, G. AGRILLO, AND G. COVIELLO",
    "predicate": "authored",
    "object": "A GPGPU Transparent Virtualization Component for High Performance Computing Clouds"
  },
  {
    "subject": "A GPGPU Transparent Virtualization Component for High Performance Computing Clouds",
    "predicate": "was presented in",
    "object": "European Conference on Parallel Processing"
  },
  {
    "subject": "European Conference on Parallel Processing",
    "predicate": "occurred in",
    "object": "2010"
  },
  {
    "subject": "V. T. RAVI, M. BECCHI, G. AGRAWAL, AND S. CHAKRADHAR",
    "predicate": "authored",
    "object": "SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK"
  },
  {
    "subject": "SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK",
    "predicate": "was published in",
    "object": "PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING"
  },
  {
    "subject": "PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING",
    "predicate": "publication year",
    "object": "2011"
  },
  {
    "subject": "GVIM",
    "predicate": "is",
    "object": "GPU-accelerated virtual machines"
  },
  {
    "subject": "GVIM",
    "predicate": "was presented in",
    "object": "Proceedings of the 3rd ACM Workshop on System-Level Virtualization for High Performance Computing"
  },
  {
    "subject": "GVIM",
    "predicate": "was published in",
    "object": "2009"
  },
  {
    "subject": "Authors",
    "predicate": "include",
    "object": "V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan"
  },
  {
    "subject": "J. DUATO, A. J. PENA, F. SILLA, R. MAYO, AND E. S. QUINTANA-ORT",
    "predicate": "authored",
    "object": "RCUDA: REDUCING THE NUMBER OF GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS"
  },
  {
    "subject": "RCUDA: REDUCING THE NUMBER OF GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS",
    "predicate": "was presented in",
    "object": "2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION"
  },
  {
    "subject": "2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION",
    "predicate": "occurred in",
    "object": "2010"
  },
  {
    "subject": "VCUDA",
    "predicate": "is",
    "object": "GPU-accelerated high-performance computing in virtual machines"
  },
  {
    "subject": "SUN and K. LI",
    "predicate": "authored",
    "object": "VCUDA: GPU-accelerated high-performance computing in virtual machines"
  },
  {
    "subject": "VCUDA",
    "predicate": "published in",
    "object": "IEEE Transactions on Computers"
  },
  {
    "subject": "MXNet",
    "predicate": "is",
    "object": "a flexible and efficient machine learning library"
  },
  {
    "subject": "MXNet",
    "predicate": "is designed for",
    "object": "heterogeneous distributed systems"
  },
  {
    "subject": "55 T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang",
    "predicate": "are authors of",
    "object": "MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems"
  },
  {
    "subject": "MXNet paper",
    "predicate": "was published in",
    "object": "arXiv preprint arXiv:1512.01274"
  },
  {
    "subject": "MXNet paper",
    "predicate": "was published in year",
    "object": "2015"
  },
  {
    "subject": "56 C. Gregg, J. Dorn, K. Hazelwood, and K. Skadron",
    "predicate": "authored",
    "object": "Fine-Grained Resource Sharing for Concurrent GPGPU Kernels"
  },
  {
    "subject": "Fine-Grained Resource Sharing for Concurrent GPGPU Kernels",
    "predicate": "was presented at",
    "object": "4th USENIX Workshop on Hot Topics in Parallelism"
  },
  {
    "subject": "Fine-Grained Resource Sharing for Concurrent GPGPU Kernels",
    "predicate": "was presented in year",
    "object": "2012"
  },
  {
    "subject": "57 S. PAI, M. J. THAZHUTHAVEETIL, AND R. GOVINDARAJAN",
    "predicate": "are authors of",
    "object": "IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS"
  },
  {
    "subject": "IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS",
    "predicate": "published in",
    "object": "ACM SIGARCH COMPUTER ARCHITECTURE NEWS"
  },
  {
    "subject": "TASO",
    "predicate": "is described in",
    "object": "58 Z. JIA, O. PADON, J. THOMAS, T. WARSZAWSKI, M. ZAHARIA, AND A. AIKEN, TASO: OPTIMIZING DEEP LEARNING COMPUTATION WITH AUTOMATIC GENERATION OF GRAPH SUBSTITUTIONS, IN ACM SOSP, 2019"
  }
]