[
  {
    "subject": "practice",
    "predicate": "is",
    "object": "dominant"
  },
  {
    "subject": "practice",
    "predicate": "is to provision",
    "object": "dedicated GPU clusters"
  },
  {
    "subject": "dedicated GPU clusters",
    "predicate": "for",
    "object": "training"
  },
  {
    "subject": "dedicated GPU clusters",
    "predicate": "for",
    "object": "inference"
  },
  {
    "subject": "training",
    "predicate": "separately",
    "object": null
  },
  {
    "subject": "inference",
    "predicate": "separately",
    "object": null
  },
  {
    "subject": "practice",
    "predicate": "is",
    "object": "dominant"
  },
  {
    "subject": "practice",
    "predicate": "is to provision",
    "object": "dedicated GPU clusters"
  },
  {
    "subject": "dedicated GPU clusters",
    "predicate": "for",
    "object": "training"
  },
  {
    "subject": "dedicated GPU clusters",
    "predicate": "for",
    "object": "inference"
  },
  {
    "subject": "training",
    "predicate": "separately",
    "object": null
  },
  {
    "subject": "inference",
    "predicate": "separately",
    "object": null
  },
  {
    "subject": "GPU Clusters",
    "predicate": "shared",
    "object": "GPU clusters"
  },
  {
    "subject": "training and inference",
    "predicate": "use",
    "object": "GPUs"
  },
  {
    "subject": "current practice",
    "predicate": "is to build",
    "object": "dedicated clusters"
  },
  {
    "subject": "dedicated clusters",
    "predicate": "for",
    "object": "training and inference separately"
  },
  {
    "subject": "GPU clusters",
    "predicate": "can be shared across",
    "object": "different applications"
  },
  {
    "subject": "GPU clusters",
    "predicate": "can be shared across",
    "object": "training and inference"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are often over-provisioned based on",
    "object": "the peak load"
  },
  {
    "subject": "GPU clusters",
    "predicate": "have limited sharing between",
    "object": "applications and task types"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are over-provisioned due to",
    "object": "the need to meet strict Service-Level Objectives (SLOs)"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "unused cycles of an inference application to be filled by training or other inference applications"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "unused cycles"
  },
  {
    "subject": "unused cycles",
    "predicate": "to be filled by",
    "object": "training or other inference applications"
  },
  {
    "subject": "unused cycles",
    "predicate": "to be filled by",
    "object": "training or other inference applications"
  },
  {
    "subject": "It",
    "predicate": "allows",
    "object": "multiple DL applications"
  },
  {
    "subject": "DL applications",
    "predicate": "time-share",
    "object": "same GPU"
  },
  {
    "subject": "same GPU",
    "predicate": "with",
    "object": "entire GPU memory"
  },
  {
    "subject": "switching overhead",
    "predicate": "is",
    "object": "millisecond-scale"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "improve",
    "object": "GPU utilization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "sacrifice",
    "object": "SLOs"
  },
  {
    "subject": "Experiments",
    "predicate": "show",
    "object": "PipeSwitch only incurs a task startup overhead of 3.66.6 ms and a total overhead of 5.434.6 ms (1050 better than NVIDIA MPS), and achieves near 100 GPU utilization"
  },
  {
    "subject": "Experiments",
    "predicate": "on",
    "object": "a variety of DL models and GPU cards"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "incurs",
    "object": "a task startup overhead of 3.66.6 ms and a total overhead of 5.434.6 ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "near 100 GPU utilization"
  },
  {
    "subject": "Experiments",
    "predicate": "show",
    "object": "PipeSwitch only incurs a task startup over-head of 3.66.6 ms and a total overhead of 5.434.6 ms (1050 better than NVIDIA MPS), and achieves near 100 GPU utilization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "focused on",
    "object": "single-GPU tasks"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "focused on",
    "object": "training"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "focused on",
    "object": "inference"
  },
  {
    "subject": "Multi-GPU inference tasks",
    "predicate": "can be supported by",
    "object": "performing PipeSwitch on each GPU with transactions"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "single-GPU training"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "asynchronous multi-GPU training"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "data parallel strategies"
  },
  {
    "subject": "preempting one GPU",
    "predicate": "does not affect",
    "object": "other GPUs"
  },
  {
    "subject": "fraction of tasks",
    "predicate": "use",
    "object": "single GPU"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is applicable to",
    "object": "tasks in real-world workloads"
  },
  {
    "subject": "use",
    "predicate": "seamlessly",
    "object": "PipeSwitch for synchronous multi-GPU training"
  },
  {
    "subject": "use",
    "predicate": "elastic synchronous training",
    "object": "dynamic changing of the number of GPUs used for training"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "high throughput"
  },
  {
    "subject": "high throughput",
    "predicate": "close to",
    "object": "the upper bound"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieving",
    "object": "near 100 GPU utilization"
  },
  {
    "subject": "We",
    "predicate": "demonstrate",
    "object": "performance of PipeSwitch"
  },
  {
    "subject": "experiments",
    "predicate": "on",
    "object": "variety of DNN models and GPU cards"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can increase",
    "object": "GPU utilization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can improve",
    "object": "agility of DL applications"
  },
  {
    "subject": "We",
    "predicate": "achieve",
    "object": "pipelined context switching"
  },
  {
    "subject": "key idea",
    "predicate": "is",
    "object": "leverage layered structure of neural network models"
  },
  {
    "subject": "neural network models",
    "predicate": "have",
    "object": "layered structure"
  },
  {
    "subject": "layered structure",
    "predicate": "is",
    "object": "leveraged"
  },
  {
    "subject": "layered structure",
    "predicate": "is",
    "object": "used for pipeline model transmission"
  },
  {
    "subject": "layered structure",
    "predicate": "is",
    "object": "used for task execution"
  },
  {
    "subject": "computation pattern",
    "predicate": "is",
    "object": "layer-by-layer"
  },
  {
    "subject": "layer-by-layer computation pattern",
    "predicate": "is",
    "object": "leveraged"
  },
  {
    "subject": "model transmission",
    "predicate": "is done over",
    "object": "PCIe"
  },
  {
    "subject": "task execution",
    "predicate": "is done in",
    "object": "GPU"
  },
  {
    "subject": "GPU",
    "predicate": "has",
    "object": "model-aware grouping"
  },
  {
    "subject": "observation",
    "predicate": "based on",
    "object": "design"
  },
  {
    "subject": "we",
    "predicate": "design",
    "object": "pipelined model transmission mechanism"
  },
  {
    "subject": "mechanism",
    "predicate": "pipelines",
    "object": "model transmission"
  },
  {
    "subject": "mechanism",
    "predicate": "pipelines",
    "object": "model computation"
  },
  {
    "subject": "model transmission",
    "predicate": "over",
    "object": "PCIe"
  },
  {
    "subject": "model computation",
    "predicate": "in",
    "object": "GPU"
  },
  {
    "subject": "transmitting",
    "predicate": "is bounded by",
    "object": "PCIe bandwidth"
  },
  {
    "subject": "transmitting",
    "predicate": "from",
    "object": "CPU to GPU"
  },
  {
    "subject": "task",
    "predicate": "from",
    "object": "CPU to GPU"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "unified memory management"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "active-standby worker switching mechanisms"
  },
  {
    "subject": "mechanisms",
    "predicate": "accompany",
    "object": "pipelining"
  },
  {
    "subject": "mechanisms",
    "predicate": "ensure",
    "object": "process-level isolation"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "active-standby mechanism"
  },
  {
    "subject": "active-standby mechanism",
    "predicate": "for",
    "object": "fast worker switching"
  },
  {
    "subject": "active-standby mechanism",
    "predicate": "for",
    "object": "process-level isolation"
  },
  {
    "subject": "We",
    "predicate": "have built",
    "object": "a PipeSwitch prototype"
  },
  {
    "subject": "We",
    "predicate": "integrated",
    "object": "it with PyTorch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "does not modify",
    "object": "model structure"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "only adds",
    "object": "hooks for PyTorch to wait for transmission or synchronize the execution"
  },
  {
    "subject": "We",
    "predicate": "have implemented",
    "object": "a system prototype for PipeSwitch with 3600 lines of code in C and Python"
  },
  {
    "subject": "We",
    "predicate": "have integrated",
    "object": "it with PyTorch 21"
  },
  {
    "subject": "Deep learning",
    "predicate": "powers",
    "object": "family of intelligent applications"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "family of intelligent applications"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "emerging family of intelligent applications"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "intelligent applications"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "applications"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "many domains"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "domains"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "retail"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "transportation"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "finance"
  },
  {
    "subject": "DL",
    "predicate": "powers",
    "object": "healthcare"
  },
  {
    "subject": "GPUs",
    "predicate": "are",
    "object": "one of the most widely-used classes of accelerators for DL"
  },
  {
    "subject": "They",
    "predicate": "provide",
    "object": "better trade-off between performance, cost and energy consumption"
  },
  {
    "subject": "CPUs",
    "predicate": "for",
    "object": "deep neural network (DNN) models"
  },
  {
    "subject": "DL workloads",
    "predicate": "include",
    "object": "throughput-intensive training tasks"
  },
  {
    "subject": "DL workloads",
    "predicate": "include",
    "object": "latency-sensitive inference tasks"
  },
  {
    "subject": "Inference tasks",
    "predicate": "cannot be served with",
    "object": "training clusters under ash crowds"
  },
  {
    "subject": "training tasks",
    "predicate": "cannot utilize",
    "object": "inference clusters"
  },
  {
    "subject": "inference load",
    "predicate": "is",
    "object": "low"
  },
  {
    "subject": "ash crowd",
    "predicate": "is",
    "object": "a crowd"
  },
  {
    "subject": "application",
    "predicate": "becomes",
    "object": "popular"
  },
  {
    "subject": "demand",
    "predicate": "grows",
    "object": "beyond operator's expectation"
  },
  {
    "subject": "training cluster",
    "predicate": "can not preempt",
    "object": "training tasks for inference tasks"
  },
  {
    "subject": "inference clusters",
    "predicate": "are often over-provisioned for",
    "object": "peak load"
  },
  {
    "subject": "inference clusters",
    "predicate": "are over-provisioned in order to meet",
    "object": "strict Service Level Objectives (SLOs)"
  },
  {
    "subject": "Inference clusters",
    "predicate": "are",
    "object": "over-provisioned for the peak load"
  },
  {
    "subject": "Inference clusters",
    "predicate": "serve",
    "object": "user requests"
  },
  {
    "subject": "Inference clusters",
    "predicate": "need to meet",
    "object": "strict SLOs"
  },
  {
    "subject": "production systems",
    "predicate": "are provisioned to",
    "object": "each application"
  },
  {
    "subject": "production systems",
    "predicate": "are provisioned on",
    "object": "per-GPU granularity"
  },
  {
    "subject": "production systems",
    "predicate": "limit",
    "object": "interference between applications"
  },
  {
    "subject": "production systems",
    "predicate": "allocate",
    "object": "GPUs"
  },
  {
    "subject": "production systems",
    "predicate": "allocate",
    "object": "applications"
  },
  {
    "subject": "production systems",
    "predicate": "allocate",
    "object": "per-GPU granularity"
  },
  {
    "subject": "production systems",
    "predicate": "limit",
    "object": "interference"
  },
  {
    "subject": "production systems",
    "predicate": "satisfy",
    "object": "SLO requirements"
  },
  {
    "subject": "DL applications",
    "predicate": "should be able to be packed to",
    "object": "same GPU server"
  },
  {
    "subject": "DL applications",
    "predicate": "maximize",
    "object": "GPU utilization"
  },
  {
    "subject": "GPU",
    "predicate": "via",
    "object": "time-sharing"
  },
  {
    "subject": "operating systems",
    "predicate": "achieve",
    "object": "high CPU utilization"
  },
  {
    "subject": "operating systems",
    "predicate": "via",
    "object": "task scheduling"
  },
  {
    "subject": "operating systems",
    "predicate": "via",
    "object": "context switching"
  },
  {
    "subject": "This",
    "predicate": "inspired by",
    "object": "OS scheduler"
  },
  {
    "subject": "This",
    "predicate": "inspired by",
    "object": "context switching"
  },
  {
    "subject": "This",
    "predicate": "inspired by",
    "object": "CPU world"
  },
  {
    "subject": "idea",
    "predicate": "has been extended to",
    "object": "cluster scheduling"
  },
  {
    "subject": "CPU time-sharing",
    "predicate": "has been extended to",
    "object": "cluster scheduling"
  },
  {
    "subject": "ne-grained time-sharing",
    "predicate": "provide",
    "object": "better utilization"
  },
  {
    "subject": "ne-grained time-sharing",
    "predicate": "provision",
    "object": "dedicated resources"
  },
  {
    "subject": "ne-grained time-sharing",
    "predicate": "provide",
    "object": "process-level isolation"
  },
  {
    "subject": "Enabling",
    "predicate": "scheduling",
    "object": "ne-grained cycles"
  },
  {
    "subject": "Google Borg 1",
    "predicate": "packs",
    "object": "online services and batch jobs"
  },
  {
    "subject": "Google Borg 1",
    "predicate": "saves",
    "object": "20-30 machines"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "GPUs"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "in the same way"
  },
  {
    "subject": "gap",
    "predicate": "is",
    "object": "GPU has high overhead when switching between tasks"
  },
  {
    "subject": "gap",
    "predicate": "is",
    "object": "GPU memory and slow switching"
  },
  {
    "subject": "GPU memory",
    "predicate": "is",
    "object": "precious"
  },
  {
    "subject": "switching",
    "predicate": "is",
    "object": "slow"
  },
  {
    "subject": "GPUs",
    "predicate": "using",
    "object": "CPUs"
  },
  {
    "subject": "requirements",
    "predicate": "have",
    "object": "strict SLOs"
  },
  {
    "subject": "SLOs",
    "predicate": "in the range of",
    "object": "tens to hundreds of milliseconds"
  },
  {
    "subject": "GPU",
    "predicate": "switches to",
    "object": "DNN model"
  },
  {
    "subject": "DNN model",
    "predicate": "has not been preloaded onto",
    "object": "GPU"
  },
  {
    "subject": "it",
    "predicate": "can take",
    "object": "multiple seconds before serving the rst inference request"
  },
  {
    "subject": "tricks",
    "predicate": "like",
    "object": "CUDA unied memory 4"
  },
  {
    "subject": "CPU applications",
    "predicate": "can be switched in",
    "object": "milliseconds or even microseconds"
  },
  {
    "subject": "existing solution",
    "predicate": "is",
    "object": "to spatially share the GPU memory"
  },
  {
    "subject": "approach",
    "predicate": "provide",
    "object": "GPU memory isolation"
  },
  {
    "subject": "approach",
    "predicate": "not provide",
    "object": "strong GPU memory isolation"
  },
  {
    "subject": "applications",
    "predicate": "have",
    "object": "GPU memory isolation"
  },
  {
    "subject": "NVIDIA Multiple Process Sharing (MPS)",
    "predicate": "allow",
    "object": "multiple processes to use the same GPU"
  },
  {
    "subject": "NVIDIA Multiple Process Sharing (MPS)",
    "predicate": "require",
    "object": "all processes data (e.g., DNN models) to be preloaded into the GPU memory"
  },
  {
    "subject": "Salus",
    "predicate": "allow",
    "object": "multiple processes to use the same GPU"
  },
  {
    "subject": "Salus",
    "predicate": "require",
    "object": "all processes data (e.g., DNN models) to be preloaded into the GPU memory"
  },
  {
    "subject": "multi-process support",
    "predicate": "is",
    "object": "from NVIDIA"
  },
  {
    "subject": "multi-process support",
    "predicate": "allows",
    "object": "inference process to share GPU with training process"
  },
  {
    "subject": "NVIDIA MPS 6",
    "predicate": "provides",
    "object": "official support"
  },
  {
    "subject": "official support",
    "predicate": "for sharing",
    "object": "a GPU"
  },
  {
    "subject": "a GPU",
    "predicate": "between",
    "object": "multiple processes"
  },
  {
    "subject": "GPU memory",
    "predicate": "is",
    "object": "much more limited than host memory"
  },
  {
    "subject": "GPU memory",
    "predicate": "cannot preload",
    "object": "many applications"
  },
  {
    "subject": "training task",
    "predicate": "consume",
    "object": "GPU memory"
  },
  {
    "subject": "training task",
    "predicate": "stops",
    "object": ""
  },
  {
    "subject": "training task",
    "predicate": "cleans",
    "object": "GPU environment"
  },
  {
    "subject": "GPU environment",
    "predicate": "freeing",
    "object": "GPU memory"
  },
  {
    "subject": "It",
    "predicate": "stops",
    "object": "the training task in the GPU"
  },
  {
    "subject": "It",
    "predicate": "starts",
    "object": "the inference task"
  },
  {
    "subject": "training task",
    "predicate": "occupies",
    "object": "entire GPU memory"
  },
  {
    "subject": "training task",
    "predicate": "does not stop",
    "object": "inference tasks"
  },
  {
    "subject": "memory footprints",
    "predicate": "are increasing",
    "object": "inference tasks"
  },
  {
    "subject": "models",
    "predicate": "are getting",
    "object": "larger"
  },
  {
    "subject": "request batching",
    "predicate": "is prevalently used to increase",
    "object": "throughput"
  },
  {
    "subject": "request batching",
    "predicate": "is used to",
    "object": "increase throughput"
  },
  {
    "subject": "request batching",
    "predicate": "increases",
    "object": "GPU memory requirement"
  },
  {
    "subject": "GPU memory requirement",
    "predicate": "of",
    "object": "inference applications"
  },
  {
    "subject": "we",
    "predicate": "argue",
    "object": "context switching design"
  },
  {
    "subject": "design",
    "predicate": "minimizes",
    "object": "switching overhead"
  },
  {
    "subject": "design",
    "predicate": "switching",
    "object": "contents on GPU memory"
  },
  {
    "subject": "approach",
    "predicate": "is",
    "object": "better"
  },
  {
    "subject": "approach",
    "predicate": "for",
    "object": "time-sharing GPUs"
  },
  {
    "subject": "existing solution",
    "predicate": "offers",
    "object": "context switching abstraction for GPU"
  },
  {
    "subject": "We",
    "predicate": "achieve",
    "object": "new technology called pipelined context switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "exploits",
    "object": "characteristics of DL applications"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "achieve",
    "object": "millisecond-scale overhead for switching tasks on GPUs"
  },
  {
    "subject": "we",
    "predicate": "face",
    "object": "a major challenge"
  },
  {
    "subject": "challenge",
    "predicate": "is",
    "object": "fast GPU context switching between different processes"
  },
  {
    "subject": "pipeline",
    "predicate": "build",
    "object": "possible"
  },
  {
    "subject": "pipeline",
    "predicate": "overlap",
    "object": "computation and GPU memory swapping"
  },
  {
    "subject": "pipeline",
    "predicate": "for",
    "object": "fast context switching"
  },
  {
    "subject": "There",
    "predicate": "is",
    "object": "no need for context switching"
  },
  {
    "subject": "application",
    "predicate": "is loaded in",
    "object": "GPU"
  },
  {
    "subject": "We",
    "predicate": "introduce",
    "object": "pipelined context switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "minimize",
    "object": "task switching overhead"
  },
  {
    "subject": "task switching overhead",
    "predicate": "on",
    "object": "GPUs"
  },
  {
    "subject": "GPUs",
    "predicate": "for",
    "object": "DL applications"
  },
  {
    "subject": "DNN models",
    "predicate": "can be held in",
    "object": "host memory"
  },
  {
    "subject": "host memory",
    "predicate": "is",
    "object": "much larger and cheaper than GPU memory"
  },
  {
    "subject": "GPU",
    "predicate": "can quickly context-switch between",
    "object": "the models either for training or inference"
  },
  {
    "subject": "enterprises",
    "predicate": "build",
    "object": "GPU clusters"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are",
    "object": "shared by multiple users"
  },
  {
    "subject": "M. Jeon",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "S. Venkataraman",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. Phanishayee",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "u. Qian",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "W. Xiao",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "F. Yang",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "Analysis",
    "predicate": "of",
    "object": "large-scale multi- tenant GPU clusters"
  },
  {
    "subject": "large-scale multi- tenant GPU clusters",
    "predicate": "for",
    "object": "DNN training workloads"
  },
  {
    "subject": "USENIX ATC",
    "predicate": "held",
    "object": "2019"
  },
  {
    "subject": "512 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "organized by",
    "object": "USENIX Association"
  },
  {
    "subject": "M. Jeon, S. Venkataraman, A. Phanishayee, J. Qian, W. Xiao, and F. Yang",
    "predicate": "authored",
    "object": "Analysis of large-scale multi- tenant GPU clusters for DNN training workloads"
  },
  {
    "subject": "Analysis of large-scale multi- tenant GPU clusters for DNN training workloads",
    "predicate": "presented at",
    "object": "USENIX ATC"
  },
  {
    "subject": "USENIX ATC",
    "predicate": "year",
    "object": "2019"
  },
  {
    "subject": "number of applications",
    "predicate": "can be multiplexed",
    "object": "not limited by the GPU memory size"
  },
  {
    "subject": "each application",
    "predicate": "is able to use",
    "object": "the entire GPU compute and memory resources during its time slice"
  },
  {
    "subject": "we",
    "predicate": "propose",
    "object": "PipeSwitch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "GPU-efficient multiplexing of many DL applications on GPU servers via fine-grained time-sharing"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "millisecond-scale latencies and high throughput as dedicated servers"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "GPU-efficient fine-grained time-sharing for multiple DL applications"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "millisecond-scale context switching latencies and high throughput"
  },
  {
    "subject": "ideas",
    "predicate": "combine into",
    "object": "system"
  },
  {
    "subject": "we",
    "predicate": "close",
    "object": "gap of GPU memory sharing and switching"
  },
  {
    "subject": "we",
    "predicate": "enable",
    "object": "design of efficient time-sharing GPU cluster for DL workloads"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "GPU-efficient multiplexing of multiple DL applications"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "on",
    "object": "GPU servers"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "GPU-efficient fine-grained time-sharing"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "for",
    "object": "multiple DL applications"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieve",
    "object": "millisecond-scale task switching time"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "DL applications on time-sharing GPUs to meet strict SLOs"
  },
  {
    "subject": "switching overhead",
    "predicate": "is",
    "object": "critical"
  },
  {
    "subject": "DL applications",
    "predicate": "to satisfy",
    "object": "strict SLO requirements"
  },
  {
    "subject": "It",
    "predicate": "exploits",
    "object": "characteristics of DL applications"
  },
  {
    "subject": "It",
    "predicate": "achieve",
    "object": "millisecond-scale task switching overhead"
  },
  {
    "subject": "It",
    "predicate": "satisfy",
    "object": "SLO requirements"
  },
  {
    "subject": "we",
    "predicate": "perform",
    "object": "measurement study"
  },
  {
    "subject": "we",
    "predicate": "profile",
    "object": "task switching overhead"
  },
  {
    "subject": "we",
    "predicate": "analyze",
    "object": "overhead of each component"
  },
  {
    "subject": "we",
    "predicate": "perform",
    "object": "a measurement study"
  },
  {
    "subject": "measurement study",
    "predicate": "prole",
    "object": "task switching overhead"
  },
  {
    "subject": "We",
    "predicate": "divide",
    "object": "the switching overhead"
  },
  {
    "subject": "the switching overhead",
    "predicate": "into",
    "object": "four components"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "old task cleaning"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "new task initialization"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "GPU memory allocation"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "model transmission via PCIe from CPU to GPU"
  },
  {
    "subject": "Instance Type",
    "predicate": "is",
    "object": "g4dn.2xlarge"
  },
  {
    "subject": "Instance Type",
    "predicate": "is",
    "object": "p3.2xlarge"
  },
  {
    "subject": "GPU Type",
    "predicate": "is",
    "object": "NVIDIA T4"
  },
  {
    "subject": "GPU Type",
    "predicate": "is",
    "object": "NVIDIA V100"
  },
  {
    "subject": "Task",
    "predicate": "Cleaning",
    "object": "155 ms"
  },
  {
    "subject": "Task",
    "predicate": "Cleaning",
    "object": "165 ms"
  },
  {
    "subject": "Task",
    "predicate": "Initialization",
    "object": "5530 ms"
  },
  {
    "subject": "Task",
    "predicate": "Initialization",
    "object": "7290 ms"
  },
  {
    "subject": "Memory Allocation",
    "predicate": "is",
    "object": "10 ms"
  },
  {
    "subject": "Memory Allocation",
    "predicate": "is",
    "object": "13 ms"
  },
  {
    "subject": "Model Transmission",
    "predicate": "is",
    "object": "91 ms"
  },
  {
    "subject": "Model Transmission",
    "predicate": "is",
    "object": "81 ms"
  },
  {
    "subject": "Total Overhead",
    "predicate": "is",
    "object": "5787 ms"
  },
  {
    "subject": "Total Overhead",
    "predicate": "is",
    "object": "7551 ms"
  },
  {
    "subject": "Inference Time",
    "predicate": "is",
    "object": "105 ms"
  },
  {
    "subject": "Inference Time",
    "predicate": "is",
    "object": "32 ms"
  },
  {
    "subject": "component",
    "predicate": "takes",
    "object": "considerable amount of time"
  },
  {
    "subject": "amount",
    "predicate": "varies",
    "object": "from tens of milliseconds to seconds"
  },
  {
    "subject": "overhead",
    "predicate": "is",
    "object": "significant"
  },
  {
    "subject": "inference task",
    "predicate": "takes",
    "object": "tens of milliseconds"
  },
  {
    "subject": "latency SLOs",
    "predicate": "are",
    "object": "typically a small multiple of the inference time"
  },
  {
    "subject": "source",
    "predicate": "is",
    "object": "contentions"
  },
  {
    "subject": "contentions",
    "predicate": "on",
    "object": "computation"
  },
  {
    "subject": "contentions",
    "predicate": "on",
    "object": "memory"
  },
  {
    "subject": "task",
    "predicate": "do not stop when",
    "object": "inference task"
  },
  {
    "subject": "We",
    "predicate": "take",
    "object": "a holistic approach"
  },
  {
    "subject": "We",
    "predicate": "exploit",
    "object": "the characteristics of DL applications"
  },
  {
    "subject": "characteristics",
    "predicate": "minimize",
    "object": "the overhead of all the components"
  },
  {
    "subject": "design",
    "predicate": "is based on",
    "object": "key observation"
  },
  {
    "subject": "observation",
    "predicate": "have",
    "object": "layered structure"
  },
  {
    "subject": "observation",
    "predicate": "have",
    "object": "computation pattern"
  },
  {
    "subject": "DNN models",
    "predicate": "are",
    "object": "usually deep"
  },
  {
    "subject": "DNN models",
    "predicate": "consisting of",
    "object": "multiple layers"
  },
  {
    "subject": "multiple layers",
    "predicate": "stacking on",
    "object": "one another"
  },
  {
    "subject": "computation",
    "predicate": "takes place",
    "object": "layer by layer"
  },
  {
    "subject": "DNN models",
    "predicate": "computation of",
    "object": "takes place"
  },
  {
    "subject": "DNN models",
    "predicate": "have",
    "object": "layered structure"
  },
  {
    "subject": "there",
    "predicate": "is",
    "object": "no need"
  },
  {
    "subject": "model",
    "predicate": "to be transmitted",
    "object": "to the GPU"
  },
  {
    "subject": "model",
    "predicate": "to be transmitted",
    "object": "before starting computation"
  },
  {
    "subject": "task",
    "predicate": "does not need to wait for",
    "object": "entire model"
  },
  {
    "subject": "task",
    "predicate": "beginning",
    "object": "computation"
  },
  {
    "subject": "task",
    "predicate": "transmitted to",
    "object": "GPU"
  },
  {
    "subject": "pipelining",
    "predicate": "introduces",
    "object": "high overhead"
  },
  {
    "subject": "pipelining",
    "predicate": "introduces",
    "object": "overhead on tensor transmission"
  },
  {
    "subject": "pipelining",
    "predicate": "introduces",
    "object": "overhead on synchronization"
  },
  {
    "subject": "Pipelining",
    "predicate": "requires",
    "object": "synchronization"
  },
  {
    "subject": "Pipelining",
    "predicate": "requires",
    "object": "per-layer granularity"
  },
  {
    "subject": "per-layer granularity",
    "predicate": "requires",
    "object": "synchronization"
  },
  {
    "subject": "We",
    "predicate": "divide",
    "object": "layers into groups"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "an optimal model-aware grouping algorithm"
  },
  {
    "subject": "algorithm",
    "predicate": "to find",
    "object": "the best grouping strategy"
  },
  {
    "subject": "algorithm",
    "predicate": "for",
    "object": "a given model"
  },
  {
    "subject": "model-aware grouping",
    "predicate": "is",
    "object": "optimal"
  },
  {
    "subject": "we",
    "predicate": "design",
    "object": "algorithm"
  },
  {
    "subject": "we",
    "predicate": "find",
    "object": "optimal grouping strategy"
  },
  {
    "subject": "optimal grouping strategy",
    "predicate": "for",
    "object": "given model"
  },
  {
    "subject": "computation of a DL task",
    "predicate": "is",
    "object": "layer by layer"
  },
  {
    "subject": "computation of a DL task",
    "predicate": "has",
    "object": "simple, regular pattern for memory allocation"
  },
  {
    "subject": "DL task",
    "predicate": "stores",
    "object": "two important types of data"
  },
  {
    "subject": "DL task",
    "predicate": "stores",
    "object": "DNN model"
  },
  {
    "subject": "DL task",
    "predicate": "stores",
    "object": "GPU memory"
  },
  {
    "subject": "DNN model",
    "predicate": "includes",
    "object": "model parameters"
  },
  {
    "subject": "DL task",
    "predicate": "stores",
    "object": "intermediate results"
  },
  {
    "subject": "GPU memory management",
    "predicate": "is",
    "object": "general-purpose"
  },
  {
    "subject": "GPU memory management",
    "predicate": "e.g.,",
    "object": "CUDA unified memory 4"
  },
  {
    "subject": "GPU memory management",
    "predicate": "is",
    "object": "overkill"
  },
  {
    "subject": "GPU memory management",
    "predicate": "incurs",
    "object": "unnecessary overhead"
  },
  {
    "subject": "NVIDIA",
    "predicate": "provides",
    "object": "CUDA unified memory"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "handle",
    "object": "memory movement"
  },
  {
    "subject": "memory movement",
    "predicate": "between",
    "object": "host memory and GPU memory"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "for",
    "object": "applications"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "unified memory management"
  },
  {
    "subject": "unified memory management",
    "predicate": "minimize",
    "object": "overhead"
  },
  {
    "subject": "dedicated memory daemon",
    "predicate": "minimize",
    "object": "overhead"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "unified memory management"
  },
  {
    "subject": "unified memory management",
    "predicate": "achieve",
    "object": "minimal memory footprint"
  },
  {
    "subject": "unified memory management",
    "predicate": "eliminate",
    "object": "extra memory copies"
  },
  {
    "subject": "daemon",
    "predicate": "pre-allocates",
    "object": "GPU memory"
  },
  {
    "subject": "daemon",
    "predicate": "re-allocates",
    "object": "it to each task"
  },
  {
    "subject": "daemon",
    "predicate": "involving",
    "object": "expensive GPU memory manager"
  },
  {
    "subject": "memory daemon",
    "predicate": "uses",
    "object": "cudaMalloc"
  },
  {
    "subject": "memory daemon",
    "predicate": "obtains",
    "object": "GPU memory"
  },
  {
    "subject": "memory daemon",
    "predicate": "allocates",
    "object": "memory to the workers"
  },
  {
    "subject": "system",
    "predicate": "starts",
    "object": "GPU memory"
  },
  {
    "subject": "DNN models",
    "predicate": "are stored",
    "object": "only once in the memory daemon"
  },
  {
    "subject": "DNN models",
    "predicate": "are stored",
    "object": "to minimize memory footprint"
  },
  {
    "subject": "memory allocation",
    "predicate": "is",
    "object": "deterministic"
  },
  {
    "subject": "memory allocation",
    "predicate": "for",
    "object": "DNN model"
  },
  {
    "subject": "memory allocation",
    "predicate": "to eliminate",
    "object": "extra memory copies"
  },
  {
    "subject": "memory allocation",
    "predicate": "reduce",
    "object": "IPC overhead"
  },
  {
    "subject": "unified memory management",
    "predicate": "requires",
    "object": "each worker"
  },
  {
    "subject": "each worker",
    "predicate": "keep",
    "object": "a copy for each DNN model"
  },
  {
    "subject": "a copy",
    "predicate": "increases",
    "object": "the memory footprint"
  },
  {
    "subject": "server",
    "predicate": "contains",
    "object": "active worker"
  },
  {
    "subject": "server",
    "predicate": "contains",
    "object": "multiple standby workers"
  },
  {
    "subject": "server",
    "predicate": "has",
    "object": "one or more standby workers"
  },
  {
    "subject": "active worker",
    "predicate": "executes",
    "object": "current task on the GPU"
  },
  {
    "subject": "standby workers",
    "predicate": "stay on",
    "object": "CPU"
  },
  {
    "subject": "standby workers",
    "predicate": "wait for",
    "object": "next task"
  },
  {
    "subject": "active worker",
    "predicate": "is",
    "object": "worker"
  },
  {
    "subject": "active worker",
    "predicate": "executes",
    "object": "task"
  },
  {
    "subject": "active worker",
    "predicate": "in",
    "object": "GPU"
  },
  {
    "subject": "worker",
    "predicate": "is",
    "object": "process"
  },
  {
    "subject": "process",
    "predicate": "executes",
    "object": "tasks"
  },
  {
    "subject": "tasks",
    "predicate": "on",
    "object": "one GPU"
  },
  {
    "subject": "active worker",
    "predicate": "completes",
    "object": "current task"
  },
  {
    "subject": "active worker",
    "predicate": "stops",
    "object": "current task"
  },
  {
    "subject": "controller",
    "predicate": "noties",
    "object": "memory daemon"
  },
  {
    "subject": "controller",
    "predicate": "noties",
    "object": "standby worker"
  },
  {
    "subject": "standby worker",
    "predicate": "load",
    "object": "task to GPU"
  },
  {
    "subject": "task",
    "predicate": "execute with",
    "object": "pipelined model transmission"
  },
  {
    "subject": "mechanism",
    "predicate": "parallelizes",
    "object": "old task cleaning"
  },
  {
    "subject": "mechanism",
    "predicate": "parallelizes",
    "object": "new task initialization"
  },
  {
    "subject": "mechanism",
    "predicate": "minimize",
    "object": "worker switching overhead"
  },
  {
    "subject": "we",
    "predicate": "have proled",
    "object": "506 14th USENIX Symposium on Operating Systems Design and Implementation USENIX Association"
  },
  {
    "subject": "Level",
    "predicate": "Process-Cleaning Initialization",
    "object": "Overhead"
  },
  {
    "subject": "Overhead",
    "predicate": "Isolation",
    "object": "Two Processes One Process Active-Standby"
  },
  {
    "subject": "Table 2",
    "predicate": "Comparison of",
    "object": "worker switching mechanisms"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "an active and standby worker switching mechanism"
  },
  {
    "subject": "mechanism",
    "predicate": "hides",
    "object": "the overhead of both task cleaning and task initialization"
  },
  {
    "subject": "mechanism",
    "predicate": "ensures",
    "object": "process-level isolation"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enforces",
    "object": "process-level isolation"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "separate worker processes"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "requires",
    "object": "us"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "address",
    "object": "new technical challenges"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "on",
    "object": "memory management"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "on",
    "object": "worker switching"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "across",
    "object": "different processes"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "aims to provide",
    "object": "fast task switching"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "aims to ensure",
    "object": "process-level isolation"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "an active worker"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "multiple standby workers"
  },
  {
    "subject": "we",
    "predicate": "keep",
    "object": "all other components of PipeSwitch the same"
  },
  {
    "subject": "we",
    "predicate": "compare",
    "object": "the following mechanisms discussed in 4.4"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is",
    "object": "active-standby worker switching mechanism"
  },
  {
    "subject": "active-standby worker switching mechanism",
    "predicate": "used by",
    "object": "PipeSwitch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "uses",
    "object": "active-standby worker switching mechanism"
  },
  {
    "subject": "active-standby worker switching mechanism",
    "predicate": "parallelize",
    "object": "old task cleaning and new task initialization"
  },
  {
    "subject": "active-standby worker switching mechanism",
    "predicate": "incurs",
    "object": "minimal overhead"
  },
  {
    "subject": "Pipelining",
    "predicate": "is",
    "object": "a canonical technique"
  },
  {
    "subject": "Pipelining",
    "predicate": "widely used in",
    "object": "computer systems"
  },
  {
    "subject": "Pipelining",
    "predicate": "to improve",
    "object": "system performance"
  },
  {
    "subject": "Pipelining",
    "predicate": "to maximize",
    "object": "resource utilization"
  },
  {
    "subject": "Pipelining",
    "predicate": "brings",
    "object": "two sources of system overheads"
  },
  {
    "subject": "Prior work",
    "predicate": "has applied",
    "object": "pipelining to distributed training"
  },
  {
    "subject": "DL systems",
    "predicate": "such as",
    "object": "PipeDream 8"
  },
  {
    "subject": "DL systems",
    "predicate": "such as",
    "object": "ByteScheduler 9"
  },
  {
    "subject": "solutions",
    "predicate": "focus on",
    "object": "inter-batch pipelining"
  },
  {
    "subject": "solutions",
    "predicate": "overlap",
    "object": "computation and gradient transmission"
  },
  {
    "subject": "computation and gradient transmission",
    "predicate": "of",
    "object": "different batches"
  },
  {
    "subject": "computation and gradient transmission",
    "predicate": "for",
    "object": "training workloads"
  },
  {
    "subject": "training workloads",
    "predicate": "of",
    "object": "the same DNN model"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "introduces",
    "object": "intra-batch pipelining"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "reduce",
    "object": "overhead of switching between different DNN models"
  },
  {
    "subject": "DNN models",
    "predicate": "can be",
    "object": "inference or training"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "new techniques"
  },
  {
    "subject": "new techniques",
    "predicate": "support",
    "object": "training"
  },
  {
    "subject": "new techniques",
    "predicate": "support",
    "object": "inference"
  },
  {
    "subject": "inference",
    "predicate": "has",
    "object": "strict SLOs"
  },
  {
    "subject": "we",
    "predicate": "make",
    "object": "contributions"
  },
  {
    "subject": "We",
    "predicate": "introduce",
    "object": "pipelined context switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "exploits",
    "object": "characteristics of DL applications"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "pipelined model transmission"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "unified memory management"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "active-standby worker switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "minimize",
    "object": "switching overhead"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "enforce",
    "object": "process-level isolation"
  },
  {
    "subject": "Pipelined context switching",
    "predicate": "includes",
    "object": "three key techniques"
  },
  {
    "subject": "three key techniques",
    "predicate": "are",
    "object": "pipelined model transmission"
  },
  {
    "subject": "three key techniques",
    "predicate": "are",
    "object": "unified memory management"
  },
  {
    "subject": "three key techniques",
    "predicate": "are",
    "object": "active-standby worker switching"
  },
  {
    "subject": "We",
    "predicate": "implement",
    "object": "a system prototype"
  },
  {
    "subject": "We",
    "predicate": "integrate",
    "object": "it with Py-Torch"
  },
  {
    "subject": "section",
    "predicate": "identify",
    "object": "inefficiencies"
  },
  {
    "subject": "section",
    "predicate": "motivate",
    "object": "running DL workloads on GPUs"
  },
  {
    "subject": "DL workloads",
    "predicate": "run on",
    "object": "GPUs"
  },
  {
    "subject": "GPUs",
    "predicate": "in",
    "object": "fine-grained time-sharing model"
  },
  {
    "subject": "We",
    "predicate": "propose to pack",
    "object": "multiple DL applications onto the same GPU"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "ne-grained time-sharing abstraction"
  },
  {
    "subject": "ne-grained time-sharing abstraction",
    "predicate": "maximize",
    "object": "GPU utilization"
  },
  {
    "subject": "task switching",
    "predicate": "enables",
    "object": "flexible fine-grained scheduling"
  },
  {
    "subject": "scheduling",
    "predicate": "improve",
    "object": "GPU utilization"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "dedicated physical forms and power supplies"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "high speed networks"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "specialized task schedulers"
  },
  {
    "subject": "500 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "held by",
    "object": "USENIX Association"
  },
  {
    "subject": "Why",
    "predicate": "build",
    "object": "a shared cluster instead of a dedicated one for each user"
  },
  {
    "subject": "USENIX Association",
    "predicate": "hosted",
    "object": "14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "related to",
    "object": "Kubernetes"
  },
  {
    "subject": "514 14th USENIX Symposium",
    "predicate": "on",
    "object": "Operating Systems Design and Implementation"
  },
  {
    "subject": "USENIX Association",
    "predicate": "hosted",
    "object": "Symposium"
  },
  {
    "subject": "main reason",
    "predicate": "is",
    "object": "to bring down the cost"
  },
  {
    "subject": "demand",
    "predicate": "is",
    "object": "not well predictable"
  },
  {
    "subject": "demand",
    "predicate": "depend on",
    "object": "progress of different developers"
  },
  {
    "subject": "demand",
    "predicate": "is",
    "object": "more predictable"
  },
  {
    "subject": "inference task",
    "predicate": "has",
    "object": "daily periodical pattern"
  },
  {
    "subject": "inference task",
    "predicate": "for",
    "object": "particular application"
  },
  {
    "subject": "daily periodical pattern",
    "predicate": "based on",
    "object": "application usage"
  },
  {
    "subject": "patterns",
    "predicate": "can vary",
    "object": "across different tasks"
  },
  {
    "subject": "shared cluster",
    "predicate": "increase",
    "object": "resource utilization"
  },
  {
    "subject": "shared cluster",
    "predicate": "via",
    "object": "time-sharing"
  },
  {
    "subject": "sharing",
    "predicate": "between",
    "object": "training and inference"
  },
  {
    "subject": "shared clusters",
    "predicate": "are not shared between",
    "object": "training and inference"
  },
  {
    "subject": "This",
    "predicate": "brings",
    "object": "several inefficiencies"
  },
  {
    "subject": "inference clusters",
    "predicate": "are",
    "object": "not always running at high utilization"
  },
  {
    "subject": "inference clusters",
    "predicate": "cannot be utilized by",
    "object": "training"
  },
  {
    "subject": "Training clusters",
    "predicate": "are equipped with",
    "object": "powerful GPUs"
  },
  {
    "subject": "Training clusters",
    "predicate": "run",
    "object": "training tasks"
  },
  {
    "subject": "training tasks",
    "predicate": "are often",
    "object": "elastic"
  },
  {
    "subject": "training tasks",
    "predicate": "do not have",
    "object": "strict deadlines"
  },
  {
    "subject": "reasons",
    "predicate": "for",
    "object": "separately provisioning"
  },
  {
    "subject": "GPUs",
    "predicate": "designed for",
    "object": "inference tasks"
  },
  {
    "subject": "GPUs",
    "predicate": "too wimpy for",
    "object": "training tasks"
  },
  {
    "subject": "arrival",
    "predicate": "has started to change with",
    "object": "new GPU hardware"
  },
  {
    "subject": "new GPU hardware",
    "predicate": "most notably",
    "object": "NVIDIA T4"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "has",
    "object": "up to 32GB GPU memory"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "has",
    "object": "15.7 TFLOPS (single-precision)"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "has",
    "object": "comparable performance with 16GB GPU memory"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "has",
    "object": "8.1 TFLOPS (single-precision)"
  },
  {
    "subject": "algorithms and systems",
    "predicate": "enable",
    "object": "multiple GPUs to accelerate training"
  },
  {
    "subject": "one GPU",
    "predicate": "is not",
    "object": "fast enough"
  },
  {
    "subject": "industry collaborator",
    "predicate": "confirms",
    "object": "observation"
  },
  {
    "subject": "industry collaborator",
    "predicate": "is",
    "object": "online service provider"
  },
  {
    "subject": "service provider",
    "predicate": "runs",
    "object": "more than 10K V100 GPUs for training"
  },
  {
    "subject": "service provider",
    "predicate": "runs",
    "object": "at least 5 as many T4 GPUs for inference"
  },
  {
    "subject": "computation power",
    "predicate": "is",
    "object": "within the same order of magnitude"
  },
  {
    "subject": "both sides",
    "predicate": "is",
    "object": "within the same order of magnitude"
  },
  {
    "subject": "inference workload",
    "predicate": "fluctuates in correlation with",
    "object": "number of active users"
  },
  {
    "subject": "inference workload",
    "predicate": "shows",
    "object": "clear peaks and valleys within each day"
  },
  {
    "subject": "peak demand",
    "predicate": "is",
    "object": "2 of the valley at midnight"
  },
  {
    "subject": "match",
    "predicate": "utilize",
    "object": "inference GPUs"
  },
  {
    "subject": "times",
    "predicate": "less busy",
    "object": null
  },
  {
    "subject": "models",
    "predicate": "require",
    "object": "daily updates"
  },
  {
    "subject": "models",
    "predicate": "require",
    "object": "latest data"
  },
  {
    "subject": "example",
    "predicate": "is",
    "object": "to ne-tune BERT using daily news"
  },
  {
    "subject": "opportunity",
    "predicate": "improving",
    "object": "GPU utilization"
  },
  {
    "subject": "Borg-like systems",
    "predicate": "utilizing",
    "object": "GPUs"
  },
  {
    "subject": "It",
    "predicate": "has",
    "object": "advantages"
  },
  {
    "subject": "It",
    "predicate": "improve",
    "object": "resource utilization"
  },
  {
    "subject": "inference and training workloads",
    "predicate": "have",
    "object": "complementary usage patterns"
  },
  {
    "subject": "Online inference services",
    "predicate": "are",
    "object": "often more idle during midnight"
  },
  {
    "subject": "training developers",
    "predicate": "would start",
    "object": "a time-consuming job at night"
  },
  {
    "subject": "inference loads",
    "predicate": "have",
    "object": "different patterns"
  },
  {
    "subject": "different models",
    "predicate": "have",
    "object": "different patterns"
  },
  {
    "subject": "different patterns",
    "predicate": "benets from",
    "object": "time sharing"
  },
  {
    "subject": "It",
    "predicate": "simplify",
    "object": "design of load balancers and schedulers"
  },
  {
    "subject": "server",
    "predicate": "able to run",
    "object": "any task"
  },
  {
    "subject": "server",
    "predicate": "have",
    "object": "low overhead to switch between different applications"
  },
  {
    "subject": "server",
    "predicate": "equipped with",
    "object": "several TB of host memory"
  },
  {
    "subject": "server",
    "predicate": "enable",
    "object": "load many applications"
  },
  {
    "subject": "task execution",
    "predicate": "require",
    "object": "GPU memory"
  },
  {
    "subject": "GPU memory",
    "predicate": "is",
    "object": "limited"
  },
  {
    "subject": "GPU memory",
    "predicate": "even on",
    "object": "high-end GPUs"
  },
  {
    "subject": "high-end GPUs",
    "predicate": "have",
    "object": "16 GB for T4"
  },
  {
    "subject": "high-end GPUs",
    "predicate": "have",
    "object": "32 GB for V100"
  },
  {
    "subject": "GPU memory",
    "predicate": "is purposed for",
    "object": "task execution"
  },
  {
    "subject": "GPU memory",
    "predicate": "is not for storing",
    "object": "the state of idle applications"
  },
  {
    "subject": "DL tasks",
    "predicate": "require",
    "object": "a large amount of memory on a GPU"
  },
  {
    "subject": "DL tasks",
    "predicate": "require",
    "object": "even all of the memory on a GPU"
  },
  {
    "subject": "DL applications",
    "predicate": "have",
    "object": "large models"
  },
  {
    "subject": "DL applications",
    "predicate": "generate",
    "object": "large amounts of intermediate results"
  },
  {
    "subject": "large amounts of intermediate results",
    "predicate": "require",
    "object": "a lot of GPU memory"
  },
  {
    "subject": "Salus 7",
    "predicate": "cannot support",
    "object": "training tasks"
  },
  {
    "subject": "Salus 7",
    "predicate": "cannot support",
    "object": "memory-intensive tasks"
  },
  {
    "subject": "Salus 7",
    "predicate": "cannot support",
    "object": "multiple inference tasks"
  },
  {
    "subject": "Salus 7",
    "predicate": "cannot support",
    "object": "large models"
  },
  {
    "subject": "state-of-the-art models",
    "predicate": "are getting",
    "object": "deeper and larger"
  },
  {
    "subject": "idle applications",
    "predicate": "can occupy",
    "object": "large memory space"
  },
  {
    "subject": "active application",
    "predicate": "should be able to utilize",
    "object": "entire GPU memory"
  },
  {
    "subject": "number of applications",
    "predicate": "should only be limited by",
    "object": "host memory size"
  },
  {
    "subject": "switching a task",
    "predicate": "require",
    "object": "heavy memory swapping"
  },
  {
    "subject": "online inference workloads",
    "predicate": "require",
    "object": "strict SLOs"
  },
  {
    "subject": "naive memory swapping",
    "predicate": "cannot meet",
    "object": "strict SLOs"
  },
  {
    "subject": "host memory",
    "predicate": "swap with",
    "object": "GPU memory"
  },
  {
    "subject": "strict SLOs",
    "predicate": "require",
    "object": "requests to be handled in small batches for low latency"
  },
  {
    "subject": "it",
    "predicate": "is common to execute",
    "object": "an inference task with a single GPU 18"
  },
  {
    "subject": "we",
    "predicate": "test",
    "object": "strawman scenario"
  },
  {
    "subject": "we",
    "predicate": "stop",
    "object": "training task"
  },
  {
    "subject": "we",
    "predicate": "start",
    "object": "inference task"
  },
  {
    "subject": "inference batch",
    "predicate": "require",
    "object": "several seconds"
  },
  {
    "subject": "inference batch",
    "predicate": "finish",
    "object": "(4.1)"
  },
  {
    "subject": "support",
    "predicate": "is",
    "object": "not optimized for DL workloads"
  },
  {
    "subject": "support",
    "predicate": "incurs",
    "object": "hundreds of milliseconds overhead"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "has",
    "object": "lower overhead"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "compared to",
    "object": "stop-and-start"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "incurs",
    "object": "several hundred milliseconds overhead"
  },
  {
    "subject": "several hundred milliseconds overhead",
    "predicate": "prevents",
    "object": "MPS from meeting strict SLOs"
  },
  {
    "subject": "USENIX Association",
    "predicate": "hosted",
    "object": "14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "Controller",
    "predicate": "is",
    "object": "Active Worker"
  },
  {
    "subject": "Active Worker",
    "predicate": "uses",
    "object": "GPU Memory Daemon"
  },
  {
    "subject": "Standby Worker",
    "predicate": "is",
    "object": "Standby Worker"
  },
  {
    "subject": "Standby Worker",
    "predicate": "performs",
    "object": "New Task"
  },
  {
    "subject": "USENIX Association",
    "predicate": "hosted",
    "object": "14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "Throughput",
    "predicate": "measured in",
    "object": "batches/sec"
  },
  {
    "subject": "Throughput",
    "predicate": "compared to",
    "object": "Upper bound PipeSwitch MPS Stop-and-start"
  },
  {
    "subject": "Throughput",
    "predicate": "measured for",
    "object": "eight p3.2xlarge instances"
  },
  {
    "subject": "They",
    "predicate": "use",
    "object": "inter-batch pipelining for training of the same task"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "introduces",
    "object": "intra-batch pipelining to fast start both USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 511 training and inference tasks"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "fast switching across tasks"
  },
  {
    "subject": "DL workloads",
    "predicate": "have",
    "object": "well-defined structures"
  },
  {
    "subject": "structure and computation pattern of DNN models",
    "predicate": "allow",
    "object": "us to highly optimize task switching"
  },
  {
    "subject": "structure and computation pattern of DNN models",
    "predicate": "allow",
    "object": "achieve millisecond-scale overhead"
  },
  {
    "subject": "we",
    "predicate": "will show",
    "object": "pipeline is feasible and effective"
  },
  {
    "subject": "pipeline",
    "predicate": "is",
    "object": "feasible and effective"
  },
  {
    "subject": "we",
    "predicate": "will need to resolve",
    "object": "other challenges"
  },
  {
    "subject": "we",
    "predicate": "will need to resolve",
    "object": "memory management"
  },
  {
    "subject": "we",
    "predicate": "will need to resolve",
    "object": "worker switching"
  },
  {
    "subject": "It",
    "predicate": "benets switching",
    "object": "not only between inference and training"
  },
  {
    "subject": "It",
    "predicate": "benets switching",
    "object": "also between inference on different models"
  },
  {
    "subject": "we",
    "predicate": "provide",
    "object": "overview"
  },
  {
    "subject": "we",
    "predicate": "provide",
    "object": "architecture"
  },
  {
    "subject": "we",
    "predicate": "provide",
    "object": "task execution"
  },
  {
    "subject": "System architecture",
    "predicate": "is",
    "object": "important"
  },
  {
    "subject": "Figure 1",
    "predicate": "shows",
    "object": "architecture"
  },
  {
    "subject": "architecture",
    "predicate": "of",
    "object": "PipeSwitch server"
  },
  {
    "subject": "Figure 2",
    "predicate": "model",
    "object": "transmission and task execution"
  },
  {
    "subject": "server",
    "predicate": "contains",
    "object": "four types of components"
  },
  {
    "subject": "server",
    "predicate": "contains",
    "object": "controller"
  },
  {
    "subject": "server",
    "predicate": "contains",
    "object": "memory daemon"
  },
  {
    "subject": "server",
    "predicate": "contains",
    "object": "active worker"
  },
  {
    "subject": "server",
    "predicate": "contains",
    "object": "multiple standby workers"
  },
  {
    "subject": "Controller",
    "predicate": "is",
    "object": "undefined"
  },
  {
    "subject": "controller",
    "predicate": "is",
    "object": "central component"
  },
  {
    "subject": "It",
    "predicate": "receives",
    "object": "tasks from clients"
  },
  {
    "subject": "It",
    "predicate": "controls",
    "object": "the memory daemon"
  },
  {
    "subject": "It",
    "predicate": "controls",
    "object": "the workers"
  },
  {
    "subject": "workers",
    "predicate": "execute",
    "object": "the tasks"
  },
  {
    "subject": "Memory daemon",
    "predicate": "is",
    "object": "a type of daemon"
  },
  {
    "subject": "Controller",
    "predicate": "is",
    "object": "daemon"
  },
  {
    "subject": "memory",
    "predicate": "is",
    "object": "daemon"
  },
  {
    "subject": "memory daemon",
    "predicate": "manages",
    "object": "GPU memory"
  },
  {
    "subject": "memory daemon",
    "predicate": "manages",
    "object": "DNN models"
  },
  {
    "subject": "server",
    "predicate": "stores",
    "object": "DNN models"
  },
  {
    "subject": "server",
    "predicate": "stores",
    "object": "in host memory"
  },
  {
    "subject": "It",
    "predicate": "allocates",
    "object": "the GPU memory to the active worker"
  },
  {
    "subject": "It",
    "predicate": "transfers",
    "object": "the model from the host memory to the GPU memory"
  },
  {
    "subject": "This",
    "predicate": "eliminates",
    "object": "the GPU environment initialization overhead"
  },
  {
    "subject": "a new task",
    "predicate": "is assigned to",
    "object": "a worker"
  },
  {
    "subject": "worker",
    "predicate": "is",
    "object": "active"
  },
  {
    "subject": "Worker",
    "predicate": "is",
    "object": "a noun"
  },
  {
    "subject": "components",
    "predicate": "should be optimized to meet",
    "object": "SLOs"
  },
  {
    "subject": "Standby worker",
    "predicate": "is",
    "object": "a worker"
  },
  {
    "subject": "standby worker",
    "predicate": "is",
    "object": "idle"
  },
  {
    "subject": "standby worker",
    "predicate": "is",
    "object": "initializing a new task"
  },
  {
    "subject": "standby worker",
    "predicate": "is",
    "object": "cleaning its environment for the previous task"
  },
  {
    "subject": "standby worker",
    "predicate": "becomes",
    "object": "new active worker"
  },
  {
    "subject": "new active worker",
    "predicate": "to execute",
    "object": "new task"
  },
  {
    "subject": "active worker",
    "predicate": "becomes",
    "object": "standby worker"
  },
  {
    "subject": "active worker",
    "predicate": "cleans",
    "object": "environment"
  },
  {
    "subject": "active worker",
    "predicate": "for",
    "object": "previous task"
  },
  {
    "subject": "new task",
    "predicate": "arrives before",
    "object": "standby worker finishes cleaning previous task"
  },
  {
    "subject": "new task",
    "predicate": "needs to wait",
    "object": "increases startup time"
  },
  {
    "subject": "Task",
    "predicate": "execution",
    "object": null
  },
  {
    "subject": "Task",
    "predicate": "initialization",
    "object": null
  },
  {
    "subject": "controller",
    "predicate": "queues",
    "object": "set of tasks"
  },
  {
    "subject": "controller",
    "predicate": "received",
    "object": "tasks"
  },
  {
    "subject": "clients",
    "predicate": "send",
    "object": "tasks"
  },
  {
    "subject": "It",
    "predicate": "uses",
    "object": "a scheduling policy"
  },
  {
    "subject": "scheduling policy",
    "predicate": "to decide",
    "object": "which task to execute next"
  },
  {
    "subject": "which task",
    "predicate": "to execute",
    "object": "next"
  },
  {
    "subject": "scheduling",
    "predicate": "is",
    "object": "preemptive"
  },
  {
    "subject": "controller",
    "predicate": "can preempt",
    "object": "current task"
  },
  {
    "subject": "controller",
    "predicate": "can preempt",
    "object": "next one"
  },
  {
    "subject": "controller",
    "predicate": "based on",
    "object": "scheduling policy"
  },
  {
    "subject": "It",
    "predicate": "supports",
    "object": "canonical scheduling policies"
  },
  {
    "subject": "canonical scheduling policies",
    "predicate": "such as",
    "object": "rst come rst serve (FCFS)"
  },
  {
    "subject": "canonical scheduling policies",
    "predicate": "such as",
    "object": "earliest deadline rst (EDF)"
  },
  {
    "subject": "It",
    "predicate": "can be extended to support",
    "object": "new policies"
  },
  {
    "subject": "We",
    "predicate": "focus on",
    "object": "fast context switching"
  },
  {
    "subject": "the specic scheduling algorithm",
    "predicate": "is orthogonal to",
    "object": "this paper"
  },
  {
    "subject": "controller",
    "predicate": "preempt",
    "object": "training task"
  },
  {
    "subject": "controller",
    "predicate": "preempt",
    "object": "inference task"
  },
  {
    "subject": "inference task",
    "predicate": "have",
    "object": "strict latency SLO"
  },
  {
    "subject": "controller",
    "predicate": "waits for",
    "object": "current task"
  },
  {
    "subject": "controller",
    "predicate": "preempts",
    "object": "active worker"
  },
  {
    "subject": "current task",
    "predicate": "nish",
    "object": "it"
  },
  {
    "subject": "it",
    "predicate": "is",
    "object": "inference"
  },
  {
    "subject": "active worker",
    "predicate": "stop",
    "object": "it"
  },
  {
    "subject": "it",
    "predicate": "is",
    "object": "training"
  },
  {
    "subject": "controller",
    "predicate": "noties",
    "object": "idle standby worker"
  },
  {
    "subject": "idle standby worker",
    "predicate": "initialize",
    "object": "environment"
  },
  {
    "subject": "idle standby worker",
    "predicate": "initialize",
    "object": "for new task"
  },
  {
    "subject": "controller",
    "predicate": "schedules",
    "object": "task"
  },
  {
    "subject": "controller",
    "predicate": "determines",
    "object": "whether to switch to another worker"
  },
  {
    "subject": "memory daemon",
    "predicate": "allocates",
    "object": "memory to the standby worker"
  },
  {
    "subject": "memory daemon",
    "predicate": "transmits",
    "object": "model used by the new task"
  },
  {
    "subject": "memory daemon",
    "predicate": "transmits",
    "object": "model from host memory to GPU memory"
  },
  {
    "subject": "memory daemon",
    "predicate": "manages",
    "object": "GPU memory"
  },
  {
    "subject": "memory daemon",
    "predicate": "transmits",
    "object": "model"
  },
  {
    "subject": "memory daemon",
    "predicate": "eliminates",
    "object": "extra memory copy"
  },
  {
    "subject": "memory daemon",
    "predicate": "transmits",
    "object": "model"
  },
  {
    "subject": "host memory",
    "predicate": "to",
    "object": "GPU memory"
  },
  {
    "subject": "memory daemon",
    "predicate": "to",
    "object": "worker"
  },
  {
    "subject": "model",
    "predicate": "is transmitted to",
    "object": "GPU"
  },
  {
    "subject": "memory daemon",
    "predicate": "needs to notify",
    "object": "worker"
  },
  {
    "subject": "memory daemon",
    "predicate": "needs to export",
    "object": "relevant GPU memory handlers to worker"
  },
  {
    "subject": "worker",
    "predicate": "can access",
    "object": "model"
  },
  {
    "subject": "worker",
    "predicate": "can execute",
    "object": "task"
  },
  {
    "subject": "memory daemon",
    "predicate": "handles",
    "object": "GPU memory allocation"
  },
  {
    "subject": "memory daemon",
    "predicate": "handles",
    "object": "model transmission"
  },
  {
    "subject": "memory daemon",
    "predicate": "creates",
    "object": "GPU memory handlers"
  },
  {
    "subject": "memory daemon",
    "predicate": "sends",
    "object": "GPU memory handlers"
  },
  {
    "subject": "memory daemon",
    "predicate": "sends",
    "object": "workers"
  },
  {
    "subject": "goal",
    "predicate": "is",
    "object": "to design a set of techniques"
  },
  {
    "subject": "techniques",
    "predicate": "based on",
    "object": "characteristics of DL applications"
  },
  {
    "subject": "techniques",
    "predicate": "to minimize",
    "object": "task switching overhead"
  },
  {
    "subject": "task switching overhead",
    "predicate": "in",
    "object": "this process"
  },
  {
    "subject": "PipeSwitch Design",
    "predicate": "perform",
    "object": "measurement study"
  },
  {
    "subject": "measurement study",
    "predicate": "prole",
    "object": "task switching overhead"
  },
  {
    "subject": "task switching overhead",
    "predicate": "break down",
    "object": "individual components"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "end-to-end experiments"
  },
  {
    "subject": "we",
    "predicate": "demonstrate",
    "object": "the benefits of PipeSwitch"
  },
  {
    "subject": "we",
    "predicate": "show",
    "object": "the effectiveness of the design choices on each component"
  },
  {
    "subject": "we",
    "predicate": "describe",
    "object": "our design"
  },
  {
    "subject": "design",
    "predicate": "minimize",
    "object": "the overhead of each component"
  },
  {
    "subject": "measurement",
    "predicate": "considers",
    "object": "scenario"
  },
  {
    "subject": "server",
    "predicate": "stops",
    "object": "training task"
  },
  {
    "subject": "server",
    "predicate": "starts",
    "object": "inference task"
  },
  {
    "subject": "server",
    "predicate": "running on",
    "object": "GPU"
  },
  {
    "subject": "DNN model",
    "predicate": "used in",
    "object": "measurement"
  },
  {
    "subject": "DNN model",
    "predicate": "is",
    "object": "ResNet152 17"
  },
  {
    "subject": "measurement",
    "predicate": "covers",
    "object": "two types of instances on Amazon AWS"
  },
  {
    "subject": "types",
    "predicate": "are",
    "object": "g4dn.2xlarge with NVIDA T4"
  },
  {
    "subject": "types",
    "predicate": "are",
    "object": "p3.2xlarge with NVIDIA V100"
  },
  {
    "subject": "inference task",
    "predicate": "has arrived at",
    "object": "server"
  },
  {
    "subject": "we",
    "predicate": "focus on",
    "object": "measuring"
  },
  {
    "subject": "we",
    "predicate": "focus on",
    "object": "time to start and execute"
  },
  {
    "subject": "we",
    "predicate": "focus on",
    "object": "it on the GPU"
  },
  {
    "subject": "We",
    "predicate": "exclude",
    "object": "the network time"
  },
  {
    "subject": "We",
    "predicate": "exclude",
    "object": "the task queueing time"
  },
  {
    "subject": "Table 1",
    "predicate": "shows",
    "object": "the results"
  },
  {
    "subject": "total times",
    "predicate": "to start",
    "object": "inference task"
  },
  {
    "subject": "inference task",
    "predicate": "on",
    "object": "GPUs"
  },
  {
    "subject": "total times",
    "predicate": "are",
    "object": "5787 ms"
  },
  {
    "subject": "total times",
    "predicate": "are",
    "object": "7551 ms"
  },
  {
    "subject": "We",
    "predicate": "break",
    "object": "the overhead down"
  },
  {
    "subject": "the overhead",
    "predicate": "down into",
    "object": "the four components"
  },
  {
    "subject": "Task",
    "predicate": "cleaning",
    "object": null
  },
  {
    "subject": "task cleaning",
    "predicate": "takes",
    "object": "time"
  },
  {
    "subject": "inference task",
    "predicate": "creates",
    "object": "environment"
  },
  {
    "subject": "inference task",
    "predicate": "initializes",
    "object": "environment"
  },
  {
    "subject": "environment",
    "predicate": "process launching",
    "object": null
  },
  {
    "subject": "environment",
    "predicate": "PyTorch CUDA runtime loading",
    "object": null
  },
  {
    "subject": "environment",
    "predicate": "CUDA context initialization",
    "object": null
  },
  {
    "subject": "Memory allocation",
    "predicate": "is",
    "object": "a process"
  },
  {
    "subject": "inference task",
    "predicate": "allocates",
    "object": "GPU memory"
  },
  {
    "subject": "inference task",
    "predicate": "for",
    "object": "neural network model"
  },
  {
    "subject": "inference task",
    "predicate": "transmits",
    "object": "model"
  },
  {
    "subject": "model",
    "predicate": "from",
    "object": "host memory"
  },
  {
    "subject": "model",
    "predicate": "to",
    "object": "GPU memory"
  },
  {
    "subject": "Model",
    "predicate": "transmission",
    "object": null
  },
  {
    "subject": "transmission",
    "predicate": "is",
    "object": "grouped"
  },
  {
    "subject": "inference time on V100",
    "predicate": "is lower than",
    "object": "that on T4"
  },
  {
    "subject": "inference time on V100",
    "predicate": "is lower than",
    "object": "total overheads"
  },
  {
    "subject": "inference time on T4",
    "predicate": "is lower than",
    "object": "total overheads"
  },
  {
    "subject": "reason",
    "predicate": "is",
    "object": "lower overhead on T4"
  },
  {
    "subject": "task switching",
    "predicate": "largely depends on",
    "object": "CPU"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "is equipped with",
    "object": "better CPU than p3.2xlarge (Intel Platinum 8259CL vs. Intel Xeon E5-2686 v4)"
  },
  {
    "subject": "strawman solution",
    "predicate": "violate",
    "object": "SLOs"
  },
  {
    "subject": "components",
    "predicate": "take",
    "object": "considerable time"
  },
  {
    "subject": "components",
    "predicate": "should be optimized to achieve",
    "object": "minimal switching overhead"
  },
  {
    "subject": "components",
    "predicate": "should be optimized to meet",
    "object": "SLOs"
  },
  {
    "subject": "PCIe bandwidth",
    "predicate": "is",
    "object": "physical limit"
  },
  {
    "subject": "physical limit",
    "predicate": "on",
    "object": "how fast task can be loaded"
  },
  {
    "subject": "task",
    "predicate": "loaded to",
    "object": "GPU"
  },
  {
    "subject": "We",
    "predicate": "exploit",
    "object": "characteristics of DL applications"
  },
  {
    "subject": "characteristics of DL applications",
    "predicate": "circumvent",
    "object": "physical limit"
  },
  {
    "subject": "computation",
    "predicate": "is performed",
    "object": "layer by layer"
  },
  {
    "subject": "inference task",
    "predicate": "performs",
    "object": "forward pass"
  },
  {
    "subject": "inference task",
    "predicate": "makes",
    "object": "prediction"
  },
  {
    "subject": "iteration",
    "predicate": "performs",
    "object": "forward pass"
  },
  {
    "subject": "iteration",
    "predicate": "performs",
    "object": "backward pass"
  },
  {
    "subject": "training task",
    "predicate": "performs",
    "object": "forward pass"
  },
  {
    "subject": "training task",
    "predicate": "performs",
    "object": "backward pass"
  },
  {
    "subject": "task",
    "predicate": "start",
    "object": "computation of a layer"
  },
  {
    "subject": "task",
    "predicate": "load",
    "object": "layer in the GPU"
  },
  {
    "subject": "task",
    "predicate": "have",
    "object": "input of the layer ready"
  },
  {
    "subject": "previous layers",
    "predicate": "finish",
    "object": "computation"
  },
  {
    "subject": "Figure 2",
    "predicate": "illustrates",
    "object": "advantage of pipelining over the strawman solution"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "requires",
    "object": "knowledge of models"
  },
  {
    "subject": "pipelining mechanism",
    "predicate": "is",
    "object": "with optimal model-aware grouping in PipeSwitch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "uses",
    "object": "model-aware grouping"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "best trade-off between pipeline overhead and efficiency"
  },
  {
    "subject": "model",
    "predicate": "transmission over",
    "object": "PCIe"
  },
  {
    "subject": "task",
    "predicate": "execution on",
    "object": "GPU"
  },
  {
    "subject": "model",
    "predicate": "transmit to",
    "object": "GPU"
  },
  {
    "subject": "task",
    "predicate": "execute on",
    "object": "GPU"
  },
  {
    "subject": "PCIe GPU",
    "predicate": "has",
    "object": "E0"
  },
  {
    "subject": "PCIe GPU",
    "predicate": "has",
    "object": "E1"
  },
  {
    "subject": "PCIe GPU",
    "predicate": "has",
    "object": "En-1"
  },
  {
    "subject": "PCIe GPU",
    "predicate": "has",
    "object": "E2"
  },
  {
    "subject": "PCIe GPU",
    "predicate": "supports",
    "object": "Pipeline model transmission and task execution"
  },
  {
    "subject": "example",
    "predicate": "shows",
    "object": "inference task"
  },
  {
    "subject": "inference task",
    "predicate": "has",
    "object": "forward pass"
  },
  {
    "subject": "forward pass",
    "predicate": "in",
    "object": "task execution"
  },
  {
    "subject": "hooks",
    "predicate": "can be automated",
    "object": null
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can be implemented as",
    "object": "a part of the DNN framework"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can gather",
    "object": "the model structure information"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can remain",
    "object": "transparent to users and cluster managers"
  },
  {
    "subject": "way",
    "predicate": "is",
    "object": "pipelining"
  },
  {
    "subject": "system",
    "predicate": "transmits",
    "object": "layers"
  },
  {
    "subject": "system",
    "predicate": "transmits",
    "object": "GPU memory"
  },
  {
    "subject": "computation",
    "predicate": "blocked",
    "object": "layer"
  },
  {
    "subject": "One",
    "predicate": "is",
    "object": "the overhead to invoke multiple calls to PCIe to transmit the data"
  },
  {
    "subject": "transmission overhead",
    "predicate": "is dominated by",
    "object": "data size"
  },
  {
    "subject": "data",
    "predicate": "combining",
    "object": "entire model to a large tensor"
  },
  {
    "subject": "entire model",
    "predicate": "to transmit together",
    "object": "large tensor"
  },
  {
    "subject": "we",
    "predicate": "divide",
    "object": "the model"
  },
  {
    "subject": "invoking",
    "predicate": "cause",
    "object": "significant extra overhead"
  },
  {
    "subject": "synchronization overhead",
    "predicate": "is",
    "object": "necessary"
  },
  {
    "subject": "synchronization overhead",
    "predicate": "between",
    "object": "transmission and computation"
  },
  {
    "subject": "computation",
    "predicate": "know",
    "object": "when a layer is ready to compute"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "grouping"
  },
  {
    "subject": "We",
    "predicate": "minimize",
    "object": "two sources of over-head"
  },
  {
    "subject": "We",
    "predicate": "combine",
    "object": "multiple layers into a group"
  },
  {
    "subject": "pipelining",
    "predicate": "is performed on",
    "object": "per-group granularity"
  },
  {
    "subject": "pipelining overhead",
    "predicate": "is paid",
    "object": "once for each group"
  },
  {
    "subject": "pipelining overhead",
    "predicate": "is paid",
    "object": "not each layer"
  },
  {
    "subject": "Grouping",
    "predicate": "introduces",
    "object": "a trade-off"
  },
  {
    "subject": "Grouping",
    "predicate": "introduces",
    "object": "between pipelining efficiency and pipelining overhead"
  },
  {
    "subject": "small groups",
    "predicate": "enables",
    "object": "more overlap between transmission and computation"
  },
  {
    "subject": "more overlap between transmission and computation",
    "predicate": "improves",
    "object": "pipelining efficiency"
  },
  {
    "subject": "more overlap between transmission and computation",
    "predicate": "pays",
    "object": "more pipelining overhead"
  },
  {
    "subject": "using big groups",
    "predicate": "has",
    "object": "minimal pipelining overhead"
  },
  {
    "subject": "using big groups",
    "predicate": "reduces",
    "object": "chance for overlapping"
  },
  {
    "subject": "Grouping",
    "predicate": "must be",
    "object": "model-aware"
  },
  {
    "subject": "models",
    "predicate": "have",
    "object": "different structures"
  },
  {
    "subject": "structures",
    "predicate": "in terms of",
    "object": "number of layers"
  },
  {
    "subject": "structures",
    "predicate": "in terms of",
    "object": "size of each layer"
  },
  {
    "subject": "we",
    "predicate": "enumerate",
    "object": "all possible combinations"
  },
  {
    "subject": "we",
    "predicate": "nd",
    "object": "optimal grouping strategy"
  },
  {
    "subject": "we",
    "predicate": "introduce",
    "object": "two pruning techniques"
  },
  {
    "subject": "pruning techniques",
    "predicate": "based on",
    "object": "two insights"
  },
  {
    "subject": "large models",
    "predicate": "have",
    "object": "hundreds of layers"
  },
  {
    "subject": "time complexity",
    "predicate": "is",
    "object": "exponential"
  },
  {
    "subject": "USENIX Association",
    "predicate": "hosted",
    "object": "14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "discussed",
    "object": "PCIe GPU lower bound of F(Group(0, i), i1)"
  },
  {
    "subject": "PCIe GPU lower bound of F(Group(0, i), i1)",
    "predicate": "involves",
    "object": "PCIe GPU Group(0, i)"
  },
  {
    "subject": "PCIe GPU Group(0, i)",
    "predicate": "includes",
    "object": "Group(i1, j)"
  },
  {
    "subject": "Group(i1, j)",
    "predicate": "is part of",
    "object": "Group(0, i)"
  },
  {
    "subject": "Group(i1, j)",
    "predicate": "is part of",
    "object": "Group(0, i)"
  },
  {
    "subject": "Group(0, i)",
    "predicate": "has",
    "object": "j, j1, , n-1"
  },
  {
    "subject": "Group(i1, j)",
    "predicate": "has",
    "object": "j, j1, , n-1"
  },
  {
    "subject": "case",
    "predicate": "should be pruned if",
    "object": "lower bound current optimal time"
  },
  {
    "subject": "cases",
    "predicate": "Prune",
    "object": "group from i to j"
  },
  {
    "subject": "cases",
    "predicate": "Prune",
    "object": "batch at least from layer i1 to j"
  },
  {
    "subject": "Figure 3",
    "predicate": "depicts",
    "object": "Examples for two pruning techniques"
  },
  {
    "subject": "we",
    "predicate": "can prune",
    "object": "the cases"
  },
  {
    "subject": "cases",
    "predicate": "group from",
    "object": "layer (i 1) to j j"
  },
  {
    "subject": "we",
    "predicate": "search for",
    "object": "j j"
  },
  {
    "subject": "we",
    "predicate": "dive into",
    "object": "details"
  },
  {
    "subject": "we",
    "predicate": "formulate",
    "object": "problem"
  },
  {
    "subject": "number of layers",
    "predicate": "be",
    "object": "n"
  },
  {
    "subject": "F(B,i)",
    "predicate": "be",
    "object": "a function"
  },
  {
    "subject": "F(B,i)",
    "predicate": "returns",
    "object": "total time of the optimal grouping strategy"
  },
  {
    "subject": "layer i",
    "predicate": "to",
    "object": "n-1"
  },
  {
    "subject": "layer 0 to i-1",
    "predicate": "have formed",
    "object": "groups represented by B"
  },
  {
    "subject": "group",
    "predicate": "is formed",
    "object": "from layer x to i"
  },
  {
    "subject": "function",
    "predicate": "applies",
    "object": "itself to nd the optimal groups from layer i1 to n-1"
  },
  {
    "subject": "function",
    "predicate": "updates",
    "object": "optgroups if the current strategy is better"
  },
  {
    "subject": "we",
    "predicate": "have",
    "object": "formula"
  },
  {
    "subject": "F",
    "predicate": "min i",
    "object": "F(group(0,i),i1)"
  },
  {
    "subject": "we",
    "predicate": "divide",
    "object": "all possible combinations into n cases"
  },
  {
    "subject": "case i",
    "predicate": "means",
    "object": "the first group contains layer 0 to i"
  },
  {
    "subject": "formula",
    "predicate": "can be applied recursively to compute",
    "object": "F(group(0,i),i1)"
  },
  {
    "subject": "insight",
    "predicate": "is",
    "object": "rst"
  },
  {
    "subject": "group",
    "predicate": "contains",
    "object": "layers"
  },
  {
    "subject": "computation",
    "predicate": "would be delayed",
    "object": "pipeline efficiency"
  },
  {
    "subject": "insight",
    "predicate": "is",
    "object": "second"
  },
  {
    "subject": "we",
    "predicate": "can pack",
    "object": "multiple layers"
  },
  {
    "subject": "layers",
    "predicate": "in",
    "object": "group"
  },
  {
    "subject": "group",
    "predicate": "based on",
    "object": "progress of computation"
  },
  {
    "subject": "progress of computation",
    "predicate": "affecting",
    "object": "pipeline efficiency"
  },
  {
    "subject": "T(i, j)",
    "predicate": "be",
    "object": "transmission and execution times for a group from layer i to j respectively"
  },
  {
    "subject": "T(i, j)",
    "predicate": "calculated based on",
    "object": "size of layer i to j and PCIe bandwidth"
  },
  {
    "subject": "E(i, j)",
    "predicate": "be",
    "object": "proled on the GPU"
  },
  {
    "subject": "overhead",
    "predicate": "included in",
    "object": "T(i, j)"
  },
  {
    "subject": "invoking multiple calls",
    "predicate": "included in",
    "object": "T(i, j)"
  },
  {
    "subject": "we",
    "predicate": "compute",
    "object": "a lower bound for the total time"
  },
  {
    "subject": "Figure 3(a)",
    "predicate": "illustrates",
    "object": "the lower bound for the total time"
  },
  {
    "subject": "each case",
    "predicate": "has",
    "object": "a total time"
  },
  {
    "subject": "Equation 1",
    "predicate": "defines",
    "object": "the total time"
  },
  {
    "subject": "lower bound",
    "predicate": "considers",
    "object": "best case"
  },
  {
    "subject": "remaining layers",
    "predicate": "combined",
    "object": "in one group"
  },
  {
    "subject": "remaining layers",
    "predicate": "combined",
    "object": "for transmission and computation"
  },
  {
    "subject": "computation",
    "predicate": "perfectly overlapped with",
    "object": "communication"
  },
  {
    "subject": "computation",
    "predicate": "happen right after",
    "object": "computation of the first group finishes"
  },
  {
    "subject": "lower bound of case i",
    "predicate": "is larger than",
    "object": "total time of the best grouping strategy found so far"
  },
  {
    "subject": "case i",
    "predicate": "can be pruned",
    "object": "recursive computation for F(group(0,i),i1)"
  },
  {
    "subject": "Figure 3(b)",
    "predicate": "shows",
    "object": "an example for this insight"
  },
  {
    "subject": "we",
    "predicate": "have",
    "object": "xed the rst group to be from layer 0 to i"
  },
  {
    "subject": "we",
    "predicate": "apply",
    "object": "Equation 1 recursively to enumerate the cases for the PCIe GPU B.delay"
  },
  {
    "subject": "group",
    "predicate": "be",
    "object": "at least from layer x to j to fill"
  },
  {
    "subject": "Group(a, i)",
    "predicate": "lower bound of",
    "object": "F(B Group(a,i), i1)"
  },
  {
    "subject": "second group",
    "predicate": "is",
    "object": null
  },
  {
    "subject": "We",
    "predicate": "can hide",
    "object": "the transmission of the second group into the computation of the rst group"
  },
  {
    "subject": "the transmission",
    "predicate": "nishes",
    "object": "no later than the computation of the rst group"
  },
  {
    "subject": "number",
    "predicate": "least",
    "object": "layers"
  },
  {
    "subject": "number",
    "predicate": "group",
    "object": "layers"
  },
  {
    "subject": "equation",
    "predicate": "compute",
    "object": "number"
  },
  {
    "subject": "group",
    "predicate": "is",
    "object": "no better than"
  },
  {
    "subject": "group",
    "predicate": "does not increase",
    "object": "pipeline efficiency"
  },
  {
    "subject": "group",
    "predicate": "has",
    "object": "higher pipeline overhead"
  },
  {
    "subject": "algorithm",
    "predicate": "runs",
    "object": "offline"
  },
  {
    "subject": "algorithm",
    "predicate": "finds",
    "object": "strategy"
  },
  {
    "subject": "strategy",
    "predicate": "used by",
    "object": "PipeSwitch"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "shows",
    "object": "pseudo code"
  },
  {
    "subject": "function FindOptGrouping",
    "predicate": "recursively nds",
    "object": "optimal grouping strategy"
  },
  {
    "subject": "optimal grouping strategy",
    "predicate": "based on",
    "object": "Equation 1"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "computes",
    "object": "the recursive function FindOptGrouping(B,x)"
  },
  {
    "subject": "It",
    "predicate": "takes",
    "object": "two inputs"
  },
  {
    "subject": "B",
    "predicate": "represents",
    "object": "the groups that have already formed"
  },
  {
    "subject": "x",
    "predicate": "is",
    "object": "the rst layer that have not formed a group"
  },
  {
    "subject": "case i k",
    "predicate": "contains",
    "object": "rst group"
  },
  {
    "subject": "rst group",
    "predicate": "contains",
    "object": "all layers from x to n1"
  },
  {
    "subject": "It",
    "predicate": "uses",
    "object": "optgroups"
  },
  {
    "subject": "optgroups",
    "predicate": "store",
    "object": "best grouping strategy"
  },
  {
    "subject": "best grouping strategy",
    "predicate": "from",
    "object": "layer x"
  },
  {
    "subject": "layer x",
    "predicate": "given",
    "object": "B"
  },
  {
    "subject": "B",
    "predicate": "is initialized to",
    "object": "none"
  },
  {
    "subject": "B",
    "predicate": "is initialized to",
    "object": "none"
  },
  {
    "subject": "none",
    "predicate": "line 2",
    "object": ""
  },
  {
    "subject": "algorithm",
    "predicate": "applies",
    "object": "second pruning insight"
  },
  {
    "subject": "algorithm",
    "predicate": "form",
    "object": "rst group"
  },
  {
    "subject": "algorithm",
    "predicate": "form",
    "object": "layer x"
  },
  {
    "subject": "algorithm",
    "predicate": "divides",
    "object": "problem"
  },
  {
    "subject": "algorithm",
    "predicate": "divides",
    "object": "cases"
  },
  {
    "subject": "cases",
    "predicate": "form",
    "object": "group"
  },
  {
    "subject": "group",
    "predicate": "from",
    "object": "layer"
  },
  {
    "subject": "Equation 3",
    "predicate": "illustrate",
    "object": "insight"
  },
  {
    "subject": "Figure 3(b)",
    "predicate": "illustrate",
    "object": "insight"
  },
  {
    "subject": "B",
    "predicate": "contains",
    "object": "one group"
  },
  {
    "subject": "B",
    "predicate": "contains",
    "object": "from layer 0 to i"
  },
  {
    "subject": "B",
    "predicate": "can contain",
    "object": "multiple groups"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "B.delay"
  },
  {
    "subject": "B.delay",
    "predicate": "denote",
    "object": "the time to which the group can be formed"
  },
  {
    "subject": "algorithm",
    "predicate": "nds",
    "object": "jbased on B.delay"
  },
  {
    "subject": "enumeration",
    "predicate": "skip",
    "object": "layers from x to j-1"
  },
  {
    "subject": "algorithm",
    "predicate": "applies",
    "object": "insight"
  },
  {
    "subject": "algorithm",
    "predicate": "compute",
    "object": "lower bound"
  },
  {
    "subject": "example",
    "predicate": "is",
    "object": "special case"
  },
  {
    "subject": "example",
    "predicate": "in",
    "object": "Equation 2"
  },
  {
    "subject": "example",
    "predicate": "in",
    "object": "Figure 3(a)"
  },
  {
    "subject": "x",
    "predicate": "is",
    "object": "0"
  },
  {
    "subject": "computation",
    "predicate": "has to wait for",
    "object": "transmission"
  },
  {
    "subject": "computation",
    "predicate": "has to wait for",
    "object": "computation of previous groups"
  },
  {
    "subject": "lower bound",
    "predicate": "is",
    "object": "bigger than current optimal time"
  },
  {
    "subject": "case i",
    "predicate": "is pruned",
    "object": "line 18-19"
  },
  {
    "subject": "it",
    "predicate": "returns",
    "object": "optgroups"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "has",
    "object": "time complexity"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "is",
    "object": "O(2n)"
  },
  {
    "subject": "worst case",
    "predicate": "needs to enumerate",
    "object": "all strategies"
  },
  {
    "subject": "pruning techniques",
    "predicate": "are",
    "object": "able to prune most of the strategies"
  },
  {
    "subject": "pruning techniques",
    "predicate": "can quickly find",
    "object": "the optimal one"
  },
  {
    "subject": "algorithm",
    "predicate": "uses",
    "object": "pruning techniques"
  },
  {
    "subject": "pruning techniques",
    "predicate": "are",
    "object": "two"
  },
  {
    "subject": "We",
    "predicate": "have",
    "object": "theorem"
  },
  {
    "subject": "theorem",
    "predicate": "for",
    "object": "algorithm"
  },
  {
    "subject": "Theorem 1",
    "predicate": "is",
    "object": "a mathematical statement"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "finds",
    "object": "optimal grouping strategy"
  },
  {
    "subject": "optimal grouping strategy",
    "predicate": "minimizes",
    "object": "total time for the pipeline"
  },
  {
    "subject": "Proof",
    "predicate": "is",
    "object": "a logical demonstration"
  },
  {
    "subject": "Proof",
    "predicate": "involves",
    "object": "establishing the truth of a statement"
  },
  {
    "subject": "Proof",
    "predicate": "can be",
    "object": "written or presented in various forms"
  },
  {
    "subject": "function",
    "predicate": "considers",
    "object": "num- ber of layers"
  },
  {
    "subject": "function",
    "predicate": "considers",
    "object": "m  n x"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "induction on m"
  },
  {
    "subject": "FindOptGrouping(B,x)",
    "predicate": "outputs",
    "object": "the optimal grouping strategy from layer x to n 1"
  },
  {
    "subject": "previous layers",
    "predicate": "have formed",
    "object": "groups represented by B"
  },
  {
    "subject": "FindOptGrouping",
    "predicate": "outputs",
    "object": "optimal strategy"
  },
  {
    "subject": "B",
    "predicate": "is",
    "object": "x"
  },
  {
    "subject": "FindOptGrouping",
    "predicate": "considers",
    "object": "k i k layers"
  },
  {
    "subject": "it",
    "predicate": "outputs",
    "object": "optimal grouping strategy"
  },
  {
    "subject": "optimal grouping strategy",
    "predicate": "based on",
    "object": "assumption"
  },
  {
    "subject": "Base case",
    "predicate": "is",
    "object": null
  },
  {
    "subject": "function",
    "predicate": "examines",
    "object": "one layer"
  },
  {
    "subject": "strategy",
    "predicate": "is",
    "object": "layer x itself"
  },
  {
    "subject": "strategy",
    "predicate": "is",
    "object": "one group"
  },
  {
    "subject": "strategy",
    "predicate": "is",
    "object": "optimal strategy"
  },
  {
    "subject": "Inductive step",
    "predicate": "is",
    "object": "a step"
  },
  {
    "subject": "algorithm",
    "predicate": "considers",
    "object": "k1 layers"
  },
  {
    "subject": "optimal strategy",
    "predicate": "is",
    "object": "one group"
  },
  {
    "subject": "cases",
    "predicate": "are",
    "object": "exclusive"
  },
  {
    "subject": "cases",
    "predicate": "cover",
    "object": "entire search space"
  },
  {
    "subject": "algorithm",
    "predicate": "outputs",
    "object": "optimal grouping strategy"
  },
  {
    "subject": "algorithm",
    "predicate": "outputs",
    "object": "optimal grouping strategy for m k 1"
  },
  {
    "subject": "technique",
    "predicate": "prunes",
    "object": "cases"
  },
  {
    "subject": "cases",
    "predicate": "lower bounds are",
    "object": "no better than the current found optimal"
  },
  {
    "subject": "technique",
    "predicate": "affect",
    "object": "optimality"
  },
  {
    "subject": "technique",
    "predicate": "prunes",
    "object": "case"
  },
  {
    "subject": "case",
    "predicate": "are from",
    "object": "rst groups"
  },
  {
    "subject": "rst groups",
    "predicate": "are from",
    "object": "layer x to j j"
  },
  {
    "subject": "cases",
    "predicate": "advance",
    "object": "computation"
  },
  {
    "subject": "computation",
    "predicate": "to",
    "object": "earlier point"
  },
  {
    "subject": "point",
    "predicate": "than",
    "object": "grouping"
  },
  {
    "subject": "grouping",
    "predicate": "from",
    "object": "x"
  },
  {
    "subject": "grouping",
    "predicate": "to",
    "object": "at least j"
  },
  {
    "subject": "pruning",
    "predicate": "affect",
    "object": "optimality"
  },
  {
    "subject": "Generality",
    "predicate": "is",
    "object": "a concept"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "achieves",
    "object": "optimality"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "for",
    "object": "a given list of layers"
  },
  {
    "subject": "This",
    "predicate": "require",
    "object": "models"
  },
  {
    "subject": "models",
    "predicate": "be",
    "object": "linear"
  },
  {
    "subject": "layers or operators",
    "predicate": "can be connected as",
    "object": "arbitrary computation graph"
  },
  {
    "subject": "layers or operators",
    "predicate": "can be connected as",
    "object": "simple chain"
  },
  {
    "subject": "Models",
    "predicate": "are",
    "object": "technically non-linear directed acyclic graph (DAGs)"
  },
  {
    "subject": "ResNet and Inception",
    "predicate": "are",
    "object": "technically non-linear directed acyclic graph (DAGs)"
  },
  {
    "subject": "execution order",
    "predicate": "is",
    "object": "an"
  },
  {
    "subject": "layers/operators",
    "predicate": "are issued to",
    "object": "GPU"
  },
  {
    "subject": "layers/operators",
    "predicate": "in",
    "object": "DAG"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "does not have",
    "object": "any special assumptions on the execution order"
  },
  {
    "subject": "It",
    "predicate": "interested in",
    "object": "nding out how to group the layers"
  },
  {
    "subject": "how",
    "predicate": "to achieve",
    "object": "high pipelining efciency"
  },
  {
    "subject": "how",
    "predicate": "to achieve",
    "object": "low pipelining over- head"
  },
  {
    "subject": "It",
    "predicate": "applies for",
    "object": "graphs with loops"
  },
  {
    "subject": "order",
    "predicate": "is based on",
    "object": "the rst time an operator is executed"
  },
  {
    "subject": "order",
    "predicate": "affect",
    "object": "correctness"
  },
  {
    "subject": "operator",
    "predicate": "be executed",
    "object": "only when it is transmitted to the GPU"
  },
  {
    "subject": "operator",
    "predicate": "be executed",
    "object": "only when the input is ready"
  },
  {
    "subject": "pipelined model transmission",
    "predicate": "is applicable to",
    "object": "general case"
  },
  {
    "subject": "we",
    "predicate": "keep",
    "object": "all other components of PipeSwitch the same"
  },
  {
    "subject": "we",
    "predicate": "compare",
    "object": "the following mechanisms discussed in 4.2"
  },
  {
    "subject": "Figure 7",
    "predicate": "depicts",
    "object": "Effectiveness of pipelined model transmission"
  },
  {
    "subject": "Task execution",
    "predicate": "requires",
    "object": "GPU memory"
  },
  {
    "subject": "Task execution",
    "predicate": "in",
    "object": "GPU"
  },
  {
    "subject": "GPU",
    "predicate": "has",
    "object": "memory management system"
  },
  {
    "subject": "GPU",
    "predicate": "provides",
    "object": "malloc function"
  },
  {
    "subject": "malloc function",
    "predicate": "is",
    "object": "similar to CPUs for memory allocation"
  },
  {
    "subject": "malloc function",
    "predicate": "e.g.",
    "object": "cudaMalloc for NVIDIA GPUs"
  },
  {
    "subject": "naive solution",
    "predicate": "is",
    "object": "GPU memory management"
  },
  {
    "subject": "task",
    "predicate": "uses",
    "object": "cudaMallocManaged function"
  },
  {
    "subject": "task",
    "predicate": "delegates",
    "object": "model transmission"
  },
  {
    "subject": "model transmission",
    "predicate": "to",
    "object": "CUDA unified memory"
  },
  {
    "subject": "we",
    "predicate": "add",
    "object": "functions for allocating GPU memory"
  },
  {
    "subject": "we",
    "predicate": "add",
    "object": "sharing the GPU USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation 507 memory to workers through CUDA IPC API"
  },
  {
    "subject": "we",
    "predicate": "add",
    "object": "getting the shared GPU memory"
  },
  {
    "subject": "worker",
    "predicate": "uses",
    "object": "cudaMalloc"
  },
  {
    "subject": "worker",
    "predicate": "allocates",
    "object": "GPU memory"
  },
  {
    "subject": "worker",
    "predicate": "transmits",
    "object": "model"
  },
  {
    "subject": "worker",
    "predicate": "to",
    "object": "GPU"
  },
  {
    "subject": "worker",
    "predicate": "allocates",
    "object": "GPU memory"
  },
  {
    "subject": "worker",
    "predicate": "transmits",
    "object": "model"
  },
  {
    "subject": "CUDA",
    "predicate": "automatically",
    "object": "transmits"
  },
  {
    "subject": "model",
    "predicate": "to",
    "object": "GPU"
  },
  {
    "subject": "solution",
    "predicate": "incurs",
    "object": "high overhead"
  },
  {
    "subject": "overhead",
    "predicate": "is for",
    "object": "DL applications"
  },
  {
    "subject": "reasons",
    "predicate": "are",
    "object": "two"
  },
  {
    "subject": "native cudaMalloc function",
    "predicate": "designed for",
    "object": "general-purpose applications"
  },
  {
    "subject": "native cudaMalloc function",
    "predicate": "may incur",
    "object": "unnecessary overhead for DL applications"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "designed for",
    "object": "general-purpose applications"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "may incur",
    "object": "unnecessary overhead for DL applications"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "is",
    "object": "not optimized for DL applications"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "introduces",
    "object": "more than one hundred milliseconds overhead than PipeSwitch"
  },
  {
    "subject": "We",
    "predicate": "exploit",
    "object": "two characteristics"
  },
  {
    "subject": "characteristics",
    "predicate": "of",
    "object": "DL applications"
  },
  {
    "subject": "DL applications",
    "predicate": "to minimize",
    "object": "GPU memory management overhead"
  },
  {
    "subject": "general-purpose GPU memory management",
    "predicate": "does not consider",
    "object": "characteristics"
  },
  {
    "subject": "general-purpose GPU memory management",
    "predicate": "is",
    "object": "heavy-weight"
  },
  {
    "subject": "DL applications",
    "predicate": "require",
    "object": "fast task switching"
  },
  {
    "subject": "amount",
    "predicate": "allocated to",
    "object": "DNN model"
  },
  {
    "subject": "amount",
    "predicate": "xed",
    "object": null
  },
  {
    "subject": "amount",
    "predicate": "does not change during",
    "object": "task execution"
  },
  {
    "subject": "training task",
    "predicate": "updates",
    "object": "model"
  },
  {
    "subject": "training task",
    "predicate": "updates",
    "object": "model parameters"
  },
  {
    "subject": "model parameters",
    "predicate": "are",
    "object": "weights of the neural network"
  },
  {
    "subject": "memory",
    "predicate": "needed to store",
    "object": "model parameters"
  },
  {
    "subject": "memory",
    "predicate": "stays",
    "object": "the same"
  },
  {
    "subject": "USENIX Association 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "is",
    "object": "inference task"
  },
  {
    "subject": "inference task",
    "predicate": "uses",
    "object": "model"
  },
  {
    "subject": "inference task",
    "predicate": "does not change",
    "object": "model"
  },
  {
    "subject": "intermediate results",
    "predicate": "change in",
    "object": "simple, regular pattern"
  },
  {
    "subject": "pattern",
    "predicate": "do not cause",
    "object": "memory fragmentation"
  },
  {
    "subject": "inference task",
    "predicate": "have",
    "object": "intermediate results"
  },
  {
    "subject": "intermediate results",
    "predicate": "be",
    "object": "outputs of each layer"
  },
  {
    "subject": "outputs of each layer",
    "predicate": "use",
    "object": "next layer"
  },
  {
    "subject": "next layer",
    "predicate": "is computed",
    "object": null
  },
  {
    "subject": "they",
    "predicate": "are no longer needed",
    "object": null
  },
  {
    "subject": "they",
    "predicate": "can be safely freed",
    "object": null
  },
  {
    "subject": "training task",
    "predicate": "differs in",
    "object": "intermediate results generated in the forward pass"
  },
  {
    "subject": "intermediate results",
    "predicate": "used by",
    "object": "backward pass"
  },
  {
    "subject": "backward pass",
    "predicate": "update",
    "object": "weights"
  },
  {
    "subject": "backward pass",
    "predicate": "consumes",
    "object": "intermediate results"
  },
  {
    "subject": "backward pass",
    "predicate": "consumes",
    "object": "results"
  },
  {
    "subject": "backward pass",
    "predicate": "consumes",
    "object": "reverse order"
  },
  {
    "subject": "forward pass",
    "predicate": "generates",
    "object": "intermediate results"
  },
  {
    "subject": "intermediate results",
    "predicate": "are",
    "object": "rst-in-last-out"
  },
  {
    "subject": "memory allocation",
    "predicate": "handled by",
    "object": "simple stack-like mechanism"
  },
  {
    "subject": "memory allocation",
    "predicate": "without causing",
    "object": "memory fragmentation"
  },
  {
    "subject": "release",
    "predicate": "handled by",
    "object": "simple stack-like mechanism"
  },
  {
    "subject": "Minimize",
    "predicate": "memory allocation overhead",
    "object": "overhead"
  },
  {
    "subject": "Minimize",
    "predicate": "memory footprint",
    "object": null
  },
  {
    "subject": "Minimize",
    "predicate": "extra memory copies",
    "object": null
  },
  {
    "subject": "avoid",
    "predicate": "extra memory copies",
    "object": null
  },
  {
    "subject": "we",
    "predicate": "design",
    "object": "a memory management mechanism"
  },
  {
    "subject": "mechanism",
    "predicate": "tailored for",
    "object": "DL applications"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "uses",
    "object": "dedicated memory daemon"
  },
  {
    "subject": "dedicated memory daemon",
    "predicate": "to manage",
    "object": "GPU memory"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "sends",
    "object": "64-bit integer offset"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "sends",
    "object": "shared GPU memory"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "sends",
    "object": "to workers"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "saves",
    "object": "223 ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "eliminating",
    "object": "memory allocation overhead"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "with",
    "object": "memory daemon"
  },
  {
    "subject": "This",
    "predicate": "eliminates",
    "object": "the overhead"
  },
  {
    "subject": "worker",
    "predicate": "use",
    "object": "cudaMalloc"
  },
  {
    "subject": "worker",
    "predicate": "get",
    "object": "a large amount of memory"
  },
  {
    "subject": "worker",
    "predicate": "store",
    "object": "their models and intermediate results"
  },
  {
    "subject": "memory daemon",
    "predicate": "needs to pass",
    "object": "memory pointers"
  },
  {
    "subject": "memory pointers",
    "predicate": "is",
    "object": "light-weight"
  },
  {
    "subject": "daemon",
    "predicate": "ensures",
    "object": "one worker owns the GPU memory"
  },
  {
    "subject": "worker",
    "predicate": "owns",
    "object": "GPU memory"
  },
  {
    "subject": "daemon",
    "predicate": "guarantees",
    "object": "memory isolation between workers"
  },
  {
    "subject": "worker",
    "predicate": "uses",
    "object": "memory pool"
  },
  {
    "subject": "memory pool",
    "predicate": "allocate",
    "object": "memory"
  },
  {
    "subject": "worker",
    "predicate": "store",
    "object": "model"
  },
  {
    "subject": "worker",
    "predicate": "store",
    "object": "intermediate results"
  },
  {
    "subject": "worker",
    "predicate": "recycles",
    "object": "memory"
  },
  {
    "subject": "intermediate results",
    "predicate": "no longer needed",
    "object": null
  },
  {
    "subject": "memory management",
    "predicate": "extends",
    "object": "Py-Torch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "extends",
    "object": "memory management"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "inserts",
    "object": "GPU memory blocks to PyTorch GPU memory pool"
  },
  {
    "subject": "PyTorch",
    "predicate": "creates",
    "object": "tensors on them"
  },
  {
    "subject": "It",
    "predicate": "designed and optimized for",
    "object": "efcient GPU memory allocation between different tasks"
  },
  {
    "subject": "memory management in PyTorch",
    "predicate": "handles",
    "object": "memory allocation for a task itself"
  },
  {
    "subject": "Replicating the models",
    "predicate": "incurs",
    "object": "high memory footprint"
  },
  {
    "subject": "Replicating the models",
    "predicate": "reduces",
    "object": "number of models a server can store"
  },
  {
    "subject": "number of models a server can store",
    "predicate": "consequently",
    "object": "types of tasks the server can execute"
  },
  {
    "subject": "storing",
    "predicate": "has",
    "object": "minimal memory footprint"
  },
  {
    "subject": "each model",
    "predicate": "is only stored",
    "object": "once"
  },
  {
    "subject": "it",
    "predicate": "incurs",
    "object": "an extra memory copy"
  },
  {
    "subject": "this process",
    "predicate": "to",
    "object": "a worker"
  },
  {
    "subject": "a worker",
    "predicate": "to start",
    "object": "a task"
  },
  {
    "subject": "it",
    "predicate": "hurts",
    "object": "the task switching time"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "stores",
    "object": "models"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "stores",
    "object": "models in the memory daemon"
  },
  {
    "subject": "server",
    "predicate": "needs to keep",
    "object": "one copy of each model"
  },
  {
    "subject": "server",
    "predicate": "needs to keep",
    "object": "one copy of each model in the host memory"
  },
  {
    "subject": "Minimize",
    "predicate": "has",
    "object": "IPC overhead"
  },
  {
    "subject": "We",
    "predicate": "leverage",
    "object": "a property of DL applications"
  },
  {
    "subject": "DL applications",
    "predicate": "minimize",
    "object": "the IPC overhead"
  },
  {
    "subject": "IPC APIs",
    "predicate": "provided by",
    "object": "GPUs"
  },
  {
    "subject": "cudaIpcOpenMemHandle",
    "predicate": "for",
    "object": "NVIDIA GPUs"
  },
  {
    "subject": "performance",
    "predicate": "measured",
    "object": "IPC APIs"
  },
  {
    "subject": "IPC APIs",
    "predicate": "incur",
    "object": "high overhead"
  },
  {
    "subject": "overhead",
    "predicate": "exacerbated by",
    "object": "pipeline"
  },
  {
    "subject": "pipeline",
    "predicate": "needs to invoke",
    "object": "IPCs frequently"
  },
  {
    "subject": "pipeline",
    "predicate": "synchronize",
    "object": "model transmission and task execution"
  },
  {
    "subject": "pipeline",
    "predicate": "invoke",
    "object": "IPCs only once"
  },
  {
    "subject": "pipeline",
    "predicate": "for",
    "object": "entire model transmission"
  },
  {
    "subject": "property",
    "predicate": "is",
    "object": "memory allocation process"
  },
  {
    "subject": "memory allocation process",
    "predicate": "is",
    "object": "deterministic"
  },
  {
    "subject": "neural network model",
    "predicate": "is",
    "object": "deterministic"
  },
  {
    "subject": "GPU memory region",
    "predicate": "is given",
    "object": "same"
  },
  {
    "subject": "model",
    "predicate": "is same as",
    "object": "same"
  },
  {
    "subject": "memory daemon and worker",
    "predicate": "use",
    "object": "same order"
  },
  {
    "subject": "memory daemon and worker",
    "predicate": "allocate",
    "object": "memory for model parameters"
  },
  {
    "subject": "memory pointers",
    "predicate": "would be",
    "object": "same"
  },
  {
    "subject": "It",
    "predicate": "is",
    "object": "easy"
  },
  {
    "subject": "to keep",
    "predicate": "the same order for",
    "object": "the memory daemon and the worker"
  },
  {
    "subject": "the neural network model",
    "predicate": "is",
    "object": "known and given"
  },
  {
    "subject": "the memory daemon",
    "predicate": "only needs to use",
    "object": "the same order to transmit the model"
  },
  {
    "subject": "the memory daemon",
    "predicate": "transmit",
    "object": "the model"
  },
  {
    "subject": "the worker",
    "predicate": "would",
    "object": "transmit the model"
  },
  {
    "subject": "memory daemon",
    "predicate": "minimize",
    "object": "usage of expensive GPU IPCs"
  },
  {
    "subject": "latency",
    "predicate": "is",
    "object": "higher"
  },
  {
    "subject": "latency",
    "predicate": "is",
    "object": "even higher"
  },
  {
    "subject": "latency",
    "predicate": "is",
    "object": "higher than no unified memory management"
  },
  {
    "subject": "It",
    "predicate": "uses",
    "object": "GPU IPC"
  },
  {
    "subject": "worker",
    "predicate": "initialize",
    "object": "GPU IPC"
  },
  {
    "subject": "worker",
    "predicate": "uses",
    "object": "cheap CPU IPCs"
  },
  {
    "subject": "worker",
    "predicate": "notify",
    "object": "pipeline group"
  },
  {
    "subject": "Pin",
    "predicate": "memory",
    "object": null
  },
  {
    "subject": "No",
    "predicate": "has",
    "object": "pin memory"
  },
  {
    "subject": "OS",
    "predicate": "swap",
    "object": "memory page to disk"
  },
  {
    "subject": "page",
    "predicate": "inactive for",
    "object": "certain amount of time"
  },
  {
    "subject": "GPUs",
    "predicate": "require",
    "object": "a page in the host memory to be pinned (or page- locked)"
  },
  {
    "subject": "host memory",
    "predicate": "to transmit",
    "object": "the data in the page to the GPU memory"
  },
  {
    "subject": "data",
    "predicate": "to transmit",
    "object": "the GPU memory"
  },
  {
    "subject": "temporary pinned page",
    "predicate": "is created for",
    "object": "transmission"
  },
  {
    "subject": "We",
    "predicate": "pin",
    "object": "the pages of the memory daemon"
  },
  {
    "subject": "We",
    "predicate": "eliminate",
    "object": "this overhead"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "is",
    "object": "desirable"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "ensures",
    "object": "one task cannot read the memory of another task"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "ensures",
    "object": "crashing of one task does not affect other tasks or the entire system"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "separate processes"
  },
  {
    "subject": "we",
    "predicate": "achieve",
    "object": "process-level isolation"
  },
  {
    "subject": "solution",
    "predicate": "is",
    "object": "naive"
  },
  {
    "subject": "solution",
    "predicate": "to use",
    "object": "separate processes"
  },
  {
    "subject": "processes",
    "predicate": "start",
    "object": "new task"
  },
  {
    "subject": "task",
    "predicate": "is stopped after",
    "object": "current task"
  },
  {
    "subject": "sequential execution",
    "predicate": "incurs",
    "object": "long delay"
  },
  {
    "subject": "sequential execution",
    "predicate": "incurs",
    "object": "old task cleaning"
  },
  {
    "subject": "sequential execution",
    "predicate": "incurs",
    "object": "new task initialization"
  },
  {
    "subject": "solution",
    "predicate": "is",
    "object": "possible"
  },
  {
    "subject": "tasks",
    "predicate": "share",
    "object": "process"
  },
  {
    "subject": "tasks",
    "predicate": "share",
    "object": "CUDA context"
  },
  {
    "subject": "task",
    "predicate": "reuse",
    "object": "GPU environment"
  },
  {
    "subject": "process",
    "predicate": "cleans",
    "object": "GPU environment"
  },
  {
    "subject": "process",
    "predicate": "is created",
    "object": "another process"
  },
  {
    "subject": "process",
    "predicate": "is initialized",
    "object": "new task"
  },
  {
    "subject": "process",
    "predicate": "cleans",
    "object": "GPU environment"
  },
  {
    "subject": "process",
    "predicate": "reuses",
    "object": "environment"
  },
  {
    "subject": "environment",
    "predicate": "for",
    "object": "old task"
  },
  {
    "subject": "environment",
    "predicate": "for",
    "object": "new task"
  },
  {
    "subject": "This",
    "predicate": "avoids",
    "object": "the new task initialization"
  },
  {
    "subject": "it",
    "predicate": "still has",
    "object": "the overhead for the current task to clean its status"
  },
  {
    "subject": "it",
    "predicate": "provide",
    "object": "process-level isolation"
  },
  {
    "subject": "tasks",
    "predicate": "have",
    "object": "process-level isolation"
  },
  {
    "subject": "worker",
    "predicate": "is",
    "object": "a separate process"
  },
  {
    "subject": "worker",
    "predicate": "initializes",
    "object": "its own GPU environment"
  },
  {
    "subject": "worker",
    "predicate": "i.e.",
    "object": "CUDA context"
  },
  {
    "subject": "worker",
    "predicate": "is",
    "object": "rst created"
  },
  {
    "subject": "current task",
    "predicate": "is stopped",
    "object": null
  },
  {
    "subject": "major job",
    "predicate": "is",
    "object": "to clear asynchronous CUDA functions queued on the GPU"
  },
  {
    "subject": "We",
    "predicate": "insert",
    "object": "synchronization points into training tasks"
  },
  {
    "subject": "number",
    "predicate": "limited",
    "object": "queued functions"
  },
  {
    "subject": "queued functions",
    "predicate": "can be quickly cleared",
    "object": ""
  },
  {
    "subject": "Synchronization points",
    "predicate": "are needed for",
    "object": "inference tasks"
  },
  {
    "subject": "inference tasks",
    "predicate": "are",
    "object": "short"
  },
  {
    "subject": "inference tasks",
    "predicate": "are not preempted",
    "object": ""
  },
  {
    "subject": "job",
    "predicate": "is",
    "object": "to free its GPU memory"
  },
  {
    "subject": "property",
    "predicate": "is",
    "object": "important"
  },
  {
    "subject": "procedure",
    "predicate": "does not modify",
    "object": "content of the memory"
  },
  {
    "subject": "procedure",
    "predicate": "only cleans",
    "object": "metadata"
  },
  {
    "subject": "metadata",
    "predicate": "is",
    "object": "GPU memory pointers"
  },
  {
    "subject": "GPU memory",
    "predicate": "managed by",
    "object": "PipeSwitch"
  },
  {
    "subject": "cleaning procedure",
    "predicate": "deletes",
    "object": "pointers pointing to the tensor data"
  },
  {
    "subject": "pointers",
    "predicate": "pointing to",
    "object": "tensor data"
  },
  {
    "subject": "pointers",
    "predicate": "pointing to",
    "object": "the actual data"
  },
  {
    "subject": "new task",
    "predicate": "transmit",
    "object": "model"
  },
  {
    "subject": "new task",
    "predicate": "transmit",
    "object": "model to GPU memory"
  },
  {
    "subject": "we",
    "predicate": "can parallelize",
    "object": "task cleaning of the current task and the pipelined model transmission of the new task"
  },
  {
    "subject": "task cleaning",
    "predicate": "of",
    "object": "the current task"
  },
  {
    "subject": "pipelined model transmission",
    "predicate": "of",
    "object": "the new task"
  },
  {
    "subject": "we",
    "predicate": "can hide",
    "object": "task cleaning overhead"
  },
  {
    "subject": "choice",
    "predicate": "is",
    "object": "optimized for performance"
  },
  {
    "subject": "choice",
    "predicate": "is",
    "object": "not a problem for a trusted environment"
  },
  {
    "subject": "latter process",
    "predicate": "read",
    "object": "memory data"
  },
  {
    "subject": "latter process",
    "predicate": "read",
    "object": "memory data of previous process"
  },
  {
    "subject": "this",
    "predicate": "is",
    "object": "concern"
  },
  {
    "subject": "operation",
    "predicate": "can be added",
    "object": "zero-out"
  },
  {
    "subject": "GPU",
    "predicate": "has",
    "object": "high memory bandwidth"
  },
  {
    "subject": "V100",
    "predicate": "has",
    "object": "900GBs"
  },
  {
    "subject": "It",
    "predicate": "incur",
    "object": "sub-millisecond overhead"
  },
  {
    "subject": "It",
    "predicate": "incur",
    "object": "zeroing-out most models like ResNet-152 (around 240MB)"
  },
  {
    "subject": "trusted environment",
    "predicate": "is",
    "object": "unnecessary to release all allocated memory"
  },
  {
    "subject": "preempted process",
    "predicate": "requires",
    "object": "entire GPU memory"
  },
  {
    "subject": "new process",
    "predicate": "does not require",
    "object": "entire GPU memory"
  },
  {
    "subject": "simple coordination",
    "predicate": "could be achieved by",
    "object": "some"
  },
  {
    "subject": "Table 2",
    "predicate": "summarizes",
    "object": "the differences between these three solutions"
  },
  {
    "subject": "controller",
    "predicate": "signals",
    "object": "current active worker"
  },
  {
    "subject": "controller",
    "predicate": "deletes",
    "object": "GPU memory allocated to it"
  },
  {
    "subject": "controller",
    "predicate": "allocates",
    "object": "GPU memory to the new active worker"
  },
  {
    "subject": "controller",
    "predicate": "notify",
    "object": "current active worker"
  },
  {
    "subject": "controller",
    "predicate": "transfers",
    "object": "parameters of the new model to the GPU"
  },
  {
    "subject": "current active worker",
    "predicate": "stop",
    "object": null
  },
  {
    "subject": "controller",
    "predicate": "receives",
    "object": "current active workers reply"
  },
  {
    "subject": "controller",
    "predicate": "ensures",
    "object": "only one active worker"
  },
  {
    "subject": "active worker",
    "predicate": "guarantee",
    "object": "exclusive occupation"
  },
  {
    "subject": "exclusive occupation",
    "predicate": "of",
    "object": "the GPU"
  },
  {
    "subject": "trade-off",
    "predicate": "between",
    "object": "number of standby workers and their GPU memory consumption"
  },
  {
    "subject": "standby worker",
    "predicate": "needs to maintain",
    "object": "its own CUDA context"
  },
  {
    "subject": "CUDA context",
    "predicate": "consumes",
    "object": "a few hundred MB GPU memory"
  },
  {
    "subject": "it",
    "predicate": "is",
    "object": "possible"
  },
  {
    "subject": "many standby workers",
    "predicate": "have",
    "object": null
  },
  {
    "subject": "there",
    "predicate": "is",
    "object": "always at least one idle standby worker"
  },
  {
    "subject": "experience",
    "predicate": "is",
    "object": "that two standby workers are sufficient"
  },
  {
    "subject": "standby workers",
    "predicate": "ensure",
    "object": "at least one idle worker"
  },
  {
    "subject": "idle worker",
    "predicate": "eliminates",
    "object": "waiting time"
  },
  {
    "subject": "waiting time",
    "predicate": "has",
    "object": "moderate GPU memory consumption"
  },
  {
    "subject": "transaction",
    "predicate": "means",
    "object": "a model is switched in or out on all of its GPUs"
  },
  {
    "subject": "model",
    "predicate": "enable or disable",
    "object": "inference"
  },
  {
    "subject": "it",
    "predicate": "does not work",
    "object": "out of the box with synchronous multi-GPU training"
  },
  {
    "subject": "We",
    "predicate": "have analyzed",
    "object": "a production GPU training trace"
  },
  {
    "subject": "a production GPU training trace",
    "predicate": "from",
    "object": "Microsoft 19,20"
  },
  {
    "subject": "tasks",
    "predicate": "total in",
    "object": "this trace"
  },
  {
    "subject": "tasks",
    "predicate": "single-GPU training",
    "object": "96,662"
  },
  {
    "subject": "jobs",
    "predicate": "account for",
    "object": "18 of total GPU hours"
  },
  {
    "subject": "share",
    "predicate": "increase in",
    "object": "future"
  },
  {
    "subject": "multi-GPU jobs",
    "predicate": "share of",
    "object": "increase"
  },
  {
    "subject": "training frameworks",
    "predicate": "do not have",
    "object": "mature support of elastic training"
  },
  {
    "subject": "research topic",
    "predicate": "remains",
    "object": "active"
  },
  {
    "subject": "research topic",
    "predicate": "is orthogonal to",
    "object": "PipeSwitch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is",
    "object": "a tool"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "performs",
    "object": "the best"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is close to",
    "object": "the lower bound"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is",
    "object": "a tool"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is",
    "object": "a tool"
  },
  {
    "subject": "solutions",
    "predicate": "are complementary to",
    "object": "PipeSwitch"
  },
  {
    "subject": "PyTorch Plugins",
    "predicate": "are",
    "object": null
  },
  {
    "subject": "https:pytorch.org",
    "predicate": "is",
    "object": "a website"
  },
  {
    "subject": "We",
    "predicate": "add",
    "object": "C and Python functions"
  },
  {
    "subject": "We",
    "predicate": "add",
    "object": "to the GPU memory management module of PyTorch"
  },
  {
    "subject": "We",
    "predicate": "add",
    "object": "functions"
  },
  {
    "subject": "functions",
    "predicate": "insert",
    "object": "received GPU memory into PyTorch GPU memory pool"
  },
  {
    "subject": "functions",
    "predicate": "clear",
    "object": "GPU memory from the pool"
  },
  {
    "subject": "functions",
    "predicate": "insert",
    "object": "received GPU memory into PyTorch GPU memory pool for a specific CUDA stream"
  },
  {
    "subject": "shared GPU memory",
    "predicate": "can be inserted into",
    "object": "PyTorch GPU memory pool"
  },
  {
    "subject": "shared GPU memory",
    "predicate": "can be inserted for",
    "object": "multiple times"
  },
  {
    "subject": "shared GPU memory",
    "predicate": "can be inserted for",
    "object": "different CUDA streams"
  },
  {
    "subject": "controller",
    "predicate": "guarantees",
    "object": "only one of these CUDA streams is active"
  },
  {
    "subject": "controller process",
    "predicate": "consists of",
    "object": "TCP thread"
  },
  {
    "subject": "controller process",
    "predicate": "consists of",
    "object": "scheduler thread"
  },
  {
    "subject": "scheduler",
    "predicate": "implemented together with",
    "object": "memory daemon"
  },
  {
    "subject": "TCP thread",
    "predicate": "accepts",
    "object": "task"
  },
  {
    "subject": "TCP thread",
    "predicate": "accepts",
    "object": "task from clients"
  },
  {
    "subject": "TCP thread",
    "predicate": "sends",
    "object": "task to scheduler thread"
  },
  {
    "subject": "scheduler thread",
    "predicate": "receives",
    "object": "task from TCP thread"
  },
  {
    "subject": "scheduler thread",
    "predicate": "allocates",
    "object": "GPU memory"
  },
  {
    "subject": "scheduler thread",
    "predicate": "shares",
    "object": "GPU memory with workers"
  },
  {
    "subject": "scheduler thread",
    "predicate": "activates",
    "object": "workers"
  },
  {
    "subject": "scheduler thread",
    "predicate": "deactivates",
    "object": "workers"
  },
  {
    "subject": "scheduler thread",
    "predicate": "sends",
    "object": "task to a worker"
  },
  {
    "subject": "scheduler thread",
    "predicate": "transfers",
    "object": "parameters for the corresponding model to GPU memory"
  },
  {
    "subject": "user",
    "predicate": "should register",
    "object": "model"
  },
  {
    "subject": "user",
    "predicate": "should notify",
    "object": "controller"
  },
  {
    "subject": "controller",
    "predicate": "to load",
    "object": "model"
  },
  {
    "subject": "model",
    "predicate": "from",
    "object": "disk"
  },
  {
    "subject": "model",
    "predicate": "to",
    "object": "CPU memory"
  },
  {
    "subject": "Parameters",
    "predicate": "transmitted to",
    "object": "GPU memory"
  },
  {
    "subject": "Parameters",
    "predicate": "transmitted in",
    "object": "groups"
  },
  {
    "subject": "groups",
    "predicate": "transmitted in",
    "object": "a pipeline"
  },
  {
    "subject": "group",
    "predicate": "is transferred",
    "object": "controller"
  },
  {
    "subject": "controller",
    "predicate": "notifies",
    "object": "worker"
  },
  {
    "subject": "worker",
    "predicate": "to start computing",
    "object": "corresponding layers"
  },
  {
    "subject": "worker process",
    "predicate": "consists of",
    "object": "two threads"
  },
  {
    "subject": "termination thread",
    "predicate": "waits for",
    "object": "termination signal"
  },
  {
    "subject": "termination thread",
    "predicate": "notifies",
    "object": "main thread"
  },
  {
    "subject": "termination signal",
    "predicate": "from",
    "object": "controller"
  },
  {
    "subject": "main thread",
    "predicate": "manages",
    "object": "DNN models"
  },
  {
    "subject": "main thread",
    "predicate": "performs",
    "object": "computation"
  },
  {
    "subject": "main thread",
    "predicate": "for",
    "object": "inference or training"
  },
  {
    "subject": "worker",
    "predicate": "requires",
    "object": "user"
  },
  {
    "subject": "worker",
    "predicate": "register",
    "object": "model"
  },
  {
    "subject": "worker",
    "predicate": "load",
    "object": "models"
  },
  {
    "subject": "worker",
    "predicate": "add",
    "object": "hooks"
  },
  {
    "subject": "hooks",
    "predicate": "wait for",
    "object": "parameter transmission"
  },
  {
    "subject": "hooks",
    "predicate": "terminate on",
    "object": "notification"
  },
  {
    "subject": "worker",
    "predicate": "loads",
    "object": "model structures"
  },
  {
    "subject": "model structures",
    "predicate": "is",
    "object": "small"
  },
  {
    "subject": "worker",
    "predicate": "loads",
    "object": "model parameters"
  },
  {
    "subject": "parameters",
    "predicate": "are stored",
    "object": "once"
  },
  {
    "subject": "parameters",
    "predicate": "are stored",
    "object": "in memory daemon"
  },
  {
    "subject": "memory daemon",
    "predicate": "has",
    "object": "minimal memory footprint"
  },
  {
    "subject": "models",
    "predicate": "are loaded",
    "object": null
  },
  {
    "subject": "models",
    "predicate": "are attached to",
    "object": "different CUDA streams"
  },
  {
    "subject": "parameters",
    "predicate": "are assigned to",
    "object": "locations in the shared GPU memory"
  },
  {
    "subject": "models",
    "predicate": "use",
    "object": "GPU memory location"
  },
  {
    "subject": "value",
    "predicate": "is",
    "object": "not valid"
  },
  {
    "subject": "controller",
    "predicate": "transfers",
    "object": "parameters"
  },
  {
    "subject": "controller",
    "predicate": "transfers",
    "object": "corresponding parameters"
  },
  {
    "subject": "controller",
    "predicate": "transfers",
    "object": "to these locations"
  },
  {
    "subject": "worker",
    "predicate": "waits for",
    "object": "scheduler"
  },
  {
    "subject": "worker",
    "predicate": "waits for",
    "object": "parameters"
  },
  {
    "subject": "worker",
    "predicate": "performs",
    "object": "inference"
  },
  {
    "subject": "worker",
    "predicate": "performs",
    "object": "training"
  },
  {
    "subject": "Setup",
    "predicate": "is",
    "object": "performed"
  },
  {
    "subject": "experiments",
    "predicate": "conducted on",
    "object": "AWS"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "two EC2 instance types"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "configured with",
    "object": "8 vCPUs (Intel Xeon E5-2686 v4)"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "configured with",
    "object": "1 GPU (NVIDIA V100 with 16 GB GPU memory)"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "configured with",
    "object": "PCIe 3.0 16"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "configured with",
    "object": "61 GB memory"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "configured with",
    "object": "8 vCPUs (Intel Platinum 8259CL)"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "configured with",
    "object": "1 GPU (NVIDIA T4 with 16 GB GPU memory)"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "configured with",
    "object": "PCIe 3.0 8"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "configured with",
    "object": "32 GB memory"
  },
  {
    "subject": "software environment",
    "predicate": "includes",
    "object": "PyTorch-1.3.0"
  },
  {
    "subject": "software environment",
    "predicate": "includes",
    "object": "torchvision-0.4.2"
  },
  {
    "subject": "software environment",
    "predicate": "includes",
    "object": "scipy-1.3.2"
  },
  {
    "subject": "software environment",
    "predicate": "includes",
    "object": "CUDA-10.1"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "PyTorch with our plugins"
  },
  {
    "subject": "PyTorch with our plugins",
    "predicate": "for",
    "object": "all mechanisms in comparison for consistency"
  },
  {
    "subject": "PyTorch with our plugins",
    "predicate": "provides",
    "object": "better results for stop-and-start"
  },
  {
    "subject": "native PyTorch from Python-PyPI",
    "predicate": "used in",
    "object": "Table 1"
  },
  {
    "subject": "Workloads",
    "predicate": "are",
    "object": null
  },
  {
    "subject": "models",
    "predicate": "include",
    "object": "ResNet152 17"
  },
  {
    "subject": "models",
    "predicate": "include",
    "object": "Inceptionv3 22"
  },
  {
    "subject": "models",
    "predicate": "include",
    "object": "Bertbase 23"
  },
  {
    "subject": "models",
    "predicate": "are",
    "object": "standard benchmarks for evaluating DL systems"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "representative configurations"
  },
  {
    "subject": "representative configurations",
    "predicate": "for",
    "object": "each model"
  },
  {
    "subject": "experiments",
    "predicate": "cover",
    "object": "training"
  },
  {
    "subject": "experiments",
    "predicate": "cover",
    "object": "inference"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "single-GPU inference and training tasks"
  },
  {
    "subject": "single-GPU inference and training tasks",
    "predicate": "discussed in",
    "object": "4.5"
  },
  {
    "subject": "Training tasks",
    "predicate": "periodically check-point",
    "object": "their models to the host memory"
  },
  {
    "subject": "Training tasks",
    "predicate": "restart",
    "object": "from the latest checkpoint after preemption"
  },
  {
    "subject": "checkpointing frequency",
    "predicate": "is set",
    "object": "according to the scheduling cycle"
  },
  {
    "subject": "training tasks",
    "predicate": "minimize",
    "object": "checkpointing overhead"
  },
  {
    "subject": "default batch size",
    "predicate": "is",
    "object": "32"
  },
  {
    "subject": "default batch size for inference",
    "predicate": "is",
    "object": "8"
  },
  {
    "subject": "Metrics",
    "predicate": "are",
    "object": "important"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "throughput"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "latency"
  },
  {
    "subject": "evaluation metrics",
    "predicate": "include",
    "object": "throughput"
  },
  {
    "subject": "evaluation metrics",
    "predicate": "include",
    "object": "latency"
  },
  {
    "subject": "We",
    "predicate": "measure",
    "object": "the end-to-end latency experienced by the client"
  },
  {
    "subject": "Figure 5",
    "predicate": "depicts",
    "object": "Total latency experienced by the client for different mechanisms"
  },
  {
    "subject": "Figure 5",
    "predicate": "shows",
    "object": "latency experienced by the client"
  },
  {
    "subject": "Table 3",
    "predicate": "shows",
    "object": "total overhead"
  },
  {
    "subject": "number",
    "predicate": "is reported with",
    "object": "average of 100 runs"
  },
  {
    "subject": "Figure 6(b)",
    "predicate": "report",
    "object": "minimum and maximum latencies"
  },
  {
    "subject": "latency",
    "predicate": "differ",
    "object": "significantly"
  },
  {
    "subject": "End-to-End Experiments",
    "predicate": "Minimizing",
    "object": "end-to-end overhead"
  },
  {
    "subject": "client",
    "predicate": "sends",
    "object": "inference task"
  },
  {
    "subject": "client",
    "predicate": "receives",
    "object": "reply"
  },
  {
    "subject": "GPU server",
    "predicate": "preempts",
    "object": "training task"
  },
  {
    "subject": "GPU server",
    "predicate": "executes",
    "object": "inference task"
  },
  {
    "subject": "GPU server",
    "predicate": "sends",
    "object": "reply"
  },
  {
    "subject": "GPU server",
    "predicate": "sends",
    "object": "reply to client"
  },
  {
    "subject": "We",
    "predicate": "compare",
    "object": "mechanisms"
  },
  {
    "subject": "model",
    "predicate": "is",
    "object": "ready"
  },
  {
    "subject": "There",
    "predicate": "is",
    "object": "no training task"
  },
  {
    "subject": "process",
    "predicate": "is loaded in",
    "object": "GPU"
  },
  {
    "subject": "required model",
    "predicate": "is loaded in",
    "object": "GPU"
  },
  {
    "subject": "solution",
    "predicate": "provides",
    "object": "lower bound"
  },
  {
    "subject": "lower bound",
    "predicate": "is",
    "object": "lowest latency"
  },
  {
    "subject": "lowest latency",
    "predicate": "achieve for",
    "object": "inference task"
  },
  {
    "subject": "Stop-and-start",
    "predicate": "is",
    "object": "a type of driving behavior"
  },
  {
    "subject": "solution",
    "predicate": "is used by",
    "object": "existing systems"
  },
  {
    "subject": "existing systems",
    "predicate": "like",
    "object": "Gandiva 24"
  },
  {
    "subject": "Gandiva 24",
    "predicate": "reported",
    "object": "similar second-scale overhead"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "is",
    "object": "a technology"
  },
  {
    "subject": "We",
    "predicate": "initialize",
    "object": "separate processes"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "is used for",
    "object": "memory swapping"
  },
  {
    "subject": "CUDA",
    "predicate": "has",
    "object": "unified memory"
  },
  {
    "subject": "CUDA Unified Memory",
    "predicate": "is",
    "object": "4"
  },
  {
    "subject": "https: devblogs.nvidia.comunified-memory-cuda- beginners",
    "predicate": "discuss",
    "object": "Unified Memory in CUDA"
  },
  {
    "subject": "This",
    "predicate": "is",
    "object": "proposed system"
  },
  {
    "subject": "properties",
    "predicate": "are described in",
    "object": "4"
  },
  {
    "subject": "14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "organized by",
    "object": "USENIX Association"
  },
  {
    "subject": "Latency",
    "predicate": "measured in",
    "object": "ms"
  },
  {
    "subject": "Ready model",
    "predicate": "included in",
    "object": "PipeSwitch MPS"
  },
  {
    "subject": "Ready model",
    "predicate": "included in",
    "object": "Stop-and-start"
  },
  {
    "subject": "Ready model",
    "predicate": "included in",
    "object": "ResNet152"
  },
  {
    "subject": "Ready model",
    "predicate": "included in",
    "object": "Inceptionv3"
  },
  {
    "subject": "Ready model",
    "predicate": "included in",
    "object": "Bertbase"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "equipped with",
    "object": "(NVIDIA V100, PCIe 3.0 16)"
  },
  {
    "subject": "Latency",
    "predicate": "is",
    "object": "5000 ms"
  },
  {
    "subject": "Latency",
    "predicate": "is",
    "object": "10000 ms"
  },
  {
    "subject": "Ready model",
    "predicate": "is",
    "object": "PipeSwitch"
  },
  {
    "subject": "Ready model",
    "predicate": "is",
    "object": "MPS"
  },
  {
    "subject": "Ready model",
    "predicate": "is",
    "object": "Stop-and-start"
  },
  {
    "subject": "Ready model",
    "predicate": "is",
    "object": "ResNet152"
  },
  {
    "subject": "Ready model",
    "predicate": "is",
    "object": "Inceptionv3"
  },
  {
    "subject": "Ready model",
    "predicate": "is",
    "object": "Bertbase"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has",
    "object": "NVIDIA T4"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has",
    "object": "PCIe 3.0 8"
  },
  {
    "subject": "ResNet152",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "Bertbase",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "0",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "20",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "40",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "60",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "80",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "100",
    "predicate": "Latency",
    "object": "ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "Per-layer pipeline",
    "object": "Grouped transmission"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "No optimization",
    "object": "(a) p3.2xlarge (NVIDIA V100, PCIe 3.0 16)"
  },
  {
    "subject": "ResNet152",
    "predicate": "has_latency",
    "object": "PipeSwitch Per-layer pipeline Grouped transmission No optimization (b) g4dn.2xlarge (NVIDIA T4, PCIe 3.0 8)"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "has_latency",
    "object": "PipeSwitch Per-layer pipeline Grouped transmission No optimization (b) g4dn.2xlarge (NVIDIA T4, PCIe 3.0 8)"
  },
  {
    "subject": "Bertbase",
    "predicate": "has_latency",
    "object": "PipeSwitch Per-layer pipeline Grouped transmission No optimization (b) g4dn.2xlarge (NVIDIA T4, PCIe 3.0 8)"
  },
  {
    "subject": "510 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "held",
    "object": "USENIX Association"
  },
  {
    "subject": "USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "featured",
    "object": "ResNet152"
  },
  {
    "subject": "USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "featured",
    "object": "Inceptionv3"
  },
  {
    "subject": "USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "featured",
    "object": "Bertbase"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has",
    "object": "NVIDIA V100"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has",
    "object": "PCIe 3.0 16"
  },
  {
    "subject": "ResNet152",
    "predicate": "has latency",
    "object": "0 ms"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "has latency",
    "object": "100 ms"
  },
  {
    "subject": "Bertbase",
    "predicate": "has latency",
    "object": "200 ms"
  },
  {
    "subject": "Latency",
    "predicate": "is measured in",
    "object": "ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "No memory management"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "No IPC optimization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "No pin memory"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "is used in",
    "object": "g4dn.2xlarge"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has GPU",
    "object": "NVIDIA T4"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has interface",
    "object": "PCIe 3.0 8"
  },
  {
    "subject": "Latency",
    "predicate": "measured in",
    "object": "ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "One process"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "Two processes"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "ResNet152"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "Inceptionv3"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "Bertbase"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has",
    "object": "NVIDIA V100"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has",
    "object": "PCIe 3.0 16"
  },
  {
    "subject": "Latency",
    "predicate": "measured in",
    "object": "ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "One process"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "Two processes"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "ResNet152"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "Inceptionv3"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "Bertbase"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "contains",
    "object": "NVIDIA T4"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "contains",
    "object": "PCIe 3.0 8"
  },
  {
    "subject": "ResNet152",
    "predicate": "has startup overhead",
    "object": "3.62 ms"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "has startup overhead",
    "object": "4.82 ms"
  },
  {
    "subject": "Bertbase",
    "predicate": "has startup overhead",
    "object": "3.62 ms"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has startup overhead",
    "object": "2.53 ms"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has startup overhead",
    "object": "6.57 ms"
  },
  {
    "subject": "We",
    "predicate": "show",
    "object": "task startup overhead for PipeSwitch in Table 4"
  },
  {
    "subject": "Table 4",
    "predicate": "is",
    "object": "difference between the time for ResNet152 Inceptionv3 Bertbase of Layers 464 189 139 Algorithm 1 1.33 s 0.18 s 0.34 s Only Pruning 1 2.09 s 0.30 s 0.88 s Only Pruning 2 3.44 h 5.07 s 24 h No Pruning 24 h 24 h 24 h"
  },
  {
    "subject": "Table 5",
    "predicate": "is",
    "object": "Effectiveness of two pruning techniques"
  },
  {
    "subject": "Salus 7",
    "predicate": "is",
    "object": "not directly comparable"
  },
  {
    "subject": "Salus 7",
    "predicate": "requires",
    "object": "models to be preloaded to the GPU"
  },
  {
    "subject": "Salus 7",
    "predicate": "has",
    "object": "several limitations"
  },
  {
    "subject": "limitations",
    "predicate": "described in",
    "object": "2.2"
  },
  {
    "subject": "performance",
    "predicate": "is similar to",
    "object": "ready model"
  },
  {
    "subject": "performance",
    "predicate": "is similar to",
    "object": "NVIDIA MPS"
  },
  {
    "subject": "model",
    "predicate": "is preloaded in",
    "object": "host memory"
  },
  {
    "subject": "total overhead",
    "predicate": "is",
    "object": "difference"
  },
  {
    "subject": "total overhead",
    "predicate": "is",
    "object": "latency"
  },
  {
    "subject": "total overhead",
    "predicate": "is",
    "object": "mechanism"
  },
  {
    "subject": "total overhead",
    "predicate": "is",
    "object": "ready model"
  },
  {
    "subject": "stop-and-start",
    "predicate": "performs",
    "object": "the worst"
  },
  {
    "subject": "stop-and-start",
    "predicate": "takes",
    "object": "several seconds"
  },
  {
    "subject": "source",
    "predicate": "is",
    "object": "CUDA context initialization and rst-time library loading operations"
  },
  {
    "subject": "source",
    "predicate": "is",
    "object": "main"
  },
  {
    "subject": "source",
    "predicate": "is",
    "object": "overhead"
  },
  {
    "subject": "operations",
    "predicate": "in",
    "object": "PyTorch"
  },
  {
    "subject": "source",
    "predicate": "is",
    "object": "GPU memory swapping"
  },
  {
    "subject": "overhead of PipeSwitch",
    "predicate": "is",
    "object": "up to 10ms"
  },
  {
    "subject": "BERT on T4",
    "predicate": "is",
    "object": "due to large model size and smaller PCIe bandwidth on T4 than that on V100"
  },
  {
    "subject": "it",
    "predicate": "takes",
    "object": "longer (120ms) to compute BERT on T4 even with the ready model"
  },
  {
    "subject": "relative overhead",
    "predicate": "is",
    "object": "acceptable"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "start computing",
    "object": "rst layer"
  },
  {
    "subject": "ready model",
    "predicate": "start computing",
    "object": null
  },
  {
    "subject": "startup overhead",
    "predicate": "is",
    "object": "only a few milliseconds"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "startup overhead"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "incurs",
    "object": "only a few milliseconds overhead for task switching"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "low latency close to the lower bound"
  },
  {
    "subject": "experiment",
    "predicate": "compare",
    "object": "throughput and end-to-end latency"
  },
  {
    "subject": "experiment",
    "predicate": "under",
    "object": "different scheduling cycles"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "ResNet152"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "ResNet152 for both training and inference"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "eight p3.2xlarge instances"
  },
  {
    "subject": "We",
    "predicate": "switch between",
    "object": "training and inference"
  },
  {
    "subject": "We",
    "predicate": "switch between",
    "object": "these two tasks"
  },
  {
    "subject": "We",
    "predicate": "switch after",
    "object": "each scheduling cycle"
  },
  {
    "subject": "Figure 6(a)",
    "predicate": "shows",
    "object": "the inference throughput"
  },
  {
    "subject": "dashed line",
    "predicate": "is",
    "object": "upper bound"
  },
  {
    "subject": "upper bound",
    "predicate": "is",
    "object": "throughput of the ready model"
  },
  {
    "subject": "throughput of the ready model",
    "predicate": "assuming",
    "object": "no task switching"
  },
  {
    "subject": "line",
    "predicate": "is",
    "object": "lower bound"
  },
  {
    "subject": "lower bound",
    "predicate": "is",
    "object": "average latency"
  },
  {
    "subject": "average latency",
    "predicate": "of",
    "object": "ready model"
  },
  {
    "subject": "ready model",
    "predicate": "assuming",
    "object": "no task switching"
  },
  {
    "subject": "throughput",
    "predicate": "is",
    "object": "nearly zero"
  },
  {
    "subject": "stop-and-start",
    "predicate": "has",
    "object": "throughput"
  },
  {
    "subject": "stop-and-start",
    "predicate": "is",
    "object": "nearly zero"
  },
  {
    "subject": "scheduling cycles",
    "predicate": "are",
    "object": "smaller than 10 s"
  },
  {
    "subject": "task switching",
    "predicate": "takes",
    "object": "several seconds"
  },
  {
    "subject": "MPS",
    "predicate": "keeps",
    "object": "poor throughput"
  },
  {
    "subject": "poor throughput",
    "predicate": "around",
    "object": "100 batches per second"
  },
  {
    "subject": "We",
    "predicate": "dene",
    "object": "GPU utilization"
  },
  {
    "subject": "GPU utilization",
    "predicate": "as",
    "object": "the ratio to the upper bound"
  },
  {
    "subject": "Figure 6(b)",
    "predicate": "shows",
    "object": "average latency"
  },
  {
    "subject": "average latency",
    "predicate": "of",
    "object": "inference tasks"
  },
  {
    "subject": "error bar",
    "predicate": "indicates",
    "object": "minimum and maximum latency"
  },
  {
    "subject": "Stop- and-start",
    "predicate": "has",
    "object": "poor latency"
  },
  {
    "subject": "rst batch",
    "predicate": "has",
    "object": "several seconds overhead"
  },
  {
    "subject": "MPS",
    "predicate": "has",
    "object": "about 80 ms average latency"
  },
  {
    "subject": "MPS",
    "predicate": "has",
    "object": "several hundred milliseconds latency for the rst batch"
  },
  {
    "subject": "Latency",
    "predicate": "is",
    "object": "7500"
  },
  {
    "subject": "Latency",
    "predicate": "is",
    "object": "10000"
  },
  {
    "subject": "PipeSwitch MPS",
    "predicate": "is",
    "object": "Stop-and-start"
  },
  {
    "subject": "Stop-and-start",
    "predicate": "has",
    "object": "1s"
  },
  {
    "subject": "Stop-and-start",
    "predicate": "has",
    "object": "2s"
  },
  {
    "subject": "Stop-and-start",
    "predicate": "has",
    "object": "5s"
  },
  {
    "subject": "Stop-and-start",
    "predicate": "has",
    "object": "10s"
  },
  {
    "subject": "Stop-and-start",
    "predicate": "has",
    "object": "30s"
  },
  {
    "subject": "Lower bound",
    "predicate": "is",
    "object": "Latency"
  },
  {
    "subject": "Figure 6",
    "predicate": "depicts",
    "object": "Throughput and latency"
  },
  {
    "subject": "Throughput and latency",
    "predicate": "measured for",
    "object": "ResNet on p3.2xlarge"
  },
  {
    "subject": "ResNet",
    "predicate": "run on",
    "object": "p3.2xlarge"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "used for",
    "object": "ResNet"
  },
  {
    "subject": "No",
    "predicate": "optimization",
    "object": null
  },
  {
    "subject": "No optimization",
    "predicate": "performs",
    "object": "the worst"
  },
  {
    "subject": "It",
    "predicate": "transmits",
    "object": "the model layer by layer (with many PCIe calls)"
  },
  {
    "subject": "It",
    "predicate": "executes",
    "object": "the task"
  },
  {
    "subject": "It",
    "predicate": "groups",
    "object": "the entire model"
  },
  {
    "subject": "It",
    "predicate": "executes",
    "object": "the task"
  },
  {
    "subject": "Per-layer pipeline",
    "predicate": "is",
    "object": "a type of pipeline"
  },
  {
    "subject": "It",
    "predicate": "transits",
    "object": "model parameters"
  },
  {
    "subject": "model parameters",
    "predicate": "layer by layer",
    "object": null
  },
  {
    "subject": "Computation",
    "predicate": "starts",
    "object": null
  },
  {
    "subject": "parameters",
    "predicate": "are transmitted",
    "object": null
  },
  {
    "subject": "Figure 7",
    "predicate": "shows",
    "object": "total time measured by the client"
  },
  {
    "subject": "total time measured by the client",
    "predicate": "for",
    "object": "an inference task"
  },
  {
    "subject": "inference task",
    "predicate": "to preempt",
    "object": "a training task"
  },
  {
    "subject": "inference task",
    "predicate": "to finish",
    "object": "its inference"
  },
  {
    "subject": "Figure 8",
    "predicate": "shows",
    "object": "total time measured by the client"
  },
  {
    "subject": "Grouped transmission",
    "predicate": "improves",
    "object": "no optimization"
  },
  {
    "subject": "Grouped transmission",
    "predicate": "combining",
    "object": "the layers of the model into one big tensor"
  },
  {
    "subject": "Grouped transmission",
    "predicate": "transmitting",
    "object": "it in one group"
  },
  {
    "subject": "pipeline",
    "predicate": "overlaps",
    "object": "transmission"
  },
  {
    "subject": "pipeline",
    "predicate": "overlaps",
    "object": "computation"
  },
  {
    "subject": "pipeline",
    "predicate": "overlaps",
    "object": "granularity"
  },
  {
    "subject": "granularity",
    "predicate": "of",
    "object": "layer"
  },
  {
    "subject": "it",
    "predicate": "has",
    "object": "PCIe overhead"
  },
  {
    "subject": "it",
    "predicate": "has",
    "object": "synchronization overhead"
  },
  {
    "subject": "models",
    "predicate": "have",
    "object": "many layers"
  },
  {
    "subject": "models",
    "predicate": "have",
    "object": "relatively light computation"
  },
  {
    "subject": "ResNet152 and Inception",
    "predicate": "are",
    "object": "models"
  },
  {
    "subject": "it",
    "predicate": "can perform",
    "object": "worse than grouped transmission"
  },
  {
    "subject": "it",
    "predicate": "can perform",
    "object": "sometimes even no pipeline"
  },
  {
    "subject": "It",
    "predicate": "reduces",
    "object": "the total time"
  },
  {
    "subject": "It",
    "predicate": "compared to",
    "object": "other solutions"
  },
  {
    "subject": "It",
    "predicate": "reduces",
    "object": "the latency by 116307 ms compared to one process"
  },
  {
    "subject": "It",
    "predicate": "reduces",
    "object": "the latency by 57 s compared to two processes"
  },
  {
    "subject": "reduction",
    "predicate": "is",
    "object": "significant"
  },
  {
    "subject": "optimizations",
    "predicate": "have been applied on",
    "object": "memory management"
  },
  {
    "subject": "optimizations",
    "predicate": "have been applied on",
    "object": "worker switching"
  },
  {
    "subject": "we",
    "predicate": "like",
    "object": "to emphasize"
  },
  {
    "subject": "we",
    "predicate": "meet",
    "object": "strict SLOs"
  },
  {
    "subject": "it",
    "predicate": "is",
    "object": "important"
  },
  {
    "subject": "it",
    "predicate": "reduce",
    "object": "all overheads"
  },
  {
    "subject": "overheads",
    "predicate": "for",
    "object": "task switching"
  },
  {
    "subject": "Table 5",
    "predicate": "shows",
    "object": "running time of Algorithm 1"
  },
  {
    "subject": "Table 5",
    "predicate": "shows",
    "object": "effects of two pruning techniques"
  },
  {
    "subject": "two pruning techniques",
    "predicate": "mentioned in",
    "object": "4.2"
  },
  {
    "subject": "number of layers",
    "predicate": "includes",
    "object": "weighted and unweighted layers"
  },
  {
    "subject": "weighted and unweighted layers",
    "predicate": "contribute to",
    "object": "computation time"
  },
  {
    "subject": "We",
    "predicate": "measure",
    "object": "parameter size"
  },
  {
    "subject": "We",
    "predicate": "measure",
    "object": "running time"
  },
  {
    "subject": "We",
    "predicate": "measure",
    "object": "each layer"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "takes",
    "object": "only several seconds to compute an optimal grouping strategy"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "takes",
    "object": "even for ResNet152 which has hundreds of layers"
  },
  {
    "subject": "pruning",
    "predicate": "does not finish",
    "object": "for all three models"
  },
  {
    "subject": "running",
    "predicate": "after",
    "object": "24 hours"
  },
  {
    "subject": "6.3",
    "predicate": "evaluate",
    "object": "effectiveness"
  },
  {
    "subject": "unified memory management",
    "predicate": "evaluate",
    "object": "effectiveness"
  },
  {
    "subject": "No",
    "predicate": "has",
    "object": "unified memory management"
  },
  {
    "subject": "Figure 8",
    "predicate": "depicts",
    "object": "Effectiveness of unified memory management"
  },
  {
    "subject": "we",
    "predicate": "keep",
    "object": "all other components of PipeSwitch the same"
  },
  {
    "subject": "we",
    "predicate": "compare",
    "object": "ve mechanisms discussed in 4.3"
  },
  {
    "subject": "No IPC optimization",
    "predicate": "is",
    "object": null
  },
  {
    "subject": "It",
    "predicate": "has",
    "object": "all optimizations on unified memory management"
  },
  {
    "subject": "pages",
    "predicate": "are not pinned to",
    "object": "main memory"
  },
  {
    "subject": "experiment",
    "predicate": "demonstrates",
    "object": "optimizations on memory management"
  },
  {
    "subject": "optimizations",
    "predicate": "are",
    "object": "effective"
  },
  {
    "subject": "unified memory management mechanism",
    "predicate": "used by",
    "object": "PipeSwitch"
  },
  {
    "subject": "IPC optimization",
    "predicate": "is",
    "object": "important"
  },
  {
    "subject": "IPC optimization",
    "predicate": "reduces",
    "object": "latency"
  },
  {
    "subject": "IPC optimization",
    "predicate": "by",
    "object": "1648 ms"
  },
  {
    "subject": "pinning",
    "predicate": "reduce",
    "object": "latency"
  },
  {
    "subject": "pinning",
    "predicate": "with",
    "object": "few milliseconds"
  },
  {
    "subject": "pages",
    "predicate": "pinned to",
    "object": "host memory"
  },
  {
    "subject": "Two processes",
    "predicate": "exist",
    "object": null
  },
  {
    "subject": "One process",
    "predicate": "is",
    "object": "being discussed"
  },
  {
    "subject": "Figure 9",
    "predicate": "depicts",
    "object": "Effectiveness of active-standby switching"
  },
  {
    "subject": "Figure 9",
    "predicate": "shows",
    "object": "the results"
  },
  {
    "subject": "Two processes",
    "predicate": "perform",
    "object": "the worst"
  },
  {
    "subject": "it",
    "predicate": "stops",
    "object": "the training task"
  },
  {
    "subject": "it",
    "predicate": "initializes",
    "object": "a new process"
  },
  {
    "subject": "a new process",
    "predicate": "for",
    "object": "the new task"
  },
  {
    "subject": "new process",
    "predicate": "needs to create",
    "object": "new CUDA environment"
  },
  {
    "subject": "new CUDA environment",
    "predicate": "dominates",
    "object": "total time"
  },
  {
    "subject": "process",
    "predicate": "reuses",
    "object": "CUDA environment"
  },
  {
    "subject": "process",
    "predicate": "pays",
    "object": "overhead"
  },
  {
    "subject": "process",
    "predicate": "cleans",
    "object": "environment"
  },
  {
    "subject": "Many frameworks",
    "predicate": "have been developed for",
    "object": "deep learning"
  },
  {
    "subject": "Many frameworks",
    "predicate": "include",
    "object": "TensorFlow 25"
  },
  {
    "subject": "Many frameworks",
    "predicate": "include",
    "object": "PyTorch 21"
  },
  {
    "subject": "Many frameworks",
    "predicate": "include",
    "object": "MXNet 26"
  },
  {
    "subject": "algorithms and systems",
    "predicate": "have been designed for",
    "object": "executing and scheduling deep learning tasks on clusters"
  },
  {
    "subject": "algorithms and systems",
    "predicate": "include",
    "object": "training and inference tasks"
  },
  {
    "subject": "scheduling solutions",
    "predicate": "are",
    "object": "orthogonal and complementary to PipeSwitch"
  },
  {
    "subject": "They",
    "predicate": "focus on",
    "object": "what scheduling decisions to make"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "focuses on",
    "object": "how to realize a scheduling decision"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "scheduler"
  },
  {
    "subject": "scheduler",
    "predicate": "change",
    "object": "resource allocation"
  },
  {
    "subject": "scheduler",
    "predicate": "more often with",
    "object": "millisecond-scale task switching"
  },
  {
    "subject": "techniques and systems",
    "predicate": "have been proposed to",
    "object": "optimize communication and improve distributed training"
  },
  {
    "subject": "techniques and systems",
    "predicate": "have been proposed to",
    "object": "optimize communication"
  },
  {
    "subject": "techniques and systems",
    "predicate": "have been proposed to",
    "object": "improve distributed training"
  },
  {
    "subject": "relevant ones",
    "predicate": "are",
    "object": "PipeDream 8"
  },
  {
    "subject": "relevant ones",
    "predicate": "are",
    "object": "ByteScheduler 9"
  },
  {
    "subject": "relevant ones",
    "predicate": "are",
    "object": "Poseidon 40"
  },
  {
    "subject": "vDNN",
    "predicate": "have",
    "object": "GPU memory management module"
  },
  {
    "subject": "SwapAdvisor",
    "predicate": "have",
    "object": "GPU memory management module"
  },
  {
    "subject": "vDNN",
    "predicate": "focus on",
    "object": "memory management for a single training task of large models"
  },
  {
    "subject": "SwapAdvisor",
    "predicate": "focus on",
    "object": "memory management for a single training task of large models"
  },
  {
    "subject": "vDNN",
    "predicate": "not directly comparable to",
    "object": "PipeSwitch"
  },
  {
    "subject": "SwapAdvisor",
    "predicate": "not directly comparable to",
    "object": "PipeSwitch"
  },
  {
    "subject": "Cluster managers",
    "predicate": "allocate",
    "object": "GPUs"
  },
  {
    "subject": "Cluster managers",
    "predicate": "allocate",
    "object": "VMs"
  },
  {
    "subject": "Cluster managers",
    "predicate": "allocate",
    "object": "containers"
  },
  {
    "subject": "Cluster managers",
    "predicate": "allocate",
    "object": "device granularity"
  },
  {
    "subject": "solutions",
    "predicate": "have been proposed",
    "object": "to share a GPU at application granularity using techniques like library interception"
  },
  {
    "subject": "techniques",
    "predicate": "using",
    "object": "library interception"
  },
  {
    "subject": "They",
    "predicate": "are",
    "object": "general-purpose"
  },
  {
    "subject": "They",
    "predicate": "focus on",
    "object": "sharing only a few kernels"
  },
  {
    "subject": "they",
    "predicate": "are",
    "object": "not suitable for deep learning applications"
  },
  {
    "subject": "deep learning applications",
    "predicate": "require",
    "object": "hundreds of kernels"
  },
  {
    "subject": "It",
    "predicate": "not designed for",
    "object": "deep learning"
  },
  {
    "subject": "It",
    "predicate": "cannot meet",
    "object": "strict SLOs of inference tasks"
  },
  {
    "subject": "efforts",
    "predicate": "on",
    "object": "GPU optimization"
  },
  {
    "subject": "efforts",
    "predicate": "improve",
    "object": "performance"
  },
  {
    "subject": "performance",
    "predicate": "of",
    "object": "running a single task"
  },
  {
    "subject": "efforts",
    "predicate": "include",
    "object": "tensor fusion"
  },
  {
    "subject": "efforts",
    "predicate": "include",
    "object": "kernel-level concurrency"
  },
  {
    "subject": "efforts",
    "predicate": "include",
    "object": "scheduling"
  },
  {
    "subject": "Acknowledgments",
    "predicate": "are",
    "object": "not provided"
  },
  {
    "subject": "We",
    "predicate": "thank",
    "object": "our shepherd Madan Musu- vathi"
  },
  {
    "subject": "We",
    "predicate": "thank",
    "object": "the anonymous reviewers"
  },
  {
    "subject": "the anonymous reviewers",
    "predicate": "provide",
    "object": "valuable feed- back"
  },
  {
    "subject": "Zhihao Bai",
    "predicate": "were supported by",
    "object": "an AWS Machine Learning Research Award"
  },
  {
    "subject": "Zhen Zhang",
    "predicate": "were supported by",
    "object": "an AWS Machine Learning Research Award"
  },
  {
    "subject": "Xin Jin",
    "predicate": "were supported by",
    "object": "an AWS Machine Learning Research Award"
  },
  {
    "subject": "A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes",
    "predicate": "authored",
    "object": "Large-scale cluster management at Google with Borg"
  },
  {
    "subject": "Large-scale cluster management at Google with Borg",
    "predicate": "presented at",
    "object": "EuroSys"
  },
  {
    "subject": "EuroSys",
    "predicate": "year",
    "object": "2015"
  },
  {
    "subject": "2 J.",
    "predicate": "is",
    "object": "a numerical value"
  },
  {
    "subject": "J.",
    "predicate": null,
    "object": null
  },
  {
    "subject": "Dean and L. A. Barroso",
    "predicate": "authored",
    "object": "The tail at scale"
  },
  {
    "subject": "The tail at scale",
    "predicate": "published in",
    "object": "Communications of the ACM"
  },
  {
    "subject": "The tail at scale",
    "predicate": "has volume",
    "object": "vol."
  },
  {
    "subject": "56",
    "predicate": "was published in",
    "object": "2013"
  },
  {
    "subject": "41",
    "predicate": "was published in",
    "object": "2013"
  },
  {
    "subject": "H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Phili- pose, A. Krishnamurthy, and R. Sundaram",
    "predicate": "are authors of",
    "object": "Nexus: A GPU cluster engine for accelerating DNN-based video analysis"
  },
  {
    "subject": "Nexus: A GPU cluster engine for accelerating DNN-based video analysis",
    "predicate": "was presented at",
    "object": "ACM SOSP, 2019"
  },
  {
    "subject": "5 A",
    "predicate": "authored",
    "object": "Ousterhout, J."
  },
  {
    "subject": "Fried, J. Behrens, A. Belay, and H. Bal-akrishnan",
    "predicate": "authored",
    "object": "Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads"
  },
  {
    "subject": "Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads",
    "predicate": "presented at",
    "object": "USENIX NSDI"
  },
  {
    "subject": "USENIX NSDI",
    "predicate": "year",
    "object": "2019"
  },
  {
    "subject": "CUDA Multi-Process Service",
    "predicate": "is",
    "object": "6"
  },
  {
    "subject": "text",
    "predicate": "is located at",
    "object": "https: docs.nvidia.comdeploypdf CUDAMultiProcessServiceOverview.pdf"
  },
  {
    "subject": "P. Yu and M. Chowdhury",
    "predicate": "authored",
    "object": "Salus: Fine-grained GPU sharing primitives for deep learning applications"
  },
  {
    "subject": "Salus: Fine-grained GPU sharing primitives for deep learning applications",
    "predicate": "presented at",
    "object": "Conference on Machine Learning and Systems"
  },
  {
    "subject": "Conference on Machine Learning and Systems",
    "predicate": "held in",
    "object": "2020"
  },
  {
    "subject": "D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia",
    "predicate": "authored",
    "object": "PipeDream: generalized pipeline parallelism for DNN training"
  },
  {
    "subject": "PipeDream: generalized pipeline parallelism for DNN training",
    "predicate": "presented at",
    "object": "ACM SOSP, 2019"
  },
  {
    "subject": "Y. Peng",
    "predicate": "is author of",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "Y. Zhu",
    "predicate": "is author of",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "Y. Chen",
    "predicate": "is author of",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "Y. Bao",
    "predicate": "is author of",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "B. Yi",
    "predicate": "is author of",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "C. Lan",
    "predicate": "is author of",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "C. Wu",
    "predicate": "is author of",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "C. Guo",
    "predicate": "is author of",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "A generic communication scheduler for distributed DNN training acceleration",
    "predicate": "was presented at",
    "object": "ACM SOSP"
  },
  {
    "subject": "A generic communication scheduler for distributed DNN training acceleration",
    "predicate": "was published in",
    "object": "2019"
  },
  {
    "subject": "J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo",
    "predicate": "authored",
    "object": "Tiresias: A GPU cluster manager for distributed deep learning"
  },
  {
    "subject": "Tiresias: A GPU cluster manager for distributed deep learning",
    "predicate": "presented at",
    "object": "USENIX NSDI"
  },
  {
    "subject": "USENIX NSDI",
    "predicate": "held in",
    "object": "2019"
  },
  {
    "subject": "H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie, and E. P. Xing",
    "predicate": "authored",
    "object": "Poseidon: An efficient communication architecture for distributed deep learning on GPU clusters"
  },
  {
    "subject": "Poseidon: An efficient communication architecture for distributed deep learning on GPU clusters",
    "predicate": "presented at",
    "object": "USENIX ATC"
  },
  {
    "subject": "USENIX ATC",
    "predicate": "held in",
    "object": "2017"
  },
  {
    "subject": "12",
    "predicate": "is",
    "object": "Amazon Web Services"
  },
  {
    "subject": "https:aws.amazon.com",
    "predicate": "is",
    "object": "a website"
  },
  {
    "subject": "13",
    "predicate": "is",
    "object": "Microsoft Azure"
  },
  {
    "subject": "https:azure.microsoft.com",
    "predicate": "is",
    "object": "a website"
  },
  {
    "subject": "14",
    "predicate": "is",
    "object": "Google Cloud Platform"
  },
  {
    "subject": "https",
    "predicate": "is",
    "object": "cloud.google.com"
  },
  {
    "subject": "A. Sergeev and M. Del Balso",
    "predicate": "developed",
    "object": "Horovod"
  },
  {
    "subject": "Horovod",
    "predicate": "is",
    "object": "fast and easy distributed deep learning in tensorow"
  },
  {
    "subject": "Horovod",
    "predicate": "was published in",
    "object": "arXiv preprint arXiv:1802.05799"
  },
  {
    "subject": "arXiv preprint arXiv:1802.05799",
    "predicate": "was published in",
    "object": "2018"
  },
  {
    "subject": "M. Li",
    "predicate": "is",
    "object": "16"
  },
  {
    "subject": "M. Li",
    "predicate": "collaborated with",
    "object": "D. G. Andersen"
  },
  {
    "subject": "M. Li",
    "predicate": "collaborated with",
    "object": "J. W. Park"
  },
  {
    "subject": "M. Li",
    "predicate": "collaborated with",
    "object": "A. J. Smola"
  },
  {
    "subject": "M. Li",
    "predicate": "collaborated with",
    "object": "A. Ahmed"
  },
  {
    "subject": "M. Li",
    "predicate": "collaborated with",
    "object": "V. Josifovski"
  },
  {
    "subject": "Long, E. J.",
    "predicate": "is",
    "object": "Shekita"
  },
  {
    "subject": "Long, E. J.",
    "predicate": "is",
    "object": "B.-Y."
  },
  {
    "subject": "Su",
    "predicate": "Scaling distributed machine learning with",
    "object": "the parameter server"
  },
  {
    "subject": "Su",
    "predicate": "published in",
    "object": "USENIX OSDI"
  },
  {
    "subject": "Su",
    "predicate": "published in",
    "object": "2014"
  },
  {
    "subject": "K. He",
    "predicate": "is",
    "object": "a researcher"
  },
  {
    "subject": "X. Zhang",
    "predicate": "is",
    "object": "a researcher"
  },
  {
    "subject": "S. Ren",
    "predicate": "is",
    "object": "a researcher"
  },
  {
    "subject": "J.",
    "predicate": "is",
    "object": "a researcher"
  },
  {
    "subject": "Sun",
    "predicate": "published",
    "object": "Deep residual learning for image recognition"
  },
  {
    "subject": "Deep residual learning for image recognition",
    "predicate": "presented at",
    "object": "IEEE Conference on Computer Vision and Pattern Recognition"
  },
  {
    "subject": "IEEE Conference on Computer Vision and Pattern Recognition",
    "predicate": "held in",
    "object": "2016"
  },
  {
    "subject": "Nvidia data center deep learning product",
    "predicate": "has",
    "object": "performance"
  },
  {
    "subject": "NVIDIA",
    "predicate": "offers",
    "object": "deep learning performance training and inference"
  },
  {
    "subject": "20 Philly",
    "predicate": "traces",
    "object": null
  },
  {
    "subject": "https:github.com/msr-fiddle",
    "predicate": "contains",
    "object": "philly-traces"
  },
  {
    "subject": "21",
    "predicate": "is",
    "object": "PyTorch"
  },
  {
    "subject": "C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna",
    "predicate": "authored",
    "object": "Rethinking the inception architecture for computer vision"
  },
  {
    "subject": "Rethinking the inception architecture for computer vision",
    "predicate": "presented at",
    "object": "IEEE Conference on Computer Vision and Pattern Recognition"
  },
  {
    "subject": "IEEE Conference on Computer Vision and Pattern Recognition",
    "predicate": "took place in",
    "object": "2016"
  },
  {
    "subject": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
    "predicate": "authored",
    "object": "BERT: Pre-training of deep bidirectional transformers for language understanding"
  },
  {
    "subject": "BERT: Pre-training of deep bidirectional transformers for language understanding",
    "predicate": "presented at",
    "object": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019"
  },
  {
    "subject": "24 W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P. Patel, X. Peng, H. Zhao, Q. Zhang, et al.",
    "predicate": "published",
    "object": "Gandiva: Introspective cluster scheduling for deep learning"
  },
  {
    "subject": "Gandiva: Introspective cluster scheduling for deep learning",
    "predicate": "presented at",
    "object": "USENIX OSDI"
  },
  {
    "subject": "USENIX OSDI",
    "predicate": "held in",
    "object": "2018"
  },
  {
    "subject": "25",
    "predicate": "is",
    "object": "TensorFlow"
  },
  {
    "subject": "TensorFlow XLA",
    "predicate": "is",
    "object": "54"
  },
  {
    "subject": "https:www.tensorflow.org",
    "predicate": "is",
    "object": "a website"
  },
  {
    "subject": "https:www.tensorflow.org",
    "predicate": "is",
    "object": "xla"
  },
  {
    "subject": "26",
    "predicate": "is",
    "object": "MXNet"
  },
  {
    "subject": "https:mxnet.apache.org",
    "predicate": "is",
    "object": "a website"
  },
  {
    "subject": "27 H. Zhang",
    "predicate": "is",
    "object": "L. Stafman"
  },
  {
    "subject": "27 H. Zhang",
    "predicate": "is",
    "object": "A."
  },
  {
    "subject": "M. J. Freedman",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "Slaq",
    "predicate": "is",
    "object": "quality-driven scheduling system"
  },
  {
    "subject": "Slaq",
    "predicate": "is used for",
    "object": "distributed machine learning"
  },
  {
    "subject": "ACM Symposium on Cloud Computing",
    "predicate": "took place in",
    "object": "2017"
  },
  {
    "subject": "Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo",
    "predicate": "are authors of",
    "object": "Optimus: an efficient dynamic resource scheduler for deep learning clusters"
  },
  {
    "subject": "Optimus: an efficient dynamic resource scheduler for deep learning clusters",
    "predicate": "was presented at",
    "object": "EuroSys"
  },
  {
    "subject": "Optimus: an efficient dynamic resource scheduler for deep learning clusters",
    "predicate": "was presented in",
    "object": "2018"
  },
  {
    "subject": "K. Mahajan",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. Balasubramanian",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. Singhvi",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "S. Venkataraman",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. Akella",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. Phanishayee",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "S. Chawla",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "Themis",
    "predicate": "is",
    "object": "title"
  },
  {
    "subject": "Themis",
    "predicate": "is",
    "object": "system"
  },
  {
    "subject": "Themis",
    "predicate": "is",
    "object": "related to GPU cluster scheduling"
  },
  {
    "subject": "USENIX NSDI",
    "predicate": "is",
    "object": "conference"
  },
  {
    "subject": "2020",
    "predicate": "is",
    "object": "year"
  },
  {
    "subject": "R. Liaw",
    "predicate": "is author of",
    "object": "HyperSched: Dynamic resource reallocation for model development on a deadline"
  },
  {
    "subject": "R. Bhardwaj",
    "predicate": "is author of",
    "object": "HyperSched: Dynamic resource reallocation for model development on a deadline"
  },
  {
    "subject": "L. Dunlap",
    "predicate": "is author of",
    "object": "HyperSched: Dynamic resource reallocation for model development on a deadline"
  },
  {
    "subject": "Y. Zou",
    "predicate": "is author of",
    "object": "HyperSched: Dynamic resource reallocation for model development on a deadline"
  },
  {
    "subject": "J. E. Gonzalez",
    "predicate": "is author of",
    "object": "HyperSched: Dynamic resource reallocation for model development on a deadline"
  },
  {
    "subject": "I. Stoica",
    "predicate": "is author of",
    "object": "HyperSched: Dynamic resource reallocation for model development on a deadline"
  },
  {
    "subject": "A. Tumanov",
    "predicate": "is author of",
    "object": "HyperSched: Dynamic resource reallocation for model development on a deadline"
  },
  {
    "subject": "HyperSched: Dynamic resource reallocation for model development on a deadline",
    "predicate": "was presented at",
    "object": "ACM Symposium on Cloud Computing"
  },
  {
    "subject": "ACM Symposium on Cloud Computing",
    "predicate": "took place in",
    "object": "2019"
  },
  {
    "subject": "R. Dathathri",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "O. Saarikivi",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "H. Chen",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "K. Laine",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "K. Lauter",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "S. Maleki",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "M. Musuvathi",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "T. Mytkowicz",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "CHET",
    "predicate": "is",
    "object": "optimizing compiler"
  },
  {
    "subject": "CHET",
    "predicate": "is",
    "object": "for fully-homomorphic neural-network inferencing"
  },
  {
    "subject": "ACM Conference on Programming Language Design and Implementation",
    "predicate": "held",
    "object": "2019"
  },
  {
    "subject": "T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze",
    "predicate": "authored",
    "object": "TVM: An automated end-to-end optimizing compiler for deep learning"
  },
  {
    "subject": "TVM: An automated end-to-end optimizing compiler for deep learning",
    "predicate": "presented at",
    "object": "USENIX OSDI"
  },
  {
    "subject": "TVM: An automated end-to-end optimizing compiler for deep learning",
    "predicate": "published in",
    "object": "2018"
  },
  {
    "subject": "Gpipe",
    "predicate": "is",
    "object": "efficient training of giant neural networks using pipeline parallelism"
  },
  {
    "subject": "Y. Huang",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "Y. Cheng",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "A. Bapna",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "O. Firat",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "D. Chen",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "M. Chen",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "H. Lee",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "J. Ngiam",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "Q. V. Le",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "Y. Wu",
    "predicate": "contributed to",
    "object": "Gpipe"
  },
  {
    "subject": "Advances in Neural Information Processing Systems",
    "predicate": "was published in",
    "object": "2019"
  },
  {
    "subject": "G. Wang",
    "predicate": "is author of",
    "object": "Blink: Fast and generic collectives for distributed ML"
  },
  {
    "subject": "S. Venkataraman",
    "predicate": "is author of",
    "object": "Blink: Fast and generic collectives for distributed ML"
  },
  {
    "subject": "A. Phanishayee",
    "predicate": "is author of",
    "object": "Blink: Fast and generic collectives for distributed ML"
  },
  {
    "subject": "J. Thelin",
    "predicate": "is author of",
    "object": "Blink: Fast and generic collectives for distributed ML"
  },
  {
    "subject": "N. Devanur",
    "predicate": "is author of",
    "object": "Blink: Fast and generic collectives for distributed ML"
  },
  {
    "subject": "I. Stoica",
    "predicate": "is author of",
    "object": "Blink: Fast and generic collectives for distributed ML"
  },
  {
    "subject": "Blink: Fast and generic collectives for distributed ML",
    "predicate": "was published in",
    "object": "Conference on Machine Learning and Systems"
  },
  {
    "subject": "Conference on Machine Learning and Systems",
    "predicate": "took place in",
    "object": "2020"
  },
  {
    "subject": "35 NVIDIA Collective Communications Library",
    "predicate": "is",
    "object": "NCCL"
  },
  {
    "subject": "https:developer.nvidia.comnccl",
    "predicate": "is",
    "object": "a website"
  },
  {
    "subject": "J. Liu, J. Wu, and D. K. Panda",
    "predicate": "developed",
    "object": "High performance RDMA-based MPI implementation"
  },
  {
    "subject": "High performance RDMA-based MPI implementation",
    "predicate": "is",
    "object": "over Infiniband"
  },
  {
    "subject": "Parallel Program",
    "predicate": "is a volume of",
    "object": "journal"
  },
  {
    "subject": "32",
    "predicate": "was published on",
    "object": "2004"
  },
  {
    "subject": "Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing",
    "predicate": "authored",
    "object": "More effective distributed ML via a stale synchronous parallel parameter server"
  },
  {
    "subject": "More effective distributed ML via a stale synchronous parallel parameter server",
    "predicate": "published in",
    "object": "Advances in Neural Information Processing Systems"
  },
  {
    "subject": "Advances in Neural Information Processing Systems",
    "predicate": "year",
    "object": "2013"
  },
  {
    "subject": "38 A",
    "predicate": "is",
    "object": "number"
  },
  {
    "subject": "A. Awan, C.-H. Chu, H. Subramoni, and D. K. Panda",
    "predicate": "authored",
    "object": "Optimized broadcast for deep learning workloads on dense-GPU inniband clusters: MPI or NCCL?"
  },
  {
    "subject": "Optimized broadcast for deep learning workloads on dense-GPU inniband clusters: MPI or NCCL?",
    "predicate": "presented at",
    "object": "Proceedings of the 25th European MPI Users Group Meeting, 2018"
  },
  {
    "subject": "39 J",
    "predicate": "is",
    "object": "a number"
  },
  {
    "subject": "A. Vishnu, C. Siegel, T. Warfel, and V. Am- atya",
    "predicate": "authored",
    "object": "GossipGraD: Scalable deep learning using gossip communication based asynchronous gradient descent"
  },
  {
    "subject": "GossipGraD: Scalable deep learning using gossip communication based asynchronous gradient descent",
    "predicate": "published in",
    "object": "CoRR"
  },
  {
    "subject": "CoRR",
    "predicate": "has volume",
    "object": "vol."
  },
  {
    "subject": "abs1803.05880",
    "predicate": "published in",
    "object": "2018"
  },
  {
    "subject": "Z. Zhang",
    "predicate": "Is",
    "object": "network the bottleneck of distributed training?"
  },
  {
    "subject": "Z. Zhang",
    "predicate": "presented at",
    "object": "ACM SIGCOMM Workshop on Network Meets AI ML (NetAI)"
  },
  {
    "subject": "Z. Zhang",
    "predicate": "date",
    "object": "August 2020"
  },
  {
    "subject": "Y. Chen, Z. Liu, B. Ren, and X. Jin",
    "predicate": "published",
    "object": "On efficient constructions of checkpoints"
  },
  {
    "subject": "Y. Chen, Z. Liu, B. Ren, and X. Jin",
    "predicate": "presented at",
    "object": "International Conference on Machine Learning (ICML)"
  },
  {
    "subject": "International Conference on Machine Learning (ICML)",
    "predicate": "held in",
    "object": "July 2020"
  },
  {
    "subject": "M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler",
    "predicate": "published",
    "object": "vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design"
  },
  {
    "subject": "M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler",
    "predicate": "presented at",
    "object": "2016 49th Annual IEEEACM International Symposium on Microarchitecture (MICRO), 2016"
  },
  {
    "subject": "C.-C. Huang, G. Jin, and J. Li",
    "predicate": "authored",
    "object": "SwapAdvisor: Pushing deep learning beyond the GPU memory limit via smart swapping"
  },
  {
    "subject": "SwapAdvisor: Pushing deep learning beyond the GPU memory limit via smart swapping",
    "predicate": "presented at",
    "object": "ACM ASPLOS"
  },
  {
    "subject": "ACM ASPLOS",
    "predicate": "held in",
    "object": "2020"
  },
  {
    "subject": "https:kubernetes.io",
    "predicate": "is",
    "object": "a website"
  },
  {
    "subject": "NVIDIA Container Runtime",
    "predicate": "for",
    "object": "Docker"
  },
  {
    "subject": "https: github.comNVIDIAnvidia-docker",
    "predicate": "is",
    "object": "a repository"
  },
  {
    "subject": "https: github.comNVIDIAnvidia-docker",
    "predicate": "belongs to",
    "object": "NVIDIA"
  },
  {
    "subject": "B. Hindman",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. Konwinski",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "M. Zaharia",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. Ghodsi",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. D. Joseph",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "R. H. Katz",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "S. Shenker",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "I. Stoica",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "Mesos",
    "predicate": "is",
    "object": "platform"
  },
  {
    "subject": "Mesos",
    "predicate": "provides",
    "object": "fine-grained resource sharing"
  },
  {
    "subject": "Mesos",
    "predicate": "is",
    "object": "used in data center"
  },
  {
    "subject": "USENIX NSDI",
    "predicate": "is",
    "object": "conference"
  },
  {
    "subject": "2011",
    "predicate": "is",
    "object": "year"
  },
  {
    "subject": "V. K. Vavilapalli",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "A. C. Murthy",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "C. Douglas",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "S. Agarwal",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "M. Konar",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "R. Evans",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "T. Graves",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "J. Lowe",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "H. Shah",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "S. Seth",
    "predicate": "is",
    "object": "author"
  },
  {
    "subject": "Apache Hadoop YARN",
    "predicate": "is",
    "object": "title"
  },
  {
    "subject": "ACM Symposium on Cloud Computing",
    "predicate": "is",
    "object": "conference"
  },
  {
    "subject": "2013",
    "predicate": "is",
    "object": "year"
  },
  {
    "subject": "G. Giunta, R. Montella, G. Agrillo, and G. Coviello",
    "predicate": "published",
    "object": "A GPGPU transparent virtualization component for high performance computing clouds"
  },
  {
    "subject": "European Conference on Parallel Processing",
    "predicate": "year",
    "object": 2010
  },
  {
    "subject": "V. T. Ravi, M. Becchi, G. Agrawal, and S. Chakradhar",
    "predicate": "are authors of",
    "object": "Supporting GPU sharing in cloud environments with a transparent runtime consolidation framework"
  },
  {
    "subject": "Supporting GPU sharing in cloud environments with a transparent runtime consolidation framework",
    "predicate": "was published in",
    "object": "Proceedings of the 20th international symposium on High performance distributed computing"
  },
  {
    "subject": "Proceedings of the 20th international symposium on High performance distributed computing",
    "predicate": "took place in",
    "object": "2011"
  },
  {
    "subject": "V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan",
    "predicate": "are authors of",
    "object": "GViM: GPU-accelerated virtual machines"
  },
  {
    "subject": "GViM: GPU-accelerated virtual machines",
    "predicate": "was presented at",
    "object": "Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing"
  },
  {
    "subject": "Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing",
    "predicate": "took place in",
    "object": "2009"
  },
  {
    "subject": "J. Duato, A. J. Pena, F. Silla, R. Mayo, and E. S. Quintana-Ort",
    "predicate": "are authors of",
    "object": "rCUDA: Reducing the number of GPU-based accelerators in high performance clusters"
  },
  {
    "subject": "rCUDA: Reducing the number of GPU-based accelerators in high performance clusters",
    "predicate": "was presented at",
    "object": "2010 International Conference on High Performance Computing Simulation"
  },
  {
    "subject": "Sun, and K. Li",
    "predicate": "published",
    "object": "vCUDA: GPU-accelerated high-performance computing in virtual machines"
  },
  {
    "subject": "vCUDA: GPU-accelerated high-performance computing in virtual machines",
    "predicate": "published in",
    "object": "IEEE Transactions on Computers"
  },
  {
    "subject": "IEEE Transactions on Computers",
    "predicate": "has volume",
    "object": "vol."
  },
  {
    "subject": "53 L. Shi",
    "predicate": "is",
    "object": "H. Chen"
  },
  {
    "subject": "53 L. Shi",
    "predicate": "is",
    "object": "J."
  },
  {
    "subject": "61",
    "predicate": "was published in",
    "object": "2011"
  },
  {
    "subject": "T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang",
    "predicate": "authored",
    "object": "MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems"
  },
  {
    "subject": "T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang",
    "predicate": "published",
    "object": "arXiv preprint arXiv:1512.01274, 2015"
  },
  {
    "subject": "C. Gregg, J. Dorn, K. Hazelwood, and K. Skadron",
    "predicate": "presented",
    "object": "as part of the 4th USENIX Workshop on Hot Topics in Parallelism"
  },
  {
    "subject": "4th USENIX Workshop on Hot Topics in Parallelism",
    "predicate": "held",
    "object": "in 2012"
  },
  {
    "subject": "S. Pai, M. J. Thazhuthaveetil, and R. Govindarajan",
    "predicate": "authored",
    "object": "Improving GPGPU concurrency with elastic kernels"
  },
  {
    "subject": "Improving GPGPU concurrency with elastic kernels",
    "predicate": "published in",
    "object": "ACM SIGARCH Computer Architecture News"
  },
  {
    "subject": "ACM SIGARCH Computer Architecture News",
    "predicate": "has volume",
    "object": "57"
  },
  {
    "subject": "Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia, and A. Aiken",
    "predicate": "published",
    "object": "TASO: optimizing deep learning computation with automatic generation of graph substitutions"
  },
  {
    "subject": "TASO: optimizing deep learning computation with automatic generation of graph substitutions",
    "predicate": "presented at",
    "object": "ACM SOSP"
  },
  {
    "subject": "ACM SOSP",
    "predicate": "held in",
    "object": "2019"
  }
]