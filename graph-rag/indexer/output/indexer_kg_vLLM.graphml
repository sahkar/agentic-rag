<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d0" for="edge" attr.name="relationship" attr.type="string"/>
<graph edgedefault="directed"><node id="EFFICIENT MEMORY MANAGEMENT FOR LARGE LANGUAGE MODEL SERVING WITH PAGEDATTENTION"/>
<node id="WOOSUK KWON, ZHUOHAN LI, SIYUAN ZHUANG, YING SHENG, LIANMIN ZHENG, CODY HAO YU, JOSEPH E. GONZALEZ, HAO ZHANG, ION STOICA"/>
<node id="WOOSUK KWON"/>
<node id="UC BERKELEY"/>
<node id="ZHUOHAN LI"/>
<node id="SIYUAN ZHUANG"/>
<node id="YING SHENG"/>
<node id="UC BERKELEY and STANFORD UNIVERSITY"/>
<node id="LIANMIN ZHENG"/>
<node id="CODY HAO YU"/>
<node id="INDEPENDENT RESEARCHER"/>
<node id="JOSEPH E. GONZALEZ"/>
<node id="HAO ZHANG"/>
<node id="UC SAN DIEGO"/>
<node id="ION STOICA"/>
<node id="HIGH THROUGHPUT SERVING OF LARGE LANGUAGE MODELS (LLMS)"/>
<node id="batching sufficiently many requests at a time"/>
<node id="existing systems"/>
<node id="the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically"/>
<node id="KV cache size"/>
<node id="number of requests"/>
<node id="memory"/>
<node id="fragmentation and redundant duplication"/>
<node id="wasted memory"/>
<node id="batch size"/>
<node id="Inefficient memory management"/>
<node id="the batch size"/>
<node id="PAGEDATTENTION"/>
<node id="an attention algorithm"/>
<node id="the classical virtual memory and paging techniques in operating systems"/>
<node id="PAGEDAT-TENTION"/>
<node id="the operating systems (OS) solution to memory fragmentation and sharing"/>
<node id="virtual memory with paging"/>
<node id="PagedAttention"/>
<node id="KV cache stored in non-contiguous paged memory"/>
<node id="the virtual memory and paging in OS"/>
<node id="the memory challenges in 3"/>
<node id="the classic idea of paging in operating systems"/>
<node id="continuous keys and values in non-contiguous memory space"/>
<node id="attention key and values vectors"/>
<node id="non-contiguous blocks in the memory"/>
<node id="THIS PAPER"/>
<node id="A NEW ATTENTION ALGORITHM"/>
<node id="ATTENTION KEYS AND VALUES TO BE STORED IN NON-CONTIGUOUS PAGED MEMORY"/>
<node id="VLLM"/>
<node id="A HIGH-THROUGHPUT LLM SERVING SYSTEM"/>
<node id="EFFICIENT MEMORY MANAGEMENT ENABLED BY PAGEDATTENTION"/>
<node id="an LLM serving system"/>
<node id="near-zero waste in KV cache memory"/>
<node id="flexible sharing of KV cache within and across requests"/>
<node id="flexible sharing of KV cache"/>
<node id="to further reduce memory usage"/>
<node id="existing LLM serving systems 31, 60"/>
<node id="the KV cache memory efficiently"/>
<node id="we"/>
<node id="the KV cache in a more flexible way as in OSS virtual memory"/>
<node id="blocks"/>
<node id="pages"/>
<node id="tokens"/>
<node id="bytes"/>
<node id="requests"/>
<node id="processes"/>
<node id="established techniques"/>
<node id="operating systems"/>
<node id="virtual memory"/>
<node id="copy-on-write"/>
<node id="established techniques such as virtual memory and copy-on-write"/>
<node id="efficiently manage KV cache"/>
<node id="handle various decoding algorithms in LLM serving"/>
<node id="LLM serving throughput by 2-4 compared to the state-of-the-art systems 31, 60"/>
<node id="model accuracy"/>
<node id="WE"/>
<node id="the performance of VLLM under a variety of workloads"/>
<node id="2-4 throughput improvements over the state-of-the-art systems"/>
<node id="The improvement"/>
<node id="longer sequences"/>
<node id="larger models"/>
<node id="more complex decoding algorithms"/>
<node id="improvements"/>
<node id="VLLMS source code"/>
<node id="https://github.com/vllm-project/vllm"/>
<node id="Many cloud companies 34, 44"/>
<node id="these applications as hosted services"/>
<node id="running these applications"/>
<node id="very expensive"/>
<node id="a large number of hardware accelerators such as GPUs"/>
<node id="processing an LLM request"/>
<node id="10 more expensive than a traditional keyword query"/>
<node id="SOSP 23"/>
<node id="OCTOBER 23-26, 2023"/>
<node id="Koblenz, Germany"/>
<node id="Copyright"/>
<node id="the owner author(s)"/>
<node id="2023"/>
<node id="ACM"/>
<node id="979-8-4007-0229-72310"/>
<node id="NVIDIA A100"/>
<node id="40GB"/>
<node id="KV cache"/>
<node id="26GB"/>
<node id="65"/>
<node id="Others"/>
<node id="20"/>
<node id="30"/>
<node id="40"/>
<node id="Memory usage"/>
<node id="GB"/>
<node id="Batch size"/>
<node id="Throughput"/>
<node id="LLM"/>
<node id="13B parameters"/>
<node id="LLM with 13B parameters"/>
<node id="memory distribution"/>
<node id="a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM"/>
<node id="THE PARAMETERS (GRAY)"/>
<node id="GPU MEMORY throughout serving"/>
<node id="the contiguous chunk of memory"/>
<node id="the KV cache of a request"/>
<node id="all available memory"/>
<node id="output length of a request"/>
<node id="decoding"/>
<node id="memory required for its KV cache"/>
<node id="as output length of a request grows at decoding"/>
<node id="available memory for incoming requests or ongoing generation for existing prompts"/>
<node id="A small amount of memory (yellow)"/>
<node id="ephemerally for activation"/>
<node id="the rapid growth curve of KV cache memory seen in existing systems 31, 60"/>
<node id="smoothing out the rapid growth curve of KV cache memory"/>
<node id="a notable boost in serving throughput"/>
<node id="The key idea behind VLLMS memory manager"/>
<node id="the virtual memory in operating systems"/>
<node id="the ideas behind virtual memory"/>
<node id="the KV cache in an LLM service"/>
<node id="cost per request of LLM serving systems"/>
<node id="more important"/>
<node id="LLMs"/>
<node id="an autoregressive transformer model"/>
<node id="this model"/>
<node id="words (tokens)"/>
<node id="the input (prompt)"/>
<node id="the previous sequence of the output tokens it has generated so far"/>
<node id="this expensive process"/>
<node id="for each request"/>
<node id="until the model outputs a termination token"/>
<node id="This sequential generation process"/>
<node id="the workload memory-bound"/>
<node id="the computation power of GPUs"/>
<node id="the serving throughput"/>
<node id="improving the throughput"/>
<node id="batching multiple requests together"/>
<node id="memory space for each request"/>
<node id="efficiently managed"/>
<node id="approximately 65 of the memory"/>
<node id="the model weights"/>
<node id="static during serving"/>
<node id="close to 30 of the memory"/>
<node id="the dynamic states of the requests"/>
<node id="these states"/>
<node id="the key and value tensors associated with the attention mechanism"/>
<node id="the key and value tensors"/>
<node id="KV cache 41"/>
<node id="the context from earlier tokens"/>
<node id="new output tokens in sequence"/>
<node id="ORCA (MAX)"/>
<node id="20.4"/>
<node id="ORCA (POW2)"/>
<node id="13.3"/>
<node id="ORCA (ORACLE)"/>
<node id="57.3"/>
<node id="8.9"/>
<node id="26.8"/>
<node id="17.9"/>
<node id="13.6"/>
<node id="41.6"/>
<node id="38.2"/>
<node id="25.2"/>
<node id="36.6"/>
<node id="96.3"/>
<node id="We"/>
<node id="the challenges in memory allocation in serving LLMs"/>
<node id="their impact on serving performance"/>
<node id="Percentage of memory"/>
<node id="other data, including activations"/>
<node id="Ephemeral tensors"/>
<node id="evaluating the LLM"/>
<node id="model weights"/>
<node id="constant"/>
<node id="activations"/>
<node id="a small fraction of the GPU memory"/>
<node id="the way the KV cache is managed"/>
<node id="critical in determining the maximum batch size"/>
<node id="KV cache memory"/>
<node id="throughput of the LLM"/>
<node id="inefficient management"/>
<node id="KV cache memory to limit batch size and throughput"/>
<node id="fine-grained batching"/>
<node id="the waste of computing"/>
<node id="requests to be batched in a more flexible way"/>
<node id="the number of requests that can be batched together"/>
<node id="GPU memory capacity"/>
<node id="the space allocated to store the KV cache"/>
<node id="The idea of virtual memory and paging"/>
<node id="managing the KV cache in LLM serving"/>
<node id="The workload"/>
<node id="dynamic memory allocation"/>
<node id="The output length"/>
<node id="not known a priori"/>
<node id="Its performance"/>
<node id="the GPU memory capacity"/>
<node id="Most deep learning frameworks 33, 39"/>
<node id="tensors to be stored in contiguous memory"/>
<node id="most operators in current deep learning frameworks 33, 39"/>
<node id="previous LLM serving systems 31, 60"/>
<node id="the KV cache of one request as a contiguous tensor across the different positions"/>
<node id="unique characteristics"/>
<node id="over time"/>
<node id="as the model generates new tokens"/>
<node id="lifetime and length of KV cache"/>
<node id="tensors in traditional deep learning workloads"/>
<node id="unlike KV cache"/>
<node id="significantly inefficient"/>
<node id="internal memory fragmentation"/>
<node id="external memory fragmentation"/>
<node id="requests actual length"/>
<node id="much shorter than its maximum length"/>
<node id="the pre-allocation"/>
<node id="inefficient"/>
<node id="the entire chunk"/>
<node id="the requests lifetime"/>
<node id="other shorter requests"/>
<node id="any part of the chunk that is currently unused"/>
<node id="External memory fragmentation"/>
<node id="significant"/>
<node id="Pre-allocated size"/>
<node id="each request"/>
<node id="the actual token states"/>
<node id="the existing systems"/>
<node id="percentage of KV cache memory used"/>
<node id="20.4 - 38.2%"/>
<node id="KV cache of one token"/>
<node id="all its previous tokens"/>
<node id="KV cache of the same token appearing at different positions in a sequence"/>
<node id="different"/>
<node id="THE TOKEN IN EACH MEMORY SLOT"/>
<node id="ITS KV CACHE"/>
<node id="the same tokens"/>
<node id="different KV cache"/>
<node id="at different positions"/>
<node id="opportunities for memory sharing"/>
<node id="LLM services"/>
<node id="advanced decoding algorithms"/>
<node id="parallel sampling"/>
<node id="beam search"/>
<node id="multiple outputs per request"/>
<node id="a range of decoding algorithms"/>
<node id="decoding algorithms"/>
<node id="users to select from"/>
<node id="varying implications for memory management complexity"/>
<node id="an LLM service"/>
<node id="more complex decoding scenarios"/>
<node id="complex accessing patterns"/>
<node id="more opportunities for memory sharing"/>
<node id="the request"/>
<node id="multiple sequences"/>
<node id="their KV cache"/>
<node id="of two requests at the same time in vLLM"/>
<node id="a request"/>
<node id="its generation"/>
<node id="its KV blocks"/>
<node id="to store the KV cache of other requests"/>
<node id="memory sharing"/>
<node id="the KV cache of the sequences"/>
<node id="separate contiguous spaces"/>
<node id="the requests KV cache into blocks"/>
<node id="each block"/>
<node id="the attention keys and values of a fixed number of tokens"/>
<node id="PAGEDATTENTION KERNEL"/>
<node id="different KV blocks separately"/>
<node id="blocks for the KV cache"/>
<node id="contiguous space"/>
<node id="THE KV CACHE MANAGER"/>
<node id="THE KV CACHE"/>
<node id="A PAGED FASHION"/>
<node id="the design of the KV cache manager in 4.2"/>
<node id="the design of the KV cache manager"/>
<node id="paged attention in 4.3"/>
<node id="the KV cache of each sequence into KV blocks"/>
<node id="the KV cache as fixed-size KV blocks"/>
<node id="fixed-size KV blocks"/>
<node id="pages in virtual memory"/>
<node id="organizing the KV cache as fixed-size KV blocks"/>
<node id="THIS DESIGN"/>
<node id="internal fragmentation"/>
<node id="relatively small blocks"/>
<node id="them on demand"/>
<node id="All blocks"/>
<node id="the same size"/>
<node id="the different sequences associated with the same request"/>
<node id="the different requests"/>
<node id="block-level memory management"/>
<node id="preemptive request scheduling"/>
<node id="block-level memory management and preemptive request scheduling"/>
<node id="PagedAttention algorithm"/>
<node id="the KV blocks to be stored in non-contiguous physical memory"/>
<node id="Storing KV blocks in non-contiguous physical memory"/>
<node id="more flexible paged memory management in VLLM"/>
<node id="the PagedAttention kernel to access the previous KV cache stored in the form of logical KV blocks"/>
<node id="the newly generated KV cache into the physical KV blocks"/>
<node id="this sharing easily"/>
<node id="this sharing"/>
<node id="its PagedAttention and Paged Memory Management"/>
<node id="popular LLMs such as GPT 5, OPT 62, and LLAMA 52"/>
<node id="varying sizes"/>
<node id="ones exceeding the memory capacity of a single GPU"/>
<node id="a distributed LLM serving engine"/>
<node id="VLLM on various scenarios"/>
<node id="previous state-of-the-art solutions such as FasterTransformer 31 and Orca 60"/>
<node id="up to 22 higher request rates compared to FasterTransformer"/>
<node id="FasterTransformer"/>
<node id="a fine-grained scheduling mechanism"/>
<node id="inefficiently like ORCA (MAX)"/>
<node id="memory fragmentation"/>
<node id="sharing"/>
<node id="more requests in a batch in parallel"/>
<node id="a 2-4 speedup compared to ORCA"/>
<node id="this section"/>
<node id="the generation and serving procedures of typical LLMs"/>
<node id="the iteration-level scheduling used in LLM serving"/>
<node id="The task of language modeling"/>
<node id="the probability of a list of tokens"/>
<node id="language"/>
<node id="a natural sequential ordering"/>
<node id="TRANSFORMERS 53"/>
<node id="the de facto standard architecture for modeling the probability above at a large scale"/>
<node id="the most important component of a transformer-based language model"/>
<node id="its self-attention layers"/>
<node id="A self-attention layer"/>
<node id="linear transformations on each position to get the query, key, and value vectors"/>
<node id="self-attention layer"/>
<node id="attention score"/>
<node id="multiplying the query vector at one position with all the key vectors before it"/>
<node id="output as the weighted average over the value vectors"/>
<node id="all other components"/>
<node id="the transformer model"/>
<node id="the embedding layer"/>
<node id="feed-forward layer"/>
<node id="layer normalization 2"/>
<node id="residual connection 22"/>
<node id="output logit computation"/>
<node id="the query, key, and value transformation in eq"/>
<node id="a conditional generation service"/>
<node id="A REQUEST TO AN LLM SERVICE"/>
<node id="A LIST OF INPUT PROMPT TOKENS"/>
<node id="the concatenation of the prompt and output lists"/>
<node id="sequence"/>
<node id="THE LLM"/>
<node id="new tokens one by one"/>
<node id="The generation process of each new token"/>
<node id="all the previous tokens in that sequence"/>
<node id="All the previous tokens in that sequence"/>
<node id="key and value vectors"/>
<node id="key and value vectors of existing tokens"/>
<node id="for generating future tokens"/>
<node id="generating future tokens"/>
<node id="A requests KV cache"/>
<node id="a series of logical KV blocks"/>
<node id="logical KV blocks"/>
<node id="left to right"/>
<node id="new tokens and their KV cache are generated"/>
<node id="the generation computation in the LLM service"/>
<node id="two phases"/>
<node id="the prompt phase"/>
<node id="the whole user prompt"/>
<node id="this process"/>
<node id="the key vectors 1"/>
<node id="the computation of the prompt phase"/>
<node id="matrix-matrix multiplication operations"/>
<node id="this phase"/>
<node id="the parallelism inherent in GPUs"/>
<node id="THE AUTOREGRESSIVE GENERATION PHASE"/>
<node id="THE REMAINING NEW TOKENS SEQUENTIALLY"/>
<node id="THE MODEL"/>
<node id="ONE TOKEN AT ITERATION"/>
<node id="key and value vectors at positions 1 to 1"/>
<node id="previous iterations"/>
<node id="new key and value vector"/>
<node id="this iteration"/>
<node id="the sequence reaches a maximum length"/>
<node id="maximum length"/>
<node id="users"/>
<node id="an end-of-sequence (EOS) token is emitted"/>
<node id="The computation at different iterations"/>
<node id="due to the data dependency"/>
<node id="matrix-vector multiplication"/>
<node id="Matrix-vector multiplication"/>
<node id="less efficient"/>
<node id="GPU computation"/>
<node id="memory-bound"/>
<node id="most portion of the latency of a single request"/>
<node id="compute utilization in serving LLMs"/>
<node id="batching multiple requests"/>
<node id="Batching the requests to an LLM service"/>
<node id="non-trivial"/>
<node id="two reasons"/>
<node id="the requests"/>
<node id="the same model weights"/>
<node id="the overhead of moving weights"/>
<node id="the requests in a batch"/>
<node id="the computational overhead"/>
<node id="the batch size is sufficiently large"/>
<node id="different times"/>
<node id="A naive batching strategy"/>
<node id="earlier requests wait for later ones"/>
<node id="the incoming requests until earlier ones finish"/>
<node id="Delaying incoming requests until earlier ones finish"/>
<node id="significant queueing delays"/>
<node id="vastly different input and output lengths"/>
<node id="A straightforward batching technique"/>
<node id="the inputs and outputs of the requests"/>
<node id="Padding the inputs and outputs of the requests"/>
<node id="equalize their lengths"/>
<node id="wasting GPU computation and memory"/>
<node id="fine-grained batching mechanisms"/>
<node id="to address this problem"/>
<node id="cellular batching 16"/>
<node id="iteration-level scheduling 60"/>
<node id="these techniques"/>
<node id="the iteration level"/>
<node id="traditional methods"/>
<node id="the request level"/>
<node id="completed requests"/>
<node id="the batch"/>
<node id="new ones"/>
<node id="a new request"/>
<node id="waiting for a single iteration"/>
<node id="waiting for the entire batch to complete"/>
<node id="special GPU kernels"/>
<node id="the need to pad the inputs and outputs"/>
<node id="the throughput of LLM serving"/>
<node id="reducing the queueing delay and the inefficiencies from padding"/>
<node id="fine-grained batching mechanisms to increase the throughput of LLM serving"/>
<node id="Our fathers"/>
<node id="EOS RESV"/>
<node id="Slots"/>
<node id="2038"/>
<node id="2"/>
<node id="KV cache states for request AS prompt"/>
<node id="7"/>
<node id="KV cache states for request BS prompt"/>
<node id="3"/>
<node id="507"/>
<node id="Request B"/>
<node id="current iteration"/>
<node id="Slot"/>
<node id="generated token"/>
<node id="Three types of memory wastes"/>
<node id="reserved, internal fragmentation, and external fragmentation"/>
<node id="Reserved, internal fragmentation, and external fragmentation"/>
<node id="to prevent other requests from fitting into the memory"/>
<node id="THE SERVING SYSTEMS THROUGHPUT"/>
<node id="MEMORY-BOUND"/>
<node id="the performance of the systems"/>
<node id="compute-bound rather than memory-bound"/>
<node id="overcoming this memory-bound"/>
<node id="the following challenges in the memory management: large KV cache"/>
<node id="KV cache of a single token"/>
<node id="800 KB of space"/>
<node id="2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16)"/>
<node id="model"/>
<node id="13B parameter OPT model 62"/>
<node id="OPT"/>
<node id="2048 tokens"/>
<node id="the memory required to store the KV cache of one request"/>
<node id="1.6 GB"/>
<node id="concurrent GPUs"/>
<node id="memory capacities in the tens of GBs"/>
<node id="GPU's computation speed"/>
<node id="memory capacity"/>
<node id="FLOPS"/>
<node id="more than 2x from NVIDIA A100 to H100"/>
<node id="GPU memory"/>
<node id="80GB maximum"/>
<node id="the memory"/>
<node id="an increasingly significant bottleneck"/>
<node id="multiple random samples from a single input prompt"/>
<node id="the KV cache of the prompt part"/>
<node id="12 of the total KV cache memory in our experiment (6.3)"/>
<node id="minimize memory usage"/>
<node id="unshared during the autoregressive generation phase"/>
<node id="different sample results and their dependence on context and position"/>
<node id="KV cache to remain unshared"/>
<node id="The extent of KV cache sharing"/>
<node id="the specific decoding algorithm employed"/>
<node id="different request beams"/>
<node id="larger portions (up to 55 memory saving) of their KV cache"/>
<node id="the sharing pattern"/>
<node id="as the decoding process advances"/>
<node id="requests to an LLM service"/>
<node id="variability in their input and output lengths"/>
<node id="a unique challenge"/>
<node id="input prompts for an LLM"/>
<node id="significantly in length"/>
<node id="resulting output lengths"/>
<node id="a priori"/>
<node id="both the input prompt and the model"/>
<node id="THE MEMORY MANAGEMENT SYSTEM"/>
<node id="A WIDE RANGE OF PROMPT LENGTHS"/>
<node id="THE SYSTEM"/>
<node id="scheduling decisions"/>
<node id="deleting the KV cache of some requests from GPU memory"/>
<node id="swapping out the KV cache of some requests from GPU memory"/>
<node id="The allocation"/>
<node id="the actual input or eventual output length of the request"/>
<node id="Output lengths from the LLM"/>
<node id="unpredictable"/>
<node id="two requests"/>
<node id="Request A"/>
<node id="2048 maximum possible sequence length"/>
<node id="maximum of 512"/>
<node id="THE CHUNK PRE-ALLOCATION SCHEME IN EXISTING SYSTEMS"/>
<node id="three primary sources of memory wastes"/>
<node id="reserved slots for future tokens"/>
<node id="internal fragmentation due to over-provisioning for potential maximum sequence lengths"/>
<node id="external fragmentation from the memory allocator like the buddy allocator"/>
<node id="THE EXTERNAL FRAGMENTATION"/>
<node id="GENERATED TOKENS"/>
<node id="before serving a request"/>
<node id="Internal fragmentation"/>
<node id="unused"/>
<node id="reserving this space for the entire requests duration"/>
<node id="the space that could otherwise be used to process other requests"/>
<node id="the average percentage of memory wastes in our experiments in Fig."/>
<node id="the actual effective memory in previous systems"/>
<node id="614 KV CACHE MANAGER"/>
<node id="SCHEDULER"/>
<node id="CPU BLOCK ALLOCATOR"/>
<node id="GPU BLOCK ALLOCATOR"/>
<node id="BLOCK TABLES"/>
<node id="WORKER 0 MODEL SHARD 0 CACHE ENGINE"/>
<node id="The architecture of VLLM"/>
<node id="Fig."/>
<node id="the system design of VLLM"/>
<node id="a distributed setting"/>
<node id="Compaction 54"/>
<node id="a potential solution to fragmentation"/>
<node id="Performing compaction in a performance-sensitive LLM serving system"/>
<node id="impractical"/>
<node id="the massive KV cache"/>
<node id="the pre-allocated chunk space for each request"/>
<node id="memory sharing specific to decoding algorithms in existing memory management systems"/>
<node id="a new attention algorithm, Page-DAttention"/>
<node id="an LLM serving engine, VLLM"/>
<node id="the challenges outlined in 3"/>
<node id="a centralized scheduler"/>
<node id="the execution of distributed GPU workers"/>
<node id="KV cache manager"/>
<node id="the physical KV cache memory on the GPU workers"/>
<node id="the instructions sent by the centralized scheduler"/>
<node id="each GPU worker"/>
<node id="the same physical block IDs"/>
<node id="a worker"/>
<node id="a portion of the KV cache for its corresponding attention heads"/>
<node id="GPU workers"/>
<node id="block table in the control message"/>
<node id="block table"/>
<node id="control message"/>
<node id="reading"/>
<node id="attention layers"/>
<node id="the PagedAttention algorithm in 4.1"/>
<node id="an example of PAGEDATTENTION in FIG."/>
<node id="this design"/>
<node id="effective memory management for various decoding methods (4.4)"/>
<node id="the variable length input and output sequences (4.5)"/>
<node id="the key and value vectors for a fixed number of tokens"/>
<node id="each block as KV 1"/>
<node id="each token"/>
<node id="a set of key and value vectors across layers and attention heads within a layer"/>
<node id="all the key and value vectors"/>
<node id="together within a single KV block"/>
<node id="the key and value vectors at different heads and layers"/>
<node id="a separate block"/>
<node id="in separate block tables"/>
<node id="THE TWO DESIGNS"/>
<node id="no performance difference"/>
<node id="the second one"/>
<node id="easy implementation"/>
<node id="our fathers"/>
<node id="four score and seven key and value vectors"/>
<node id="the effect of block size in 7.2"/>
<node id="4"/>
<node id="the following block-wise computation"/>
<node id="IS"/>
<node id="ATTENTION SCORE on -TH KV BLOCK"/>
<node id="THE KEY AND VALUE VECTORS"/>
<node id="THREE BLOCKS"/>
<node id="THE THREE BLOCKS"/>
<node id="THE PHYSICAL MEMORY"/>
<node id="the kernel"/>
<node id="the query vector of the query token (forth) and the key vectors in a block"/>
<node id="the attention score"/>
<node id="the value vectors in a block"/>
<node id="the final attention output"/>
<node id="OS"/>
<node id="memory into fixed-sized pages"/>
<node id="user programs logical pages to physical pages"/>
<node id="CONTIGUOUS LOGICAL PAGES"/>
<node id="NON-CONTIGUOUS PHYSICAL MEMORY PAGES"/>
<node id="USER PROGRAMS"/>
<node id="MEMORY as though it were CONTIGUOUS"/>
<node id="physical memory space"/>
<node id="in advance"/>
<node id="the OS"/>
<node id="to dynamically allocate physical pages as needed"/>
<node id="THE LAST KV BLOCKS UNFILLED POSITIONS"/>
<node id="FUTURE GENERATIONS"/>
<node id="A block engine"/>
<node id="a contiguous chunk of GPU DRAM"/>
<node id="a block engine"/>
<node id="Fathers"/>
<node id="four score and seven years ago"/>
<node id="Physical KV blocks"/>
<node id="GPU DRAM"/>
<node id="Block 0, Block 1, Block 2, Block 3, Block 4, Block 5, Block 6, Block 7, Block 8"/>
<node id="Logical KV blocks"/>
<node id="Physical block number"/>
<node id="BLOCK TABLE"/>
<node id="TRANSLATION IN VLLM"/>
<node id="The KV block manager"/>
<node id="block tables"/>
<node id="the mapping between logical and physical KV blocks of each request"/>
<node id="each block table entry"/>
<node id="the corresponding physical blocks of a logical block"/>
<node id="the number of filled positions"/>
<node id="Separating logical and physical KV blocks"/>
<node id="VLLM to dynamically grow the KV cache memory without reserving it for all positions in advance"/>
<node id="most memory waste in existing systems"/>
<node id="all the blocks"/>
<node id="a new physical block"/>
<node id="all previous blocks are full"/>
<node id="all the memory wastes for a request within one block"/>
<node id="all the memory"/>
<node id="the sharing of most of the space used to store the prompts KV cache across multiple output samples"/>
<node id="the final logical block"/>
<node id="a copy-on-write mechanism"/>
<node id="VLLMS physical block sharing"/>
<node id="frequent memory copy overhead"/>
<node id="the complex memory sharing between different sequences"/>
<node id="the complex memory sharing"/>
<node id="a common mapping layer"/>
<node id="logical blocks to physical blocks"/>
<node id="introducing the VLLMS techniques"/>
<node id="the performance"/>
<node id="the degradation of performance"/>
<node id="the extra overhead of memory indirection and non-contiguous block memory"/>
<node id="the new token"/>
<node id="physical blocks 7 and 1"/>
<node id="the first autoregressive decoding step"/>
<node id="4.3"/>
<node id="how PagedAttention and VLLM handle basic decoding algorithms"/>
<node id="PagedAttention and VLLM"/>
<node id="basic decoding algorithms"/>
<node id="greedy decoding and sampling"/>
<node id="one user prompt as input"/>
<node id="a single output sequence"/>
<node id="the prompt"/>
<node id="7 tokens"/>
<node id="the first 2 logical KV blocks (0 and 1) to 2 physical KV blocks (7 and 1, respectively)"/>
<node id="the KV cache of the prompts and the first output token"/>
<node id="the KV cache of the prompts and the first output token with a conventional self-attention algorithm"/>
<node id="the KV cache of the first 4 tokens in logical block 0"/>
<node id="the KV cache of the following 3 tokens in logical block 1"/>
<node id="new physical blocks to logical blocks"/>
<node id="free physical blocks for new tokens"/>
<node id="a set of sequences to evict"/>
<node id="their KV cache to the CPU"/>
<node id="THE REMAINING SLOT"/>
<node id="THE SUBSEQUENT AUTOREGRESSIVE GENERATION PHASE"/>
<node id="one slot"/>
<node id="available in the last logical block"/>
<node id="newly generated KV cache"/>
<node id="there"/>
<node id="newly generated KV cache in a new logical block"/>
<node id="a new physical block (physical block 3)"/>
<node id="this mapping in the block table"/>
<node id="last logical block"/>
<node id="full"/>
<node id="all the input tokens of the current iteration"/>
<node id="all tokens"/>
<node id="prompt phase four score and seven years ago our fathers brought"/>
<node id="block 0, block 1, block 2, block 3"/>
<node id="physical KV blocks"/>
<node id="request A"/>
<node id="request B"/>
<node id="figure 7"/>
<node id="mentioned"/>
<node id="Storing multiple tokens within a KV block (block size 1)"/>
<node id="the PagedAttention kernel to process the KV cache across more positions in parallel"/>
<node id="Processing the KV cache across more positions in parallel"/>
<node id="hardware utilization"/>
<node id="latency"/>
<node id="a larger block size"/>
<node id="an example of VLLM managing the memory for two sequences"/>
<node id="THE LOGICAL BLOCKS OF THE TWO SEQUENCES"/>
<node id="DIFFERENT PHYSICAL BLOCKS"/>
<node id="THE SPACE RESERVED BY THE BLOCK ENGINE IN GPU WORKERS"/>
<node id="The neighboring logical blocks of both sequences"/>
<node id="contiguous in physical GPU memory"/>
<node id="The space of physical blocks"/>
<node id="both sequences"/>
<node id="multiple sampled outputs for a single input prompt"/>
<node id="a favorite output from various candidates"/>
<node id="A request 616"/>
<node id="A1"/>
<node id="block 0"/>
<node id="Our mothers"/>
<node id="A2"/>
<node id="Copy-on-write"/>
<node id="ref count 2"/>
<node id="Figure 8"/>
<node id="the described blocks"/>
<node id="the more general case in which a request generates multiple sequences"/>
<node id="one request"/>
<node id="multiple samples sharing the same input prompt"/>
<node id="multiple samples"/>
<node id="the same input prompt"/>
<node id="the KV cache of the prompt"/>
<node id="in parallel sampling"/>
<node id="all parallel sequences in a request"/>
<node id="the KV cache for the prompt"/>
<node id="8"/>
<node id="an example of parallel decoding for two outputs"/>
<node id="both outputs"/>
<node id="the same prompt"/>
<node id="one copy of the prompt's state at the prompt phase"/>
<node id="the logical blocks for the prompts of both sequences"/>
<node id="the same physical blocks"/>
<node id="the logical block 0 and 1 of both sequences"/>
<node id="physical blocks 7 and 1, respectively"/>
<node id="a single physical block"/>
<node id="multiple logical blocks"/>
<node id="a reference count for each physical block"/>
<node id="reference counts for physical block 7"/>
<node id="the two outputs"/>
<node id="different output tokens"/>
<node id="separate storage for KV cache"/>
<node id="the reference count of the corresponding physical block (physical block 1) is greater than 1"/>
<node id="the block engine to copy the information from physical block 1"/>
<node id="the reference count to 1"/>
<node id="Sample A2"/>
<node id="physical block 1"/>
<node id="The reference count"/>
<node id="1"/>
<node id="newly generated KV cache to physical block 1"/>
<node id="sharing physical blocks across multiple samples"/>
<node id="greatly reduce memory usage"/>
<node id="memory usage"/>
<node id="long input prompts"/>
<node id="the top-most appropriate translations output by the LLM"/>
<node id="LLM tasks like machine translation 59"/>
<node id="machine translation"/>
<node id="Beam search 49"/>
<node id="the most probable output sequence from an LLM"/>
<node id="the computational complexity of fully traversing the block"/>
<node id="Beam search"/>
<node id="each candidate sequence in the beam"/>
<node id="all possible tokens"/>
<node id="their respective probabilities using the LLM"/>
<node id="the top-most probable sequences out of candidates"/>
<node id="V"/>
<node id="the vocabulary size"/>
<node id="THE ALGORITHM"/>
<node id="THE BEAM WIDTH PARAMETER"/>
<node id="THE NUMBER OF TOP CANDIDATES RETAINED AT EVERY STEP"/>
<node id="initial prompt blocks"/>
<node id="other blocks across different candidates"/>
<node id="Sharing patterns"/>
<node id="the process tree in the OS created by compound forks"/>
<node id="the KV blocks for a beam search example with 4"/>
<node id="each candidate sequence"/>
<node id="4 full logical blocks prior to the iteration illustrated as the dotted line"/>
<node id="ALL BEAM CANDIDATES"/>
<node id="THE FIRST BLOCK 0 (I.E., PROMPT)"/>
<node id="CANDIDATE 3"/>
<node id="others from the second block"/>
<node id="all candidates"/>
<node id="blocks 0, 1, 3"/>
<node id="candidates 0 and 1"/>
<node id="block 6"/>
<node id="top-4 probable candidates"/>
<node id="candidates 1 and 2"/>
<node id="original candidates 0 and 3"/>
<node id="the top candidates"/>
<node id="their logical blocks"/>
<node id="freed"/>
<node id="the reference counts of corresponding physical blocks"/>
<node id="reduced"/>
<node id="new physical blocks (blocks 9-12)"/>
<node id="the new KV cache from the new candidates"/>
<node id="Previous LLM serving systems"/>
<node id="frequent memory copies of the KV cache across the beam candidates"/>
<node id="Candidate 3"/>
<node id="a large portion of Candidate 2's KV cache"/>
<node id="to continue generation"/>
<node id="most blocks of different beam candidates"/>
<node id="in VLLM"/>
<node id="THE SAME STRATEGY"/>
<node id="BEAM SEARCH"/>
<node id="PREFIX SHARING BY VLLM"/>
<node id="THE COPY-ON-WRITE MECHANISM"/>
<node id="the newly generated tokens are within an old shared block"/>
<node id="the newly generated tokens"/>
<node id="an old shared block"/>
<node id="the copy-on-write mechanism"/>
<node id="parallel decoding"/>
<node id="LLM user"/>
<node id="a (long) description of the task including instructions and example inputs and outputs"/>
<node id="system prompt 36"/>
<node id="THE DESCRIPTION"/>
<node id="THE ACTUAL TASK INPUT"/>
<node id="THE DESCRIPTION AND THE ACTUAL TASK INPUT"/>
<node id="THE PROMPT OF THE REQUEST"/>
<node id="SEA OTTER"/>
<node id="LOUTRE DE MER"/>
<node id="PEPPERMINT"/>
<node id="MENTHE POIVRE"/>
<node id="PLUSH GIRAFE"/>
<node id="GIRAFE EN PELUCHE"/>
<node id="CHEESE"/>
<node id="FROMAGE"/>
<node id="I LOVE YOU"/>
<node id="JE T'AIME"/>
<node id="THE EXAMPLES"/>
<node id="5"/>
<node id="10"/>
<node id="an example"/>
<node id="the shared prefix"/>
<node id="prompt engineering"/>
<node id="tuning the shared prefix via prompt engineering"/>
<node id="the accuracy of the downstream tasks 26, 27"/>
<node id="many user prompts"/>
<node id="a prefix"/>
<node id="the LLM service provider"/>
<node id="the KV cache of the prefix in advance"/>
<node id="storing the KV cache of the prefix in advance"/>
<node id="the redundant computation spent on the prefix"/>
<node id="reserving a set of physical blocks for a set of predefined shared prefixes by the LLM service provider"/>
<node id="shared library across processes"/>
<node id="A user input prompt with the shared prefix"/>
<node id="its logical blocks to the cached physical blocks"/>
<node id="The last block"/>
<node id="THE PROMPT PHASE COMPUTATION"/>
<node id="the users task input"/>
<node id="THE DECODING METHODS DISCUSSED EARLIER"/>
<node id="DIVERSE MEMORY SHARING AND ACCESSING PATTERNS"/>
<node id="the simultaneous processing of requests with different decoding preferences"/>
<node id="THE LLM AND ITS EXECUTION KERNEL"/>
<node id="a list of physical block IDs for each sequence"/>
<node id="sharing patterns across sequences"/>
<node id="this approach"/>
<node id="the batching opportunities for requests with different sampling requirements"/>
<node id="the system's overall throughput"/>
<node id="a subset of requests when the request traffic surpasses the system's capacity"/>
<node id="the earliest arrived requests are served first"/>
<node id="the latest requests are preempted first"/>
<node id="first-come-first-serve (FCFS) scheduling policy"/>
<node id="all requests"/>
<node id="fairness"/>
<node id="starvation"/>
<node id="the GPUs physical blocks to store the newly generated KV cache"/>
<node id="block size"/>
<node id="too small"/>
<node id="the GPUs parallelism for reading and processing KV cache"/>
<node id="two classic questions"/>
<node id="two techniques: swapping"/>
<node id="Eviction policies"/>
<node id="heuristics"/>
<node id="which block will be accessed furthest in the future"/>
<node id="that block"/>
<node id="all blocks of a sequence"/>
<node id="together"/>
<node id="an all-or-nothing eviction policy"/>
<node id="all-or-nothing eviction policy"/>
<node id="either evict all or none of the blocks of a sequence"/>
<node id="MULTIPLE SEQUENCES WITHIN ONE REQUEST"/>
<node id="gang-scheduled as a sequence group"/>
<node id="sequences within one sequence group"/>
<node id="true"/>
<node id="due to potential memory sharing across those sequences"/>
<node id="classic technique"/>
<node id="most virtual memory implementations"/>
<node id="evicted pages to a swap space on the disk"/>
<node id="evicted blocks to the CPU memory"/>
<node id="a CPU block allocator"/>
<node id="CPU block allocator"/>
<node id="the physical blocks swapped to CPU RAM"/>
<node id="GPU block allocator"/>
<node id="a sequence"/>
<node id="its blocks"/>
<node id="new requests"/>
<node id="all preempted sequences are completed"/>
<node id="once"/>
<node id="the blocks of a preempted sequence"/>
<node id="to continue the processing of that sequence"/>
<node id="the number of blocks swapped to the CPU RAM"/>
<node id="the number of total physical blocks in the GPU RAM"/>
<node id="the swap space on the CPU RAM"/>
<node id="the GPU memory allocated for the KV cache"/>
<node id="the KV cache"/>
<node id="the preempted sequences are rescheduled"/>
<node id="recomputation latency"/>
<node id="significantly lower than the original latency"/>
<node id="tokens generated at decoding"/>
<node id="the original user prompt as a new prompt"/>
<node id="their KV cache at all positions"/>
<node id="one prompt phase iteration"/>
<node id="performances of swapping and recomputation"/>
<node id="bandwidth between CPU RAM and GPU memory"/>
<node id="computation power of the GPU"/>
<node id="the speeds of swapping and recomputation in 7.3"/>
<node id="recomputation"/>
<node id="swapping"/>
<node id="recomputation and swapping"/>
<node id="recovery mechanisms of VLLM"/>
<node id="many LLMs"/>
<node id="parameter sizes exceeding the capacity of a single GPU"/>
<node id="a memory manager"/>
<node id="distributed memory"/>
<node id="distributed settings"/>
<node id="Megatron-LM style tensor model parallelism strategy on Transformers 47"/>
<node id="THIS STRATEGY"/>
<node id="AN SPMD (SINGLE PROGRAM MULTIPLE DATA) EXECUTION SCHEDULE"/>
<node id="THE LINEAR LAYERS"/>
<node id="618"/>
<node id="THE DETAILED MODEL SIZES AND SERVER CONFIGURATIONS"/>
<node id="TABLE 1"/>
<node id="MODEL SIZE 13B"/>
<node id="GPUS A100 4"/>
<node id="MODEL SIZE 66B"/>
<node id="GPUS A100 8"/>
<node id="MODEL SIZE 175B"/>
<node id="GPUS A100-80GB 8"/>
<node id="TOTAL GPU MEMORY 40 GB"/>
<node id="TOTAL GPU MEMORY 160 GB"/>
<node id="TOTAL GPU MEMORY 640 GB"/>
<node id="PARAMETER SIZE 26 GB"/>
<node id="PARAMETER SIZE 132 GB"/>
<node id="PARAMETER SIZE 346 GB"/>
<node id="MEMORY FOR KV CACHE 12 GB"/>
<node id="MEMORY FOR KV CACHE 21 GB"/>
<node id="MEMORY FOR KV CACHE 264 GB"/>
<node id="GPUs"/>
<node id="intermediate results"/>
<node id="synchronize"/>
<node id="all-reduce operation"/>
<node id="block-wise matrix multiplication"/>
<node id="the attention operator"/>
<node id="the attention head dimension"/>
<node id="each SPMD process"/>
<node id="a subset of attention heads in multi-head attention"/>
<node id="each model shard"/>
<node id="the same set of input tokens"/>
<node id="model parallel execution"/>
<node id="each model shard processes the same set of input tokens"/>
<node id="the KV cache for the same positions"/>
<node id="a single KV cache manager within the centralized scheduler"/>
<node id="Different GPU workers"/>
<node id="the manager"/>
<node id="the mapping from logical blocks to physical blocks"/>
<node id="THIS COMMON MAPPING"/>
<node id="GPU WORKERS to execute the model with the physical blocks provided by the scheduler for each input request"/>
<node id="the scheduler"/>
<node id="the message with input token ids for each request in the batch"/>
<node id="the block table for each request"/>
<node id="THE SCHEDULER"/>
<node id="THIS CONTROL MESSAGE TO THE GPU WORKERS"/>
<node id="sampled tokens of this iteration"/>
<node id="the model with the input token ids"/>
<node id="memory management"/>
<node id="all the memory management information at the beginning of each decoding iteration along with the step inputs"/>
<node id="an end-to-end serving system"/>
<node id="a FastAPI frontend"/>
<node id="a GPU-based inference engine"/>
<node id="THE FRONTEND"/>
<node id="THE OPENAI API 34 INTERFACE"/>
<node id="USERS to customize sampling parameters for each request"/>
<node id="sampling parameters"/>
<node id="the maximum sequence length"/>
<node id="the beam width"/>
<node id="THE VLLM ENGINE"/>
<node id="8.5K lines of Python"/>
<node id="2K lines of CCUDA code"/>
<node id="model executor"/>
<node id="INPUT AND OUTPUT LENGTH DISTRIBUTIONS"/>
<node id="(A) SHAREGPT DATASET"/>
<node id="(B) ALPACA DATASET"/>
<node id="THE SHAREGPT DATASET"/>
<node id="THE ALPACA DATASET"/>
<node id="PYTORCH"/>
<node id="39"/>
<node id="TRANSFORMERS"/>
<node id="58"/>
<node id="NCCL 32 for tensor communication across the distributed GPU workers"/>
<node id="memory access patterns that are not efficiently supported by existing systems"/>
<node id="several GPU kernels for optimizing PagedAttention"/>
<node id="The dynamic block mapping in PagedAttention"/>
<node id="the performance of the GPU operations involving the stored KV cache"/>
<node id="The GPU operations involving the stored KV cache"/>
<node id="block readwrites and attention"/>
<node id="FUSED RE-SHAPE"/>
<node id="BLOCK WRITE"/>
<node id="new KV cache"/>
<node id="a memory layout optimized for block read"/>
<node id="the block table"/>
<node id="them into a single kernel"/>
<node id="fusing them into a single kernel"/>
<node id="kernel launch overheads"/>
<node id="a kernel that batches the copy operations for different blocks into a single kernel launch"/>
<node id="the attention kernel in FasterTransformer 31"/>
<node id="KV cache according to the block table"/>
<node id="attention operations on the fly"/>
<node id="a GPU warp to read each block"/>
<node id="assigning a GPU warp to read each block"/>
<node id="coalesced memory access"/>
<node id="variable sequence lengths within a request batch"/>
<node id="BLOCK COPY OPERATIONS"/>
<node id="DISCONTINUOUS BLOCKS"/>
<node id="using the cudamemcpyasync API"/>
<node id="numerous invocations of small data movements"/>
<node id="various decoding algorithms"/>
<node id="three key methods"/>
<node id="fork, append, and free"/>
<node id="THE FORK METHOD"/>
<node id="A NEW SEQUENCE FROM AN EXISTING ONE"/>
<node id="THE APPEND METHOD"/>
<node id="A NEW TOKEN TO THE SEQUENCE"/>
<node id="THE FREE METHOD"/>
<node id="THE SEQUENCE"/>
<node id="multiple output sequences"/>
<node id="the fork method"/>
<node id="the single input sequence"/>
<node id="VLLM creating multiple output sequences from the single input sequence using the fork method"/>
<node id="future decoding algorithms"/>
<node id="combining these methods"/>
<node id="OPT-13B"/>
<node id="1 GPU"/>
<node id="SHAREGPT"/>
<node id="OPT-66B"/>
<node id="4 GPUS"/>
<node id="OPT-175B"/>
<node id="8 GPUS"/>
<node id="ALPACA"/>
<node id="Latency"/>
<node id="NORMALIZED LATENCY (STOKEN)"/>
<node id="Request rate"/>
<node id="REQUEST RATE (REQS)"/>
<node id="Latency methods"/>
<node id="FASTERTRANSFORMER"/>
<node id="A"/>
<node id="D"/>
<node id="request rate (reqs)"/>
<node id="method for normalized latency (stoken)"/>
<node id="0, 5, 10, 15, 20, 25, 30, 35"/>
<node id="7.00, 9.81, 13.62, 30.42"/>
<node id="0, 25, 50, 75, 100, 125, 150"/>
<node id="7.00, 43.24, 72.75, 132.44"/>
<node id="SINGLE SEQUENCE GENERATION"/>
<node id="OPT models"/>
<node id="ShareGPT and Alpaca dataset"/>
<node id="OPT 62 models with 13B parameters"/>
<node id="OPT 62 models with 66B parameters"/>
<node id="OPT 62 models with 175B parameters"/>
<node id="LLAMA 52 with 13B parameters"/>
<node id="OPT 62 models"/>
<node id="66B parameters"/>
<node id="175B parameters"/>
<node id="LLAMA 52"/>
<node id="our evaluation"/>
<node id="13B"/>
<node id="popular sizes for LLMs"/>
<node id="66B"/>
<node id="13B and 66B"/>
<node id="an LLM leaderboard 38"/>
<node id="175B"/>
<node id="the size of the famous GPT-3 5 model"/>
<node id="workloads"/>
<node id="ShareGPT 51 and Alpaca 50 datasets"/>
<node id="input and output texts of real LLM services"/>
<node id="a collection of user-shared conversations with ChatGPT"/>
<node id="the chatting history and user query using the ShareGPT dataset"/>
<node id="an instruction dataset"/>
<node id="GPT-3.5 with Self-Instruct 57"/>
<node id="the datasets"/>
<node id="their input and output lengths"/>
<node id="synthesize client requests"/>
<node id="request arrival times using Poisson distribution with different request rates"/>
<node id="these datasets"/>
<node id="timestamps"/>
<node id="BASELINE 1"/>
<node id="FASTERTRANSFORMER 31"/>
<node id="a distributed inference engine highly optimized for latency"/>
<node id="its own scheduler"/>
<node id="a custom scheduler"/>
<node id="a dynamic batching mechanism"/>
<node id="the existing serving systems such as Triton 30"/>
<node id="a maximum batch size as large as possible for each experiment"/>
<node id="maximum batch size"/>
<node id="the three ORCA baselines"/>
<node id="similarly"/>
<node id="ORCA 60"/>
<node id="a state-of-the-art LLM serving system"/>
<node id="throughput"/>
<node id="our own version of ORCA"/>
<node id="ORCA"/>
<node id="not publicly available for use"/>
<node id="three versions of ORCA"/>
<node id="how much it over-reserves the space for request outputs"/>
<node id="ORACLE"/>
<node id="the buddy allocation algorithm"/>
<node id="the memory address to store KV cache"/>
<node id="the system"/>
<node id="the knowledge of the lengths of the outputs that will be actually generated for the requests"/>
<node id="upper-bound performance"/>
<node id="upper-bound performance of ORCA"/>
<node id="infeasible to achieve in practice"/>
<node id="the space for outputs by at most 2"/>
<node id="true output length"/>
<node id="25"/>
<node id="the space up to the maximum sequence length of the model"/>
<node id="the maximum sequence length of the model"/>
<node id="serving throughput"/>
<node id="normalized latency of the systems"/>
<node id="the mean of every request's end-to-end latency divided by its output length"/>
<node id="different request rates"/>
<node id="A HIGH-THROUGHPUT SERVING SYSTEM"/>
<node id="low normalized latency against high request rates"/>
<node id="the systems with 1-hour traces"/>
<node id="15-minute traces for the OPT-175B model as an exception"/>
<node id="reason"/>
<node id="cost limit"/>
<node id="the Alpaca dataset"/>
<node id="Parallel generation and beam search"/>
<node id="VLLM with basic sampling (one sample per request)"/>
<node id="evaluation"/>
<node id="three models and two datasets"/>
<node id="12"/>
<node id="the results on the ShareGPT dataset"/>
<node id="the ShareGPT dataset"/>
<node id="the curves"/>
<node id="that as the request rate increases, the latency initially increases at a gradual pace but then suddenly explodes"/>
<node id="the latency"/>
<node id="a gradual pace"/>
<node id="suddenly"/>
<node id="request rate"/>
<node id="capacity of the serving system"/>
<node id="queue length"/>
<node id="infinitely"/>
<node id="latency of the requests"/>
<node id="similar latencies"/>
<node id="ShareGPT dataset"/>
<node id="13A, FOR OPT-13B VLLM"/>
<node id="2.2 more requests at the same time than ORCA (ORACLE)"/>
<node id="4.3 more requests than ORCA (MAX)"/>
<node id="1.67 higher throughput than ORCA (Oracle) when the one-shot prefix is shared"/>
<node id="ORCA (Oracle)"/>
<node id="3.58 higher"/>
<node id="VLLMS PAGEDATTENTION"/>
<node id="the memory usage"/>
<node id="batching more requests than ORCA"/>
<node id="The iteration-level scheduling in ORCA 60"/>
<node id="a complementary technique to PagedAttention in VLLM"/>
<node id="Both systems"/>
<node id="the GPU utilization"/>
<node id="by scheduling and interleaving the requests"/>
<node id="Scheduling and interleaving the requests in ORCA"/>
<node id="more requests to be processed in parallel"/>
<node id="by increasing memory utilization"/>
<node id="Increasing memory utilization in VLLM"/>
<node id="the working sets of more requests to fit into memory"/>
<node id="VLLMS advantage over ORCA (ORACLE) and ORCA (POW2)"/>
<node id="less pronounced"/>
<node id="The model and server configuration for OPT-175B"/>
<node id="large GPU memory space available to store KV cache"/>
<node id="The Alpaca dataset"/>
<node id="short sequences"/>
<node id="ORCA (ORACLE) and ORCA (POW2)"/>
<node id="a large number of requests"/>
<node id="inefficiencies in their memory management"/>
<node id="OUTPUT SEQUENCES"/>
<node id="2, 4, 6"/>
<node id="MEMORY SAVING (A) PARALLEL SAMPLING"/>
<node id="6.09, 8.53, 9.79"/>
<node id="BEAM WIDTH"/>
<node id="0, 20, 40, 60"/>
<node id="MEMORY SAVING (B) BEAM SEARCH"/>
<node id="37.56, 53.13, 55.16"/>
<node id="FIGURE"/>
<node id="15"/>
<node id="sharing KV blocks"/>
<node id="average amount of memory saving"/>
<node id="OPT-13B for the Alpaca trace"/>
<node id="memory sharing in page-dattention"/>
<node id="two popular sampling methods: parallel sampling and beam search"/>
<node id="6.1 - 9.8 memory saving on parallel sampling"/>
<node id="37.6 - 55.2 memory saving on beam search"/>
<node id="the same experiments with the ShareGPT dataset"/>
<node id="16.2 - 30.5 memory saving on parallel sampling"/>
<node id="44.3 - 66.3 memory saving on beam search"/>
<node id="more improvement over the ORCA baselines with a larger number of sequences to sample"/>
<node id="2 higher request rates compared to the three ORCA baselines"/>
<node id="14"/>
<node id="the results for beam search with different beam widths"/>
<node id="more sharing"/>
<node id="even greater performance benefits"/>
<node id="ORCA (Oracle) on OPT-13B and the Alpaca dataset"/>
<node id="Improvement of VLLM over ORCA"/>
<node id="1.3 in basic sampling"/>
<node id="2.3 in beam search with a width of 6"/>
<node id="the amount of memory saving"/>
<node id="the number of blocks we saved by sharing divided by the number of total blocks without sharing"/>
<node id="the case a prefix is shared among different input prompts"/>
<node id="input prompts"/>
<node id="a common prefix"/>
<node id="THE PREFIX"/>
<node id="(A) 1 EXAMPLE WITH 80 TOKENS"/>
<node id="(B) 5 EXAMPLES WITH 341 TOKENS"/>
<node id="the model"/>
<node id="LLAMA-13B 52"/>
<node id="multilingual"/>
<node id="the WMT16 4 English-to-German translation dataset for the workload"/>
<node id="two prefixes that include an instruction and a few translation examples"/>
<node id="The first prefix"/>
<node id="a single example (i.e., one-shot)"/>
<node id="The other prefix"/>
<node id="5 examples (i.e., few-shot)"/>
<node id="CHATBOT"/>
<node id="LLMS"/>
<node id="the model generate a response"/>
<node id="a response"/>
<node id="the chatting history and the last user query into a prompt"/>
<node id="OPT-13B model"/>
<node id="limited context length"/>
<node id="the prompt to the last 1024 tokens"/>
<node id="at most 1024 tokens"/>
<node id="the KV cache between different conversation rounds"/>
<node id="storing the KV cache between conversation rounds"/>
<node id="the space for other requests between the conversation rounds"/>
<node id="many long conversations"/>
<node id="input prompts for most requests"/>
<node id="1024 tokens"/>
<node id="ORCA baselines"/>
<node id="space for 1024 tokens for the request outputs"/>
<node id="Buddy allocation algorithm"/>
<node id="ORCA baselines reserve the space for 1024 tokens for the request outputs"/>
<node id="64 128 256 context length"/>
<node id="kernel latency (us)"/>
<node id="Latency of attention kernels"/>
<node id="VLLM (BS 8), FT (BS 8), VLLM (BS 32), FT (BS 32)"/>
<node id="BLOCK SIZE"/>
<node id="1, 2, 4, 8, 16, 32, 64, 128, 256"/>
<node id="0.0, 2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5"/>
<node id="SHAREGPT ALPACA (B)"/>
<node id="END-TO-END LATENCY"/>
<node id="DIFFERENT BLOCK SIZES"/>
<node id="various aspects of VLLM"/>
<node id="the design choices we make with ablation experiments"/>
<node id="the problem of memory fragmentation and reservation"/>
<node id="OUR GPU KERNELS (5)"/>
<node id="extra overheads of accessing the block table"/>
<node id="executing extra branches"/>
<node id="handling variable sequence lengths"/>
<node id="18A"/>
<node id="2026 higher attention kernel latency"/>
<node id="highly-optimized FasterTransformer implementation"/>
<node id="the overhead"/>
<node id="small"/>
<node id="the other operators in the model, such as linear"/>
<node id="vLLM significantly outperform FasterTransformer in end-to-end performance"/>
<node id="choice of block size"/>
<node id="a substantial impact on the performance of VLLM"/>
<node id="too large"/>
<node id="the performance of VLLM with different block sizes"/>
<node id="the ShareGPT and Alpaca traces with basic sampling under fixed request rates"/>
<node id="block sizes from 16 to 128"/>
<node id="the best performance"/>
<node id="block size 16 and 32"/>
<node id="in the Alpaca trace"/>
<node id="larger block sizes"/>
<node id="the sequences"/>
<node id="shorter than the block sizes"/>
<node id="block size 16"/>
<node id="efficiently utilize the GPU"/>
<node id="avoid significant internal fragmentation in most workloads"/>
<node id="its default block size as 16"/>
<node id="Time"/>
<node id="milliseconds (ms)"/>
<node id="Microbenchmark"/>
<node id="Normalized latency"/>
<node id="0.0 to 2.5 (stoken)"/>
<node id="Figure 19"/>
<node id="Microbenchmark and End-to-End Performance data"/>
<node id="Performance metrics"/>
<node id="Recompute, Swap In, Swap Out"/>
<node id="overhead"/>
<node id="different block sizes"/>
<node id="the overhead of recomputation"/>
<node id="constant across different block sizes"/>
<node id="the KV blocks"/>
<node id="the block size is small"/>
<node id="the block size is large"/>
<node id="recomputation overhead"/>
<node id="20% of swapping's latency"/>
<node id="their end-to-end performance"/>
<node id="their overheads"/>
<node id="excessive overhead with small block sizes"/>
<node id="small block sizes"/>
<node id="numerous small data transfers between CPU and GPU"/>
<node id="the effective PCIe bandwidth"/>
<node id="the two methods"/>
<node id="comparable end-to-end performance for medium block sizes from 16 to 64"/>
<node id="the overhead of memory indirection in paging"/>
<node id="fusing the GPU kernels for memory access operations with those for other operations such as attention"/>
<node id="Tensor shapes"/>
<node id="typically static"/>
<node id="Memory allocation"/>
<node id="optimized ahead of time"/>
<node id="an increase in memory efficiency"/>
<node id="any performance improvement"/>
<node id="primarily compute-bound"/>
<node id="VLLMs techniques being applied to other workloads with similar properties to LLM serving"/>
<node id="the idea of virtual memory and paging"/>
<node id="the application-specific semantics"/>
<node id="VLLMS all-or-nothing swap-out policy"/>
<node id="the fact that processing a request requires all of its corresponding token states to be stored in GPU memory"/>
<node id="recomputation method"/>
<node id="another example"/>
<node id="evicted blocks"/>
<node id="Model serving"/>
<node id="an active area of research in recent years"/>
<node id="Numerous systems"/>
<node id="diverse aspects of deep learning model deployment"/>
<node id="batching"/>
<node id="serving single or multiple models"/>
<node id="caching"/>
<node id="placement"/>
<node id="scheduling"/>
<node id="DVABATCH 12"/>
<node id="multi-entry multi-exit batching"/>
<node id="REEF 21 AND SHEP-HERD 61"/>
<node id="preemption for serving"/>
<node id="ALPASERVE 28"/>
<node id="model parallelism for statistical multiplexing"/>
<node id="ALPASERVE"/>
<node id="statistical multiplexing with model parallelism for deep learning serving"/>
<node id="general systems"/>
<node id="the auto-regressive property and token state of LLM inference"/>
<node id="SERVING SYSTEMS"/>
<node id="numerous specialized serving systems"/>
<node id="the transformer architecture"/>
<node id="THESE SYSTEMS"/>
<node id="GPU KERNEL OPTIMIZATIONS 1, 29, 31, 56"/>
<node id="ADVANCED BATCHING MECHANISMS 14, 60"/>
<node id="MODEL PARALLELISM 1, 41, 60"/>
<node id="PARAMETER SHARING 64"/>
<node id="efficient serving"/>
<node id="our approach"/>
<node id="fine-grained scheduling and interleaving of the requests like in ORCA"/>
<node id="memory management more challenging"/>
<node id="techniques proposed in VLLM"/>
<node id="even more crucial"/>
<node id="The widening gap between the compute capability and memory capacity of accelerators"/>
<node id="memory to become a bottleneck for both training and inference"/>
<node id="FLEXGEN 46"/>
<node id="how to swap weights and token states for LLM inference with 623 limited GPU memory"/>
<node id="the online serving settings"/>
<node id="OLLA 48"/>
<node id="the lifetime and location of tensors"/>
<node id="fragmentation"/>
<node id="fine-grained block-level management"/>
<node id="online serving"/>
<node id="FLASHAT-TENTION 13"/>
<node id="tiling and kernel optimizations"/>
<node id="the peak memory of attention computation"/>
<node id="IO costs"/>
<node id="a new idea of block-level memory management in the context of online serving"/>
<node id="Xiaoxuan Liu"/>
<node id="Zhifeng Chen"/>
<node id="Yan-Ping Huang"/>
<node id="Anonymous SOSP reviewers"/>
<node id="our shepherd, Lidong Zhou"/>
<node id="Xiaoxuan Liu, Zhifeng Chen, Yan-Ping Huang, Anonymous SOSP reviewers, and our shepherd Lidong Zhou"/>
<node id="insightful feedback"/>
<node id="This research"/>
<node id="Andreessen Horowitz"/>
<node id="Anyscale"/>
<node id="Astronomer"/>
<node id="Google"/>
<node id="IBM"/>
<node id="Intel"/>
<node id="Lacework"/>
<node id="Microsoft"/>
<node id="Mohamed Bin Zayed University of Artificial Intelligence"/>
<node id="Samsung SDS"/>
<node id="Uber"/>
<node id="VMware"/>
<node id="REFERENCES"/>
<node id="REZA YAZDANI AMINABADI, SAMYAM RAJBHANDARI, MINJIA ZHANG, AMMAR AHMAD AWAN, CHENG LI, DU LI, ELTON ZHENG, JEFF RASLEY, SHADEN SMITH, OLATUNJI RUWASE, ET AL."/>
<node id="DEEPSPEED INFERENCE"/>
<node id="efficient inference of transformer models at unprecedented scale"/>
<node id="ARXIV PREPRINT"/>
<node id="ARXIV:1607.06450"/>
<node id="ARXIV PREPRINT ARXIV:1607.06450"/>
<node id="2016"/>
<node id="ARXIV:2107.03374"/>
<node id="ARXIV PREPRINT ARXIV:2107.03374"/>
<node id="2021"/>
<node id="ARXIV:1604.06174"/>
<node id="ARXIV PREPRINT ARXIV:1604.06174"/>
<node id="ARXIV:2204.02311"/>
<node id="ARXIV PREPRINT ARXIV:2204.02311"/>
<node id="2022"/>
<node id="ARXIV:2302.11665"/>
<node id="ARXIV PREPRINT ARXIV:2302.11665"/>
<node id="ARXIV:1712.06139"/>
<node id="ARXIV PREPRINT ARXIV:1712.06139"/>
<node id="2017"/>
<node id="ARXIV:2303.06865"/>
<node id="ARXIV PREPRINT ARXIV:2303.06865"/>
<node id="ARXIV:1909.08053"/>
<node id="ARXIV PREPRINT ARXIV:1909.08053"/>
<node id="2019"/>
<node id="ARXIV:2302.13971"/>
<node id="ARXIV PREPRINT ARXIV:2302.13971"/>
<node id="ARXIV:2212.10560"/>
<node id="ARXIV PREPRINT ARXIV:2212.10560"/>
<node id="ARXIV:1609.08144"/>
<node id="ARXIV PREPRINT ARXIV:1609.08144"/>
<node id="ARXIV:2205.01068"/>
<node id="ARXIV PREPRINT ARXIV:2205.01068"/>
<node id="YOSHUA BENGIO"/>
<node id="RJEAN DUCHARME"/>
<node id="PASCAL VINCENT"/>
<node id="A NEURAL PROBABILISTIC LANGUAGE MODEL"/>
<node id="a model"/>
<node id="4 OND REJ BOJAR"/>
<node id="RAJEN CHATTERJEE"/>
<node id="CHRISTIAN FEDERMANN"/>
<node id="YVETTE GRAHAM"/>
<node id="BARRY HADDOW"/>
<node id="MATTHIAS HUCK"/>
<node id="ANTONIO JIMENO YEPES"/>
<node id="PHILIPP KOEHN"/>
<node id="VARVARA LOGACHEVA"/>
<node id="CHRISTOF MONZ"/>
<node id="MATTEO NEGRI"/>
<node id="AURELIE NEVEOL"/>
<node id="MARIANA NEVES"/>
<node id="MARTIN POPEL"/>
<node id="MATT POST"/>
<node id="RAPHAEL RUBINO"/>
<node id="CAROLINA SCARTON"/>
<node id="LUCIA SPECIA"/>
<node id="MARCO TURCHI"/>
<node id="KARIN VERSPOOR"/>
<node id="MARCOS ZAMPIERI"/>
<node id="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS"/>
<node id="BERLIN"/>
<node id="GERMANY"/>
<node id="131198"/>
<node id="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301"/>
<node id="Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al."/>
<node id="Language models"/>
<node id="few-shot learners"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35"/>
<node id="16344-16359"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS"/>
<node id="27"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27"/>
<node id="2014"/>
<node id="14 JIARUI FANG"/>
<node id="the text"/>
<node id="YANG YU"/>
<node id="CHENGDUO ZHAO"/>
<node id="JIE ZHOU"/>
<node id="64"/>
<node id="ZHE ZHOU"/>
<node id="XUECHAO WEI"/>
<node id="JIEJING ZHANG"/>
<node id="GUANGYU SUN"/>
<node id="Training deep nets"/>
<node id="sublinear memory cost"/>
<node id="VICUNA"/>
<node id="an open-source chatbot"/>
<node id="GPT-4"/>
<node id="90 ChatGPT quality"/>
<node id="ORGBLOG2023-03-30-VICUNA 9"/>
<node id="Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al."/>
<node id="PALM"/>
<node id="scaling language modeling with Pathways"/>
<node id="INFERLINE"/>
<node id="latency-aware provisioning and scaling for prediction serving pipelines"/>
<node id="Proceedings"/>
<node id="the 11th ACM Symposium on Cloud Computing"/>
<node id="CLIPPER"/>
<node id="a low-latency online prediction serving system"/>
<node id="DVABATCH"/>
<node id="DIVERSITY-AWARE MULTI-ENTRY MULTI-EXIT BATCHING"/>
<node id="EFFICIENT PROCESSING OF DNN SERVICES ON GPUS"/>
<node id="USENIX ANNUAL TECHNICAL CONFERENCE"/>
<node id="USENIX ATC 22"/>
<node id="TURBOTRANSFORMERS"/>
<node id="an efficient GPU serving system for transformer models"/>
<node id="26TH ACM SIGPLAN SYMPOSIUM"/>
<node id="PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING"/>
<node id="the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming"/>
<node id="FASTAPI"/>
<node id="a web framework"/>
<node id="URL"/>
<node id="https://github.com/tiangolo/fastapi"/>
<node id="17 AMIR GHOLAMI"/>
<node id="ZHEWEI YAO"/>
<node id="SEHOON KIM"/>
<node id="MICHAEL W MAHONEY"/>
<node id="KURT KEUTZER"/>
<node id="18"/>
<node id="GITHUB"/>
<node id="GitHub Copilot"/>
<node id="GitHub"/>
<node id="HTTPS:BARD.GOOGLE.COM"/>
<node id="Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, Jonathan Mace"/>
<node id="MICROSECOND-SCALE PREEMPTION"/>
<node id="CONCURRENT GPU-ACCELERATED DNN INFERENCES"/>
<node id="DEEP RESIDUAL LEARNING"/>
<node id="IMAGE RECOGNITION"/>
<node id="SWAPADVISOR"/>
<node id="deep learning beyond the GPU memory limit via smart swapping"/>
<node id="The twenty-fifth international conference"/>
<node id="Architectural Support for Programming Languages and Operating Systems"/>
<node id="24 PARAS JAIN"/>
<node id="AJAY JAIN"/>
<node id="ANIRUDDHA NRUSIMHA"/>
<node id="AMIR GHOLAMI"/>
<node id="PIETER ABBEEL"/>
<node id="JOSEPH GONZALEZ"/>
<node id="TOM KILBURN"/>
<node id="DAVID BG EDWARDS"/>
<node id="MICHAEL J LANIGAN"/>
<node id="FRANK H SUMNER"/>
<node id="The power of scale"/>
<node id="parameter-efficient prompt tuning"/>
<node id="RAMMER"/>
<node id="holistic deep learning compiler optimizations with rTasks"/>
<node id="NVIDIA"/>
<node id="31"/>
<node id="32"/>
<node id="NVIDIA Triton Inference Server"/>
<node id="https://developer.nvidia.com"/>
<node id="https://github.com/NVIDIA/FASTERTRANSFORMER"/>
<node id="NCCL"/>
<node id="THE NVIDIA COLLECTIVE COMMUNICATION LIBRARY"/>
<node id="TENSORFLOW-SERVING"/>
<node id="flexible, high-performance ML serving"/>
<node id="OPENAI"/>
<node id="34"/>
<node id="ARXIV:2303.08774"/>
<node id="CS.CL"/>
<node id="38"/>
<node id="LMSYS ORG"/>
<node id="CHATBOT ARENA LEADERBOARD WEEK 8"/>
<node id="MT-BENCH and VICUNA-33B"/>
<node id="ADAM PASZKE, SAM GROSS, FRANCISCO MASSA, ADAM LERER, JAMES BRADBURY, GREGORY CHANAN, TREVOR KILLEEN, ZEMING LIN, NATALIA GIMELSHEIN, LUCA ANTIGA, ET AL."/>
<node id="an imperative style, high-performance deep learning library"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32"/>
<node id="POET"/>
<node id="training neural networks on tiny devices with integrated rematerialization and paging"/>
<node id="PMLR"/>
<node id="1757317583"/>
<node id="41"/>
<node id="REINER POPE"/>
<node id="SHOLTO DOUGLAS"/>
<node id="AAKANKSHA CHOWDHERY"/>
<node id="JACOB DEVLIN"/>
<node id="JAMES BRADBURY"/>
<node id="ANSELM LEVSKAYA"/>
<node id="JONATHAN HEEK"/>
<node id="KEFAN XIAO"/>
<node id="SHIVANI AGRAWAL"/>
<node id="JEFF DEAN"/>
<node id="Amazon Web Services"/>
<node id="https://www.reuters.com/technology/tech-giants-ai-like-bing-bard-poses-billion-dollar-search-problem-2023-02-22"/>
<node id="Authors"/>
<node id="Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram"/>
<node id="Website"/>
<node id="https://aws.amazon.com/bedrock"/>
<node id="NEXUS"/>
<node id="a GPU cluster engine"/>
<node id="accelerating DNN-based video analysis"/>
<node id="HIGH-THROUGHPUT GENERATIVE INFERENCE"/>
<node id="LARGE LANGUAGE MODELS with a SINGLE GPU"/>
<node id="MEGATRON-LM"/>
<node id="training multi-billion parameter language models using model parallelism"/>
<node id="OLLA"/>
<node id="the lifetime and location of arrays"/>
<node id="optimizing the lifetime and location of arrays"/>
<node id="the memory usage of neural networks"/>
<node id="DOI"/>
<node id="10.48550/ARXIV.2210.12924"/>
<node id="Ilya Sutskever"/>
<node id="Oriol Vinyals"/>
<node id="Quoc V Le"/>
<node id="STANFORD ALPACA"/>
<node id="an instruction-following LLaMA model"/>
<node id="HTTPS"/>
<node id="GITHUB.COM/TATSU-LAB/STANFORD-ALPACA"/>
<node id="HUGO TOUVRON"/>
<node id="HTTPS:SHAREGPT.COM 52"/>
<node id="THIBAUT LAVRIL"/>
<node id="GAUTIER IZACARD"/>
<node id="XAVIER MARTINET"/>
<node id="MARIE-ANNE LACHAUX"/>
<node id="TIMOTHE LACROIX"/>
<node id="BAPTISTE ROZIRE"/>
<node id="NAMAN GOYAL"/>
<node id="ERIC HAMBRO"/>
<node id="FAISAL AZHAR"/>
<node id="LLAMA"/>
<node id="open and efficient foundation language models"/>
<node id="PACMAN"/>
<node id="an efficient compaction approach"/>
<node id="log-structured key-value store"/>
<node id="persistent memory"/>
<node id="SUPERNEURONS"/>
<node id="dynamic GPU memory management for training deep neural networks"/>
<node id="LIGHTSEQ"/>
<node id="a high performance inference library for transformers"/>
<node id="the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers"/>
<node id="SELF-INSTRUCT"/>
<node id="aligning language model with self generated instructions"/>
<node id="Google's Neural Machine Translation System"/>
<node id="the gap between human and machine translation"/>
<node id="a distributed serving system for transformer-based generative models"/>
<node id="Susan Zhang"/>
<node id="NSDI23 conference"/>
<node id="Stephen Roller"/>
<node id="Naman Goyal"/>
<node id="Mikel Artetxe"/>
<node id="Moya Chen"/>
<node id="Shuohui Chen"/>
<node id="Christopher Dewan"/>
<node id="Mona Diab"/>
<node id="Xian Li"/>
<node id="Xi Victoria Lin"/>
<node id="OPEN PRE-TRAINED TRANSFORMER LANGUAGE MODELS"/>
<node id="ALPA"/>
<node id="inter-operator parallelism"/>
<node id="intra-operator parallelism"/>
<node id="distributed deep learning"/>
<node id="PETS"/>
<node id="A unified framework for parameter-efficient transformers serving"/>
<edge source="EFFICIENT MEMORY MANAGEMENT FOR LARGE LANGUAGE MODEL SERVING WITH PAGEDATTENTION" target="WOOSUK KWON, ZHUOHAN LI, SIYUAN ZHUANG, YING SHENG, LIANMIN ZHENG, CODY HAO YU, JOSEPH E. GONZALEZ, HAO ZHANG, ION STOICA">
  <data key="d0">is authored by</data>
</edge>
<edge source="WOOSUK KWON" target="UC BERKELEY">
  <data key="d0">affiliated with</data>
</edge>
<edge source="ZHUOHAN LI" target="UC BERKELEY">
  <data key="d0">affiliated with</data>
</edge>
<edge source="SIYUAN ZHUANG" target="UC BERKELEY">
  <data key="d0">affiliated with</data>
</edge>
<edge source="YING SHENG" target="UC BERKELEY and STANFORD UNIVERSITY">
  <data key="d0">affiliated with</data>
</edge>
<edge source="LIANMIN ZHENG" target="UC BERKELEY">
  <data key="d0">affiliated with</data>
</edge>
<edge source="CODY HAO YU" target="INDEPENDENT RESEARCHER">
  <data key="d0">affiliated with</data>
</edge>
<edge source="JOSEPH E. GONZALEZ" target="UC BERKELEY">
  <data key="d0">affiliated with</data>
</edge>
<edge source="HAO ZHANG" target="UC SAN DIEGO">
  <data key="d0">affiliated with</data>
</edge>
<edge source="ION STOICA" target="UC BERKELEY">
  <data key="d0">affiliated with</data>
</edge>
<edge source="ION STOICA" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="HIGH THROUGHPUT SERVING OF LARGE LANGUAGE MODELS (LLMS)" target="batching sufficiently many requests at a time">
  <data key="d0">requires</data>
</edge>
<edge source="existing systems" target="the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically">
  <data key="d0">struggle because</data>
</edge>
<edge source="existing systems" target="significantly inefficient">
  <data key="d0">are</data>
</edge>
<edge source="existing systems" target="internal memory fragmentation">
  <data key="d0">suffer from</data>
</edge>
<edge source="existing systems" target="external memory fragmentation">
  <data key="d0">suffer from</data>
</edge>
<edge source="existing systems" target="opportunities for memory sharing">
  <data key="d0">cannot exploit</data>
</edge>
<edge source="existing systems" target="the simultaneous processing of requests with different decoding preferences">
  <data key="d0">cannot do efficiently</data>
</edge>
<edge source="KV cache size" target="number of requests">
  <data key="d0">grows quickly with</data>
</edge>
<edge source="memory" target="fragmentation and redundant duplication">
  <data key="d0">can be wasted by</data>
</edge>
<edge source="wasted memory" target="batch size">
  <data key="d0">limits</data>
</edge>
<edge source="Inefficient memory management" target="the batch size">
  <data key="d0">can decrease</data>
</edge>
<edge source="PAGEDATTENTION" target="an attention algorithm">
  <data key="d0">is</data>
</edge>
<edge source="PAGEDATTENTION" target="the classical virtual memory and paging techniques in operating systems">
  <data key="d0">is inspired by</data>
</edge>
<edge source="PAGEDATTENTION" target="the memory challenges in 3">
  <data key="d0">is introduced to address</data>
</edge>
<edge source="PAGEDATTENTION" target="the classic idea of paging in operating systems">
  <data key="d0">is inspired by</data>
</edge>
<edge source="PAGEDATTENTION" target="A NEW ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGEDATTENTION" target="ATTENTION KEYS AND VALUES TO BE STORED IN NON-CONTIGUOUS PAGED MEMORY">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="PAGEDATTENTION" target="the requests KV cache into blocks">
  <data key="d0">divides</data>
</edge>
<edge source="PAGEDATTENTION" target="the KV cache of each sequence into KV blocks">
  <data key="d0">partitions</data>
</edge>
<edge source="PAGEDAT-TENTION" target="an attention algorithm">
  <data key="d0">is</data>
</edge>
<edge source="PAGEDAT-TENTION" target="the operating systems (OS) solution to memory fragmentation and sharing">
  <data key="d0">is inspired by</data>
</edge>
<edge source="the operating systems (OS) solution to memory fragmentation and sharing" target="virtual memory with paging">
  <data key="d0">is</data>
</edge>
<edge source="PagedAttention" target="KV cache stored in non-contiguous paged memory">
  <data key="d0">operates on</data>
</edge>
<edge source="PagedAttention" target="the virtual memory and paging in OS">
  <data key="d0">is inspired by</data>
</edge>
<edge source="PagedAttention" target="continuous keys and values in non-contiguous memory space">
  <data key="d0">allows storing</data>
</edge>
<edge source="PagedAttention" target="organizing the KV cache as fixed-size KV blocks">
  <data key="d0">enables</data>
</edge>
<edge source="PagedAttention" target="memory access patterns that are not efficiently supported by existing systems">
  <data key="d0">introduces</data>
</edge>
<edge source="PagedAttention" target="the problem of memory fragmentation and reservation">
  <data key="d0">resolves</data>
</edge>
<edge source="PagedAttention" target="vLLM significantly outperform FasterTransformer in end-to-end performance">
  <data key="d0">makes</data>
</edge>
<edge source="attention key and values vectors" target="non-contiguous blocks in the memory">
  <data key="d0">are stored as</data>
</edge>
<edge source="THIS PAPER" target="PAGEDATTENTION">
  <data key="d0">PROPOSES</data>
</edge>
<edge source="THIS PAPER" target="VLLM">
  <data key="d0">PRESENTS</data>
</edge>
<edge source="THIS PAPER" target="a new idea of block-level memory management in the context of online serving">
  <data key="d0">introduces</data>
</edge>
<edge source="VLLM" target="A HIGH-THROUGHPUT LLM SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="EFFICIENT MEMORY MANAGEMENT ENABLED BY PAGEDATTENTION">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="an LLM serving system">
  <data key="d0">is</data>
</edge>
<edge source="VLLM" target="near-zero waste in KV cache memory">
  <data key="d0">achieves</data>
</edge>
<edge source="VLLM" target="flexible sharing of KV cache within and across requests">
  <data key="d0">achieves</data>
</edge>
<edge source="VLLM" target="LLM serving throughput by 2-4 compared to the state-of-the-art systems 31, 60">
  <data key="d0">improves</data>
</edge>
<edge source="VLLM" target="model accuracy">
  <data key="d0">does not affect</data>
</edge>
<edge source="VLLM" target="2-4 throughput improvements over the state-of-the-art systems">
  <data key="d0">achieves</data>
</edge>
<edge source="VLLM" target="the rapid growth curve of KV cache memory seen in existing systems 31, 60">
  <data key="d0">smooths out</data>
</edge>
<edge source="VLLM" target="the ideas behind virtual memory">
  <data key="d0">uses</data>
</edge>
<edge source="VLLM" target="8.9">
  <data key="d0">KV CACHE USAGE</data>
</edge>
<edge source="VLLM" target="41.6">
  <data key="d0">TOKEN STATES RESERVATION</data>
</edge>
<edge source="VLLM" target="96.3">
  <data key="d0">INTERNAL FRAG.</data>
</edge>
<edge source="VLLM" target="block-level memory management">
  <data key="d0">uses</data>
</edge>
<edge source="VLLM" target="preemptive request scheduling">
  <data key="d0">uses</data>
</edge>
<edge source="VLLM" target="the PagedAttention kernel to access the previous KV cache stored in the form of logical KV blocks">
  <data key="d0">uses</data>
</edge>
<edge source="VLLM" target="the newly generated KV cache into the physical KV blocks">
  <data key="d0">saves</data>
</edge>
<edge source="VLLM" target="this sharing easily">
  <data key="d0">can realize</data>
</edge>
<edge source="VLLM" target="memory">
  <data key="d0">can save</data>
</edge>
<edge source="VLLM" target="popular LLMs such as GPT 5, OPT 62, and LLAMA 52">
  <data key="d0">supports</data>
</edge>
<edge source="VLLM" target="a distributed LLM serving engine">
  <data key="d0">is</data>
</edge>
<edge source="VLLM" target="PagedAttention">
  <data key="d0">uses algorithm</data>
</edge>
<edge source="VLLM" target="previous state-of-the-art solutions such as FasterTransformer 31 and Orca 60">
  <data key="d0">outperforms</data>
</edge>
<edge source="VLLM" target="up to 22 higher request rates compared to FasterTransformer">
  <data key="d0">can sustain</data>
</edge>
<edge source="VLLM" target="memory fragmentation">
  <data key="d0">reduces</data>
</edge>
<edge source="VLLM" target="sharing">
  <data key="d0">enables</data>
</edge>
<edge source="VLLM" target="more requests in a batch in parallel">
  <data key="d0">runs</data>
</edge>
<edge source="VLLM" target="a 2-4 speedup compared to ORCA">
  <data key="d0">achieves</data>
</edge>
<edge source="VLLM" target="the challenges outlined in 3">
  <data key="d0">tackle</data>
</edge>
<edge source="VLLM" target="a centralized scheduler">
  <data key="d0">adopts</data>
</edge>
<edge source="VLLM" target="all the memory wastes for a request within one block">
  <data key="d0">limits</data>
</edge>
<edge source="VLLM" target="all the memory">
  <data key="d0">can effectively utilize</data>
</edge>
<edge source="VLLM" target="the sharing of most of the space used to store the prompts KV cache across multiple output samples">
  <data key="d0">enables</data>
</edge>
<edge source="VLLM" target="the complex memory sharing between different sequences">
  <data key="d0">conceals</data>
</edge>
<edge source="VLLM" target="the new token">
  <data key="d0">generates</data>
</edge>
<edge source="VLLM" target="the first 2 logical KV blocks (0 and 1) to 2 physical KV blocks (7 and 1, respectively)">
  <data key="d0">maps</data>
</edge>
<edge source="VLLM" target="the KV cache of the prompts and the first output token">
  <data key="d0">generates</data>
</edge>
<edge source="VLLM" target="the KV cache of the prompts and the first output token with a conventional self-attention algorithm">
  <data key="d0">generates</data>
</edge>
<edge source="VLLM" target="the KV cache of the first 4 tokens in logical block 0">
  <data key="d0">stores</data>
</edge>
<edge source="VLLM" target="the KV cache of the following 3 tokens in logical block 1">
  <data key="d0">stores</data>
</edge>
<edge source="VLLM" target="new physical blocks to logical blocks">
  <data key="d0">dynamically assigns</data>
</edge>
<edge source="VLLM" target="free physical blocks for new tokens">
  <data key="d0">exhausts</data>
</edge>
<edge source="VLLM" target="a set of sequences to evict">
  <data key="d0">selects</data>
</edge>
<edge source="VLLM" target="their KV cache to the CPU">
  <data key="d0">transfers</data>
</edge>
<edge source="VLLM" target="newly generated KV cache in a new logical block">
  <data key="d0">stores</data>
</edge>
<edge source="VLLM" target="a new physical block (physical block 3)">
  <data key="d0">allocates</data>
</edge>
<edge source="VLLM" target="this mapping in the block table">
  <data key="d0">stores</data>
</edge>
<edge source="VLLM" target="all the input tokens of the current iteration">
  <data key="d0">concatenates</data>
</edge>
<edge source="VLLM" target="the reference count of the corresponding physical block (physical block 1) is greater than 1">
  <data key="d0">recognizes</data>
</edge>
<edge source="VLLM" target="the block engine to copy the information from physical block 1">
  <data key="d0">instructs</data>
</edge>
<edge source="VLLM" target="the reference count to 1">
  <data key="d0">decreases</data>
</edge>
<edge source="VLLM" target="the KV blocks for a beam search example with 4">
  <data key="d0">manages</data>
</edge>
<edge source="VLLM" target="new physical blocks (blocks 9-12)">
  <data key="d0">allocates</data>
</edge>
<edge source="VLLM" target="reserving a set of physical blocks for a set of predefined shared prefixes by the LLM service provider">
  <data key="d0">achieves</data>
</edge>
<edge source="VLLM" target="the simultaneous processing of requests with different decoding preferences">
  <data key="d0">facilitates</data>
</edge>
<edge source="VLLM" target="a subset of requests when the request traffic surpasses the system's capacity">
  <data key="d0">must prioritize</data>
</edge>
<edge source="VLLM" target="requests">
  <data key="d0">needs to preempt</data>
</edge>
<edge source="VLLM" target="the earliest arrived requests are served first">
  <data key="d0">ensures</data>
</edge>
<edge source="VLLM" target="the latest requests are preempted first">
  <data key="d0">ensures</data>
</edge>
<edge source="VLLM" target="first-come-first-serve (FCFS) scheduling policy">
  <data key="d0">adopts</data>
</edge>
<edge source="VLLM" target="the GPUs physical blocks to store the newly generated KV cache">
  <data key="d0">can run out of</data>
</edge>
<edge source="VLLM" target="the GPUs parallelism for reading and processing KV cache">
  <data key="d0">may not fully utilize</data>
</edge>
<edge source="VLLM" target="two classic questions">
  <data key="d0">needs to answer</data>
</edge>
<edge source="VLLM" target="a CPU block allocator">
  <data key="d0">includes</data>
</edge>
<edge source="VLLM" target="a sequence">
  <data key="d0">preempts</data>
</edge>
<edge source="VLLM" target="its blocks">
  <data key="d0">evicts</data>
</edge>
<edge source="VLLM" target="new requests">
  <data key="d0">stops accepting</data>
</edge>
<edge source="VLLM" target="all preempted sequences are completed">
  <data key="d0">stops accepting new requests until</data>
</edge>
<edge source="VLLM" target="recomputation">
  <data key="d0">supports</data>
</edge>
<edge source="VLLM" target="swapping">
  <data key="d0">supports</data>
</edge>
<edge source="VLLM" target="distributed settings">
  <data key="d0">is effective in</data>
</edge>
<edge source="VLLM" target="Megatron-LM style tensor model parallelism strategy on Transformers 47">
  <data key="d0">supports</data>
</edge>
<edge source="VLLM" target="a single KV cache manager within the centralized scheduler">
  <data key="d0">features</data>
</edge>
<edge source="VLLM" target="an end-to-end serving system">
  <data key="d0">is</data>
</edge>
<edge source="VLLM" target="a FastAPI frontend">
  <data key="d0">has</data>
</edge>
<edge source="VLLM" target="a GPU-based inference engine">
  <data key="d0">has</data>
</edge>
<edge source="VLLM" target="various decoding algorithms">
  <data key="d0">implements</data>
</edge>
<edge source="VLLM" target="three key methods">
  <data key="d0">uses</data>
</edge>
<edge source="VLLM" target="multiple output sequences">
  <data key="d0">creates</data>
</edge>
<edge source="VLLM" target="the fork method">
  <data key="d0">uses</data>
</edge>
<edge source="VLLM" target="method for normalized latency (stoken)">
  <data key="d0">is a</data>
</edge>
<edge source="VLLM" target="0, 5, 10, 15, 20, 25, 30, 35">
  <data key="d0">has batched requests</data>
</edge>
<edge source="VLLM" target="7.00, 9.81, 13.62, 30.42">
  <data key="d0">has values</data>
</edge>
<edge source="VLLM" target="0, 25, 50, 75, 100, 125, 150">
  <data key="d0">has batched requests</data>
</edge>
<edge source="VLLM" target="7.00, 43.24, 72.75, 132.44">
  <data key="d0">has values</data>
</edge>
<edge source="VLLM" target="ORCA (ORACLE)">
  <data key="d0">can sustain 1.72 to 7 times higher request rates compared to</data>
</edge>
<edge source="VLLM" target="ORCA (MAX)">
  <data key="d0">can sustain 2.78 times higher request rates compared to</data>
</edge>
<edge source="VLLM" target="similar latencies">
  <data key="d0">maintains</data>
</edge>
<edge source="VLLM" target="ShareGPT dataset">
  <data key="d0">performance measured on</data>
</edge>
<edge source="VLLM" target="1.67 higher throughput than ORCA (Oracle) when the one-shot prefix is shared">
  <data key="d0">achieves</data>
</edge>
<edge source="VLLM" target="ORCA (Oracle)">
  <data key="d0">achieves higher throughput than</data>
</edge>
<edge source="VLLM" target="3.58 higher">
  <data key="d0">achieves throughput</data>
</edge>
<edge source="VLLM" target="by increasing memory utilization">
  <data key="d0">achieves increased GPU utilization and throughput</data>
</edge>
<edge source="VLLM" target="more improvement over the ORCA baselines with a larger number of sequences to sample">
  <data key="d0">brings</data>
</edge>
<edge source="VLLM" target="2 higher request rates compared to the three ORCA baselines">
  <data key="d0">can sustain</data>
</edge>
<edge source="VLLM" target="even greater performance benefits">
  <data key="d0">demonstrates</data>
</edge>
<edge source="VLLM" target="ORCA (Oracle) on OPT-13B and the Alpaca dataset">
  <data key="d0">improves over</data>
</edge>
<edge source="VLLM" target="the case a prefix is shared among different input prompts">
  <data key="d0">is explored for effectiveness</data>
</edge>
<edge source="VLLM" target="64 128 256 context length">
  <data key="d0">can effectively handle</data>
</edge>
<edge source="VLLM" target="kernel latency (us)">
  <data key="d0">has latency measured in</data>
</edge>
<edge source="VLLM" target="its default block size as 16">
  <data key="d0">sets</data>
</edge>
<edge source="VLLM" target="the overhead of memory indirection in paging">
  <data key="d0">mitigates</data>
</edge>
<edge source="VLLM" target="fusing the GPU kernels for memory access operations with those for other operations such as attention">
  <data key="d0">mitigates the overhead by</data>
</edge>
<edge source="VLLM" target="the idea of virtual memory and paging">
  <data key="d0">re-interprets and augments</data>
</edge>
<edge source="VLLM" target="the application-specific semantics">
  <data key="d0">leverages</data>
</edge>
<edge source="flexible sharing of KV cache" target="to further reduce memory usage">
  <data key="d0">purpose</data>
</edge>
<edge source="existing LLM serving systems 31, 60" target="the KV cache memory efficiently">
  <data key="d0">fall short of managing</data>
</edge>
<edge source="we" target="the KV cache in a more flexible way as in OSS virtual memory">
  <data key="d0">can manage</data>
</edge>
<edge source="we" target="the average percentage of memory wastes in our experiments in Fig.">
  <data key="d0">visualize</data>
</edge>
<edge source="we" target="a new attention algorithm, Page-DAttention">
  <data key="d0">develop</data>
</edge>
<edge source="we" target="an LLM serving engine, VLLM">
  <data key="d0">build</data>
</edge>
<edge source="we" target="each block as KV 1">
  <data key="d0">denote</data>
</edge>
<edge source="we" target="an example of VLLM managing the memory for two sequences">
  <data key="d0">show</data>
</edge>
<edge source="we" target="the more general case in which a request generates multiple sequences">
  <data key="d0">assume</data>
</edge>
<edge source="we" target="one copy of the prompt's state at the prompt phase">
  <data key="d0">reserve space for</data>
</edge>
<edge source="we" target="a reference count for each physical block">
  <data key="d0">introduce</data>
</edge>
<edge source="we" target="two techniques: swapping">
  <data key="d0">consider</data>
</edge>
<edge source="we" target="an all-or-nothing eviction policy">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="evicted blocks to the CPU memory">
  <data key="d0">copy</data>
</edge>
<edge source="we" target="the KV cache">
  <data key="d0">recompute</data>
</edge>
<edge source="we" target="them into a single kernel">
  <data key="d0">fuse</data>
</edge>
<edge source="we" target="a kernel that batches the copy operations for different blocks into a single kernel launch">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="a GPU warp to read each block">
  <data key="d0">assign</data>
</edge>
<edge source="we" target="variable sequence lengths within a request batch">
  <data key="d0">add support for</data>
</edge>
<edge source="we" target="request arrival times using Poisson distribution with different request rates">
  <data key="d0">generate</data>
</edge>
<edge source="we" target="a maximum batch size as large as possible for each experiment">
  <data key="d0">set</data>
</edge>
<edge source="we" target="our own version of ORCA">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="normalized latency of the systems">
  <data key="d0">measure</data>
</edge>
<edge source="we" target="the systems with 1-hour traces">
  <data key="d0">evaluate</data>
</edge>
<edge source="we" target="15-minute traces for the OPT-175B model as an exception">
  <data key="d0">use</data>
</edge>
<edge source="we" target="VLLM with basic sampling (one sample per request)">
  <data key="d0">evaluate the performance of</data>
</edge>
<edge source="we" target="the WMT16 4 English-to-German translation dataset for the workload">
  <data key="d0">use</data>
</edge>
<edge source="we" target="two prefixes that include an instruction and a few translation examples">
  <data key="d0">synthesize</data>
</edge>
<edge source="we" target="the model generate a response">
  <data key="d0">let</data>
</edge>
<edge source="we" target="the prompt to the last 1024 tokens">
  <data key="d0">cut</data>
</edge>
<edge source="we" target="at most 1024 tokens">
  <data key="d0">let the model generate</data>
</edge>
<edge source="we" target="various aspects of VLLM">
  <data key="d0">study</data>
</edge>
<edge source="we" target="the design choices we make with ablation experiments">
  <data key="d0">evaluate</data>
</edge>
<edge source="we" target="their end-to-end performance">
  <data key="d0">evaluate</data>
</edge>
<edge source="we" target="their overheads">
  <data key="d0">microbenchmark</data>
</edge>
<edge source="we" target="VLLMs techniques being applied to other workloads with similar properties to LLM serving">
  <data key="d0">would be excited to see</data>
</edge>
<edge source="blocks" target="pages">
  <data key="d0">are</data>
</edge>
<edge source="blocks" target="a memory layout optimized for block read">
  <data key="d0">are reshaped to</data>
</edge>
<edge source="tokens" target="bytes">
  <data key="d0">are</data>
</edge>
<edge source="requests" target="processes">
  <data key="d0">are</data>
</edge>
<edge source="requests" target="different times">
  <data key="d0">may arrive at</data>
</edge>
<edge source="requests" target="vastly different input and output lengths">
  <data key="d0">may have</data>
</edge>
<edge source="established techniques" target="operating systems">
  <data key="d0">are inspired by</data>
</edge>
<edge source="established techniques" target="virtual memory">
  <data key="d0">include</data>
</edge>
<edge source="established techniques" target="copy-on-write">
  <data key="d0">include</data>
</edge>
<edge source="established techniques such as virtual memory and copy-on-write" target="efficiently manage KV cache">
  <data key="d0">can be adapted to</data>
</edge>
<edge source="established techniques such as virtual memory and copy-on-write" target="handle various decoding algorithms in LLM serving">
  <data key="d0">can be adapted to</data>
</edge>
<edge source="WE" target="the performance of VLLM under a variety of workloads">
  <data key="d0">evaluate</data>
</edge>
<edge source="WE" target="the KV cache as fixed-size KV blocks">
  <data key="d0">organize</data>
</edge>
<edge source="WE" target="an example of PAGEDATTENTION in FIG.">
  <data key="d0">show</data>
</edge>
<edge source="WE" target="the second one">
  <data key="d0">choose</data>
</edge>
<edge source="WE" target="NCCL 32 for tensor communication across the distributed GPU workers">
  <data key="d0">use</data>
</edge>
<edge source="WE" target="OPT 62 models with 13B parameters">
  <data key="d0">use</data>
</edge>
<edge source="WE" target="OPT 62 models with 66B parameters">
  <data key="d0">use</data>
</edge>
<edge source="WE" target="OPT 62 models with 175B parameters">
  <data key="d0">use</data>
</edge>
<edge source="WE" target="LLAMA 52 with 13B parameters">
  <data key="d0">use</data>
</edge>
<edge source="WE" target="our evaluation">
  <data key="d0">use for</data>
</edge>
<edge source="WE" target="6.1 - 9.8 memory saving on parallel sampling">
  <data key="d0">show</data>
</edge>
<edge source="WE" target="37.6 - 55.2 memory saving on beam search">
  <data key="d0">show</data>
</edge>
<edge source="The improvement" target="longer sequences">
  <data key="d0">is more pronounced with</data>
</edge>
<edge source="The improvement" target="larger models">
  <data key="d0">is more pronounced with</data>
</edge>
<edge source="The improvement" target="more complex decoding algorithms">
  <data key="d0">is more pronounced with</data>
</edge>
<edge source="improvements" target="longer sequences">
  <data key="d0">are more pronounced with</data>
</edge>
<edge source="improvements" target="larger models">
  <data key="d0">are more pronounced with</data>
</edge>
<edge source="improvements" target="more complex decoding algorithms">
  <data key="d0">are more pronounced with</data>
</edge>
<edge source="VLLMS source code" target="https://github.com/vllm-project/vllm">
  <data key="d0">is publicly available at</data>
</edge>
<edge source="Many cloud companies 34, 44" target="these applications as hosted services">
  <data key="d0">are racing to provide</data>
</edge>
<edge source="running these applications" target="very expensive">
  <data key="d0">is</data>
</edge>
<edge source="running these applications" target="a large number of hardware accelerators such as GPUs">
  <data key="d0">requires</data>
</edge>
<edge source="processing an LLM request" target="10 more expensive than a traditional keyword query">
  <data key="d0">can be</data>
</edge>
<edge source="SOSP 23" target="OCTOBER 23-26, 2023">
  <data key="d0">date</data>
</edge>
<edge source="SOSP 23" target="Koblenz, Germany">
  <data key="d0">location</data>
</edge>
<edge source="Copyright" target="the owner author(s)">
  <data key="d0">held by</data>
</edge>
<edge source="Copyright" target="2023">
  <data key="d0">year</data>
</edge>
<edge source="ACM" target="979-8-4007-0229-72310">
  <data key="d0">has ISBN</data>
</edge>
<edge source="NVIDIA A100" target="40GB">
  <data key="d0">has memory size</data>
</edge>
<edge source="KV cache" target="26GB">
  <data key="d0">size</data>
</edge>
<edge source="KV cache" target="65">
  <data key="d0">parameter</data>
</edge>
<edge source="KV cache" target="unique characteristics">
  <data key="d0">has</data>
</edge>
<edge source="KV cache" target="over time">
  <data key="d0">dynamically grows and shrinks</data>
</edge>
<edge source="KV cache" target="as the model generates new tokens">
  <data key="d0">grows and shrinks</data>
</edge>
<edge source="KV cache" target="of two requests at the same time in vLLM">
  <data key="d0">is stored</data>
</edge>
<edge source="KV cache" target="unshared during the autoregressive generation phase">
  <data key="d0">should remain</data>
</edge>
<edge source="Others" target="20">
  <data key="d0">parameter</data>
</edge>
<edge source="Others" target="30">
  <data key="d0">parameter</data>
</edge>
<edge source="Others" target="40">
  <data key="d0">parameter</data>
</edge>
<edge source="Memory usage" target="GB">
  <data key="d0">unit</data>
</edge>
<edge source="Batch size" target="requests">
  <data key="d0">unit</data>
</edge>
<edge source="Throughput" target="tokens">
  <data key="d0">unit</data>
</edge>
<edge source="LLM" target="13B parameters">
  <data key="d0">has</data>
</edge>
<edge source="LLM" target="multiple sampled outputs for a single input prompt">
  <data key="d0">generates</data>
</edge>
<edge source="LLM with 13B parameters" target="NVIDIA A100">
  <data key="d0">is served on</data>
</edge>
<edge source="memory distribution" target="a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM">
  <data key="d0">is illustrated for</data>
</edge>
<edge source="THE PARAMETERS (GRAY)" target="GPU MEMORY throughout serving">
  <data key="d0">persist in</data>
</edge>
<edge source="the contiguous chunk of memory" target="the KV cache of a request">
  <data key="d0">is used to store</data>
</edge>
<edge source="all available memory" target="KV cache">
  <data key="d0">was allocated to</data>
</edge>
<edge source="output length of a request" target="decoding">
  <data key="d0">grows at</data>
</edge>
<edge source="memory required for its KV cache" target="as output length of a request grows at decoding">
  <data key="d0">expands</data>
</edge>
<edge source="memory required for its KV cache" target="available memory for incoming requests or ongoing generation for existing prompts">
  <data key="d0">may exhaust</data>
</edge>
<edge source="A small amount of memory (yellow)" target="ephemerally for activation">
  <data key="d0">is used</data>
</edge>
<edge source="smoothing out the rapid growth curve of KV cache memory" target="a notable boost in serving throughput">
  <data key="d0">leads to</data>
</edge>
<edge source="The key idea behind VLLMS memory manager" target="the virtual memory in operating systems">
  <data key="d0">is analogous to</data>
</edge>
<edge source="the ideas behind virtual memory" target="the KV cache in an LLM service">
  <data key="d0">to manage</data>
</edge>
<edge source="cost per request of LLM serving systems" target="more important">
  <data key="d0">is becoming</data>
</edge>
<edge source="LLMs" target="an autoregressive transformer model">
  <data key="d0">have at their core</data>
</edge>
<edge source="LLMs" target="a conditional generation service">
  <data key="d0">are deployed as</data>
</edge>
<edge source="this model" target="words (tokens)">
  <data key="d0">generates</data>
</edge>
<edge source="this model" target="the input (prompt)">
  <data key="d0">generates words based on</data>
</edge>
<edge source="this model" target="the previous sequence of the output tokens it has generated so far">
  <data key="d0">generates words based on</data>
</edge>
<edge source="this expensive process" target="for each request">
  <data key="d0">is repeated</data>
</edge>
<edge source="this expensive process" target="until the model outputs a termination token">
  <data key="d0">is repeated</data>
</edge>
<edge source="This sequential generation process" target="the workload memory-bound">
  <data key="d0">makes</data>
</edge>
<edge source="This sequential generation process" target="the computation power of GPUs">
  <data key="d0">underutilizes</data>
</edge>
<edge source="This sequential generation process" target="the serving throughput">
  <data key="d0">limits</data>
</edge>
<edge source="improving the throughput" target="batching multiple requests together">
  <data key="d0">is possible by</data>
</edge>
<edge source="memory space for each request" target="efficiently managed">
  <data key="d0">should be</data>
</edge>
<edge source="approximately 65 of the memory" target="the model weights">
  <data key="d0">is allocated for</data>
</edge>
<edge source="the model weights" target="static during serving">
  <data key="d0">remain</data>
</edge>
<edge source="close to 30 of the memory" target="the dynamic states of the requests">
  <data key="d0">is used to store</data>
</edge>
<edge source="these states" target="the key and value tensors associated with the attention mechanism">
  <data key="d0">consist of</data>
</edge>
<edge source="the key and value tensors" target="KV cache 41">
  <data key="d0">are referred to as</data>
</edge>
<edge source="KV cache 41" target="the context from earlier tokens">
  <data key="d0">represent</data>
</edge>
<edge source="KV cache 41" target="new output tokens in sequence">
  <data key="d0">generate</data>
</edge>
<edge source="ORCA (MAX)" target="20.4">
  <data key="d0">KV CACHE USAGE</data>
</edge>
<edge source="ORCA (MAX)" target="26.8">
  <data key="d0">TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (MAX)" target="38.2">
  <data key="d0">INTERNAL FRAG.</data>
</edge>
<edge source="ORCA (MAX)" target="method for normalized latency (stoken)">
  <data key="d0">is a</data>
</edge>
<edge source="ORCA (MAX)" target="0, 5, 10, 15, 20, 25, 30, 35">
  <data key="d0">has batched requests</data>
</edge>
<edge source="ORCA (MAX)" target="7.00, 9.81, 13.62, 30.42">
  <data key="d0">has values</data>
</edge>
<edge source="ORCA (MAX)" target="0, 25, 50, 75, 100, 125, 150">
  <data key="d0">has batched requests</data>
</edge>
<edge source="ORCA (MAX)" target="7.00, 43.24, 72.75, 132.44">
  <data key="d0">has values</data>
</edge>
<edge source="ORCA (POW2)" target="13.3">
  <data key="d0">KV CACHE USAGE</data>
</edge>
<edge source="ORCA (POW2)" target="17.9">
  <data key="d0">TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (POW2)" target="25.2">
  <data key="d0">INTERNAL FRAG.</data>
</edge>
<edge source="ORCA (POW2)" target="method for normalized latency (stoken)">
  <data key="d0">is a</data>
</edge>
<edge source="ORCA (POW2)" target="0, 5, 10, 15, 20, 25, 30, 35">
  <data key="d0">has batched requests</data>
</edge>
<edge source="ORCA (POW2)" target="7.00, 9.81, 13.62, 30.42">
  <data key="d0">has values</data>
</edge>
<edge source="ORCA (POW2)" target="0, 25, 50, 75, 100, 125, 150">
  <data key="d0">has batched requests</data>
</edge>
<edge source="ORCA (POW2)" target="7.00, 43.24, 72.75, 132.44">
  <data key="d0">has values</data>
</edge>
<edge source="ORCA (ORACLE)" target="57.3">
  <data key="d0">KV CACHE USAGE</data>
</edge>
<edge source="ORCA (ORACLE)" target="13.6">
  <data key="d0">TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (ORACLE)" target="36.6">
  <data key="d0">INTERNAL FRAG.</data>
</edge>
<edge source="ORCA (ORACLE)" target="method for normalized latency (stoken)">
  <data key="d0">is a</data>
</edge>
<edge source="ORCA (ORACLE)" target="0, 5, 10, 15, 20, 25, 30, 35">
  <data key="d0">has batched requests</data>
</edge>
<edge source="ORCA (ORACLE)" target="7.00, 9.81, 13.62, 30.42">
  <data key="d0">has values</data>
</edge>
<edge source="ORCA (ORACLE)" target="0, 25, 50, 75, 100, 125, 150">
  <data key="d0">has batched requests</data>
</edge>
<edge source="ORCA (ORACLE)" target="7.00, 43.24, 72.75, 132.44">
  <data key="d0">has values</data>
</edge>
<edge source="We" target="the challenges in memory allocation in serving LLMs">
  <data key="d0">identify</data>
</edge>
<edge source="We" target="their impact on serving performance">
  <data key="d0">quantify</data>
</edge>
<edge source="We" target="the design of the KV cache manager in 4.2">
  <data key="d0">show</data>
</edge>
<edge source="We" target="VLLM on various scenarios">
  <data key="d0">evaluate</data>
</edge>
<edge source="We" target="the PagedAttention algorithm in 4.1">
  <data key="d0">describe</data>
</edge>
<edge source="We" target="the effect of block size in 7.2">
  <data key="d0">study</data>
</edge>
<edge source="We" target="the speeds of swapping and recomputation in 7.3">
  <data key="d0">examine</data>
</edge>
<edge source="We" target="several GPU kernels for optimizing PagedAttention">
  <data key="d0">develop</data>
</edge>
<edge source="We" target="the attention kernel in FasterTransformer 31">
  <data key="d0">adapt</data>
</edge>
<edge source="We" target="the chatting history and user query using the ShareGPT dataset">
  <data key="d0">synthesize</data>
</edge>
<edge source="We" target="the datasets">
  <data key="d0">tokenize</data>
</edge>
<edge source="We" target="their input and output lengths">
  <data key="d0">use</data>
</edge>
<edge source="We" target="synthesize client requests">
  <data key="d0">use their input and output lengths to</data>
</edge>
<edge source="We" target="a custom scheduler">
  <data key="d0">implement</data>
</edge>
<edge source="We" target="three versions of ORCA">
  <data key="d0">implement</data>
</edge>
<edge source="We" target="serving throughput">
  <data key="d0">focus on</data>
</edge>
<edge source="We" target="the KV cache between different conversation rounds">
  <data key="d0">do not store</data>
</edge>
<edge source="We" target="the performance of VLLM with different block sizes">
  <data key="d0">evaluate</data>
</edge>
<edge source="We" target="the ShareGPT and Alpaca traces with basic sampling under fixed request rates">
  <data key="d0">use</data>
</edge>
<edge source="We" target="Xiaoxuan Liu">
  <data key="d0">would like to thank</data>
</edge>
<edge source="We" target="Zhifeng Chen">
  <data key="d0">would like to thank</data>
</edge>
<edge source="We" target="Yan-Ping Huang">
  <data key="d0">would like to thank</data>
</edge>
<edge source="We" target="Anonymous SOSP reviewers">
  <data key="d0">would like to thank</data>
</edge>
<edge source="We" target="our shepherd, Lidong Zhou">
  <data key="d0">would like to thank</data>
</edge>
<edge source="Percentage of memory" target="other data, including activations">
  <data key="d0">is used for</data>
</edge>
<edge source="Ephemeral tensors" target="evaluating the LLM">
  <data key="d0">are created when</data>
</edge>
<edge source="model weights" target="constant">
  <data key="d0">are</data>
</edge>
<edge source="activations" target="a small fraction of the GPU memory">
  <data key="d0">occupy</data>
</edge>
<edge source="the way the KV cache is managed" target="critical in determining the maximum batch size">
  <data key="d0">is</data>
</edge>
<edge source="KV cache memory" target="batch size">
  <data key="d0">can limit</data>
</edge>
<edge source="KV cache memory" target="throughput of the LLM">
  <data key="d0">can limit</data>
</edge>
<edge source="KV cache memory" target="the actual token states">
  <data key="d0">is used to store</data>
</edge>
<edge source="inefficient management" target="KV cache memory to limit batch size and throughput">
  <data key="d0">causes</data>
</edge>
<edge source="fine-grained batching" target="the waste of computing">
  <data key="d0">reduces</data>
</edge>
<edge source="fine-grained batching" target="requests to be batched in a more flexible way">
  <data key="d0">enables</data>
</edge>
<edge source="the number of requests that can be batched together" target="GPU memory capacity">
  <data key="d0">is constrained by</data>
</edge>
<edge source="GPU memory capacity" target="the space allocated to store the KV cache">
  <data key="d0">particularly constrains</data>
</edge>
<edge source="The idea of virtual memory and paging" target="managing the KV cache in LLM serving">
  <data key="d0">is effective for</data>
</edge>
<edge source="The workload" target="dynamic memory allocation">
  <data key="d0">requires</data>
</edge>
<edge source="The output length" target="not known a priori">
  <data key="d0">is</data>
</edge>
<edge source="Its performance" target="the GPU memory capacity">
  <data key="d0">is bound by</data>
</edge>
<edge source="Most deep learning frameworks 33, 39" target="tensors to be stored in contiguous memory">
  <data key="d0">require</data>
</edge>
<edge source="most operators in current deep learning frameworks 33, 39" target="tensors to be stored in contiguous memory">
  <data key="d0">require</data>
</edge>
<edge source="previous LLM serving systems 31, 60" target="the KV cache of one request as a contiguous tensor across the different positions">
  <data key="d0">store</data>
</edge>
<edge source="lifetime and length of KV cache" target="not known a priori">
  <data key="d0">are</data>
</edge>
<edge source="tensors in traditional deep learning workloads" target="unlike KV cache">
  <data key="d0">are</data>
</edge>
<edge source="requests actual length" target="much shorter than its maximum length">
  <data key="d0">can be</data>
</edge>
<edge source="the pre-allocation" target="inefficient">
  <data key="d0">is</data>
</edge>
<edge source="the entire chunk" target="the requests lifetime">
  <data key="d0">is reserved during</data>
</edge>
<edge source="other shorter requests" target="any part of the chunk that is currently unused">
  <data key="d0">cannot utilize</data>
</edge>
<edge source="External memory fragmentation" target="significant">
  <data key="d0">can be</data>
</edge>
<edge source="Pre-allocated size" target="each request">
  <data key="d0">can be different for</data>
</edge>
<edge source="the actual token states" target="the existing systems">
  <data key="d0">are stored in</data>
</edge>
<edge source="percentage of KV cache memory used" target="20.4 - 38.2%">
  <data key="d0">is</data>
</edge>
<edge source="KV cache of one token" target="all its previous tokens">
  <data key="d0">depends on</data>
</edge>
<edge source="KV cache of the same token appearing at different positions in a sequence" target="different">
  <data key="d0">will be</data>
</edge>
<edge source="THE TOKEN IN EACH MEMORY SLOT" target="ITS KV CACHE">
  <data key="d0">represents</data>
</edge>
<edge source="the same tokens" target="different KV cache">
  <data key="d0">can have</data>
</edge>
<edge source="different KV cache" target="at different positions">
  <data key="d0">occur</data>
</edge>
<edge source="LLM services" target="advanced decoding algorithms">
  <data key="d0">use</data>
</edge>
<edge source="LLM services" target="a range of decoding algorithms">
  <data key="d0">offer</data>
</edge>
<edge source="LLM services" target="a unique challenge">
  <data key="d0">face</data>
</edge>
<edge source="advanced decoding algorithms" target="parallel sampling">
  <data key="d0">include</data>
</edge>
<edge source="advanced decoding algorithms" target="beam search">
  <data key="d0">include</data>
</edge>
<edge source="advanced decoding algorithms" target="multiple outputs per request">
  <data key="d0">generate</data>
</edge>
<edge source="parallel sampling" target="VLLM creating multiple output sequences from the single input sequence using the fork method">
  <data key="d0">involves</data>
</edge>
<edge source="decoding algorithms" target="users to select from">
  <data key="d0">are for</data>
</edge>
<edge source="decoding algorithms" target="varying implications for memory management complexity">
  <data key="d0">have</data>
</edge>
<edge source="an LLM service" target="more complex decoding scenarios">
  <data key="d0">must offer</data>
</edge>
<edge source="an LLM service" target="more opportunities for memory sharing">
  <data key="d0">must offer</data>
</edge>
<edge source="more complex decoding scenarios" target="complex accessing patterns">
  <data key="d0">exhibit</data>
</edge>
<edge source="the request" target="multiple sequences">
  <data key="d0">consists of</data>
</edge>
<edge source="multiple sequences" target="their KV cache">
  <data key="d0">can partially share</data>
</edge>
<edge source="a request" target="its generation">
  <data key="d0">finishes</data>
</edge>
<edge source="a request" target="once">
  <data key="d0">completes</data>
</edge>
<edge source="its KV blocks" target="to store the KV cache of other requests">
  <data key="d0">can be freed</data>
</edge>
<edge source="memory sharing" target="the existing systems">
  <data key="d0">is not possible in</data>
</edge>
<edge source="memory sharing" target="the different sequences associated with the same request">
  <data key="d0">occurs across</data>
</edge>
<edge source="memory sharing" target="the different requests">
  <data key="d0">occurs across</data>
</edge>
<edge source="the KV cache of the sequences" target="separate contiguous spaces">
  <data key="d0">is stored in</data>
</edge>
<edge source="each block" target="the attention keys and values of a fixed number of tokens">
  <data key="d0">can contain</data>
</edge>
<edge source="each block" target="the key and value vectors for a fixed number of tokens">
  <data key="d0">contains</data>
</edge>
<edge source="PAGEDATTENTION KERNEL" target="different KV blocks separately">
  <data key="d0">identifies and fetches</data>
</edge>
<edge source="blocks for the KV cache" target="contiguous space">
  <data key="d0">are not necessarily stored in</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="THE KV CACHE">
  <data key="d0">manages</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="A PAGED FASHION">
  <data key="d0">manages in</data>
</edge>
<edge source="A PAGED FASHION" target="PAGEDATTENTION">
  <data key="d0">is enabled by</data>
</edge>
<edge source="the design of the KV cache manager" target="paged attention in 4.3">
  <data key="d0">facilitates</data>
</edge>
<edge source="fixed-size KV blocks" target="pages in virtual memory">
  <data key="d0">are like</data>
</edge>
<edge source="THIS DESIGN" target="internal fragmentation">
  <data key="d0">alleviates</data>
</edge>
<edge source="THIS DESIGN" target="relatively small blocks">
  <data key="d0">uses</data>
</edge>
<edge source="THIS DESIGN" target="them on demand">
  <data key="d0">allocates</data>
</edge>
<edge source="All blocks" target="the same size">
  <data key="d0">have</data>
</edge>
<edge source="block-level memory management and preemptive request scheduling" target="PagedAttention">
  <data key="d0">are co-designed with</data>
</edge>
<edge source="PagedAttention algorithm" target="the KV blocks to be stored in non-contiguous physical memory">
  <data key="d0">allows</data>
</edge>
<edge source="PagedAttention algorithm" target="physical blocks 7 and 1">
  <data key="d0">operates on</data>
</edge>
<edge source="Storing KV blocks in non-contiguous physical memory" target="more flexible paged memory management in VLLM">
  <data key="d0">enables</data>
</edge>
<edge source="this sharing" target="its PagedAttention and Paged Memory Management">
  <data key="d0">is via</data>
</edge>
<edge source="popular LLMs such as GPT 5, OPT 62, and LLAMA 52" target="varying sizes">
  <data key="d0">have</data>
</edge>
<edge source="varying sizes" target="ones exceeding the memory capacity of a single GPU">
  <data key="d0">include</data>
</edge>
<edge source="FasterTransformer" target="a fine-grained scheduling mechanism">
  <data key="d0">does not utilize</data>
</edge>
<edge source="FasterTransformer" target="inefficiently like ORCA (MAX)">
  <data key="d0">manages memory</data>
</edge>
<edge source="FasterTransformer" target="its own scheduler">
  <data key="d0">does not have</data>
</edge>
<edge source="this section" target="the generation and serving procedures of typical LLMs">
  <data key="d0">describes</data>
</edge>
<edge source="this section" target="the iteration-level scheduling used in LLM serving">
  <data key="d0">describes</data>
</edge>
<edge source="The task of language modeling" target="the probability of a list of tokens">
  <data key="d0">is to model</data>
</edge>
<edge source="language" target="a natural sequential ordering">
  <data key="d0">has</data>
</edge>
<edge source="TRANSFORMERS 53" target="the de facto standard architecture for modeling the probability above at a large scale">
  <data key="d0">have become</data>
</edge>
<edge source="the most important component of a transformer-based language model" target="its self-attention layers">
  <data key="d0">is</data>
</edge>
<edge source="A self-attention layer" target="linear transformations on each position to get the query, key, and value vectors">
  <data key="d0">applies</data>
</edge>
<edge source="self-attention layer" target="attention score">
  <data key="d0">computes</data>
</edge>
<edge source="self-attention layer" target="output as the weighted average over the value vectors">
  <data key="d0">computes</data>
</edge>
<edge source="attention score" target="multiplying the query vector at one position with all the key vectors before it">
  <data key="d0">is computed by</data>
</edge>
<edge source="all other components" target="the transformer model">
  <data key="d0">are in</data>
</edge>
<edge source="all other components" target="the embedding layer">
  <data key="d0">include</data>
</edge>
<edge source="all other components" target="feed-forward layer">
  <data key="d0">include</data>
</edge>
<edge source="all other components" target="layer normalization 2">
  <data key="d0">include</data>
</edge>
<edge source="all other components" target="residual connection 22">
  <data key="d0">include</data>
</edge>
<edge source="all other components" target="output logit computation">
  <data key="d0">include</data>
</edge>
<edge source="all other components" target="the query, key, and value transformation in eq">
  <data key="d0">include</data>
</edge>
<edge source="A REQUEST TO AN LLM SERVICE" target="A LIST OF INPUT PROMPT TOKENS">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="the concatenation of the prompt and output lists" target="sequence">
  <data key="d0">is referred to as</data>
</edge>
<edge source="THE LLM" target="new tokens one by one">
  <data key="d0">can only sample and generate</data>
</edge>
<edge source="The generation process of each new token" target="all the previous tokens in that sequence">
  <data key="d0">depends on</data>
</edge>
<edge source="All the previous tokens in that sequence" target="key and value vectors">
  <data key="d0">have</data>
</edge>
<edge source="key and value vectors of existing tokens" target="for generating future tokens">
  <data key="d0">are often cached</data>
</edge>
<edge source="generating future tokens" target="KV cache">
  <data key="d0">is known as</data>
</edge>
<edge source="A requests KV cache" target="a series of logical KV blocks">
  <data key="d0">is represented as</data>
</edge>
<edge source="logical KV blocks" target="left to right">
  <data key="d0">are filled from</data>
</edge>
<edge source="logical KV blocks" target="new tokens and their KV cache are generated">
  <data key="d0">are filled as</data>
</edge>
<edge source="the generation computation in the LLM service" target="two phases">
  <data key="d0">can be decomposed into</data>
</edge>
<edge source="the prompt phase" target="the whole user prompt">
  <data key="d0">takes</data>
</edge>
<edge source="this process" target="the key vectors 1">
  <data key="d0">generates</data>
</edge>
<edge source="the computation of the prompt phase" target="matrix-matrix multiplication operations">
  <data key="d0">can be parallelized using</data>
</edge>
<edge source="this phase" target="the parallelism inherent in GPUs">
  <data key="d0">can efficiently use</data>
</edge>
<edge source="this phase" target="the sequence reaches a maximum length">
  <data key="d0">completes when</data>
</edge>
<edge source="this phase" target="an end-of-sequence (EOS) token is emitted">
  <data key="d0">completes when</data>
</edge>
<edge source="this phase" target="GPU computation">
  <data key="d0">underutilizes</data>
</edge>
<edge source="this phase" target="memory-bound">
  <data key="d0">becomes</data>
</edge>
<edge source="this phase" target="most portion of the latency of a single request">
  <data key="d0">is responsible for</data>
</edge>
<edge source="THE AUTOREGRESSIVE GENERATION PHASE" target="THE REMAINING NEW TOKENS SEQUENTIALLY">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THE MODEL" target="ONE TOKEN AT ITERATION">
  <data key="d0">takes as input</data>
</edge>
<edge source="key and value vectors at positions 1 to 1" target="previous iterations">
  <data key="d0">are cached at</data>
</edge>
<edge source="new key and value vector" target="this iteration">
  <data key="d0">are computed at</data>
</edge>
<edge source="maximum length" target="users">
  <data key="d0">is specified by</data>
</edge>
<edge source="maximum length" target="LLMs">
  <data key="d0">is limited by</data>
</edge>
<edge source="users" target="multiple random samples from a single input prompt">
  <data key="d0">request</data>
</edge>
<edge source="users" target="a favorite output from various candidates">
  <data key="d0">can choose</data>
</edge>
<edge source="users" target="the top-most appropriate translations output by the LLM">
  <data key="d0">expect</data>
</edge>
<edge source="The computation at different iterations" target="due to the data dependency">
  <data key="d0">cannot be parallelized</data>
</edge>
<edge source="The computation at different iterations" target="matrix-vector multiplication">
  <data key="d0">often uses</data>
</edge>
<edge source="Matrix-vector multiplication" target="less efficient">
  <data key="d0">is</data>
</edge>
<edge source="compute utilization in serving LLMs" target="batching multiple requests">
  <data key="d0">can be improved by</data>
</edge>
<edge source="Batching the requests to an LLM service" target="non-trivial">
  <data key="d0">is</data>
</edge>
<edge source="Batching the requests to an LLM service" target="two reasons">
  <data key="d0">is non-trivial for</data>
</edge>
<edge source="the requests" target="the same model weights">
  <data key="d0">share</data>
</edge>
<edge source="the overhead of moving weights" target="the requests in a batch">
  <data key="d0">is amortized across</data>
</edge>
<edge source="the overhead of moving weights" target="the computational overhead">
  <data key="d0">can be overwhelmed by</data>
</edge>
<edge source="the computational overhead" target="the batch size is sufficiently large">
  <data key="d0">overwhelms the overhead of moving weights when</data>
</edge>
<edge source="A naive batching strategy" target="earlier requests wait for later ones">
  <data key="d0">would either make</data>
</edge>
<edge source="A naive batching strategy" target="the incoming requests until earlier ones finish">
  <data key="d0">would either delay</data>
</edge>
<edge source="Delaying incoming requests until earlier ones finish" target="significant queueing delays">
  <data key="d0">leads to</data>
</edge>
<edge source="A straightforward batching technique" target="the inputs and outputs of the requests">
  <data key="d0">would pad</data>
</edge>
<edge source="Padding the inputs and outputs of the requests" target="equalize their lengths">
  <data key="d0">to do what</data>
</edge>
<edge source="Padding the inputs and outputs of the requests" target="wasting GPU computation and memory">
  <data key="d0">results in</data>
</edge>
<edge source="fine-grained batching mechanisms" target="to address this problem">
  <data key="d0">have been proposed</data>
</edge>
<edge source="fine-grained batching mechanisms" target="cellular batching 16">
  <data key="d0">include</data>
</edge>
<edge source="fine-grained batching mechanisms" target="iteration-level scheduling 60">
  <data key="d0">include</data>
</edge>
<edge source="fine-grained batching mechanisms" target="the throughput of LLM serving">
  <data key="d0">increase</data>
</edge>
<edge source="these techniques" target="the iteration level">
  <data key="d0">operate at</data>
</edge>
<edge source="traditional methods" target="the request level">
  <data key="d0">work at</data>
</edge>
<edge source="completed requests" target="the batch">
  <data key="d0">are removed from</data>
</edge>
<edge source="new ones" target="the batch">
  <data key="d0">are added</data>
</edge>
<edge source="a new request" target="waiting for a single iteration">
  <data key="d0">can be processed after</data>
</edge>
<edge source="a new request" target="waiting for the entire batch to complete">
  <data key="d0">cannot be processed after</data>
</edge>
<edge source="special GPU kernels" target="the need to pad the inputs and outputs">
  <data key="d0">eliminate</data>
</edge>
<edge source="reducing the queueing delay and the inefficiencies from padding" target="fine-grained batching mechanisms to increase the throughput of LLM serving">
  <data key="d0">causes</data>
</edge>
<edge source="Our fathers" target="EOS RESV">
  <data key="d0">brought forth</data>
</edge>
<edge source="Our fathers" target="block 0">
  <data key="d0">are associated with</data>
</edge>
<edge source="Slots" target="2038">
  <data key="d0">never used (internal fragmentation)</data>
</edge>
<edge source="Slots" target="2">
  <data key="d0">future used (reserved)</data>
</edge>
<edge source="Slots" target="507">
  <data key="d0">never used (internal fragmentation)</data>
</edge>
<edge source="KV cache states for request AS prompt" target="7">
  <data key="d0">are</data>
</edge>
<edge source="KV cache states for request BS prompt" target="3">
  <data key="d0">are</data>
</edge>
<edge source="3" target="two requests">
  <data key="d0">illustrates</data>
</edge>
<edge source="Request B" target="current iteration">
  <data key="d0">is</data>
</edge>
<edge source="Request B" target="maximum of 512">
  <data key="d0">has</data>
</edge>
<edge source="Slot" target="generated token">
  <data key="d0">is for</data>
</edge>
<edge source="Three types of memory wastes" target="reserved, internal fragmentation, and external fragmentation">
  <data key="d0">are</data>
</edge>
<edge source="Reserved, internal fragmentation, and external fragmentation" target="to prevent other requests from fitting into the memory">
  <data key="d0">exist</data>
</edge>
<edge source="THE SERVING SYSTEMS THROUGHPUT" target="MEMORY-BOUND">
  <data key="d0">is</data>
</edge>
<edge source="the performance of the systems" target="compute-bound rather than memory-bound">
  <data key="d0">becomes</data>
</edge>
<edge source="overcoming this memory-bound" target="the following challenges in the memory management: large KV cache">
  <data key="d0">requires addressing</data>
</edge>
<edge source="KV cache of a single token" target="800 KB of space">
  <data key="d0">demands</data>
</edge>
<edge source="800 KB of space" target="2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16)">
  <data key="d0">is calculated as</data>
</edge>
<edge source="model" target="13B parameter OPT model 62">
  <data key="d0">is</data>
</edge>
<edge source="OPT" target="2048 tokens">
  <data key="d0">can generate sequences up to</data>
</edge>
<edge source="OPT" target="OPEN PRE-TRAINED TRANSFORMER LANGUAGE MODELS">
  <data key="d0">is</data>
</edge>
<edge source="the memory required to store the KV cache of one request" target="1.6 GB">
  <data key="d0">can be as much as</data>
</edge>
<edge source="concurrent GPUs" target="memory capacities in the tens of GBs">
  <data key="d0">have</data>
</edge>
<edge source="GPU's computation speed" target="memory capacity">
  <data key="d0">grows faster than</data>
</edge>
<edge source="FLOPS" target="more than 2x from NVIDIA A100 to H100">
  <data key="d0">increases by</data>
</edge>
<edge source="GPU memory" target="80GB maximum">
  <data key="d0">stays at</data>
</edge>
<edge source="the memory" target="an increasingly significant bottleneck">
  <data key="d0">will become</data>
</edge>
<edge source="the KV cache of the prompt part" target="12 of the total KV cache memory in our experiment (6.3)">
  <data key="d0">accounts for</data>
</edge>
<edge source="the KV cache of the prompt part" target="minimize memory usage">
  <data key="d0">can be shared to</data>
</edge>
<edge source="different sample results and their dependence on context and position" target="KV cache to remain unshared">
  <data key="d0">cause</data>
</edge>
<edge source="The extent of KV cache sharing" target="the specific decoding algorithm employed">
  <data key="d0">depends on</data>
</edge>
<edge source="different request beams" target="larger portions (up to 55 memory saving) of their KV cache">
  <data key="d0">can share</data>
</edge>
<edge source="the sharing pattern" target="as the decoding process advances">
  <data key="d0">evolves</data>
</edge>
<edge source="requests to an LLM service" target="variability in their input and output lengths">
  <data key="d0">exhibit</data>
</edge>
<edge source="input prompts for an LLM" target="significantly in length">
  <data key="d0">can vary</data>
</edge>
<edge source="resulting output lengths" target="a priori">
  <data key="d0">are not known</data>
</edge>
<edge source="resulting output lengths" target="both the input prompt and the model">
  <data key="d0">are contingent on</data>
</edge>
<edge source="THE MEMORY MANAGEMENT SYSTEM" target="A WIDE RANGE OF PROMPT LENGTHS">
  <data key="d0">REQUIRES TO ACCOMMODATE</data>
</edge>
<edge source="THE SYSTEM" target="scheduling decisions">
  <data key="d0">needs to make</data>
</edge>
<edge source="scheduling decisions" target="deleting the KV cache of some requests from GPU memory">
  <data key="d0">include</data>
</edge>
<edge source="scheduling decisions" target="swapping out the KV cache of some requests from GPU memory">
  <data key="d0">include</data>
</edge>
<edge source="The allocation" target="the actual input or eventual output length of the request">
  <data key="d0">is irrespective of</data>
</edge>
<edge source="Output lengths from the LLM" target="unpredictable">
  <data key="d0">are</data>
</edge>
<edge source="Request A" target="2048 maximum possible sequence length">
  <data key="d0">has</data>
</edge>
<edge source="THE CHUNK PRE-ALLOCATION SCHEME IN EXISTING SYSTEMS" target="three primary sources of memory wastes">
  <data key="d0">has</data>
</edge>
<edge source="three primary sources of memory wastes" target="reserved slots for future tokens">
  <data key="d0">include</data>
</edge>
<edge source="three primary sources of memory wastes" target="internal fragmentation due to over-provisioning for potential maximum sequence lengths">
  <data key="d0">include</data>
</edge>
<edge source="three primary sources of memory wastes" target="external fragmentation from the memory allocator like the buddy allocator">
  <data key="d0">include</data>
</edge>
<edge source="THE EXTERNAL FRAGMENTATION" target="GENERATED TOKENS">
  <data key="d0">will never be used for</data>
</edge>
<edge source="THE EXTERNAL FRAGMENTATION" target="before serving a request">
  <data key="d0">is known</data>
</edge>
<edge source="Internal fragmentation" target="unused">
  <data key="d0">remains</data>
</edge>
<edge source="reserving this space for the entire requests duration" target="the space that could otherwise be used to process other requests">
  <data key="d0">occupies</data>
</edge>
<edge source="the actual effective memory in previous systems" target="20.4">
  <data key="d0">can be as low as</data>
</edge>
<edge source="614 KV CACHE MANAGER" target="SCHEDULER">
  <data key="d0">includes</data>
</edge>
<edge source="614 KV CACHE MANAGER" target="CPU BLOCK ALLOCATOR">
  <data key="d0">includes</data>
</edge>
<edge source="614 KV CACHE MANAGER" target="GPU BLOCK ALLOCATOR">
  <data key="d0">includes</data>
</edge>
<edge source="614 KV CACHE MANAGER" target="BLOCK TABLES">
  <data key="d0">includes</data>
</edge>
<edge source="614 KV CACHE MANAGER" target="WORKER 0 MODEL SHARD 0 CACHE ENGINE">
  <data key="d0">includes</data>
</edge>
<edge source="The architecture of VLLM" target="Fig.">
  <data key="d0">is shown in</data>
</edge>
<edge source="the system design of VLLM" target="a distributed setting">
  <data key="d0">works in</data>
</edge>
<edge source="Compaction 54" target="a potential solution to fragmentation">
  <data key="d0">has been proposed as</data>
</edge>
<edge source="Performing compaction in a performance-sensitive LLM serving system" target="impractical">
  <data key="d0">is</data>
</edge>
<edge source="Performing compaction in a performance-sensitive LLM serving system" target="the massive KV cache">
  <data key="d0">is impractical due to</data>
</edge>
<edge source="the pre-allocated chunk space for each request" target="memory sharing specific to decoding algorithms in existing memory management systems">
  <data key="d0">prevents</data>
</edge>
<edge source="a centralized scheduler" target="the execution of distributed GPU workers">
  <data key="d0">coordinates</data>
</edge>
<edge source="KV cache manager" target="the physical KV cache memory on the GPU workers">
  <data key="d0">manages</data>
</edge>
<edge source="KV cache manager" target="the instructions sent by the centralized scheduler">
  <data key="d0">manages through</data>
</edge>
<edge source="each GPU worker" target="the same physical block IDs">
  <data key="d0">has</data>
</edge>
<edge source="a worker" target="a portion of the KV cache for its corresponding attention heads">
  <data key="d0">stores</data>
</edge>
<edge source="GPU workers" target="KV cache">
  <data key="d0">read</data>
</edge>
<edge source="GPU workers" target="block table in the control message">
  <data key="d0">read according to</data>
</edge>
<edge source="GPU workers" target="a block engine">
  <data key="d0">have</data>
</edge>
<edge source="GPU workers" target="sampled tokens of this iteration">
  <data key="d0">send</data>
</edge>
<edge source="GPU workers" target="the model with the input token ids">
  <data key="d0">start to execute</data>
</edge>
<edge source="GPU workers" target="memory management">
  <data key="d0">do not need to synchronize on</data>
</edge>
<edge source="GPU workers" target="all the memory management information at the beginning of each decoding iteration along with the step inputs">
  <data key="d0">need to receive</data>
</edge>
<edge source="block table" target="control message">
  <data key="d0">is in</data>
</edge>
<edge source="reading" target="attention layers">
  <data key="d0">occurs in</data>
</edge>
<edge source="this design" target="effective memory management for various decoding methods (4.4)">
  <data key="d0">facilitates</data>
</edge>
<edge source="this design" target="the variable length input and output sequences (4.5)">
  <data key="d0">handles</data>
</edge>
<edge source="each token" target="a set of key and value vectors across layers and attention heads within a layer">
  <data key="d0">has</data>
</edge>
<edge source="all the key and value vectors" target="together within a single KV block">
  <data key="d0">can be managed</data>
</edge>
<edge source="the key and value vectors at different heads and layers" target="a separate block">
  <data key="d0">can each have</data>
</edge>
<edge source="the key and value vectors at different heads and layers" target="in separate block tables">
  <data key="d0">can be managed</data>
</edge>
<edge source="THE TWO DESIGNS" target="no performance difference">
  <data key="d0">have</data>
</edge>
<edge source="the second one" target="easy implementation">
  <data key="d0">is chosen for</data>
</edge>
<edge source="our fathers" target="four score and seven key and value vectors">
  <data key="d0">brought forth</data>
</edge>
<edge source="4" target="the following block-wise computation">
  <data key="d0">can be transformed into</data>
</edge>
<edge source="IS" target="ATTENTION SCORE on -TH KV BLOCK">
  <data key="d0">the row vector of</data>
</edge>
<edge source="THE KEY AND VALUE VECTORS" target="THREE BLOCKS">
  <data key="d0">are spread across</data>
</edge>
<edge source="THE THREE BLOCKS" target="THE PHYSICAL MEMORY">
  <data key="d0">are not contiguous on</data>
</edge>
<edge source="the kernel" target="the query vector of the query token (forth) and the key vectors in a block">
  <data key="d0">multiplies</data>
</edge>
<edge source="the kernel" target="the attention score">
  <data key="d0">computes</data>
</edge>
<edge source="the kernel" target="the value vectors in a block">
  <data key="d0">multiplies with</data>
</edge>
<edge source="the kernel" target="the final attention output">
  <data key="d0">derives</data>
</edge>
<edge source="OS" target="memory into fixed-sized pages">
  <data key="d0">partitions</data>
</edge>
<edge source="OS" target="user programs logical pages to physical pages">
  <data key="d0">maps</data>
</edge>
<edge source="OS" target="shared library across processes">
  <data key="d0">handles</data>
</edge>
<edge source="CONTIGUOUS LOGICAL PAGES" target="NON-CONTIGUOUS PHYSICAL MEMORY PAGES">
  <data key="d0">can correspond to</data>
</edge>
<edge source="USER PROGRAMS" target="MEMORY as though it were CONTIGUOUS">
  <data key="d0">can access</data>
</edge>
<edge source="physical memory space" target="in advance">
  <data key="d0">needs not to be fully reserved</data>
</edge>
<edge source="the OS" target="to dynamically allocate physical pages as needed">
  <data key="d0">enables</data>
</edge>
<edge source="THE LAST KV BLOCKS UNFILLED POSITIONS" target="FUTURE GENERATIONS">
  <data key="d0">ARE RESERVED FOR</data>
</edge>
<edge source="A block engine" target="a contiguous chunk of GPU DRAM">
  <data key="d0">allocates</data>
</edge>
<edge source="Fathers" target="four score and seven years ago">
  <data key="d0">brought</data>
</edge>
<edge source="Physical KV blocks" target="GPU DRAM">
  <data key="d0">are on</data>
</edge>
<edge source="Physical KV blocks" target="Block 0, Block 1, Block 2, Block 3, Block 4, Block 5, Block 6, Block 7, Block 8">
  <data key="d0">include</data>
</edge>
<edge source="Physical KV blocks" target="block 0">
  <data key="d0">include</data>
</edge>
<edge source="Logical KV blocks" target="Physical block number">
  <data key="d0">are represented by</data>
</edge>
<edge source="Logical KV blocks" target="block 0">
  <data key="d0">include</data>
</edge>
<edge source="BLOCK TABLE" target="TRANSLATION IN VLLM">
  <data key="d0">is used in</data>
</edge>
<edge source="The KV block manager" target="block tables">
  <data key="d0">maintains</data>
</edge>
<edge source="The KV block manager" target="the mapping between logical and physical KV blocks of each request">
  <data key="d0">maintains</data>
</edge>
<edge source="each block table entry" target="the corresponding physical blocks of a logical block">
  <data key="d0">records</data>
</edge>
<edge source="each block table entry" target="the number of filled positions">
  <data key="d0">records</data>
</edge>
<edge source="Separating logical and physical KV blocks" target="VLLM to dynamically grow the KV cache memory without reserving it for all positions in advance">
  <data key="d0">allows</data>
</edge>
<edge source="Separating logical and physical KV blocks" target="most memory waste in existing systems">
  <data key="d0">eliminates</data>
</edge>
<edge source="all the blocks" target="left to right">
  <data key="d0">are filled from</data>
</edge>
<edge source="a new physical block" target="all previous blocks are full">
  <data key="d0">is only allocated when</data>
</edge>
<edge source="the final logical block" target="a copy-on-write mechanism">
  <data key="d0">is managed by</data>
</edge>
<edge source="VLLMS physical block sharing" target="frequent memory copy overhead">
  <data key="d0">reduces</data>
</edge>
<edge source="the complex memory sharing" target="a common mapping layer">
  <data key="d0">is via</data>
</edge>
<edge source="a common mapping layer" target="logical blocks to physical blocks">
  <data key="d0">translates</data>
</edge>
<edge source="introducing the VLLMS techniques" target="the performance">
  <data key="d0">may degrade</data>
</edge>
<edge source="the performance" target="primarily compute-bound">
  <data key="d0">is</data>
</edge>
<edge source="the degradation of performance" target="the extra overhead of memory indirection and non-contiguous block memory">
  <data key="d0">is due to</data>
</edge>
<edge source="the new token" target="the first autoregressive decoding step">
  <data key="d0">is generated in</data>
</edge>
<edge source="4.3" target="how PagedAttention and VLLM handle basic decoding algorithms">
  <data key="d0">shows</data>
</edge>
<edge source="PagedAttention and VLLM" target="basic decoding algorithms">
  <data key="d0">handle</data>
</edge>
<edge source="basic decoding algorithms" target="greedy decoding and sampling">
  <data key="d0">include</data>
</edge>
<edge source="greedy decoding and sampling" target="one user prompt as input">
  <data key="d0">take</data>
</edge>
<edge source="greedy decoding and sampling" target="a single output sequence">
  <data key="d0">generate</data>
</edge>
<edge source="the prompt" target="7 tokens">
  <data key="d0">has</data>
</edge>
<edge source="THE REMAINING SLOT" target="THE SUBSEQUENT AUTOREGRESSIVE GENERATION PHASE">
  <data key="d0">is reserved for</data>
</edge>
<edge source="one slot" target="available in the last logical block">
  <data key="d0">remains</data>
</edge>
<edge source="newly generated KV cache" target="there">
  <data key="d0">is stored</data>
</edge>
<edge source="last logical block" target="full">
  <data key="d0">is</data>
</edge>
<edge source="all tokens" target="prompt phase four score and seven years ago our fathers brought">
  <data key="d0">are for</data>
</edge>
<edge source="block 0, block 1, block 2, block 3" target="physical KV blocks">
  <data key="d0">are</data>
</edge>
<edge source="request A" target="request B">
  <data key="d0">is related to</data>
</edge>
<edge source="figure 7" target="mentioned">
  <data key="d0">is</data>
</edge>
<edge source="Storing multiple tokens within a KV block (block size 1)" target="the PagedAttention kernel to process the KV cache across more positions in parallel">
  <data key="d0">enables</data>
</edge>
<edge source="Processing the KV cache across more positions in parallel" target="hardware utilization">
  <data key="d0">increases</data>
</edge>
<edge source="Processing the KV cache across more positions in parallel" target="latency">
  <data key="d0">reduces</data>
</edge>
<edge source="a larger block size" target="memory fragmentation">
  <data key="d0">increases</data>
</edge>
<edge source="THE LOGICAL BLOCKS OF THE TWO SEQUENCES" target="DIFFERENT PHYSICAL BLOCKS">
  <data key="d0">ARE MAPPED TO</data>
</edge>
<edge source="DIFFERENT PHYSICAL BLOCKS" target="THE SPACE RESERVED BY THE BLOCK ENGINE IN GPU WORKERS">
  <data key="d0">ARE WITHIN</data>
</edge>
<edge source="The neighboring logical blocks of both sequences" target="contiguous in physical GPU memory">
  <data key="d0">do not need to be</data>
</edge>
<edge source="The space of physical blocks" target="both sequences">
  <data key="d0">can be effectively utilized by</data>
</edge>
<edge source="A request 616" target="A1">
  <data key="d0">samples</data>
</edge>
<edge source="A1" target="four score and seven years ago">
  <data key="d0">refers to</data>
</edge>
<edge source="Our mothers" target="block 0">
  <data key="d0">are associated with</data>
</edge>
<edge source="A2" target="copy-on-write">
  <data key="d0">is a sample</data>
</edge>
<edge source="A2" target="newly generated KV cache to physical block 1">
  <data key="d0">writes</data>
</edge>
<edge source="Copy-on-write" target="ref count 2">
  <data key="d0">has</data>
</edge>
<edge source="Figure 8" target="the described blocks">
  <data key="d0">illustrates</data>
</edge>
<edge source="one request" target="multiple samples sharing the same input prompt">
  <data key="d0">includes</data>
</edge>
<edge source="multiple samples" target="the same input prompt">
  <data key="d0">share</data>
</edge>
<edge source="the KV cache of the prompt" target="in parallel sampling">
  <data key="d0">is shared</data>
</edge>
<edge source="all parallel sequences in a request" target="the KV cache for the prompt">
  <data key="d0">can share</data>
</edge>
<edge source="8" target="an example of parallel decoding for two outputs">
  <data key="d0">shows</data>
</edge>
<edge source="both outputs" target="the same prompt">
  <data key="d0">share</data>
</edge>
<edge source="the logical blocks for the prompts of both sequences" target="the same physical blocks">
  <data key="d0">are mapped to</data>
</edge>
<edge source="the logical block 0 and 1 of both sequences" target="physical blocks 7 and 1, respectively">
  <data key="d0">are mapped to</data>
</edge>
<edge source="a single physical block" target="multiple logical blocks">
  <data key="d0">can be mapped to</data>
</edge>
<edge source="reference counts for physical block 7" target="2">
  <data key="d0">are</data>
</edge>
<edge source="the two outputs" target="different output tokens">
  <data key="d0">sample</data>
</edge>
<edge source="the two outputs" target="separate storage for KV cache">
  <data key="d0">need</data>
</edge>
<edge source="Sample A2" target="physical block 1">
  <data key="d0">writes to</data>
</edge>
<edge source="The reference count" target="1">
  <data key="d0">is reduced to</data>
</edge>
<edge source="sharing physical blocks across multiple samples" target="greatly reduce memory usage">
  <data key="d0">can</data>
</edge>
<edge source="memory usage" target="long input prompts">
  <data key="d0">is especially reduced for</data>
</edge>
<edge source="LLM tasks like machine translation 59" target="machine translation">
  <data key="d0">include</data>
</edge>
<edge source="Beam search 49" target="the most probable output sequence from an LLM">
  <data key="d0">is widely used to decode</data>
</edge>
<edge source="Beam search 49" target="the computational complexity of fully traversing the block">
  <data key="d0">mitigates</data>
</edge>
<edge source="Beam search" target="each candidate sequence in the beam">
  <data key="d0">expands</data>
</edge>
<edge source="Beam search" target="all possible tokens">
  <data key="d0">considers</data>
</edge>
<edge source="Beam search" target="their respective probabilities using the LLM">
  <data key="d0">computes</data>
</edge>
<edge source="Beam search" target="the top-most probable sequences out of candidates">
  <data key="d0">retains</data>
</edge>
<edge source="Beam search" target="initial prompt blocks">
  <data key="d0">facilitates sharing</data>
</edge>
<edge source="Beam search" target="other blocks across different candidates">
  <data key="d0">facilitates sharing</data>
</edge>
<edge source="V" target="the vocabulary size">
  <data key="d0">is</data>
</edge>
<edge source="THE ALGORITHM" target="THE BEAM WIDTH PARAMETER">
  <data key="d0">RELIES ON</data>
</edge>
<edge source="THE BEAM WIDTH PARAMETER" target="THE NUMBER OF TOP CANDIDATES RETAINED AT EVERY STEP">
  <data key="d0">DETERMINES</data>
</edge>
<edge source="Sharing patterns" target="as the decoding process advances">
  <data key="d0">dynamically change</data>
</edge>
<edge source="Sharing patterns" target="the process tree in the OS created by compound forks">
  <data key="d0">are similar to</data>
</edge>
<edge source="each candidate sequence" target="4 full logical blocks prior to the iteration illustrated as the dotted line">
  <data key="d0">has used</data>
</edge>
<edge source="ALL BEAM CANDIDATES" target="THE FIRST BLOCK 0 (I.E., PROMPT)">
  <data key="d0">SHARE</data>
</edge>
<edge source="CANDIDATE 3" target="others from the second block">
  <data key="d0">digresses from</data>
</edge>
<edge source="all candidates" target="blocks 0, 1, 3">
  <data key="d0">share</data>
</edge>
<edge source="candidates 0 and 1" target="block 6">
  <data key="d0">share</data>
</edge>
<edge source="top-4 probable candidates" target="candidates 1 and 2">
  <data key="d0">originate from</data>
</edge>
<edge source="original candidates 0 and 3" target="the top candidates">
  <data key="d0">are no longer among</data>
</edge>
<edge source="their logical blocks" target="freed">
  <data key="d0">are</data>
</edge>
<edge source="the reference counts of corresponding physical blocks" target="reduced">
  <data key="d0">are</data>
</edge>
<edge source="new physical blocks (blocks 9-12)" target="the new KV cache from the new candidates">
  <data key="d0">store</data>
</edge>
<edge source="Previous LLM serving systems" target="frequent memory copies of the KV cache across the beam candidates">
  <data key="d0">require</data>
</edge>
<edge source="Candidate 3" target="a large portion of Candidate 2's KV cache">
  <data key="d0">would need to copy</data>
</edge>
<edge source="Candidate 3" target="to continue generation">
  <data key="d0">would need to copy a large portion of Candidate 2's KV cache</data>
</edge>
<edge source="most blocks of different beam candidates" target="in VLLM">
  <data key="d0">can be shared</data>
</edge>
<edge source="THE SAME STRATEGY" target="BEAM SEARCH">
  <data key="d0">is applied in</data>
</edge>
<edge source="THE SAME STRATEGY" target="PREFIX SHARING BY VLLM">
  <data key="d0">is applied in</data>
</edge>
<edge source="BEAM SEARCH" target="more sharing">
  <data key="d0">allows for</data>
</edge>
<edge source="THE COPY-ON-WRITE MECHANISM" target="the newly generated tokens are within an old shared block">
  <data key="d0">is applied only when</data>
</edge>
<edge source="the newly generated tokens" target="an old shared block">
  <data key="d0">are within</data>
</edge>
<edge source="the copy-on-write mechanism" target="parallel decoding">
  <data key="d0">is applied in</data>
</edge>
<edge source="LLM user" target="a (long) description of the task including instructions and example inputs and outputs">
  <data key="d0">provides</data>
</edge>
<edge source="a (long) description of the task including instructions and example inputs and outputs" target="system prompt 36">
  <data key="d0">is also known as</data>
</edge>
<edge source="THE DESCRIPTION" target="THE ACTUAL TASK INPUT">
  <data key="d0">is concatenated with</data>
</edge>
<edge source="THE DESCRIPTION AND THE ACTUAL TASK INPUT" target="THE PROMPT OF THE REQUEST">
  <data key="d0">form</data>
</edge>
<edge source="SEA OTTER" target="LOUTRE DE MER">
  <data key="d0">translates to</data>
</edge>
<edge source="PEPPERMINT" target="MENTHE POIVRE">
  <data key="d0">translates to</data>
</edge>
<edge source="PLUSH GIRAFE" target="GIRAFE EN PELUCHE">
  <data key="d0">translates to</data>
</edge>
<edge source="CHEESE" target="FROMAGE">
  <data key="d0">translates to</data>
</edge>
<edge source="I LOVE YOU" target="JE T'AIME">
  <data key="d0">translates to</data>
</edge>
<edge source="THE EXAMPLES" target="5">
  <data key="d0">ARE ADOPTED FROM</data>
</edge>
<edge source="10" target="an example">
  <data key="d0">shows</data>
</edge>
<edge source="the shared prefix" target="prompt engineering">
  <data key="d0">can be tuned via</data>
</edge>
<edge source="tuning the shared prefix via prompt engineering" target="the accuracy of the downstream tasks 26, 27">
  <data key="d0">improves</data>
</edge>
<edge source="many user prompts" target="a prefix">
  <data key="d0">share</data>
</edge>
<edge source="the LLM service provider" target="the KV cache of the prefix in advance">
  <data key="d0">can store</data>
</edge>
<edge source="storing the KV cache of the prefix in advance" target="the redundant computation spent on the prefix">
  <data key="d0">reduces</data>
</edge>
<edge source="A user input prompt with the shared prefix" target="its logical blocks to the cached physical blocks">
  <data key="d0">can map</data>
</edge>
<edge source="The last block" target="copy-on-write">
  <data key="d0">is marked</data>
</edge>
<edge source="THE PROMPT PHASE COMPUTATION" target="the users task input">
  <data key="d0">needs to execute on</data>
</edge>
<edge source="THE DECODING METHODS DISCUSSED EARLIER" target="DIVERSE MEMORY SHARING AND ACCESSING PATTERNS">
  <data key="d0">exhibit</data>
</edge>
<edge source="THE LLM AND ITS EXECUTION KERNEL" target="a list of physical block IDs for each sequence">
  <data key="d0">see</data>
</edge>
<edge source="THE LLM AND ITS EXECUTION KERNEL" target="sharing patterns across sequences">
  <data key="d0">do not need to handle</data>
</edge>
<edge source="this approach" target="the batching opportunities for requests with different sampling requirements">
  <data key="d0">broadens</data>
</edge>
<edge source="this approach" target="the system's overall throughput">
  <data key="d0">increases</data>
</edge>
<edge source="first-come-first-serve (FCFS) scheduling policy" target="all requests">
  <data key="d0">applies to</data>
</edge>
<edge source="first-come-first-serve (FCFS) scheduling policy" target="fairness">
  <data key="d0">ensures</data>
</edge>
<edge source="first-come-first-serve (FCFS) scheduling policy" target="starvation">
  <data key="d0">prevents</data>
</edge>
<edge source="block size" target="too small">
  <data key="d0">is</data>
</edge>
<edge source="block size" target="too large">
  <data key="d0">is</data>
</edge>
<edge source="Eviction policies" target="heuristics">
  <data key="d0">use</data>
</edge>
<edge source="Eviction policies" target="that block">
  <data key="d0">evict</data>
</edge>
<edge source="heuristics" target="which block will be accessed furthest in the future">
  <data key="d0">predict</data>
</edge>
<edge source="all blocks of a sequence" target="together">
  <data key="d0">are accessed</data>
</edge>
<edge source="all-or-nothing eviction policy" target="either evict all or none of the blocks of a sequence">
  <data key="d0">means</data>
</edge>
<edge source="MULTIPLE SEQUENCES WITHIN ONE REQUEST" target="gang-scheduled as a sequence group">
  <data key="d0">are</data>
</edge>
<edge source="sequences within one sequence group" target="true">
  <data key="d0">are preempted or rescheduled together</data>
</edge>
<edge source="sequences within one sequence group" target="due to potential memory sharing across those sequences">
  <data key="d0">are preempted or rescheduled together</data>
</edge>
<edge source="classic technique" target="most virtual memory implementations">
  <data key="d0">is used by</data>
</edge>
<edge source="most virtual memory implementations" target="evicted pages to a swap space on the disk">
  <data key="d0">copy</data>
</edge>
<edge source="CPU block allocator" target="the physical blocks swapped to CPU RAM">
  <data key="d0">manages</data>
</edge>
<edge source="GPU block allocator" target="VLLM">
  <data key="d0">is part of</data>
</edge>
<edge source="its blocks" target="memory">
  <data key="d0">are freed from</data>
</edge>
<edge source="the blocks of a preempted sequence" target="to continue the processing of that sequence">
  <data key="d0">are brought back in</data>
</edge>
<edge source="the number of blocks swapped to the CPU RAM" target="the number of total physical blocks in the GPU RAM">
  <data key="d0">never exceeds</data>
</edge>
<edge source="the swap space on the CPU RAM" target="the GPU memory allocated for the KV cache">
  <data key="d0">is bounded by</data>
</edge>
<edge source="the KV cache" target="the preempted sequences are rescheduled">
  <data key="d0">is recomputed when</data>
</edge>
<edge source="recomputation latency" target="significantly lower than the original latency">
  <data key="d0">can be</data>
</edge>
<edge source="tokens generated at decoding" target="the original user prompt as a new prompt">
  <data key="d0">can be concatenated with</data>
</edge>
<edge source="their KV cache at all positions" target="one prompt phase iteration">
  <data key="d0">can be generated in</data>
</edge>
<edge source="performances of swapping and recomputation" target="bandwidth between CPU RAM and GPU memory">
  <data key="d0">depend on</data>
</edge>
<edge source="performances of swapping and recomputation" target="computation power of the GPU">
  <data key="d0">depend on</data>
</edge>
<edge source="recomputation" target="the KV blocks">
  <data key="d0">does not utilize</data>
</edge>
<edge source="recomputation" target="the block size is small">
  <data key="d0">is more efficient when</data>
</edge>
<edge source="swapping" target="the block size is large">
  <data key="d0">is more efficient when</data>
</edge>
<edge source="swapping" target="excessive overhead with small block sizes">
  <data key="d0">incurs</data>
</edge>
<edge source="recomputation and swapping" target="recovery mechanisms of VLLM">
  <data key="d0">are</data>
</edge>
<edge source="many LLMs" target="parameter sizes exceeding the capacity of a single GPU">
  <data key="d0">have</data>
</edge>
<edge source="a memory manager" target="distributed memory">
  <data key="d0">is capable of handling</data>
</edge>
<edge source="THIS STRATEGY" target="AN SPMD (SINGLE PROGRAM MULTIPLE DATA) EXECUTION SCHEDULE">
  <data key="d0">adheres to</data>
</edge>
<edge source="THE LINEAR LAYERS" target="618">
  <data key="d0">are partitioned</data>
</edge>
<edge source="THE DETAILED MODEL SIZES AND SERVER CONFIGURATIONS" target="TABLE 1">
  <data key="d0">are shown in</data>
</edge>
<edge source="MODEL SIZE 13B" target="GPUS A100 4">
  <data key="d0">requires</data>
</edge>
<edge source="MODEL SIZE 13B" target="TOTAL GPU MEMORY 40 GB">
  <data key="d0">has</data>
</edge>
<edge source="MODEL SIZE 13B" target="PARAMETER SIZE 26 GB">
  <data key="d0">has</data>
</edge>
<edge source="MODEL SIZE 13B" target="MEMORY FOR KV CACHE 12 GB">
  <data key="d0">has</data>
</edge>
<edge source="MODEL SIZE 66B" target="GPUS A100 8">
  <data key="d0">requires</data>
</edge>
<edge source="MODEL SIZE 66B" target="TOTAL GPU MEMORY 160 GB">
  <data key="d0">has</data>
</edge>
<edge source="MODEL SIZE 66B" target="PARAMETER SIZE 132 GB">
  <data key="d0">has</data>
</edge>
<edge source="MODEL SIZE 66B" target="MEMORY FOR KV CACHE 21 GB">
  <data key="d0">has</data>
</edge>
<edge source="MODEL SIZE 175B" target="GPUS A100-80GB 8">
  <data key="d0">requires</data>
</edge>
<edge source="MODEL SIZE 175B" target="TOTAL GPU MEMORY 640 GB">
  <data key="d0">has</data>
</edge>
<edge source="MODEL SIZE 175B" target="PARAMETER SIZE 346 GB">
  <data key="d0">has</data>
</edge>
<edge source="MODEL SIZE 175B" target="MEMORY FOR KV CACHE 264 GB">
  <data key="d0">has</data>
</edge>
<edge source="GPUs" target="intermediate results">
  <data key="d0">synchronize</data>
</edge>
<edge source="synchronize" target="all-reduce operation">
  <data key="d0">method</data>
</edge>
<edge source="block-wise matrix multiplication" target="GPUs">
  <data key="d0">performed by</data>
</edge>
<edge source="the attention operator" target="the attention head dimension">
  <data key="d0">is split on</data>
</edge>
<edge source="each SPMD process" target="a subset of attention heads in multi-head attention">
  <data key="d0">takes care of</data>
</edge>
<edge source="each model shard" target="the same set of input tokens">
  <data key="d0">processes</data>
</edge>
<edge source="the same set of input tokens" target="the KV cache for the same positions">
  <data key="d0">requires</data>
</edge>
<edge source="model parallel execution" target="each model shard processes the same set of input tokens">
  <data key="d0">is used</data>
</edge>
<edge source="Different GPU workers" target="the manager">
  <data key="d0">share</data>
</edge>
<edge source="Different GPU workers" target="the mapping from logical blocks to physical blocks">
  <data key="d0">share</data>
</edge>
<edge source="THIS COMMON MAPPING" target="GPU WORKERS to execute the model with the physical blocks provided by the scheduler for each input request">
  <data key="d0">allows</data>
</edge>
<edge source="the scheduler" target="the message with input token ids for each request in the batch">
  <data key="d0">prepares</data>
</edge>
<edge source="the scheduler" target="the block table for each request">
  <data key="d0">prepares</data>
</edge>
<edge source="THE SCHEDULER" target="THIS CONTROL MESSAGE TO THE GPU WORKERS">
  <data key="d0">BROADCASTS</data>
</edge>
<edge source="sampled tokens of this iteration" target="the scheduler">
  <data key="d0">are sent back to</data>
</edge>
<edge source="THE FRONTEND" target="THE OPENAI API 34 INTERFACE">
  <data key="d0">extends</data>
</edge>
<edge source="THE FRONTEND" target="USERS to customize sampling parameters for each request">
  <data key="d0">allows</data>
</edge>
<edge source="sampling parameters" target="the maximum sequence length">
  <data key="d0">include</data>
</edge>
<edge source="sampling parameters" target="the beam width">
  <data key="d0">include</data>
</edge>
<edge source="THE VLLM ENGINE" target="8.5K lines of Python">
  <data key="d0">is written in</data>
</edge>
<edge source="THE VLLM ENGINE" target="2K lines of CCUDA code">
  <data key="d0">is written in</data>
</edge>
<edge source="model executor" target="popular LLMs such as GPT 5, OPT 62, and LLAMA 52">
  <data key="d0">implements</data>
</edge>
<edge source="INPUT AND OUTPUT LENGTH DISTRIBUTIONS" target="(A) SHAREGPT DATASET">
  <data key="d0">are of</data>
</edge>
<edge source="INPUT AND OUTPUT LENGTH DISTRIBUTIONS" target="(B) ALPACA DATASET">
  <data key="d0">are of</data>
</edge>
<edge source="THE SHAREGPT DATASET" target="THE ALPACA DATASET">
  <data key="d0">has higher variance than</data>
</edge>
<edge source="THE SHAREGPT DATASET" target="a collection of user-shared conversations with ChatGPT">
  <data key="d0">is</data>
</edge>
<edge source="THE ALPACA DATASET" target="an instruction dataset">
  <data key="d0">is</data>
</edge>
<edge source="THE ALPACA DATASET" target="GPT-3.5 with Self-Instruct 57">
  <data key="d0">is generated by</data>
</edge>
<edge source="PYTORCH" target="39">
  <data key="d0">version</data>
</edge>
<edge source="PYTORCH" target="an imperative style, high-performance deep learning library">
  <data key="d0">is</data>
</edge>
<edge source="39" target="ADAM PASZKE, SAM GROSS, FRANCISCO MASSA, ADAM LERER, JAMES BRADBURY, GREGORY CHANAN, TREVOR KILLEEN, ZEMING LIN, NATALIA GIMELSHEIN, LUCA ANTIGA, ET AL.">
  <data key="d0">includes authors</data>
</edge>
<edge source="TRANSFORMERS" target="58">
  <data key="d0">version</data>
</edge>
<edge source="The dynamic block mapping in PagedAttention" target="the performance of the GPU operations involving the stored KV cache">
  <data key="d0">affects</data>
</edge>
<edge source="The GPU operations involving the stored KV cache" target="block readwrites and attention">
  <data key="d0">include</data>
</edge>
<edge source="FUSED RE-SHAPE" target="BLOCK WRITE">
  <data key="d0">and</data>
</edge>
<edge source="new KV cache" target="blocks">
  <data key="d0">are split into</data>
</edge>
<edge source="new KV cache" target="the block table">
  <data key="d0">are saved at positions specified by</data>
</edge>
<edge source="fusing them into a single kernel" target="kernel launch overheads">
  <data key="d0">minimizes</data>
</edge>
<edge source="the attention kernel in FasterTransformer 31" target="KV cache according to the block table">
  <data key="d0">reads</data>
</edge>
<edge source="the attention kernel in FasterTransformer 31" target="attention operations on the fly">
  <data key="d0">performs</data>
</edge>
<edge source="assigning a GPU warp to read each block" target="coalesced memory access">
  <data key="d0">ensures</data>
</edge>
<edge source="BLOCK COPY OPERATIONS" target="THE COPY-ON-WRITE MECHANISM">
  <data key="d0">are issued by</data>
</edge>
<edge source="BLOCK COPY OPERATIONS" target="DISCONTINUOUS BLOCKS">
  <data key="d0">may operate on</data>
</edge>
<edge source="using the cudamemcpyasync API" target="numerous invocations of small data movements">
  <data key="d0">can lead to</data>
</edge>
<edge source="three key methods" target="fork, append, and free">
  <data key="d0">are</data>
</edge>
<edge source="THE FORK METHOD" target="A NEW SEQUENCE FROM AN EXISTING ONE">
  <data key="d0">CREATES</data>
</edge>
<edge source="THE APPEND METHOD" target="A NEW TOKEN TO THE SEQUENCE">
  <data key="d0">APPENDS</data>
</edge>
<edge source="THE FREE METHOD" target="THE SEQUENCE">
  <data key="d0">DELETES</data>
</edge>
<edge source="multiple output sequences" target="the single input sequence">
  <data key="d0">are created from</data>
</edge>
<edge source="future decoding algorithms" target="combining these methods">
  <data key="d0">can be supported by</data>
</edge>
<edge source="OPT-13B" target="1 GPU">
  <data key="d0">uses</data>
</edge>
<edge source="OPT-13B" target="SHAREGPT">
  <data key="d0">runs on</data>
</edge>
<edge source="OPT-13B" target="ALPACA">
  <data key="d0">runs on</data>
</edge>
<edge source="OPT-13B" target="the Alpaca dataset">
  <data key="d0">is used on</data>
</edge>
<edge source="OPT-66B" target="4 GPUS">
  <data key="d0">uses</data>
</edge>
<edge source="OPT-66B" target="SHAREGPT">
  <data key="d0">runs on</data>
</edge>
<edge source="OPT-66B" target="ALPACA">
  <data key="d0">runs on</data>
</edge>
<edge source="OPT-175B" target="8 GPUS">
  <data key="d0">uses</data>
</edge>
<edge source="OPT-175B" target="SHAREGPT">
  <data key="d0">runs on</data>
</edge>
<edge source="OPT-175B" target="ALPACA">
  <data key="d0">runs on</data>
</edge>
<edge source="Latency" target="NORMALIZED LATENCY (STOKEN)">
  <data key="d0">is measured in</data>
</edge>
<edge source="NORMALIZED LATENCY (STOKEN)" target="0.0, 2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5">
  <data key="d0">includes values</data>
</edge>
<edge source="Request rate" target="REQUEST RATE (REQS)">
  <data key="d0">is measured in</data>
</edge>
<edge source="Latency methods" target="FASTERTRANSFORMER">
  <data key="d0">include</data>
</edge>
<edge source="Latency methods" target="ORCA (MAX)">
  <data key="d0">include</data>
</edge>
<edge source="Latency methods" target="ORCA (POW2)">
  <data key="d0">include</data>
</edge>
<edge source="Latency methods" target="ORCA (ORACLE)">
  <data key="d0">include</data>
</edge>
<edge source="Latency methods" target="VLLM">
  <data key="d0">include</data>
</edge>
<edge source="FASTERTRANSFORMER" target="https://github.com/NVIDIA/FASTERTRANSFORMER">
  <data key="d0">is hosted on</data>
</edge>
<edge source="A" target="2">
  <data key="d0">has parallel size</data>
</edge>
<edge source="A" target="request rate (reqs)">
  <data key="d0">measures</data>
</edge>
<edge source="D" target="2">
  <data key="d0">has beam width</data>
</edge>
<edge source="SINGLE SEQUENCE GENERATION" target="OPT models">
  <data key="d0">is performed with</data>
</edge>
<edge source="SINGLE SEQUENCE GENERATION" target="ShareGPT and Alpaca dataset">
  <data key="d0">is performed on</data>
</edge>
<edge source="OPT 62 models" target="13B parameters">
  <data key="d0">have</data>
</edge>
<edge source="OPT 62 models" target="66B parameters">
  <data key="d0">have</data>
</edge>
<edge source="OPT 62 models" target="175B parameters">
  <data key="d0">have</data>
</edge>
<edge source="LLAMA 52" target="13B parameters">
  <data key="d0">has</data>
</edge>
<edge source="13B" target="popular sizes for LLMs">
  <data key="d0">are</data>
</edge>
<edge source="13B" target="the Alpaca dataset">
  <data key="d0">shows the results on</data>
</edge>
<edge source="66B" target="popular sizes for LLMs">
  <data key="d0">are</data>
</edge>
<edge source="13B and 66B" target="an LLM leaderboard 38">
  <data key="d0">are shown in</data>
</edge>
<edge source="175B" target="the size of the famous GPT-3 5 model">
  <data key="d0">is</data>
</edge>
<edge source="workloads" target="ShareGPT 51 and Alpaca 50 datasets">
  <data key="d0">are synthesized based on</data>
</edge>
<edge source="workloads" target="different request rates">
  <data key="d0">have</data>
</edge>
<edge source="ShareGPT 51 and Alpaca 50 datasets" target="input and output texts of real LLM services">
  <data key="d0">contain</data>
</edge>
<edge source="these datasets" target="timestamps">
  <data key="d0">do not include</data>
</edge>
<edge source="BASELINE 1" target="FASTERTRANSFORMER">
  <data key="d0">is</data>
</edge>
<edge source="FASTERTRANSFORMER 31" target="a distributed inference engine highly optimized for latency">
  <data key="d0">is</data>
</edge>
<edge source="a custom scheduler" target="a dynamic batching mechanism">
  <data key="d0">has</data>
</edge>
<edge source="a dynamic batching mechanism" target="the existing serving systems such as Triton 30">
  <data key="d0">is similar to</data>
</edge>
<edge source="maximum batch size" target="the GPU memory capacity">
  <data key="d0">is set according to</data>
</edge>
<edge source="the three ORCA baselines" target="similarly">
  <data key="d0">behave</data>
</edge>
<edge source="ORCA 60" target="a state-of-the-art LLM serving system">
  <data key="d0">is</data>
</edge>
<edge source="ORCA 60" target="throughput">
  <data key="d0">is optimized for</data>
</edge>
<edge source="ORCA 60" target="our approach">
  <data key="d0">is most relevant to</data>
</edge>
<edge source="ORCA" target="not publicly available for use">
  <data key="d0">is</data>
</edge>
<edge source="ORCA" target="ORACLE">
  <data key="d0">is also known as</data>
</edge>
<edge source="ORCA" target="the buddy allocation algorithm">
  <data key="d0">uses</data>
</edge>
<edge source="ORCA" target="upper-bound performance">
  <data key="d0">has</data>
</edge>
<edge source="ORCA" target="by scheduling and interleaving the requests">
  <data key="d0">achieves increased GPU utilization and throughput</data>
</edge>
<edge source="ORCA" target="a distributed serving system for transformer-based generative models">
  <data key="d0">is</data>
</edge>
<edge source="three versions of ORCA" target="how much it over-reserves the space for request outputs">
  <data key="d0">are based on</data>
</edge>
<edge source="the buddy allocation algorithm" target="the memory address to store KV cache">
  <data key="d0">determines</data>
</edge>
<edge source="the system" target="the knowledge of the lengths of the outputs that will be actually generated for the requests">
  <data key="d0">has</data>
</edge>
<edge source="the system" target="the space for outputs by at most 2">
  <data key="d0">over-reserves</data>
</edge>
<edge source="the system" target="the space up to the maximum sequence length of the model">
  <data key="d0">reserves</data>
</edge>
<edge source="upper-bound performance of ORCA" target="infeasible to achieve in practice">
  <data key="d0">is</data>
</edge>
<edge source="true output length" target="25">
  <data key="d0">is</data>
</edge>
<edge source="25" target="TOM KILBURN">
  <data key="d0">includes authors</data>
</edge>
<edge source="25" target="DAVID BG EDWARDS">
  <data key="d0">includes authors</data>
</edge>
<edge source="25" target="MICHAEL J LANIGAN">
  <data key="d0">includes authors</data>
</edge>
<edge source="25" target="FRANK H SUMNER">
  <data key="d0">includes authors</data>
</edge>
<edge source="the maximum sequence length of the model" target="2048 tokens">
  <data key="d0">is</data>
</edge>
<edge source="normalized latency of the systems" target="the mean of every request's end-to-end latency divided by its output length">
  <data key="d0">is</data>
</edge>
<edge source="A HIGH-THROUGHPUT SERVING SYSTEM" target="low normalized latency against high request rates">
  <data key="d0">should retain</data>
</edge>
<edge source="reason" target="cost limit">
  <data key="d0">is</data>
</edge>
<edge source="the Alpaca dataset" target="the ShareGPT dataset">
  <data key="d0">follows a similar trend to</data>
</edge>
<edge source="Parallel generation and beam search" target="OPT-13B">
  <data key="d0">are performed with</data>
</edge>
<edge source="evaluation" target="three models and two datasets">
  <data key="d0">is performed on</data>
</edge>
<edge source="12" target="the results on the ShareGPT dataset">
  <data key="d0">shows</data>
</edge>
<edge source="the curves" target="that as the request rate increases, the latency initially increases at a gradual pace but then suddenly explodes">
  <data key="d0">illustrate</data>
</edge>
<edge source="the latency" target="a gradual pace">
  <data key="d0">initially increases at</data>
</edge>
<edge source="the latency" target="suddenly">
  <data key="d0">then explodes</data>
</edge>
<edge source="request rate" target="capacity of the serving system">
  <data key="d0">surpasses</data>
</edge>
<edge source="queue length" target="infinitely">
  <data key="d0">continues to grow</data>
</edge>
<edge source="latency of the requests" target="infinitely">
  <data key="d0">continues to grow</data>
</edge>
<edge source="ShareGPT dataset" target="many long conversations">
  <data key="d0">contains</data>
</edge>
<edge source="13A, FOR OPT-13B VLLM" target="2.2 more requests at the same time than ORCA (ORACLE)">
  <data key="d0">processes</data>
</edge>
<edge source="13A, FOR OPT-13B VLLM" target="4.3 more requests than ORCA (MAX)">
  <data key="d0">processes</data>
</edge>
<edge source="VLLMS PAGEDATTENTION" target="the memory usage">
  <data key="d0">can efficiently manage</data>
</edge>
<edge source="VLLMS PAGEDATTENTION" target="batching more requests than ORCA">
  <data key="d0">enable</data>
</edge>
<edge source="The iteration-level scheduling in ORCA 60" target="a complementary technique to PagedAttention in VLLM">
  <data key="d0">is</data>
</edge>
<edge source="Both systems" target="the GPU utilization">
  <data key="d0">aim to increase</data>
</edge>
<edge source="Both systems" target="the throughput of LLM serving">
  <data key="d0">aim to increase</data>
</edge>
<edge source="Scheduling and interleaving the requests in ORCA" target="more requests to be processed in parallel">
  <data key="d0">allows</data>
</edge>
<edge source="Increasing memory utilization in VLLM" target="the working sets of more requests to fit into memory">
  <data key="d0">allows</data>
</edge>
<edge source="VLLMS advantage over ORCA (ORACLE) and ORCA (POW2)" target="less pronounced">
  <data key="d0">is</data>
</edge>
<edge source="The model and server configuration for OPT-175B" target="large GPU memory space available to store KV cache">
  <data key="d0">allows for</data>
</edge>
<edge source="The Alpaca dataset" target="short sequences">
  <data key="d0">has</data>
</edge>
<edge source="ORCA (ORACLE) and ORCA (POW2)" target="a large number of requests">
  <data key="d0">can batch</data>
</edge>
<edge source="ORCA (ORACLE) and ORCA (POW2)" target="inefficiencies in their memory management">
  <data key="d0">can batch requests despite</data>
</edge>
<edge source="OUTPUT SEQUENCES" target="2, 4, 6">
  <data key="d0">values</data>
</edge>
<edge source="MEMORY SAVING (A) PARALLEL SAMPLING" target="6.09, 8.53, 9.79">
  <data key="d0">values</data>
</edge>
<edge source="BEAM WIDTH" target="0, 20, 40, 60">
  <data key="d0">values</data>
</edge>
<edge source="MEMORY SAVING (B) BEAM SEARCH" target="37.56, 53.13, 55.16">
  <data key="d0">values</data>
</edge>
<edge source="FIGURE" target="15">
  <data key="d0">number</data>
</edge>
<edge source="15" target="the amount of memory saving">
  <data key="d0">plots</data>
</edge>
<edge source="sharing KV blocks" target="average amount of memory saving">
  <data key="d0">results in</data>
</edge>
<edge source="average amount of memory saving" target="OPT-13B for the Alpaca trace">
  <data key="d0">occurs when serving</data>
</edge>
<edge source="memory sharing in page-dattention" target="two popular sampling methods: parallel sampling and beam search">
  <data key="d0">is evaluated with</data>
</edge>
<edge source="the same experiments with the ShareGPT dataset" target="16.2 - 30.5 memory saving on parallel sampling">
  <data key="d0">showed</data>
</edge>
<edge source="the same experiments with the ShareGPT dataset" target="44.3 - 66.3 memory saving on beam search">
  <data key="d0">showed</data>
</edge>
<edge source="14" target="the results for beam search with different beam widths">
  <data key="d0">shows</data>
</edge>
<edge source="Improvement of VLLM over ORCA" target="1.3 in basic sampling">
  <data key="d0">goes from</data>
</edge>
<edge source="Improvement of VLLM over ORCA" target="2.3 in beam search with a width of 6">
  <data key="d0">goes to</data>
</edge>
<edge source="the amount of memory saving" target="the number of blocks we saved by sharing divided by the number of total blocks without sharing">
  <data key="d0">is computed by</data>
</edge>
<edge source="input prompts" target="a common prefix">
  <data key="d0">share</data>
</edge>
<edge source="THE PREFIX" target="(A) 1 EXAMPLE WITH 80 TOKENS">
  <data key="d0">includes</data>
</edge>
<edge source="THE PREFIX" target="(B) 5 EXAMPLES WITH 341 TOKENS">
  <data key="d0">includes</data>
</edge>
<edge source="the model" target="LLAMA-13B 52">
  <data key="d0">uses</data>
</edge>
<edge source="the model" target="a response">
  <data key="d0">generate</data>
</edge>
<edge source="LLAMA-13B 52" target="multilingual">
  <data key="d0">is</data>
</edge>
<edge source="The first prefix" target="a single example (i.e., one-shot)">
  <data key="d0">includes</data>
</edge>
<edge source="The other prefix" target="5 examples (i.e., few-shot)">
  <data key="d0">includes</data>
</edge>
<edge source="CHATBOT" target="LLMS">
  <data key="d0">is one of the most important applications of</data>
</edge>
<edge source="a response" target="the chatting history and the last user query into a prompt">
  <data key="d0">generated by concatenating</data>
</edge>
<edge source="OPT-13B model" target="limited context length">
  <data key="d0">has</data>
</edge>
<edge source="storing the KV cache between conversation rounds" target="the space for other requests between the conversation rounds">
  <data key="d0">would occupy</data>
</edge>
<edge source="input prompts for most requests" target="1024 tokens">
  <data key="d0">have</data>
</edge>
<edge source="ORCA baselines" target="space for 1024 tokens for the request outputs">
  <data key="d0">reserve</data>
</edge>
<edge source="Buddy allocation algorithm" target="ORCA baselines reserve the space for 1024 tokens for the request outputs">
  <data key="d0">causes</data>
</edge>
<edge source="Latency of attention kernels" target="VLLM (BS 8), FT (BS 8), VLLM (BS 32), FT (BS 32)">
  <data key="d0">is shown for</data>
</edge>
<edge source="BLOCK SIZE" target="1, 2, 4, 8, 16, 32, 64, 128, 256">
  <data key="d0">includes values</data>
</edge>
<edge source="SHAREGPT ALPACA (B)" target="END-TO-END LATENCY">
  <data key="d0">measures</data>
</edge>
<edge source="END-TO-END LATENCY" target="DIFFERENT BLOCK SIZES">
  <data key="d0">varies with</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="extra overheads of accessing the block table">
  <data key="d0">involve</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="executing extra branches">
  <data key="d0">involve</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="handling variable sequence lengths">
  <data key="d0">involve</data>
</edge>
<edge source="18A" target="2026 higher attention kernel latency">
  <data key="d0">leads to</data>
</edge>
<edge source="2026 higher attention kernel latency" target="highly-optimized FasterTransformer implementation">
  <data key="d0">compared to</data>
</edge>
<edge source="the overhead" target="small">
  <data key="d0">is</data>
</edge>
<edge source="the overhead" target="the attention operator">
  <data key="d0">only affects</data>
</edge>
<edge source="the overhead" target="the other operators in the model, such as linear">
  <data key="d0">does not affect</data>
</edge>
<edge source="choice of block size" target="a substantial impact on the performance of VLLM">
  <data key="d0">can have</data>
</edge>
<edge source="block sizes from 16 to 128" target="the best performance">
  <data key="d0">lead to</data>
</edge>
<edge source="block size 16 and 32" target="in the Alpaca trace">
  <data key="d0">work well</data>
</edge>
<edge source="larger block sizes" target="the performance">
  <data key="d0">significantly degrade</data>
</edge>
<edge source="the sequences" target="shorter than the block sizes">
  <data key="d0">become</data>
</edge>
<edge source="block size 16" target="efficiently utilize the GPU">
  <data key="d0">is large enough to</data>
</edge>
<edge source="block size 16" target="avoid significant internal fragmentation in most workloads">
  <data key="d0">is small enough to</data>
</edge>
<edge source="Time" target="milliseconds (ms)">
  <data key="d0">measured in</data>
</edge>
<edge source="Microbenchmark" target="1, 2, 4, 8, 16, 32, 64, 128, 256">
  <data key="d0">has block sizes</data>
</edge>
<edge source="Normalized latency" target="0.0 to 2.5 (stoken)">
  <data key="d0">ranges from</data>
</edge>
<edge source="Figure 19" target="Microbenchmark and End-to-End Performance data">
  <data key="d0">contains</data>
</edge>
<edge source="Performance metrics" target="Recompute, Swap In, Swap Out">
  <data key="d0">include</data>
</edge>
<edge source="overhead" target="recomputation and swapping">
  <data key="d0">is for</data>
</edge>
<edge source="overhead" target="different block sizes">
  <data key="d0">varies with</data>
</edge>
<edge source="the overhead of recomputation" target="constant across different block sizes">
  <data key="d0">remains</data>
</edge>
<edge source="recomputation overhead" target="20% of swapping's latency">
  <data key="d0">is never higher than</data>
</edge>
<edge source="small block sizes" target="numerous small data transfers between CPU and GPU">
  <data key="d0">result in</data>
</edge>
<edge source="numerous small data transfers between CPU and GPU" target="the effective PCIe bandwidth">
  <data key="d0">limit</data>
</edge>
<edge source="the two methods" target="comparable end-to-end performance for medium block sizes from 16 to 64">
  <data key="d0">exhibit</data>
</edge>
<edge source="Tensor shapes" target="typically static">
  <data key="d0">are</data>
</edge>
<edge source="Memory allocation" target="optimized ahead of time">
  <data key="d0">can be</data>
</edge>
<edge source="an increase in memory efficiency" target="any performance improvement">
  <data key="d0">may not result in</data>
</edge>
<edge source="VLLMS all-or-nothing swap-out policy" target="the fact that processing a request requires all of its corresponding token states to be stored in GPU memory">
  <data key="d0">exploits</data>
</edge>
<edge source="recomputation method" target="another example">
  <data key="d0">is</data>
</edge>
<edge source="recomputation method" target="evicted blocks">
  <data key="d0">recovers</data>
</edge>
<edge source="recomputation method" target="OS">
  <data key="d0">is not feasible in</data>
</edge>
<edge source="Model serving" target="an active area of research in recent years">
  <data key="d0">has been</data>
</edge>
<edge source="Numerous systems" target="diverse aspects of deep learning model deployment">
  <data key="d0">are proposed to tackle</data>
</edge>
<edge source="batching" target="serving single or multiple models">
  <data key="d0">is for</data>
</edge>
<edge source="caching" target="serving single or multiple models">
  <data key="d0">is for</data>
</edge>
<edge source="placement" target="serving single or multiple models">
  <data key="d0">is for</data>
</edge>
<edge source="scheduling" target="serving single or multiple models">
  <data key="d0">is for</data>
</edge>
<edge source="DVABATCH 12" target="multi-entry multi-exit batching">
  <data key="d0">introduces</data>
</edge>
<edge source="REEF 21 AND SHEP-HERD 61" target="preemption for serving">
  <data key="d0">propose</data>
</edge>
<edge source="ALPASERVE 28" target="model parallelism for statistical multiplexing">
  <data key="d0">utilizes</data>
</edge>
<edge source="ALPASERVE" target="statistical multiplexing with model parallelism for deep learning serving">
  <data key="d0">is</data>
</edge>
<edge source="general systems" target="the auto-regressive property and token state of LLM inference">
  <data key="d0">fail to take into account</data>
</edge>
<edge source="SERVING SYSTEMS" target="TRANSFORMERS">
  <data key="d0">are specialized for</data>
</edge>
<edge source="numerous specialized serving systems" target="the transformer architecture">
  <data key="d0">have been developed for</data>
</edge>
<edge source="THESE SYSTEMS" target="GPU KERNEL OPTIMIZATIONS 1, 29, 31, 56">
  <data key="d0">utilize</data>
</edge>
<edge source="THESE SYSTEMS" target="ADVANCED BATCHING MECHANISMS 14, 60">
  <data key="d0">utilize</data>
</edge>
<edge source="THESE SYSTEMS" target="MODEL PARALLELISM 1, 41, 60">
  <data key="d0">utilize</data>
</edge>
<edge source="THESE SYSTEMS" target="PARAMETER SHARING 64">
  <data key="d0">utilize</data>
</edge>
<edge source="THESE SYSTEMS" target="efficient serving">
  <data key="d0">utilize</data>
</edge>
<edge source="fine-grained scheduling and interleaving of the requests like in ORCA" target="memory management more challenging">
  <data key="d0">makes</data>
</edge>
<edge source="techniques proposed in VLLM" target="even more crucial">
  <data key="d0">are</data>
</edge>
<edge source="The widening gap between the compute capability and memory capacity of accelerators" target="memory to become a bottleneck for both training and inference">
  <data key="d0">has caused</data>
</edge>
<edge source="FLEXGEN 46" target="how to swap weights and token states for LLM inference with 623 limited GPU memory">
  <data key="d0">studies</data>
</edge>
<edge source="FLEXGEN 46" target="the online serving settings">
  <data key="d0">does not target</data>
</edge>
<edge source="OLLA 48" target="the lifetime and location of tensors">
  <data key="d0">optimizes</data>
</edge>
<edge source="OLLA 48" target="fragmentation">
  <data key="d0">reduces</data>
</edge>
<edge source="OLLA 48" target="fine-grained block-level management">
  <data key="d0">does not do</data>
</edge>
<edge source="OLLA 48" target="online serving">
  <data key="d0">does not do</data>
</edge>
<edge source="FLASHAT-TENTION 13" target="tiling and kernel optimizations">
  <data key="d0">applies</data>
</edge>
<edge source="tiling and kernel optimizations" target="the peak memory of attention computation">
  <data key="d0">reduce</data>
</edge>
<edge source="tiling and kernel optimizations" target="IO costs">
  <data key="d0">reduce</data>
</edge>
<edge source="Xiaoxuan Liu, Zhifeng Chen, Yan-Ping Huang, Anonymous SOSP reviewers, and our shepherd Lidong Zhou" target="insightful feedback">
  <data key="d0">provided</data>
</edge>
<edge source="This research" target="Andreessen Horowitz">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Anyscale">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Astronomer">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Google">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="IBM">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Intel">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Lacework">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Microsoft">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Mohamed Bin Zayed University of Artificial Intelligence">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Samsung SDS">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="Uber">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="This research" target="VMware">
  <data key="d0">is partly supported by</data>
</edge>
<edge source="REFERENCES" target="REZA YAZDANI AMINABADI, SAMYAM RAJBHANDARI, MINJIA ZHANG, AMMAR AHMAD AWAN, CHENG LI, DU LI, ELTON ZHENG, JEFF RASLEY, SHADEN SMITH, OLATUNJI RUWASE, ET AL.">
  <data key="d0">include authors</data>
</edge>
<edge source="DEEPSPEED INFERENCE" target="efficient inference of transformer models at unprecedented scale">
  <data key="d0">enables</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1607.06450">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2107.03374">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1604.06174">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2204.02311">
  <data key="d0">identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2302.11665">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1712.06139">
  <data key="d0">identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2303.06865">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1909.08053">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2302.13971">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2212.10560">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1609.08144">
  <data key="d0">identifier</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2205.01068">
  <data key="d0">has identifier</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:1607.06450" target="2016">
  <data key="d0">was published in</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:2107.03374" target="2021">
  <data key="d0">was published in</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:1604.06174" target="2016">
  <data key="d0">was published in</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:2204.02311" target="2022">
  <data key="d0">year</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:2302.11665" target="2023">
  <data key="d0">published in year</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:1712.06139" target="2017">
  <data key="d0">year</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:2303.06865" target="2023">
  <data key="d0">published in year</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:1909.08053" target="2019">
  <data key="d0">was published in</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:2302.13971" target="2023">
  <data key="d0">publication year</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:2212.10560" target="2022">
  <data key="d0">publication year</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:1609.08144" target="2016">
  <data key="d0">year</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:2205.01068" target="2022">
  <data key="d0">publication year</data>
</edge>
<edge source="YOSHUA BENGIO" target="RJEAN DUCHARME">
  <data key="d0">is an author with</data>
</edge>
<edge source="YOSHUA BENGIO" target="PASCAL VINCENT">
  <data key="d0">is an author with</data>
</edge>
<edge source="RJEAN DUCHARME" target="PASCAL VINCENT">
  <data key="d0">is an author with</data>
</edge>
<edge source="A NEURAL PROBABILISTIC LANGUAGE MODEL" target="a model">
  <data key="d0">is</data>
</edge>
<edge source="4 OND REJ BOJAR" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="RAJEN CHATTERJEE" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="CHRISTIAN FEDERMANN" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="YVETTE GRAHAM" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="BARRY HADDOW" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="MATTHIAS HUCK" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="ANTONIO JIMENO YEPES" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="PHILIPP KOEHN" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="VARVARA LOGACHEVA" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="CHRISTOF MONZ" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="MATTEO NEGRI" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="AURELIE NEVEOL" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="MARIANA NEVES" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="MARTIN POPEL" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="MATT POST" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="RAPHAEL RUBINO" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="CAROLINA SCARTON" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="LUCIA SPECIA" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="MARCO TURCHI" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="KARIN VERSPOOR" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="MARCOS ZAMPIERI" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS" target="BERLIN">
  <data key="d0">is located in</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS" target="GERMANY">
  <data key="d0">is located in</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS" target="131198">
  <data key="d0">has identifier</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.">
  <data key="d0">has authors</data>
</edge>
<edge source="Language models" target="few-shot learners">
  <data key="d0">are</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35" target="2022">
  <data key="d0">is published in</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35" target="16344-16359">
  <data key="d0">has page range</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="27">
  <data key="d0">is volume</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="32">
  <data key="d0">is volume</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27" target="2014">
  <data key="d0">was published in</data>
</edge>
<edge source="14 JIARUI FANG" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="YANG YU" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="CHENGDUO ZHAO" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="JIE ZHOU" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="64" target="ZHE ZHOU">
  <data key="d0">is associated with</data>
</edge>
<edge source="64" target="XUECHAO WEI">
  <data key="d0">is associated with</data>
</edge>
<edge source="64" target="JIEJING ZHANG">
  <data key="d0">is associated with</data>
</edge>
<edge source="64" target="GUANGYU SUN">
  <data key="d0">is associated with</data>
</edge>
<edge source="Training deep nets" target="sublinear memory cost">
  <data key="d0">has</data>
</edge>
<edge source="VICUNA" target="an open-source chatbot">
  <data key="d0">is</data>
</edge>
<edge source="VICUNA" target="GPT-4">
  <data key="d0">is impressing</data>
</edge>
<edge source="VICUNA" target="90 ChatGPT quality">
  <data key="d0">has</data>
</edge>
<edge source="ORGBLOG2023-03-30-VICUNA 9" target="Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.">
  <data key="d0">has authors</data>
</edge>
<edge source="PALM" target="scaling language modeling with Pathways">
  <data key="d0">is about</data>
</edge>
<edge source="INFERLINE" target="latency-aware provisioning and scaling for prediction serving pipelines">
  <data key="d0">is about</data>
</edge>
<edge source="Proceedings" target="the 11th ACM Symposium on Cloud Computing">
  <data key="d0">are of</data>
</edge>
<edge source="Proceedings" target="the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming">
  <data key="d0">are of</data>
</edge>
<edge source="Proceedings" target="the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers">
  <data key="d0">of</data>
</edge>
<edge source="CLIPPER" target="a low-latency online prediction serving system">
  <data key="d0">is</data>
</edge>
<edge source="DVABATCH" target="DIVERSITY-AWARE MULTI-ENTRY MULTI-EXIT BATCHING">
  <data key="d0">is</data>
</edge>
<edge source="DVABATCH" target="EFFICIENT PROCESSING OF DNN SERVICES ON GPUS">
  <data key="d0">is used for</data>
</edge>
<edge source="USENIX ANNUAL TECHNICAL CONFERENCE" target="2022">
  <data key="d0">occurred in</data>
</edge>
<edge source="USENIX ANNUAL TECHNICAL CONFERENCE" target="USENIX ATC 22">
  <data key="d0">abbreviated as</data>
</edge>
<edge source="TURBOTRANSFORMERS" target="an efficient GPU serving system for transformer models">
  <data key="d0">is</data>
</edge>
<edge source="26TH ACM SIGPLAN SYMPOSIUM" target="PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING">
  <data key="d0">is on</data>
</edge>
<edge source="FASTAPI" target="15">
  <data key="d0">is</data>
</edge>
<edge source="FASTAPI" target="a web framework">
  <data key="d0">is</data>
</edge>
<edge source="URL" target="https://github.com/tiangolo/fastapi">
  <data key="d0">is</data>
</edge>
<edge source="17 AMIR GHOLAMI" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="ZHEWEI YAO" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="SEHOON KIM" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="MICHAEL W MAHONEY" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="KURT KEUTZER" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="KURT KEUTZER" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="18" target="GITHUB">
  <data key="d0">is associated with</data>
</edge>
<edge source="GitHub Copilot" target="GitHub">
  <data key="d0">is a feature of</data>
</edge>
<edge source="GitHub Copilot" target="Google">
  <data key="d0">is associated with</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, Jonathan Mace">
  <data key="d0">has authors</data>
</edge>
<edge source="MICROSECOND-SCALE PREEMPTION" target="CONCURRENT GPU-ACCELERATED DNN INFERENCES">
  <data key="d0">is for</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING" target="IMAGE RECOGNITION">
  <data key="d0">is used for</data>
</edge>
<edge source="SWAPADVISOR" target="deep learning beyond the GPU memory limit via smart swapping">
  <data key="d0">pushes</data>
</edge>
<edge source="The twenty-fifth international conference" target="Architectural Support for Programming Languages and Operating Systems">
  <data key="d0">is on</data>
</edge>
<edge source="24 PARAS JAIN" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="AJAY JAIN" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="ANIRUDDHA NRUSIMHA" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="AMIR GHOLAMI" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="PIETER ABBEEL" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="JOSEPH GONZALEZ" target="true">
  <data key="d0">is a person</data>
</edge>
<edge source="The power of scale" target="parameter-efficient prompt tuning">
  <data key="d0">is for</data>
</edge>
<edge source="RAMMER" target="holistic deep learning compiler optimizations with rTasks">
  <data key="d0">enables</data>
</edge>
<edge source="NVIDIA" target="30">
  <data key="d0">is</data>
</edge>
<edge source="NVIDIA" target="31">
  <data key="d0">is</data>
</edge>
<edge source="32" target="NVIDIA">
  <data key="d0">is associated with</data>
</edge>
<edge source="NVIDIA Triton Inference Server" target="https://developer.nvidia.com">
  <data key="d0">website</data>
</edge>
<edge source="NCCL" target="THE NVIDIA COLLECTIVE COMMUNICATION LIBRARY">
  <data key="d0">is</data>
</edge>
<edge source="TENSORFLOW-SERVING" target="flexible, high-performance ML serving">
  <data key="d0">is</data>
</edge>
<edge source="OPENAI" target="34">
  <data key="d0">is</data>
</edge>
<edge source="ARXIV:2303.08774" target="CS.CL">
  <data key="d0">has category</data>
</edge>
<edge source="ARXIV:2303.08774" target="38">
  <data key="d0">has number</data>
</edge>
<edge source="ARXIV:2303.08774" target="LMSYS ORG">
  <data key="d0">associated with</data>
</edge>
<edge source="CHATBOT ARENA LEADERBOARD WEEK 8" target="MT-BENCH and VICUNA-33B">
  <data key="d0">introduces</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32" target="2019">
  <data key="d0">was published in</data>
</edge>
<edge source="POET" target="training neural networks on tiny devices with integrated rematerialization and paging">
  <data key="d0">is about</data>
</edge>
<edge source="PMLR" target="1757317583">
  <data key="d0">has identifier</data>
</edge>
<edge source="41" target="REINER POPE">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="SHOLTO DOUGLAS">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="AAKANKSHA CHOWDHERY">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="JACOB DEVLIN">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="JAMES BRADBURY">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="ANSELM LEVSKAYA">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="JONATHAN HEEK">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="KEFAN XIAO">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="SHIVANI AGRAWAL">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="JEFF DEAN">
  <data key="d0">includes</data>
</edge>
<edge source="Amazon Web Services" target="https://www.reuters.com/technology/tech-giants-ai-like-bing-bard-poses-billion-dollar-search-problem-2023-02-22">
  <data key="d0">is mentioned in</data>
</edge>
<edge source="Authors" target="Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram">
  <data key="d0">include</data>
</edge>
<edge source="Authors" target="Ilya Sutskever">
  <data key="d0">include</data>
</edge>
<edge source="Authors" target="Oriol Vinyals">
  <data key="d0">include</data>
</edge>
<edge source="Authors" target="Quoc V Le">
  <data key="d0">include</data>
</edge>
<edge source="Website" target="https://aws.amazon.com/bedrock">
  <data key="d0">is</data>
</edge>
<edge source="NEXUS" target="a GPU cluster engine">
  <data key="d0">is</data>
</edge>
<edge source="NEXUS" target="accelerating DNN-based video analysis">
  <data key="d0">purpose</data>
</edge>
<edge source="HIGH-THROUGHPUT GENERATIVE INFERENCE" target="LARGE LANGUAGE MODELS with a SINGLE GPU">
  <data key="d0">is performed on</data>
</edge>
<edge source="MEGATRON-LM" target="training multi-billion parameter language models using model parallelism">
  <data key="d0">is used for</data>
</edge>
<edge source="OLLA" target="the lifetime and location of arrays">
  <data key="d0">optimizes</data>
</edge>
<edge source="optimizing the lifetime and location of arrays" target="the memory usage of neural networks">
  <data key="d0">reduces</data>
</edge>
<edge source="DOI" target="10.48550/ARXIV.2210.12924">
  <data key="d0">is</data>
</edge>
<edge source="STANFORD ALPACA" target="an instruction-following LLaMA model">
  <data key="d0">is</data>
</edge>
<edge source="HTTPS" target="GITHUB.COM/TATSU-LAB/STANFORD-ALPACA">
  <data key="d0">is</data>
</edge>
<edge source="HUGO TOUVRON" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="THIBAUT LAVRIL" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="GAUTIER IZACARD" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="XAVIER MARTINET" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="MARIE-ANNE LACHAUX" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="TIMOTHE LACROIX" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="BAPTISTE ROZIRE" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="NAMAN GOYAL" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="ERIC HAMBRO" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="FAISAL AZHAR" target="HTTPS:SHAREGPT.COM 52">
  <data key="d0">is listed at</data>
</edge>
<edge source="LLAMA" target="open and efficient foundation language models">
  <data key="d0">is</data>
</edge>
<edge source="PACMAN" target="an efficient compaction approach">
  <data key="d0">is</data>
</edge>
<edge source="PACMAN" target="log-structured key-value store">
  <data key="d0">is for</data>
</edge>
<edge source="log-structured key-value store" target="persistent memory">
  <data key="d0">is on</data>
</edge>
<edge source="SUPERNEURONS" target="dynamic GPU memory management for training deep neural networks">
  <data key="d0">is about</data>
</edge>
<edge source="LIGHTSEQ" target="a high performance inference library for transformers">
  <data key="d0">is</data>
</edge>
<edge source="SELF-INSTRUCT" target="aligning language model with self generated instructions">
  <data key="d0">is about</data>
</edge>
<edge source="Google's Neural Machine Translation System" target="the gap between human and machine translation">
  <data key="d0">bridges</data>
</edge>
<edge source="Susan Zhang" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Stephen Roller" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Naman Goyal" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Mikel Artetxe" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Moya Chen" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Shuohui Chen" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Christopher Dewan" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Mona Diab" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Xian Li" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="Xi Victoria Lin" target="NSDI23 conference">
  <data key="d0">is a presenter at</data>
</edge>
<edge source="ALPA" target="inter-operator parallelism">
  <data key="d0">automates</data>
</edge>
<edge source="ALPA" target="intra-operator parallelism">
  <data key="d0">automates</data>
</edge>
<edge source="ALPA" target="distributed deep learning">
  <data key="d0">is used for</data>
</edge>
<edge source="PETS" target="A unified framework for parameter-efficient transformers serving">
  <data key="d0">is</data>
</edge>
</graph></graphml>