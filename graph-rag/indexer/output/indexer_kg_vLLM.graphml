<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d0" for="edge" attr.name="relationship" attr.type="string"/>
<graph edgedefault="directed"><node id="Efficient Memory Management for Large Language Model Serving with PagedAttention"/>
<node id="Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica"/>
<node id="Woosuk Kwon"/>
<node id="UC Berkeley"/>
<node id="Zhuohan Li"/>
<node id="Siyuan Zhuang"/>
<node id="Ying Sheng"/>
<node id="UC Berkeley and Stanford University"/>
<node id="Lianmin Zheng"/>
<node id="Cody Hao Yu"/>
<node id="Independent Researcher"/>
<node id="Joseph E. Gonzalez"/>
<node id="Hao Zhang"/>
<node id="UC San Diego"/>
<node id="Ion Stoica"/>
<node id="High throughput serving of large language models (LLMs)"/>
<node id="batching sufficiently many requests at a time"/>
<node id="existing systems"/>
<node id="because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically"/>
<node id="The KV Cache size"/>
<node id="the number of requests"/>
<node id="this memory"/>
<node id="fragmentation and redundant duplication"/>
<node id="wasting of this memory"/>
<node id="inefficient management"/>
<node id="the batch size"/>
<node id="inefficient memory management"/>
<node id="PagedAttention"/>
<node id="an attention algorithm"/>
<node id="the classical virtual memory and paging techniques in operating systems"/>
<node id="the operating systems (OS) solution to memory fragmentation and sharing"/>
<node id="virtual memory with paging"/>
<node id="KV cache stored in non-contiguous paged memory"/>
<node id="the virtual memory and paging in OS"/>
<node id="the classic idea of paging in operating systems"/>
<node id="the memory challenges in 3"/>
<node id="continuous keys and values in non-contiguous memory space"/>
<node id="traditional attention algorithms"/>
<node id="PagedAttention algorithm"/>
<node id="attention key and values vectors stored as non-contiguous blocks in the memory"/>
<node id="This paper"/>
<node id="a new attention algorithm"/>
<node id="attention keys and values to be stored in non-contiguous paged memory"/>
<node id="vLLM"/>
<node id="a high-throughput LLM serving system"/>
<node id="efficient memory management enabled by PagedAttention"/>
<node id="an LLM serving system"/>
<node id="near-zero waste in KV cache memory"/>
<node id="flexible sharing of KV cache within and across requests"/>
<node id="flexible sharing of KV cache"/>
<node id="to further reduce memory usage"/>
<node id="existing LLM serving systems 31, 60"/>
<node id="managing the KV cache memory efficiently"/>
<node id="we"/>
<node id="the KV cache in a more flexible way"/>
<node id="blocks"/>
<node id="pages"/>
<node id="tokens"/>
<node id="bytes"/>
<node id="requests"/>
<node id="processes"/>
<node id="a high-throughput distributed LLM serving engine"/>
<node id="KV cache"/>
<node id="memory management in existing systems"/>
<node id="established techniques"/>
<node id="operating systems"/>
<node id="virtual memory"/>
<node id="copy-on-write"/>
<node id="efficiently manage KV cache"/>
<node id="handle various decoding algorithms in LLM serving"/>
<node id="the throughput of popular LLMs by 2-4"/>
<node id="the same level of latency compared to the state-of-the-art systems"/>
<node id="state-of-the-art systems"/>
<node id="FasterTransformer and Orca"/>
<node id="LLM serving throughput by 2-4 compared to the state-of-the-art systems 31, 60"/>
<node id="model accuracy"/>
<node id="the performance of vLLM under a variety of workloads"/>
<node id="2-4 throughput improvements over the state-of-the-art systems"/>
<node id="The improvement"/>
<node id="longer sequences"/>
<node id="larger models"/>
<node id="more complex decoding algorithms"/>
<node id="The improvements"/>
<node id="vLLMs source code"/>
<node id="https://github.com/vllm-project/vllm"/>
<node id="large language models (LLMs) like GPT 5, 37 and PaLM 9"/>
<node id="new applications such as programming assistants 6, 18 and universal chatbots 19, 35"/>
<node id="programming assistants 6, 18 and universal chatbots 19, 35"/>
<node id="our work and daily routines"/>
<node id="Many cloud companies 34, 44"/>
<node id="these applications as hosted services"/>
<node id="running these applications"/>
<node id="very expensive"/>
<node id="a large number of hardware accelerators such as GPUs"/>
<node id="processing an LLM request"/>
<node id="10 more expensive than a traditional keyword query"/>
<node id="SOSP 23"/>
<node id="October 23-26, 2023"/>
<node id="Koblenz, Germany"/>
<node id="Copyright"/>
<node id="the ownerauthor(s)"/>
<node id="2023"/>
<node id="ACM"/>
<node id="979-8-4007-0229-72310"/>
<node id="DOI"/>
<node id="https://doi.org/10.1145/3600006.3613165"/>
<node id="NVIDIA A100"/>
<node id="40GB"/>
<node id="Parameters"/>
<node id="26GB"/>
<node id="KV Cache"/>
<node id="30"/>
<node id="Others"/>
<node id="20"/>
<node id="40"/>
<node id="Memory usage"/>
<node id="GB"/>
<node id="Batch size"/>
<node id="Throughput"/>
<node id="Existing systems"/>
<node id="Memory layout"/>
<node id="an LLM with 13B parameters on NVIDIA A100"/>
<node id="1 (left)"/>
<node id="the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM"/>
<node id="The parameters (gray)"/>
<node id="GPU memory throughout serving"/>
<node id="The memory for the KV cache (red)"/>
<node id="per serving request"/>
<node id="the contiguous chunk of memory"/>
<node id="storing the KV cache of a request in contiguous space"/>
<node id="all available memory"/>
<node id="output length of a request"/>
<node id="decoding"/>
<node id="memory required for its KV cache"/>
<node id="as output length of a request grows at decoding"/>
<node id="available memory for incoming requests or ongoing generation for existing prompts"/>
<node id="A small amount of memory (yellow)"/>
<node id="ephemerally for activation"/>
<node id="the rapid growth curve of KV cache memory seen in existing systems 31, 60"/>
<node id="a notable boost in serving throughput"/>
<node id="The key idea behind vLLMs memory manager"/>
<node id="the virtual memory 25 in operating systems"/>
<node id="the ideas behind virtual memory to manage the KV cache in an LLM service"/>
<node id="cost per request"/>
<node id="more important"/>
<node id="LLM serving systems"/>
<node id="LLMs"/>
<node id="an autoregressive Transformer model"/>
<node id="This model"/>
<node id="words (tokens)"/>
<node id="one at a time"/>
<node id="the input (prompt)"/>
<node id="the previous sequence of the outputs tokens it has generated so far"/>
<node id="this expensive process"/>
<node id="for each request"/>
<node id="the model"/>
<node id="a termination token"/>
<node id="This sequential generation process"/>
<node id="the workload memory-bound"/>
<node id="the computation power of GPUs"/>
<node id="the serving throughput"/>
<node id="Improving the throughput"/>
<node id="batching multiple requests together"/>
<node id="memory space for each request"/>
<node id="efficiently managed"/>
<node id="One exception"/>
<node id="Fig."/>
<node id="Approximately 65 of the memory"/>
<node id="the model weights"/>
<node id="static during serving"/>
<node id="Close to 30 of the memory"/>
<node id="the dynamic states of the requests"/>
<node id="these states"/>
<node id="the key and value tensors associated with the attention mechanism"/>
<node id="the key and value tensors"/>
<node id="KV cache 41"/>
<node id="the context from earlier tokens"/>
<node id="generate new output tokens in sequence"/>
<node id="611 Orca (Max)"/>
<node id="20.4"/>
<node id="Orca (Pow2)"/>
<node id="13.3"/>
<node id="Orca (Oracle)"/>
<node id="57.3"/>
<node id="8.9"/>
<node id="26.8"/>
<node id="17.9"/>
<node id="13.6"/>
<node id="41.6"/>
<node id="38.2"/>
<node id="25.2"/>
<node id="36.6"/>
<node id="96.3"/>
<node id="Average percentage of memory wastes"/>
<node id="different LLM serving systems during the experiment in 6.2"/>
<node id="Percentage of memory"/>
<node id="other data, including activations"/>
<node id="Activations"/>
<node id="the ephemeral tensors created when evaluating the LLM"/>
<node id="model weights"/>
<node id="constant"/>
<node id="activations"/>
<node id="a small fraction of the GPU memory"/>
<node id="the way the KV cache is managed"/>
<node id="critical in determining the maximum batch size"/>
<node id="KV cache memory"/>
<node id="batch size"/>
<node id="throughput of the LLM"/>
<node id="limiting batch size and throughput"/>
<node id="KV cache memory is managed inefficiently"/>
<node id="fine-grained batching"/>
<node id="the waste of computing"/>
<node id="requests to be batched in a more flexible way"/>
<node id="the number of requests that can be batched together"/>
<node id="GPU memory capacity"/>
<node id="the space allocated to store the KV cache"/>
<node id="The idea of virtual memory and paging"/>
<node id="managing the KV cache in LLM serving"/>
<node id="The workload"/>
<node id="dynamic memory allocation"/>
<node id="The output length"/>
<node id="not known a priori"/>
<node id="Performance"/>
<node id="the GPU memory capacity"/>
<node id="most deep learning frameworks 33, 39"/>
<node id="tensors to be stored in contiguous memory"/>
<node id="most operators in current deep learning frameworks"/>
<node id="previous LLM serving systems"/>
<node id="the KV cache of one request as a contiguous tensor across the different positions"/>
<node id="unique characteristics"/>
<node id="as the model generates new tokens"/>
<node id="lifetime and length of KV cache"/>
<node id="existing systems approach"/>
<node id="significantly inefficient in two ways"/>
<node id="existing systems 31, 60"/>
<node id="internal and external memory fragmentation"/>
<node id="the requests actual length"/>
<node id="much shorter than its maximum length"/>
<node id="pre-allocation"/>
<node id="inefficient"/>
<node id="entire chunk"/>
<node id="requests lifetime"/>
<node id="other shorter requests"/>
<node id="any part of the chunk that is currently unused"/>
<node id="external memory fragmentation"/>
<node id="significant"/>
<node id="pre-allocated size"/>
<node id="each request"/>
<node id="the actual token states"/>
<node id="20.4 - 38.2 of the KV cache memory"/>
<node id="KV cache of one token"/>
<node id="all its previous tokens"/>
<node id="the KV cache of the same token appearing at different positions in a sequence"/>
<node id="different"/>
<node id="The token in each memory slot"/>
<node id="its KV cache"/>
<node id="the same tokens"/>
<node id="different KV cache"/>
<node id="at different positions"/>
<node id="the existing systems"/>
<node id="the opportunities for memory sharing"/>
<node id="LLM services"/>
<node id="advanced decoding algorithms"/>
<node id="parallel sampling"/>
<node id="beam search"/>
<node id="multiple outputs per request"/>
<node id="a range of decoding algorithms"/>
<node id="users"/>
<node id="decoding algorithms"/>
<node id="varying implications for memory management complexity"/>
<node id="an LLM service"/>
<node id="more complex decoding scenarios"/>
<node id="complex accessing patterns"/>
<node id="more opportunities for memory sharing"/>
<node id="the request"/>
<node id="multiple sequences"/>
<node id="their KV cache"/>
<node id="the KV cache of two requests at the same time"/>
<node id="a request"/>
<node id="its generation"/>
<node id="its KV blocks"/>
<node id="the KV cache of other requests"/>
<node id="memory sharing"/>
<node id="the KV cache of the sequences"/>
<node id="separate contiguous spaces"/>
<node id="the requests KV cache into blocks"/>
<node id="each block"/>
<node id="the attention keys and values of a fixed number of tokens"/>
<node id="PagedAttention kernel"/>
<node id="different KV blocks"/>
<node id="blocks for the KV cache in PagedAttention"/>
<node id="contiguous space"/>
<node id="The KV cache manager"/>
<node id="the KV cache"/>
<node id="paged fashion"/>
<node id="paged fashion management of the KV cache"/>
<node id="the design of the KV cache manager in 4.2"/>
<node id="the design of the KV cache manager"/>
<node id="PagedAttention in 4.3"/>
<node id="the KV cache of each sequence into KV blocks"/>
<node id="organizing the KV cache as fixed-size KV blocks"/>
<node id="fixed-size KV blocks"/>
<node id="pages in virtual memory"/>
<node id="This design"/>
<node id="internal fragmentation"/>
<node id="relatively small blocks"/>
<node id="relatively small blocks on demand"/>
<node id="all blocks"/>
<node id="the same size"/>
<node id="the different sequences associated with the same request"/>
<node id="the different requests"/>
<node id="KV blocks to be stored in non-contiguous physical memory"/>
<node id="storing KV blocks in non-contiguous physical memory"/>
<node id="more flexible paged memory management in vLLM"/>
<node id="PagedAttention kernel to access the previous KV cache"/>
<node id="previous KV cache"/>
<node id="logical KV blocks"/>
<node id="newly generated KV cache into the physical KV blocks"/>
<node id="this sharing easily via its PagedAttention and paged memory management"/>
<node id="memory via its PagedAttention and paged memory management"/>
<node id="popular LLMs such as GPT 5, OPT 62, and LLaMA 52"/>
<node id="varying sizes"/>
<node id="ones exceeding the memory capacity of a single GPU"/>
<node id="the challenges in memory allocation in serving LLMs"/>
<node id="their impact on serving performance"/>
<node id="We"/>
<node id="a distributed LLM serving engine"/>
<node id="up to 22 higher request rates compared to FasterTransformer"/>
<node id="FasterTransformer"/>
<node id="a fine-grained scheduling mechanism"/>
<node id="inefficiently like Orca (Max)"/>
<node id="memory fragmentation"/>
<node id="sharing"/>
<node id="more requests in a batch in parallel"/>
<node id="a 2-4 speedup compared to Orca"/>
<node id="this section"/>
<node id="the generation and serving procedures of typical LLMs"/>
<node id="the iteration-level scheduling used in LLM serving"/>
<node id="language modeling"/>
<node id="the probability of a list of tokens"/>
<node id="language"/>
<node id="a natural sequential ordering"/>
<node id="Transformers 53"/>
<node id="the de facto standard architecture for modeling the probability above at a large scale"/>
<node id="The most important component of a Transformer-based language model"/>
<node id="its self-attention layers"/>
<node id="a self-attention layer"/>
<node id="linear transformations on each position to get the query, key, and value vectors"/>
<node id="the self-attention layer"/>
<node id="the attention score"/>
<node id="multiplying the query vector at one position with all the key vectors before it"/>
<node id="the output as the weighted average over the value vectors"/>
<node id="all other components in the Transformer model"/>
<node id="embedding layer"/>
<node id="feed-forward layer"/>
<node id="layer normalization 2"/>
<node id="residual connection 22"/>
<node id="output logit computation"/>
<node id="query, key, and value transformation in Eq."/>
<node id="a conditional generation service"/>
<node id="A request to an LLM service"/>
<node id="a list of input prompt tokens"/>
<node id="the concatenation of the prompt and output lists as sequence"/>
<node id="the LLM"/>
<node id="new tokens one by one"/>
<node id="the generation process of each new token"/>
<node id="all the previous tokens in that sequence"/>
<node id="their key and value vectors"/>
<node id="key and value vectors of existing tokens"/>
<node id="generating future tokens"/>
<node id="A requests KV cache"/>
<node id="a series of logical KV blocks"/>
<node id="left to right"/>
<node id="new tokens and their KV cache are generated"/>
<node id="generation computation in the LLM service"/>
<node id="two phases"/>
<node id="The prompt phase"/>
<node id="the whole user prompt"/>
<node id="the computation of the prompt phase"/>
<node id="matrix-matrix multiplication operations"/>
<node id="this phase"/>
<node id="the parallelism inherent in GPUs"/>
<node id="The autoregressive generation phase"/>
<node id="the remaining new tokens sequentially"/>
<node id="key and value vectors at positions 1 to 1"/>
<node id="previous iterations"/>
<node id="new key and value vector"/>
<node id="this iteration"/>
<node id="This phase"/>
<node id="the sequence reaches a maximum length"/>
<node id="maximum length"/>
<node id="an end-of-sequence (eos) token is emitted"/>
<node id="The computation at different iterations"/>
<node id="parallelized"/>
<node id="the data dependency"/>
<node id="matrix-vector multiplication"/>
<node id="less efficient"/>
<node id="GPU computation"/>
<node id="memory-bound"/>
<node id="most portion of the latency of a single request"/>
<node id="compute utilization in serving LLMs"/>
<node id="batching multiple requests"/>
<node id="batching the requests to an LLM service"/>
<node id="non-trivial"/>
<node id="the same model weights"/>
<node id="the overhead of moving weights"/>
<node id="the requests in a batch"/>
<node id="the computational overhead"/>
<node id="the batch size being sufficiently large"/>
<node id="different times"/>
<node id="A naive batching strategy"/>
<node id="earlier requests wait for later ones"/>
<node id="delay the incoming requests until earlier ones finish"/>
<node id="delaying the incoming requests until earlier ones finish"/>
<node id="significant queueing delays"/>
<node id="vastly different input and output lengths"/>
<node id="A straightforward batching technique"/>
<node id="the inputs and outputs of the requests"/>
<node id="padding the inputs and outputs of the requests"/>
<node id="equalize their lengths"/>
<node id="GPU computation and memory"/>
<node id="fine-grained batching mechanisms"/>
<node id="cellular batching"/>
<node id="iteration-level scheduling"/>
<node id="to address this problem"/>
<node id="these techniques"/>
<node id="the iteration level"/>
<node id="traditional methods"/>
<node id="the request level"/>
<node id="completed requests"/>
<node id="the batch"/>
<node id="new ones"/>
<node id="a new request"/>
<node id="waiting for a single iteration"/>
<node id="waiting for the entire batch to complete"/>
<node id="the need to pad the inputs and outputs"/>
<node id="special GPU kernels"/>
<node id="queueing delay and the inefficiencies from padding"/>
<node id="the throughput of LLM serving"/>
<node id="our fathers"/>
<node id="613 Four score and seven years ago"/>
<node id="You"/>
<node id="once"/>
<node id="2038 slots"/>
<node id="never used (internal fragmentation)"/>
<node id="2 slots"/>
<node id="future used (reserved)"/>
<node id="External fragmentation"/>
<node id="memory"/>
<node id="7 KV cache states"/>
<node id="request As prompt"/>
<node id="3 KV cache states"/>
<node id="request Bs prompt"/>
<node id="1 slot"/>
<node id="507 slots"/>
<node id="never used (Internal fragmentation)"/>
<node id="Request B"/>
<node id="current iteration"/>
<node id="generated token"/>
<node id="Figure 3"/>
<node id="illustration"/>
<node id="Three types of memory wastes"/>
<node id="reserved, internal fragmentation, and external fragmentation"/>
<node id="to prevent other requests from fitting into the memory"/>
<node id="serving systems throughput"/>
<node id="the performance of the systems"/>
<node id="compute-bound rather than memory-bound"/>
<node id="Overcoming this memory-bound"/>
<node id="the following challenges in the memory management"/>
<node id="the challenges in the memory management"/>
<node id="Large KV cache"/>
<node id="KV cache of a single token"/>
<node id="800 KB of space"/>
<node id="2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16)"/>
<node id="model"/>
<node id="13B parameter OPT model 62"/>
<node id="OPT"/>
<node id="2048 tokens"/>
<node id="the memory required to store the KV cache of one request"/>
<node id="1.6 GB"/>
<node id="Concurrent GPUs"/>
<node id="memory capacities in the tens of GBs"/>
<node id="GPUs computation speed"/>
<node id="memory capacity"/>
<node id="FLOPS"/>
<node id="more than 2x from NVIDIA A100 to H100"/>
<node id="GPU memory"/>
<node id="80GB maximum"/>
<node id="an increasingly significant bottleneck"/>
<node id="multiple random samples from a single input prompt"/>
<node id="a typical use case in program suggestion 18"/>
<node id="the KV cache of the prompt part"/>
<node id="12 of the total KV cache memory in our experiment (6.3)"/>
<node id="minimize memory usage"/>
<node id="unshared during the autoregressive generation phase"/>
<node id="different sample results and their dependence on context and position"/>
<node id="KV cache to remain unshared"/>
<node id="The extent of KV cache sharing"/>
<node id="the specific decoding algorithm employed"/>
<node id="different request beams"/>
<node id="larger portions of their KV cache"/>
<node id="larger portions"/>
<node id="55 memory saving"/>
<node id="sharing pattern"/>
<node id="as the decoding process advances"/>
<node id="Scheduling"/>
<node id="unknown input output lengths"/>
<node id="The requests to an LLM service"/>
<node id="variability in their input and output lengths"/>
<node id="a unique challenge"/>
<node id="input prompts for an LLM"/>
<node id="significantly in length"/>
<node id="resulting output lengths"/>
<node id="a priori"/>
<node id="both the input prompt and the model"/>
<node id="the memory management system"/>
<node id="a wide range of prompt lengths"/>
<node id="The system"/>
<node id="scheduling decisions"/>
<node id="deleting or swapping out the KV cache of some requests from GPU memory"/>
<node id="allocation"/>
<node id="the request's maximum possible sequence length"/>
<node id="the actual input or eventual output length of the request"/>
<node id="request A"/>
<node id="2048"/>
<node id="request B"/>
<node id="512"/>
<node id="The chunk pre-allocation scheme in existing systems"/>
<node id="three primary sources of memory wastes"/>
<node id="reserved slots for future tokens"/>
<node id="internal fragmentation due to over-provisioning for potential maximum sequence lengths"/>
<node id="external fragmentation from the memory allocator like the buddy allocator"/>
<node id="The external fragmentation"/>
<node id="generated tokens"/>
<node id="serving a request"/>
<node id="Internal fragmentation"/>
<node id="unused"/>
<node id="reserving this space for the entire requests duration"/>
<node id="the space that could otherwise be used to process other requests"/>
<node id="the average percentage of memory wastes in our experiments in Fig."/>
<node id="actual effective memory in previous systems"/>
<node id="614 KV Cache Manager"/>
<node id="vLLM system"/>
<node id="Scheduler"/>
<node id="CPU Block Allocator"/>
<node id="GPU Block Allocator"/>
<node id="Block tables"/>
<node id="Worker 0"/>
<node id="Model Shard 0 Cache Engine"/>
<node id="Figure 4"/>
<node id="vLLM system overview"/>
<node id="a CPU block allocator"/>
<node id="CPU block allocator"/>
<node id="the physical blocks swapped to CPU RAM"/>
<node id="GPU block allocator"/>
<node id="compaction 54"/>
<node id="a potential solution to fragmentation"/>
<node id="performing compaction in a performance-sensitive LLM serving system"/>
<node id="impractical"/>
<node id="the massive KV cache"/>
<node id="pre-allocated chunk space for each request"/>
<node id="memory sharing specific to decoding algorithms in existing memory management systems"/>
<node id="a new attention algorithm, PagedAttention"/>
<node id="an LLM serving engine, vLLM"/>
<node id="the challenges outlined in 3"/>
<node id="The architecture of vLLM"/>
<node id="the general applicability of vLLM on them"/>
<node id="a centralized scheduler"/>
<node id="the execution of distributed GPU workers"/>
<node id="the KV cache manager"/>
<node id="the physical KV cache memory on the GPU workers"/>
<node id="the instructions sent by the centralized scheduler"/>
<node id="each GPU worker"/>
<node id="the same physical block IDs"/>
<node id="a worker"/>
<node id="a portion of the KV cache for its corresponding attention heads"/>
<node id="GPU workers"/>
<node id="the block table in the control message"/>
<node id="the block table"/>
<node id="the control message"/>
<node id="the attention layers"/>
<node id="the PagedAttention algorithm"/>
<node id="4.1"/>
<node id="an example of PagedAttention in Fig."/>
<node id="this design"/>
<node id="effective memory management for various decoding methods (4.4)"/>
<node id="the variable length input and output sequences (4.5)"/>
<node id="the system design of vLLM"/>
<node id="a distributed setting"/>
<node id="Each block"/>
<node id="the key and value vectors for a fixed number of tokens"/>
<node id="the key and value vectors for a fixed number of tokens as KV"/>
<node id="Each token"/>
<node id="a set of key and value vectors across layers and attention heads within a layer"/>
<node id="All the key and value vectors"/>
<node id="a single KV block"/>
<node id="the key and value vectors at different heads and layers"/>
<node id="a separate block"/>
<node id="separate block tables"/>
<node id="The two designs"/>
<node id="no performance difference"/>
<node id="the second one"/>
<node id="easy implementation"/>
<node id="Four score and seven"/>
<node id="the effect of block size in 7.2"/>
<node id="key block"/>
<node id="((1)1"/>
<node id="The attention computation"/>
<node id="Eq."/>
<node id="4"/>
<node id="the following block-wise computation"/>
<node id="The key and value vectors"/>
<node id="three blocks"/>
<node id="The three blocks"/>
<node id="contiguous on the physical memory"/>
<node id="the kernel"/>
<node id="the query vector of the query token (forth) and the key vectors in a block"/>
<node id="the value vectors in a block"/>
<node id="the final attention output"/>
<node id="OS"/>
<node id="memory into fixed-sized pages"/>
<node id="user programs logical pages to physical pages"/>
<node id="Contiguous logical pages"/>
<node id="non-contiguous physical memory pages"/>
<node id="user programs"/>
<node id="memory as though it were contiguous"/>
<node id="physical memory space"/>
<node id="fully reserved in advance"/>
<node id="the OS"/>
<node id="to dynamically allocate physical pages as needed"/>
<node id="The last KV blocks unfilled positions"/>
<node id="future generations"/>
<node id="block engine"/>
<node id="a contiguous chunk of GPU DRAM"/>
<node id="fathers"/>
<node id="Four score and seven years ago"/>
<node id="Physical KV blocks"/>
<node id="GPU DRAM"/>
<node id="Block table translation"/>
<node id="The KV block manager"/>
<node id="block tables"/>
<node id="the mapping between logical and physical KV blocks of each request"/>
<node id="Each block table entry"/>
<node id="the corresponding physical blocks of a logical block"/>
<node id="the number of filled positions"/>
<node id="Separating logical and physical KV blocks"/>
<node id="vLLM to dynamically grow the KV cache memory without reserving it for all positions in advance"/>
<node id="most memory waste in existing systems"/>
<node id="all the blocks"/>
<node id="from left to right"/>
<node id="a new physical block"/>
<node id="only when all previous blocks are full"/>
<node id="all the memory wastes for a request within one block"/>
<node id="all the memory effectively"/>
<node id="the sharing of most of the space used to store the prompts KV cache across multiple output samples"/>
<node id="the final logical block"/>
<node id="a copy-on-write mechanism"/>
<node id="vLLMs physical block sharing"/>
<node id="frequent memory copy overhead"/>
<node id="the complex memory sharing between different sequences"/>
<node id="a common mapping layer"/>
<node id="logical blocks to physical blocks"/>
<node id="introducing the vLLMs techniques"/>
<node id="the performance"/>
<node id="the degradation of performance"/>
<node id="the extra overhead of memory indirection and non-contiguous block memory"/>
<node id="an example"/>
<node id="The example"/>
<node id="the new token"/>
<node id="physical blocks 7 and 1"/>
<node id="the first autoregressive decoding step"/>
<node id="4.3"/>
<node id="PagedAttention and vLLM handle basic decoding algorithms"/>
<node id="basic decoding algorithms"/>
<node id="greedy decoding and sampling"/>
<node id="one user prompt"/>
<node id="a single output sequence"/>
<node id="The prompt"/>
<node id="7 tokens"/>
<node id="the first 2 logical KV blocks (0 and 1) to 2 physical KV blocks (7 and 1, respectively)"/>
<node id="the KV cache of the prompts and the first output token"/>
<node id="a conventional self-attention algorithm (e.g., 13)"/>
<node id="the prefill step"/>
<node id="vLLM generating the KV cache of the prompts and the first output token"/>
<node id="the KV cache of the first 4 tokens in logical block 0"/>
<node id="the KV cache of the following 3 tokens in logical block 1"/>
<node id="new physical blocks to logical blocks"/>
<node id="free physical blocks for new tokens"/>
<node id="a set of sequences to evict"/>
<node id="their KV cache to the CPU"/>
<node id="The remaining slot"/>
<node id="the subsequent autoregressive generation phase"/>
<node id="one slot"/>
<node id="available in the last logical block"/>
<node id="newly generated KV cache"/>
<node id="in the last logical block"/>
<node id="a set of candidate sequences for batching"/>
<node id="the physical blocks for the newly required logical blocks"/>
<node id="all the input tokens of the current iteration"/>
<node id="all tokens"/>
<node id="prompt phase"/>
<node id="Four score and seven years ago our fathers brought"/>
<node id="Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8"/>
<node id="Logical KV blocks"/>
<node id="Block 0 Block 1 Block 2"/>
<node id="Storing multiple tokens within a KV block (block size 1)"/>
<node id="the PagedAttention kernel to process the KV cache across more positions in parallel"/>
<node id="Processing the KV cache across more positions in parallel"/>
<node id="hardware utilization"/>
<node id="latency"/>
<node id="a larger block size"/>
<node id="batching"/>
<node id="the throughput"/>
<node id="the memory for two sequences"/>
<node id="The logical blocks of the two sequences"/>
<node id="different physical blocks within the space reserved by the block engine in GPU workers"/>
<node id="The neighboring logical blocks of both sequences"/>
<node id="contiguous in physical GPU memory"/>
<node id="The space of physical blocks"/>
<node id="both sequences"/>
<node id="an LLM"/>
<node id="multiple sampled outputs for a single input prompt"/>
<node id="a favorite output from various candidates"/>
<node id="request"/>
<node id="616 Sample A1"/>
<node id="Sample A1"/>
<node id="Four score and seven years ago our fathers"/>
<node id="Block 0"/>
<node id="Four score and seven years ago our mothers"/>
<node id="Sample A2"/>
<node id="Copy-on-write"/>
<node id="Ref count: 2"/>
<node id="Figure 8"/>
<node id="in the text"/>
<node id="one request"/>
<node id="multiple samples sharing the same input prompt"/>
<node id="multiple samples"/>
<node id="the same input prompt"/>
<node id="the KV cache of the prompt to be shared"/>
<node id="all parallel sequences in a request"/>
<node id="the KV cache for the prompt"/>
<node id="8"/>
<node id="an example of parallel decoding for two outputs"/>
<node id="both outputs"/>
<node id="the same prompt"/>
<node id="one copy of the prompts state at the prompt phase"/>
<node id="the logical blocks for the prompts of both sequences"/>
<node id="the same physical blocks"/>
<node id="logical block 0 of both sequences"/>
<node id="physical block 7"/>
<node id="a single physical block"/>
<node id="multiple logical blocks"/>
<node id="a reference count for each physical block"/>
<node id="reference counts for physical block 7"/>
<node id="2"/>
<node id="the two outputs"/>
<node id="different output tokens"/>
<node id="separate storage for KV cache"/>
<node id="a copy-on-write mechanism at the block granularity"/>
<node id="copy-on-write mechanism"/>
<node id="physical blocks that need modification by multiple sequences"/>
<node id="the copy-on-write technique in OS virtual memory"/>
<node id="copy-on-write technique in OS virtual memory"/>
<node id="when forking a process"/>
<node id="sample A1"/>
<node id="its last logical block (logical block 1)"/>
<node id="the reference count of the corresponding physical block (physical block 1) is greater than 1"/>
<node id="a new physical block (physical block 3)"/>
<node id="the block engine to copy the information from physical block 1"/>
<node id="the reference count to 1"/>
<node id="sample A2"/>
<node id="physical block 1"/>
<node id="reference count"/>
<node id="1"/>
<node id="A2"/>
<node id="newly generated KV cache to physical block 1"/>
<node id="sharing physical blocks across multiple samples"/>
<node id="memory usage"/>
<node id="greatly"/>
<node id="memory usage reduction"/>
<node id="long input prompts"/>
<node id="top-most appropriate translations output by the LLM in LLM tasks like machine translation 59"/>
<node id="Beam search 49"/>
<node id="the most probable output sequence from an LLM"/>
<node id="the computational complexity of fully traversing the Block 10 Block 11 Block 1 Block 3 Block 6 Block 7 Block 5 Block 0 Block 2 Block 4 Block 8 Block 9 Block 12"/>
<node id="each candidate sequence in the beam during decoding"/>
<node id="all possible tokens"/>
<node id="respective probabilities using the LLM"/>
<node id="the top-most probable sequences out of candidates"/>
<node id="candidates"/>
<node id="vocabulary size"/>
<node id="The algorithm"/>
<node id="the beam width parameter"/>
<node id="the number of top candidates retained at every step"/>
<node id="initial prompt blocks"/>
<node id="other blocks across different candidates"/>
<node id="sharing patterns"/>
<node id="the process tree in the OS created by compound forks"/>
<node id="the KV blocks for a beam search example with 4"/>
<node id="each candidate sequence"/>
<node id="4 full logical blocks"/>
<node id="the iteration"/>
<node id="the dotted line"/>
<node id="All beam candidates"/>
<node id="the first block 0 (i.e., prompt)"/>
<node id="Candidate 3"/>
<node id="others from the second block"/>
<node id="Candidates 0-2"/>
<node id="the first 3 blocks"/>
<node id="at the fourth block"/>
<node id="all candidates"/>
<node id="blocks 0, 1, 3"/>
<node id="candidates 0 and 1"/>
<node id="block 6"/>
<node id="top-4 probable candidates"/>
<node id="candidates 1 and 2"/>
<node id="original candidates 0 and 3"/>
<node id="the top candidates"/>
<node id="their logical blocks"/>
<node id="freed"/>
<node id="the reference counts of corresponding physical blocks"/>
<node id="reduced"/>
<node id="all physical blocks whose reference counts reach 0 (blocks 2, 4, 5, 8)"/>
<node id="new physical blocks (blocks 9-12)"/>
<node id="the new KV cache from the new candidates"/>
<node id="Previous LLM serving systems"/>
<node id="frequent memory copies of the KV cache across the beam candidates"/>
<node id="candidate 3"/>
<node id="a large portion of candidate 2's KV cache"/>
<node id="to continue generation"/>
<node id="most blocks of different beam candidates"/>
<node id="in vLLM"/>
<node id="The same strategy"/>
<node id="prefix sharing"/>
<node id="beam search and prefix sharing"/>
<node id="LLM user"/>
<node id="a (long) description of the task including instructions and example inputs and outputs"/>
<node id="system prompt 36"/>
<node id="The description"/>
<node id="the actual task input"/>
<node id="The description and the actual task input"/>
<node id="the prompt of the request"/>
<node id="sea otter"/>
<node id="loutre de mer"/>
<node id="peppermint"/>
<node id="menthe poivre"/>
<node id="plush girafe"/>
<node id="girafe en peluche"/>
<node id="cheese"/>
<node id="fromage"/>
<node id="I love you"/>
<node id="Je tamie"/>
<node id="Shared prompt"/>
<node id="machine translation"/>
<node id="The examples"/>
<node id="5. on the full prompt"/>
<node id="10"/>
<node id="the shared prefix"/>
<node id="prompt engineering"/>
<node id="tuning the shared prefix via prompt engineering"/>
<node id="the accuracy of the downstream tasks 26, 27"/>
<node id="many user prompts"/>
<node id="a prefix"/>
<node id="the LLM service provider"/>
<node id="the KV cache of the prefix in advance"/>
<node id="storing the KV cache of the prefix in advance"/>
<node id="the redundant computation spent on the prefix"/>
<node id="reserving a set of physical blocks for a set of predefined shared prefixes by the LLM service provider"/>
<node id="LLM service provider"/>
<node id="a set of physical blocks for a set of predefined shared prefixes"/>
<node id="shared library across processes"/>
<node id="A user input prompt with the shared prefix"/>
<node id="its logical blocks to the cached physical blocks"/>
<node id="the last block"/>
<node id="The prompt phase computation"/>
<node id="the users task input"/>
<node id="The decoding methods discussed earlier"/>
<node id="diverse memory sharing and accessing patterns"/>
<node id="the simultaneous processing of requests with different decoding preferences"/>
<node id="The LLM and its execution kernel"/>
<node id="a list of physical block IDs for each sequence"/>
<node id="sharing patterns across sequences"/>
<node id="this approach"/>
<node id="the batching opportunities for requests with different sampling requirements"/>
<node id="the systems overall throughput"/>
<node id="request traffic"/>
<node id="systems capacity"/>
<node id="a subset of requests"/>
<node id="the earliest arrived requests are served first"/>
<node id="the latest requests are preempted first"/>
<node id="first-come-first-serve (FCFS) scheduling policy"/>
<node id="all requests"/>
<node id="fairness"/>
<node id="starvation"/>
<node id="the GPUs physical blocks to store the newly generated KV cache"/>
<node id="block size"/>
<node id="too small"/>
<node id="GPUs parallelism for reading and processing KV cache"/>
<node id="two classic questions"/>
<node id="two techniques: Swapping"/>
<node id="eviction policies"/>
<node id="heuristics"/>
<node id="which block will be accessed furthest in the future"/>
<node id="that block"/>
<node id="all blocks of a sequence"/>
<node id="together"/>
<node id="an all-or-nothing eviction policy"/>
<node id="all-or-nothing eviction policy"/>
<node id="either evict all or none of the blocks of a sequence"/>
<node id="multiple sequences within one request"/>
<node id="a sequence group"/>
<node id="The sequences within one sequence group"/>
<node id="due to potential memory sharing across those sequences"/>
<node id="classic technique"/>
<node id="most virtual memory implementations"/>
<node id="evicted pages to a swap space on the disk"/>
<node id="evicted blocks to the CPU memory"/>
<node id="a sequence"/>
<node id="its blocks"/>
<node id="new requests"/>
<node id="all preempted sequences are completed"/>
<node id="the blocks of a preempted sequence"/>
<node id="to continue the processing of that sequence"/>
<node id="the number of blocks swapped to the CPU RAM"/>
<node id="the number of total physical blocks in the GPU RAM"/>
<node id="the swap space on the CPU RAM"/>
<node id="the GPU memory allocated for the KV cache"/>
<node id="when the preempted sequences are rescheduled"/>
<node id="recomputation latency"/>
<node id="significantly lower than the original latency"/>
<node id="tokens generated at decoding"/>
<node id="the original user prompt as a new prompt"/>
<node id="their KV cache at all positions"/>
<node id="one prompt phase iteration"/>
<node id="performances of swapping and recomputation"/>
<node id="the bandwidth between CPU RAM and GPU memory"/>
<node id="the computation power of the GPU"/>
<node id="the speeds of swapping and recomputation in 7.3"/>
<node id="recomputation"/>
<node id="swapping"/>
<node id="recomputation and swapping"/>
<node id="recovery mechanisms"/>
<node id="Many LLMs"/>
<node id="parameter sizes exceeding the capacity of a single GPU"/>
<node id="distributed settings"/>
<node id="Megatron-LM style tensor model parallelism strategy on Transformers 47"/>
<node id="This strategy"/>
<node id="an SPMD (Single Program Multiple Data) execution schedule"/>
<node id="The detailed model sizes and server configurations"/>
<node id="Table 1"/>
<node id="Model size 13B"/>
<node id="A100 4A100 8A100-80GB"/>
<node id="40 GB"/>
<node id="Model size 66B"/>
<node id="160 GB"/>
<node id="Model size 175B"/>
<node id="640 GB"/>
<node id="26 GB"/>
<node id="132 GB"/>
<node id="346 GB"/>
<node id="12 GB"/>
<node id="21 GB"/>
<node id="264 GB"/>
<node id="GPUs"/>
<node id="intermediate results"/>
<node id="synchronize"/>
<node id="all-reduce operation"/>
<node id="KV cache slots"/>
<node id="15.7K, 9.7K, 60.1K"/>
<node id="block-wise matrix multiplication"/>
<node id="the attention operator"/>
<node id="the attention head dimension"/>
<node id="each SPMD process"/>
<node id="a subset of attention heads in multi-head attention"/>
<node id="each model shard"/>
<node id="the same set of input tokens"/>
<node id="the KV Cache for the same positions"/>
<node id="model parallel execution"/>
<node id="each model shard processing the same set of input tokens"/>
<node id="a single KV cache manager within the centralized scheduler"/>
<node id="Different GPU workers"/>
<node id="the manager"/>
<node id="the mapping from logical blocks to physical blocks"/>
<node id="This common mapping"/>
<node id="GPU workers to execute the model with the physical blocks provided by the scheduler for each input request"/>
<node id="the scheduler"/>
<node id="the message with input token IDs for each request in the batch"/>
<node id="the block table for each request"/>
<node id="this control message to the GPU workers"/>
<node id="the GPU workers"/>
<node id="the sampled tokens of this iteration back to the scheduler"/>
<node id="the model with the input token IDs"/>
<node id="all-reduce communication primitive"/>
<node id="coordination of the scheduler"/>
<node id="memory management"/>
<node id="all the memory management information at the beginning of each decoding iteration along with the step inputs"/>
<node id="an end-to-end serving system"/>
<node id="a FastAPI 15 frontend"/>
<node id="a GPU-based inference engine"/>
<node id="The frontend"/>
<node id="the OpenAI API 34 interface"/>
<node id="users to customize sampling parameters for each request"/>
<node id="sampling parameters"/>
<node id="the maximum sequence length"/>
<node id="the beam width"/>
<node id="vLLM engine"/>
<node id="8.5K lines of Python"/>
<node id="2K lines of CCUDA code"/>
<node id="control-related components including the scheduler and the block manager in Python"/>
<node id="custom CUDA kernels for key operations such as PagedAttention"/>
<node id="popular LLMs"/>
<node id="the model executor"/>
<node id="Input and output length distributions"/>
<node id="ShareGPT dataset"/>
<node id="Alpaca dataset"/>
<node id="the ShareGPT dataset"/>
<node id="8.4 longer input prompts on average than the Alpaca dataset"/>
<node id="5.8 longer outputs on average than the Alpaca dataset"/>
<node id="higher variance than the Alpaca dataset"/>
<node id="PyTorch"/>
<node id="39"/>
<node id="Transformers"/>
<node id="58"/>
<node id="NCCL 32 for tensor communication across the distributed GPU workers"/>
<node id="memory access patterns that are not efficiently supported by existing systems"/>
<node id="several GPU kernels for optimizing PagedAttention"/>
<node id="The dynamic block mapping in PagedAttention"/>
<node id="the performance of the GPU operations involving the stored KV cache"/>
<node id="the GPU operations involving the stored KV cache"/>
<node id="block readwrites and attention"/>
<node id="Fused re-shape"/>
<node id="block write"/>
<node id="Fused block copy"/>
<node id="3"/>
<node id="new KV cache"/>
<node id="a memory layout optimized for block read"/>
<node id="positions specified by the block table"/>
<node id="every Transformer layer"/>
<node id="them into a single kernel"/>
<node id="kernel launch overheads"/>
<node id="fusing them into a single kernel"/>
<node id="a kernel that batches the copy operations for different blocks into a single kernel launch"/>
<node id="the attention kernel in FasterTransformer 31"/>
<node id="KV cache according to the block table"/>
<node id="attention operations on the fly"/>
<node id="a GPU warp to read each block"/>
<node id="a GPU warp"/>
<node id="we assign a GPU warp to read each block"/>
<node id="to ensure coalesced memory access"/>
<node id="variable sequence lengths within a request batch"/>
<node id="Block copy operations"/>
<node id="the copy-on-write mechanism"/>
<node id="discontinuous blocks"/>
<node id="use of the cudaMemcpyAsync API"/>
<node id="numerous invocations of small data movements"/>
<node id="various decoding algorithms"/>
<node id="fork, append, and free"/>
<node id="The fork method"/>
<node id="a new sequence from an existing one"/>
<node id="The append method"/>
<node id="a new token to the sequence"/>
<node id="the free method"/>
<node id="the sequence"/>
<node id="multiple output sequences from the single input sequence using the fork method in parallel sampling"/>
<node id="future decoding algorithms"/>
<node id="combining these methods"/>
<node id="OPT-13B"/>
<node id="1 GPU"/>
<node id="ShareGPT"/>
<node id="OPT-66B"/>
<node id="4 GPUs"/>
<node id="OPT-175B"/>
<node id="8 GPUs"/>
<node id="Alpaca"/>
<node id="Normalized latency"/>
<node id="stoken"/>
<node id="Request rate"/>
<node id="reqs"/>
<node id="Figure 12"/>
<node id="Normalized latency and Request rate for OPT models with ShareGPT and Alpaca"/>
<node id="parallel generation"/>
<node id="Orca"/>
<node id="Max"/>
<node id="Pow2"/>
<node id="Oracle"/>
<node id="Figure 14"/>
<node id="request rate and normalized latency for parallel generation and beam search"/>
<node id="Single sequence generation"/>
<node id="OPT models"/>
<node id="Orca (Max), Orca (Pow2), Orca (Oracle), vLLM"/>
<node id="Batched requests"/>
<node id="0, 5, 10, 15, 20, 25, 30, 35"/>
<node id="0, 25, 50, 75, 100, 125, 150"/>
<node id="Performance values on ShareGPT dataset"/>
<node id="7.00, 9.81, 13.62, 30.42"/>
<node id="Performance values on Alpaca dataset"/>
<node id="7.00, 43.24, 72.75, 132.44"/>
<node id="Average number of batched requests"/>
<node id="OPT-13B for the ShareGPT (2 reqss) and Alpaca (30 reqss) traces"/>
<node id="Experimental Setup"/>
<node id="Model and server configurations"/>
<node id="13B"/>
<node id="popular sizes for LLMs"/>
<node id="66B"/>
<node id="13B and 66B"/>
<node id="an LLM leaderboard 38"/>
<node id="175B"/>
<node id="the size of the famous GPT-3 5 model"/>
<node id="A2 instances with NVIDIA A100 GPUs on Google Cloud Platform"/>
<node id="The ShareGPT dataset"/>
<node id="a collection of user-shared conversations with ChatGPT"/>
<node id="the chatting history and user query using the ShareGPT dataset"/>
<node id="The Alpaca dataset"/>
<node id="an instruction dataset"/>
<node id="GPT-3.5 with self-instruct 57"/>
<node id="the datasets"/>
<node id="their input and output lengths"/>
<node id="client requests"/>
<node id="these datasets"/>
<node id="timestamps"/>
<node id="request arrival times"/>
<node id="Poisson distribution with different request rates"/>
<node id="Baseline 1"/>
<node id="FasterTransformer 31"/>
<node id="a distributed inference engine highly optimized for latency"/>
<node id="its own scheduler"/>
<node id="a custom scheduler with a dynamic batching mechanism"/>
<node id="dynamic batching mechanism"/>
<node id="the existing serving systems such as Triton 30"/>
<node id="a maximum batch size as large as possible for each experiment"/>
<node id="maximum batch size"/>
<node id="The scheduler"/>
<node id="number of earliest arrived requests"/>
<node id="FasterTransformer for processing"/>
<node id="Baseline 2"/>
<node id="the three Orca baselines"/>
<node id="similarly"/>
<node id="Orca 60"/>
<node id="a state-of-the-art LLM serving system"/>
<node id="throughput"/>
<node id="publicly available for use"/>
<node id="our own version of Orca"/>
<node id="three versions of Orca"/>
<node id="how much it over-reserves the space for request outputs"/>
<node id="One version"/>
<node id="the buddy allocation algorithm"/>
<node id="the memory address to store KV cache"/>
<node id="the system"/>
<node id="the knowledge of the lengths of the outputs that will be actually generated for the requests"/>
<node id="the upper-bound performance of Orca"/>
<node id="infeasible to achieve in practice"/>
<node id="the space for outputs"/>
<node id="true output length"/>
<node id="25"/>
<node id="the space up to the maximum sequence length of the model"/>
<node id="the maximum sequence length of the model"/>
<node id="serving throughput"/>
<node id="normalized latency of the systems"/>
<node id="the mean of every request's end-to-end latency divided by its output length"/>
<node id="normalized latency measurement"/>
<node id="workloads with different request rates"/>
<node id="A high-throughput serving system"/>
<node id="low normalized latency against high request rates"/>
<node id="the systems with 1-hour traces"/>
<node id="15-minute traces for the OPT-175B model"/>
<node id="15-minute traces"/>
<node id="the OPT-175B model"/>
<node id="use of 15-minute traces"/>
<node id="the cost limit"/>
<node id="Parallel generation and beam search"/>
<node id="12"/>
<node id="the results on the ShareGPT dataset"/>
<node id="13b"/>
<node id="the results on the Alpaca dataset"/>
<node id="the Alpaca dataset"/>
<node id="a similar trend to the ShareGPT dataset"/>
<node id="The curves"/>
<node id="that as the request rate increases, the latency initially increases at a gradual pace but then suddenly explodes"/>
<node id="request rate"/>
<node id="latency initially increases at a gradual pace"/>
<node id="at a gradual pace initially"/>
<node id="suddenly"/>
<node id="capacity of the serving system"/>
<node id="queue length"/>
<node id="infinitely"/>
<node id="latency of the requests"/>
<node id="1.72.7 higher request rates compared to Orca (Oracle) on the ShareGPT dataset"/>
<node id="2.78 higher request rates compared to Orca (Max) on the ShareGPT dataset"/>
<node id="similar latencies"/>
<node id="OPT-13B vLLM"/>
<node id="2.2 more requests at the same time than Orca (Oracle)"/>
<node id="4.3 more requests at the same time than Orca (Max)"/>
<node id="1.67 higher throughput than Orca (Oracle) when the one-shot prefix is shared"/>
<node id="3.58 higher throughput than Orca (Oracle)"/>
<node id="vLLMs PagedAttention"/>
<node id="memory usage efficiently"/>
<node id="batching more requests than Orca"/>
<node id="The iteration-level scheduling in Orca 60"/>
<node id="a complementary technique to PagedAttention in vLLM"/>
<node id="Both systems"/>
<node id="the GPU utilization"/>
<node id="scheduling and interleaving the requests"/>
<node id="Scheduling and interleaving the requests"/>
<node id="more requests to be processed in parallel"/>
<node id="increasing memory utilization"/>
<node id="Increasing memory utilization"/>
<node id="the working sets of more requests to fit into memory"/>
<node id="vLLMs"/>
<node id="Orca (Oracle) and Orca (Pow2)"/>
<node id="advantage of vLLMs over Orca (Oracle) and Orca (Pow2)"/>
<node id="less pronounced in 12 (f)"/>
<node id="model and server configuration for OPT-175B"/>
<node id="large GPU memory space available to store KV cache"/>
<node id="short sequences"/>
<node id="a large number of requests"/>
<node id="inefficiencies in their memory management"/>
<node id="Output sequences"/>
<node id="0 4 8 12"/>
<node id="Memory saving (a) Parallel sampling"/>
<node id="6.09 8.53 9.79"/>
<node id="Beam width"/>
<node id="0 20 40 60"/>
<node id="Memory saving (b) Beam search"/>
<node id="37.56 53.13 55.16"/>
<node id="Figure 15"/>
<node id="Memory saving for Parallel sampling and Beam search"/>
<node id="Average amount of memory saving"/>
<node id="sharing KV blocks"/>
<node id="OPT-13B for the Alpaca trace"/>
<node id="the effectiveness of memory sharing in PagedAttention with two popular sampling methods: parallel sampling and beam search"/>
<node id="6.1 - 9.8 memory saving on parallel sampling"/>
<node id="37.6 - 55.2 memory saving on beam search"/>
<node id="more improvement over the Orca baselines with a larger number of sequences to sample"/>
<node id="2 higher request rates compared to the three Orca baselines"/>
<node id="14"/>
<node id="the results for beam search with different beam widths"/>
<node id="more sharing"/>
<node id="even greater performance benefits"/>
<node id="Orca (Oracle) on OPT-13B and the Alpaca dataset"/>
<node id="improvement of vLLM over Orca (Oracle) on OPT-13B and the Alpaca dataset"/>
<node id="1.3 in basic sampling"/>
<node id="2.3 in beam search with a width of 6"/>
<node id="the case a prefix is shared among different input prompts"/>
<node id="input prompts"/>
<node id="a common prefix"/>
<node id="The prefix"/>
<node id="1 example with 80 tokens"/>
<node id="5 examples with 341 tokens"/>
<node id="LLaMA-13B 52"/>
<node id="multilingual"/>
<node id="the WMT16 4 English-to-German translation dataset"/>
<node id="two prefixes that include an instruction and a few translation examples"/>
<node id="The first prefix"/>
<node id="a single example"/>
<node id="one-shot"/>
<node id="the other prefix"/>
<node id="5 examples"/>
<node id="few-shot"/>
<node id="Chatbot"/>
<node id="the model generate a response"/>
<node id="a response"/>
<node id="the chatting history and the last user query into a prompt"/>
<node id="OPT-13B model"/>
<node id="limited context length"/>
<node id="the prompt to the last 1024 tokens"/>
<node id="the model generate at most 1024 tokens"/>
<node id="the KV cache between different conversation rounds"/>
<node id="storing the KV cache between different conversation rounds"/>
<node id="the space for other requests between the conversation rounds"/>
<node id="many long conversations"/>
<node id="input prompts for most requests"/>
<node id="1024 tokens"/>
<node id="Orca baselines"/>
<node id="space for 1024 tokens for the request outputs"/>
<node id="buddy allocation algorithm"/>
<node id="how they predict the output lengths"/>
<node id="64 128 256 context length"/>
<node id="Latency of attention kernels"/>
<node id="vLLM (bs 8), FT (bs 8), vLLM (bs 32), FT (bs 32)"/>
<node id="Block size"/>
<node id="1, 2, 4, 8, 16, 32, 64, 128, 256"/>
<node id="Normalized latency (stoken)"/>
<node id="0.0, 2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5"/>
<node id="ShareGPT Alpaca (b)"/>
<node id="End-to-end latency with different block sizes"/>
<node id="various aspects of vLLM"/>
<node id="the design choices we make with ablation experiments"/>
<node id="the problem of memory fragmentation and reservation"/>
<node id="our GPU kernels (5)"/>
<node id="extra overheads of accessing the block table"/>
<node id="executing extra branches"/>
<node id="handling variable sequence lengths"/>
<node id="18a"/>
<node id="2026 higher attention kernel latency"/>
<node id="highly-optimized FasterTransformer implementation"/>
<node id="the overhead"/>
<node id="small"/>
<node id="the other operators in the model"/>
<node id="Linear"/>
<node id="vLLM significantly outperform FasterTransformer in end-to-end performance"/>
<node id="The choice of block size"/>
<node id="a substantial impact on the performance of vLLM"/>
<node id="too large"/>
<node id="the performance of vLLM with different block sizes"/>
<node id="the ShareGPT and Alpaca traces with basic sampling under fixed request rates"/>
<node id="block sizes from 16 to 128"/>
<node id="the best performance"/>
<node id="block sizes"/>
<node id="the ShareGPT trace"/>
<node id="block size 16 and 32"/>
<node id="in the Alpaca trace"/>
<node id="larger block sizes"/>
<node id="sequences"/>
<node id="shorter than the block sizes"/>
<node id="block size 16"/>
<node id="efficiently utilize the GPU"/>
<node id="avoid significant internal fragmentation in most workloads"/>
<node id="its default block size as 16"/>
<node id="Time"/>
<node id="ms"/>
<node id="Microbenchmark"/>
<node id="Recompute, Swap in, Swap out, Swap in out"/>
<node id="End-to-end performance"/>
<node id="Recompute, Swap"/>
<node id="Figure 19"/>
<node id="Microbenchmark and End-to-end performance"/>
<node id="Overhead"/>
<node id="recomputation and swapping for different block sizes"/>
<node id="the overhead of recomputation"/>
<node id="constant across different block sizes"/>
<node id="the KV blocks"/>
<node id="the block size is small"/>
<node id="the block size is large"/>
<node id="recomputation overhead"/>
<node id="20 of swappings latency"/>
<node id="OPT-13B with the ShareGPT traces at the same request rate"/>
<node id="their end-to-end performance"/>
<node id="their overheads"/>
<node id="excessive overhead with small block sizes"/>
<node id="small block sizes"/>
<node id="numerous small data transfers between CPU and GPU"/>
<node id="the effective PCIe bandwidth"/>
<node id="the two methods"/>
<node id="comparable end-to-end performance for medium block sizes from 16 to 64"/>
<node id="virtual memory and paging technique"/>
<node id="other GPU workloads"/>
<node id="the overhead of memory indirection in paging"/>
<node id="fusing the GPU kernels for memory access operations with those for other operations such as attention"/>
<node id="tensor shapes"/>
<node id="typically static in DNN training"/>
<node id="memory allocation"/>
<node id="optimized ahead of time"/>
<node id="an increase in memory efficiency"/>
<node id="any performance improvement"/>
<node id="performance"/>
<node id="primarily compute-bound"/>
<node id="vLLMs techniques being applied to other workloads with similar properties to LLM serving"/>
<node id="LLM-specific optimizations"/>
<node id="virtual memory and paging"/>
<node id="the idea of virtual memory and paging"/>
<node id="the application-specific semantics"/>
<node id="vLLMs all-or-nothing swap-out policy"/>
<node id="the fact that processing a request requires all of its corresponding token states to be stored in GPU memory"/>
<node id="recomputation method"/>
<node id="recover the evicted blocks"/>
<node id="9"/>
<node id="Related Work General model serving systems"/>
<node id="Model serving"/>
<node id="an active area of research in recent years"/>
<node id="numerous systems"/>
<node id="to tackle diverse aspects of deep learning model deployment"/>
<node id="Clipper"/>
<node id="general model serving system"/>
<node id="TensorFlow Serving"/>
<node id="Nexus"/>
<node id="InferLine"/>
<node id="Clockwork"/>
<node id="serving single or multiple models"/>
<node id="caching"/>
<node id="placement"/>
<node id="scheduling"/>
<node id="DVABatch 12"/>
<node id="multi-entry multi-exit batching"/>
<node id="REEF 21 and Shep- herd 61"/>
<node id="preemption for serving"/>
<node id="AlpaServe 28"/>
<node id="model parallelism for statistical multiplexing"/>
<node id="AlpaServe"/>
<node id="Statistical Multiplexing with Model Parallelism for Deep Learning Serving"/>
<node id="these general systems"/>
<node id="the auto-regressive property and token state of LLM inference"/>
<node id="Specialized serving systems"/>
<node id="transformers"/>
<node id="numerous specialized serving systems"/>
<node id="the transformer architecture"/>
<node id="These systems"/>
<node id="GPU kernel optimizations 1, 29, 31, 56"/>
<node id="advanced batching mechanisms 14, 60"/>
<node id="model parallelism 1, 41, 60"/>
<node id="parameter sharing 64"/>
<node id="GPU kernel optimizations, advanced batching mechanisms, model parallelism, and parameter sharing for efficient serving"/>
<node id="most relevant to our approach"/>
<node id="the fine-grained scheduling and interleaving of the requests like in Orca"/>
<node id="memory management more challenging"/>
<node id="the techniques proposed in vLLM"/>
<node id="even more crucial"/>
<node id="The widening gap between the compute capability and memory capacity of accelerators"/>
<node id="memory to become a bottleneck for both training and inference"/>
<node id="Swapping 23, 42, 55, recomputation 7, 24 and their combination 40"/>
<node id="the peak memory of training"/>
<node id="FlexGen 46"/>
<node id="how to swap weights and token states for LLM inference with 623 limited GPU memory"/>
<node id="the online serving settings"/>
<node id="OLLA 48"/>
<node id="the lifetime and location of tensors"/>
<node id="fragmentation"/>
<node id="fine-grained block-level management"/>
<node id="online serving"/>
<node id="FlashAttention 13"/>
<node id="tiling and kernel optimizations"/>
<node id="the peak memory of attention computation"/>
<node id="IO costs"/>
<node id="a new idea of block-level memory management in the context of online serving"/>
<node id="Xiaoxuan Liu"/>
<node id="Zhifeng Chen"/>
<node id="Yan-ping Huang"/>
<node id="anonymous SOSP reviewers"/>
<node id="our shepherd, Lidong Zhou"/>
<node id="Xiaoxuan Liu, Zhifeng Chen, Yan-ping Huang, anonymous SOSP reviewers, and our shepherd, Lidong Zhou"/>
<node id="insightful feedback"/>
<node id="This research"/>
<node id="gifts from Andreessen Horowitz"/>
<node id="gifts from Anyscale"/>
<node id="gifts from Astronomer"/>
<node id="gifts from Google"/>
<node id="gifts from IBM"/>
<node id="gifts from Intel"/>
<node id="gifts from Lacework"/>
<node id="gifts from Microsoft"/>
<node id="gifts from Mohamed Bin Zayed University of Artificial Intelligence"/>
<node id="gifts from Samsung SDS"/>
<node id="gifts from Uber"/>
<node id="gifts from VMware"/>
<node id="DeepSpeed Inference"/>
<node id="efficient inference of transformer models at unprecedented scale"/>
<node id="arXiv preprint"/>
<node id="arXiv:2207.00032"/>
<node id="arXiv preprint arXiv:2207.00032"/>
<node id="2022"/>
<node id="arXiv:1607.06450"/>
<node id="arXiv preprint arXiv:1607.06450"/>
<node id="2016"/>
<node id="arXiv:2107.03374"/>
<node id="arXiv preprint arXiv:2107.03374"/>
<node id="2021"/>
<node id="arXiv:1604.06174"/>
<node id="arXiv preprint arXiv:1604.06174"/>
<node id="arXiv:2204.02311"/>
<node id="arXiv preprint arXiv:2204.02311"/>
<node id="arXiv:2104.08691"/>
<node id="arXiv preprint arXiv:2104.08691"/>
<node id="arXiv:2101.00190"/>
<node id="arXiv preprint arXiv:2101.00190"/>
<node id="arXiv:2302.11665"/>
<node id="arXiv preprint arXiv:2302.11665"/>
<node id="arXiv:1712.06139"/>
<node id="arXiv preprint arXiv:1712.06139"/>
<node id="2017"/>
<node id="arXiv:2211.05102"/>
<node id="arXiv preprint arXiv:2211.05102"/>
<node id="arXiv:2303.06865"/>
<node id="arXiv preprint arXiv:2303.06865"/>
<node id="arXiv:1909.08053"/>
<node id="arXiv preprint arXiv:1909.08053"/>
<node id="2019"/>
<node id="arXiv:2302.13971"/>
<node id="arXiv preprint arXiv:2302.13971"/>
<node id="arXiv:2212.10560"/>
<node id="arXiv preprint arXiv:2212.10560"/>
<node id="arXiv:1609.08144"/>
<node id="arXiv preprint arXiv:1609.08144"/>
<node id="arXiv:2205.01068"/>
<node id="arXiv preprint arXiv:2205.01068"/>
<node id="Jimmy Lei Ba"/>
<node id="Jamie Ryan Kiros and Geoffrey E Hinton"/>
<node id="Layer normalization"/>
<node id="a technique"/>
<node id="Yoshua Bengio"/>
<node id="Rjean Ducharme and Pascal Vincent"/>
<node id="A neural probabilistic language model"/>
<node id="a language model"/>
<node id="Advances in neural information processing systems"/>
<node id="13"/>
<node id="Advances in neural information processing systems 13"/>
<node id="2000"/>
<node id="Findings"/>
<node id="the 2016 Conference on Machine Translation"/>
<node id="Association for Computational Linguistics"/>
<node id="Berlin, Germany"/>
<node id="Language models"/>
<node id="few-shot learners"/>
<node id="Advances in Neural Information Processing Systems"/>
<node id="35"/>
<node id="Advances in Neural Information Processing Systems 35"/>
<node id="Advances in Neural Information Processing Systems 35 (2022)"/>
<node id="16344-16359"/>
<node id="Advances in neural information processing systems 27"/>
<node id="2014"/>
<node id="Advances in neural information processing systems 30"/>
<node id="6 Mark Chen"/>
<node id="Jerry Tworek"/>
<node id="Heewoo Jun"/>
<node id="Qiming Yuan"/>
<node id="Henrique Ponde de Oliveira Pinto"/>
<node id="Jared Kaplan"/>
<node id="Harri Edwards"/>
<node id="Yuri Burda"/>
<node id="Nicholas Joseph"/>
<node id="Greg Brockman"/>
<node id="large language models"/>
<node id="code"/>
<node id="7"/>
<node id="Tianqi Chen"/>
<node id="Bing Xu"/>
<node id="Chiyuan Zhang"/>
<node id="Carlos Guestrin"/>
<node id="22"/>
<node id="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun"/>
<node id="64"/>
<node id="Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun"/>
<node id="Training deep nets"/>
<node id="sublinear memory cost"/>
<node id="28 Zhuohan Li"/>
<node id="the text"/>
<node id="Yinmin Zhong"/>
<node id="Vincent Liu"/>
<node id="Xin Jin"/>
<node id="Yanping Huang"/>
<node id="Joseph E Gonzalez"/>
<node id="46 Ying Sheng"/>
<node id="Binhang Yuan"/>
<node id="Max Ryabinin"/>
<node id="Daniel Y Fu"/>
<node id="Zhiqiang Xie"/>
<node id="Beidi Chen"/>
<node id="Clark Barrett"/>
<node id="59"/>
<node id="Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al."/>
<node id="Vicuna"/>
<node id="an open-source chatbot"/>
<node id="GPT-4"/>
<node id="90 ChatGPT quality"/>
<node id="orgblog2023-03-30-vicuna"/>
<node id="Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al."/>
<node id="Palm"/>
<node id="Scaling language modeling with pathways"/>
<node id="10 Daniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey Zumar, Ion Stoica, Joseph Gonzalez, and Alexey Tumanov"/>
<node id="persons"/>
<node id="Daniel Crankshaw"/>
<node id="Xin Wang"/>
<node id="Guilio Zhou"/>
<node id="Michael J Franklin"/>
<node id="latency-aware provisioning and scaling for prediction serving pipelines"/>
<node id="Proceedings"/>
<node id="the 11th ACM Symposium on Cloud Computing"/>
<node id="A Low-Latency Online Prediction Serving System"/>
<node id="20th USENIX Symposium on Networked Systems Design and Implementation"/>
<node id="NSDI 23"/>
<node id="DVABatch"/>
<node id="Diversity-aware Multi-Entry Multi-Exit Batching"/>
<node id="Efficient Processing of DNN Services on GPUs"/>
<node id="USENIX Annual Technical Conference"/>
<node id="USENIX ATC 22"/>
<node id="Flashattention"/>
<node id="Fast and memory-efficient exact attention with io-awareness"/>
<node id="TurboTransformers"/>
<node id="an efficient GPU serving system for transformer models"/>
<node id="the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming"/>
<node id="Proceedings of the 23rd ACM SIGPLAN symposium"/>
<node id="principles and practice of parallel programming"/>
<node id="FastAPI"/>
<node id="15"/>
<node id="rnn inference"/>
<node id="low latency"/>
<node id="17"/>
<node id="Amir Gholami"/>
<node id="Zhewei Yao"/>
<node id="Sehoon Kim"/>
<node id="Michael W Mahoney"/>
<node id="Kurt Keutzer"/>
<node id="18"/>
<node id="Github"/>
<node id="https:bard.google.com"/>
<node id="Arpan Gujarati"/>
<node id="2023 document"/>
<node id="Reza Karimi"/>
<node id="Safya Alzayat"/>
<node id="Wei Hao"/>
<node id="Antoine Kaufmann"/>
<node id="Ymir Vigfusson"/>
<node id="Jonathan Mace"/>
<node id="Serving DNNs"/>
<node id="Performance Predictability from the Bottom Up"/>
<node id="14th USENIX Symposium on Operating Systems Design and Implementation"/>
<node id="OSDI 20"/>
<node id="the 14th USENIX Conference on Operating Systems Design and Implementation"/>
<node id="16th USENIX Symposium on Operating Systems Design and Implementation"/>
<node id="OSDI 22"/>
<node id="Microsecond-scale Preemption"/>
<node id="Concurrent GPU-accelerated DNN Inferences"/>
<node id="Deep residual learning"/>
<node id="image recognition"/>
<node id="the IEEE conference on computer vision and pattern recognition"/>
<node id="Swapadvisor"/>
<node id="deep learning beyond the GPU memory limit via smart swapping"/>
<node id="the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems"/>
<node id="24 Paras Jain"/>
<node id="Paras Jain"/>
<node id="Ajay Jain"/>
<node id="Aniruddha Nrusimha"/>
<node id="Pieter Abbeel"/>
<node id="Joseph Gonzalez"/>
<node id="Check-mate"/>
<node id="breaking the memory wall with optimal tensor rematerialization"/>
<node id="Tom Kilburn"/>
<node id="David BG Edwards"/>
<node id="Michael J Lanigan"/>
<node id="Frank H Sumner"/>
<node id="One-level storage system"/>
<node id="a storage system"/>
<node id="Brian Lester"/>
<node id="26"/>
<node id="Rami Al-Rfou"/>
<node id="Noah Constant"/>
<node id="The power of scale"/>
<node id="parameter-efficient prompt tuning"/>
<node id="Prefix-tuning"/>
<node id="optimizing continuous prompts for generation"/>
<node id="Rammer"/>
<node id="holistic deep learning compiler optimizations with rtasks"/>
<node id="NVIDIA"/>
<node id="31"/>
<node id="32"/>
<node id="Triton Inference Server"/>
<node id="n. d."/>
<node id="nvidia-triton-inference-server"/>
<node id="https://developer.nvidia.com"/>
<node id="https://github.com/NVIDIA/FasterTransformer"/>
<node id="NCCL"/>
<node id="The NVIDIA Collective Communication Library"/>
<node id="Tensorflow-serving"/>
<node id="flexible, high-performance ml serving"/>
<node id="OpenAI"/>
<node id="34"/>
<node id="OpenAI API"/>
<node id="https://openai.com/blog/openai-api in 2020"/>
<node id="ChatGPT blog post in 2022"/>
<node id="ChatGPT blog post"/>
<node id="https://openai.com/blog/chatgpt"/>
<node id="arXiv:2303.08774"/>
<node id="cs.CL"/>
<node id="38"/>
<node id="LMSYS ORG"/>
<node id="Chatbot Arena Leaderboard Week 8"/>
<node id="MT-Bench and Vicuna-33B"/>
<node id="Adam Paszke"/>
<node id="Sam Gross"/>
<node id="Francisco Massa"/>
<node id="Adam Lerer"/>
<node id="James Bradbury"/>
<node id="Gregory Chanan"/>
<node id="Trevor Killeen"/>
<node id="Zeming Lin"/>
<node id="Natalia Gimelshein"/>
<node id="Luca Antiga"/>
<node id="Pytorch"/>
<node id="an imperative style, high-performance deep learning library"/>
<node id="Advances in neural information processing systems 32"/>
<node id="POET"/>
<node id="Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging"/>
<node id="PMLR"/>
<node id="1757317583"/>
<node id="41"/>
<node id="Reiner Pope"/>
<node id="Sholto Douglas"/>
<node id="Aakanksha Chowdhery"/>
<node id="Jacob Devlin"/>
<node id="Anselm Levskaya"/>
<node id="Jonathan Heek"/>
<node id="Kefan Xiao"/>
<node id="Shivani Agrawal"/>
<node id="Jeff Dean"/>
<node id="ZeRO-Offload"/>
<node id="a method for democratizing billion-scale model training"/>
<node id="Amazon Web Services"/>
<node id="2023. https:www.reuters.comtechnologytech-giants-ai-like-bing-bard-poses-billion-dollar-search-problem-2023-02-22"/>
<node id="https://aws.amazon.com/bedrock"/>
<node id="Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram"/>
<node id="a work referenced as 45"/>
<node id="a GPU cluster engine"/>
<node id="accelerating DNN-based video analysis"/>
<node id="the 27th ACM Symposium on Operating Systems Principles"/>
<node id="High-throughput Generative Inference"/>
<node id="Large Language Models"/>
<node id="High-throughput Generative Inference of Large Language Models"/>
<node id="a Single GPU"/>
<node id="47"/>
<node id="Mohammad Shoeybi"/>
<node id="Mostofa Patwary"/>
<node id="Raul Puri"/>
<node id="Patrick LeGresley"/>
<node id="Jared Casper"/>
<node id="Bryan Catanzaro"/>
<node id="Megatron-lm"/>
<node id="training multi-billion parameter language models"/>
<node id="model parallelism"/>
<node id="48"/>
<node id="Benoit Steiner, Mostafa Elhoushi, Jacob Kahn, and James Hegarty"/>
<node id="OLLA"/>
<node id="Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks"/>
<node id="Ilya Sutskever, Oriol Vinyals, and Quoc V Le"/>
<node id="arXiv.2210.12924"/>
<node id="Sequence to sequence learning"/>
<node id="neural networks"/>
<node id="50"/>
<node id="Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto"/>
<node id="57 Yizhong Wang"/>
<node id="Yeganeh Kordi"/>
<node id="Swaroop Mishra"/>
<node id="Alisa Liu"/>
<node id="Noah A Smith"/>
<node id="Daniel Khashabi"/>
<node id="Hannaneh Hajishirzi"/>
<node id="Stanford Alpaca"/>
<node id="an Instruction-following LLaMA model"/>
<node id="https://github.com/tatsu-lab/stanford-alpaca"/>
<node id="a URL"/>
<node id="Hugo Touvron"/>
<node id="2023 document on https:sharegpt.com"/>
<node id="Thibaut Lavril"/>
<node id="Gautier Izacard"/>
<node id="Xavier Martinet"/>
<node id="Marie-Anne Lachaux"/>
<node id="Timothe Lacroix"/>
<node id="Baptiste Rozire"/>
<node id="Naman Goyal"/>
<node id="Eric Hambro"/>
<node id="Faisal Azhar"/>
<node id="Llama"/>
<node id="Open and efficient foundation language models"/>
<node id="53"/>
<node id="Ashish Vaswani"/>
<node id="Noam Shazeer"/>
<node id="Niki Parmar"/>
<node id="Jakob Uszkoreit"/>
<node id="Llion Jones"/>
<node id="Aidan N Gomez"/>
<node id="ukasz Kaiser"/>
<node id="Illia Polosukhin"/>
<node id="Attention"/>
<node id="all you need"/>
<node id="Pacman"/>
<node id="An Efficient Compaction Approach for Log-Structured Key-Value Store on Persistent Memory"/>
<node id="Superneurons"/>
<node id="Dynamic GPU memory management for training deep neural networks"/>
<node id="LightSeq"/>
<node id="A High Performance Inference Library for Transformers"/>
<node id="the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers"/>
<node id="Self-Instruct"/>
<node id="Aligning Language Model with Self Generated Instructions"/>
<node id="Thomas Wolf"/>
<node id="Lysandre Debut"/>
<node id="Victor Sanh"/>
<node id="Julien Chaumond"/>
<node id="Clement Delangue"/>
<node id="Anthony Moi"/>
<node id="Pierric Cistac"/>
<node id="Tim Rault"/>
<node id="Rmi Louf"/>
<node id="Morgan Funtowicz"/>
<node id="state-of-the-art natural language processing"/>
<node id="Proceedings of the 2020 conference on empirical methods in natural language processing"/>
<node id="system demonstrations"/>
<node id="Google's neural machine translation system"/>
<node id="the gap between human and machine translation"/>
<node id="60"/>
<node id="Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun"/>
<node id="A Distributed Serving System for Transformer-Based Generative Models"/>
<node id="SHEPHERD"/>
<node id="Serving DNNs in the Wild"/>
<node id="USENIX Association"/>
<node id="Boston, MA"/>
<node id="https://www.usenix.org/conference/nsdi23/presentation/zhang-hong"/>
<node id="Susan Zhang"/>
<node id="Stephen Roller"/>
<node id="Mikel Artetxe"/>
<node id="Moya Chen"/>
<node id="Shuohui Chen"/>
<node id="Christopher Dewan"/>
<node id="Mona Diab"/>
<node id="Xian Li"/>
<node id="Xi Victoria Lin"/>
<node id="Opt"/>
<node id="Open pre-trained transformer language models"/>
<node id="Alpa"/>
<node id="Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning"/>
<node id="PetS"/>
<node id="A Unified Framework for Parameter-Efficient Transformers Serving"/>
<edge source="Efficient Memory Management for Large Language Model Serving with PagedAttention" target="Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica">
  <data key="d0">is authored by</data>
</edge>
<edge source="Woosuk Kwon" target="UC Berkeley">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Zhuohan Li" target="UC Berkeley">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Zhuohan Li" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Siyuan Zhuang" target="UC Berkeley">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Ying Sheng" target="UC Berkeley and Stanford University">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Ying Sheng" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Lianmin Zheng" target="UC Berkeley">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Lianmin Zheng" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Cody Hao Yu" target="Independent Researcher">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Joseph E. Gonzalez" target="UC Berkeley">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Hao Zhang" target="UC San Diego">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Hao Zhang" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Ion Stoica" target="UC Berkeley">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Ion Stoica" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Ion Stoica" target="Ion Stoica">
  <data key="d0">is a person</data>
</edge>
<edge source="High throughput serving of large language models (LLMs)" target="batching sufficiently many requests at a time">
  <data key="d0">requires</data>
</edge>
<edge source="existing systems" target="because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically">
  <data key="d0">struggle</data>
</edge>
<edge source="existing systems" target="KV cache memory">
  <data key="d0">use</data>
</edge>
<edge source="existing systems" target="the simultaneous processing of requests with different decoding preferences">
  <data key="d0">cannot efficiently do</data>
</edge>
<edge source="The KV Cache size" target="the number of requests">
  <data key="d0">grows quickly with</data>
</edge>
<edge source="this memory" target="fragmentation and redundant duplication">
  <data key="d0">can be wasted by</data>
</edge>
<edge source="fragmentation and redundant duplication" target="the batch size">
  <data key="d0">limit</data>
</edge>
<edge source="wasting of this memory" target="inefficient management">
  <data key="d0">is caused by</data>
</edge>
<edge source="inefficient memory management" target="the batch size">
  <data key="d0">can decrease</data>
</edge>
<edge source="PagedAttention" target="an attention algorithm">
  <data key="d0">is</data>
</edge>
<edge source="PagedAttention" target="the classical virtual memory and paging techniques in operating systems">
  <data key="d0">is inspired by</data>
</edge>
<edge source="PagedAttention" target="the operating systems (OS) solution to memory fragmentation and sharing">
  <data key="d0">is inspired by</data>
</edge>
<edge source="PagedAttention" target="KV cache stored in non-contiguous paged memory">
  <data key="d0">operates on</data>
</edge>
<edge source="PagedAttention" target="the virtual memory and paging in OS">
  <data key="d0">is inspired by</data>
</edge>
<edge source="PagedAttention" target="the classic idea of paging in operating systems">
  <data key="d0">is inspired by</data>
</edge>
<edge source="PagedAttention" target="the memory challenges in 3">
  <data key="d0">is introduced to address</data>
</edge>
<edge source="PagedAttention" target="continuous keys and values in non-contiguous memory space">
  <data key="d0">allows storing</data>
</edge>
<edge source="PagedAttention" target="traditional attention algorithms">
  <data key="d0">is unlike</data>
</edge>
<edge source="PagedAttention" target="a new attention algorithm">
  <data key="d0">is</data>
</edge>
<edge source="PagedAttention" target="attention keys and values to be stored in non-contiguous paged memory">
  <data key="d0">allows</data>
</edge>
<edge source="PagedAttention" target="the requests KV cache into blocks">
  <data key="d0">divides</data>
</edge>
<edge source="PagedAttention" target="paged fashion management of the KV cache">
  <data key="d0">enables</data>
</edge>
<edge source="PagedAttention" target="the KV cache of each sequence into KV blocks">
  <data key="d0">partitions</data>
</edge>
<edge source="PagedAttention" target="organizing the KV cache as fixed-size KV blocks">
  <data key="d0">enables</data>
</edge>
<edge source="PagedAttention" target="memory access patterns that are not efficiently supported by existing systems">
  <data key="d0">introduces</data>
</edge>
<edge source="PagedAttention" target="the problem of memory fragmentation and reservation">
  <data key="d0">resolves</data>
</edge>
<edge source="PagedAttention" target="vLLM significantly outperform FasterTransformer in end-to-end performance">
  <data key="d0">makes</data>
</edge>
<edge source="the operating systems (OS) solution to memory fragmentation and sharing" target="virtual memory with paging">
  <data key="d0">is</data>
</edge>
<edge source="PagedAttention algorithm" target="attention key and values vectors stored as non-contiguous blocks in the memory">
  <data key="d0">has</data>
</edge>
<edge source="PagedAttention algorithm" target="KV blocks to be stored in non-contiguous physical memory">
  <data key="d0">allows</data>
</edge>
<edge source="This paper" target="PagedAttention">
  <data key="d0">proposes</data>
</edge>
<edge source="This paper" target="vLLM">
  <data key="d0">presents</data>
</edge>
<edge source="This paper" target="a new idea of block-level memory management in the context of online serving">
  <data key="d0">introduces</data>
</edge>
<edge source="vLLM" target="a high-throughput LLM serving system">
  <data key="d0">is</data>
</edge>
<edge source="vLLM" target="efficient memory management enabled by PagedAttention">
  <data key="d0">has</data>
</edge>
<edge source="vLLM" target="an LLM serving system">
  <data key="d0">is</data>
</edge>
<edge source="vLLM" target="near-zero waste in KV cache memory">
  <data key="d0">achieves</data>
</edge>
<edge source="vLLM" target="flexible sharing of KV cache within and across requests">
  <data key="d0">achieves</data>
</edge>
<edge source="vLLM" target="a high-throughput distributed LLM serving engine">
  <data key="d0">is</data>
</edge>
<edge source="vLLM" target="PagedAttention">
  <data key="d0">is built on top of</data>
</edge>
<edge source="vLLM" target="the throughput of popular LLMs by 2-4">
  <data key="d0">improves</data>
</edge>
<edge source="vLLM" target="the same level of latency compared to the state-of-the-art systems">
  <data key="d0">has</data>
</edge>
<edge source="vLLM" target="LLM serving throughput by 2-4 compared to the state-of-the-art systems 31, 60">
  <data key="d0">improves</data>
</edge>
<edge source="vLLM" target="model accuracy">
  <data key="d0">does not affect</data>
</edge>
<edge source="vLLM" target="2-4 throughput improvements over the state-of-the-art systems">
  <data key="d0">achieves</data>
</edge>
<edge source="vLLM" target="the rapid growth curve of KV cache memory seen in existing systems 31, 60">
  <data key="d0">smooths out</data>
</edge>
<edge source="vLLM" target="a notable boost in serving throughput">
  <data key="d0">leads to</data>
</edge>
<edge source="vLLM" target="the ideas behind virtual memory to manage the KV cache in an LLM service">
  <data key="d0">uses</data>
</edge>
<edge source="vLLM" target="8.9">
  <data key="d0">has KV cache usage</data>
</edge>
<edge source="vLLM" target="41.6">
  <data key="d0">has Token states</data>
</edge>
<edge source="vLLM" target="96.3">
  <data key="d0">has Reservation</data>
</edge>
<edge source="vLLM" target="the KV cache of two requests at the same time">
  <data key="d0">stores</data>
</edge>
<edge source="vLLM" target="PagedAttention kernel to access the previous KV cache">
  <data key="d0">uses</data>
</edge>
<edge source="vLLM" target="newly generated KV cache into the physical KV blocks">
  <data key="d0">saves</data>
</edge>
<edge source="vLLM" target="this sharing easily via its PagedAttention and paged memory management">
  <data key="d0">can realize</data>
</edge>
<edge source="vLLM" target="memory via its PagedAttention and paged memory management">
  <data key="d0">can save</data>
</edge>
<edge source="vLLM" target="popular LLMs such as GPT 5, OPT 62, and LLaMA 52">
  <data key="d0">supports</data>
</edge>
<edge source="vLLM" target="a distributed LLM serving engine">
  <data key="d0">is</data>
</edge>
<edge source="vLLM" target="up to 22 higher request rates compared to FasterTransformer">
  <data key="d0">can sustain</data>
</edge>
<edge source="vLLM" target="memory fragmentation">
  <data key="d0">reduces</data>
</edge>
<edge source="vLLM" target="sharing">
  <data key="d0">enables</data>
</edge>
<edge source="vLLM" target="more requests in a batch in parallel">
  <data key="d0">runs</data>
</edge>
<edge source="vLLM" target="a 2-4 speedup compared to Orca">
  <data key="d0">achieves</data>
</edge>
<edge source="vLLM" target="a CPU block allocator">
  <data key="d0">includes</data>
</edge>
<edge source="vLLM" target="the challenges outlined in 3">
  <data key="d0">tackle</data>
</edge>
<edge source="vLLM" target="a centralized scheduler">
  <data key="d0">adopts</data>
</edge>
<edge source="vLLM" target="all the memory wastes for a request within one block">
  <data key="d0">limits</data>
</edge>
<edge source="vLLM" target="all the memory effectively">
  <data key="d0">can utilize</data>
</edge>
<edge source="vLLM" target="the sharing of most of the space used to store the prompts KV cache across multiple output samples">
  <data key="d0">enables</data>
</edge>
<edge source="vLLM" target="the complex memory sharing between different sequences">
  <data key="d0">conceals</data>
</edge>
<edge source="vLLM" target="a common mapping layer">
  <data key="d0">uses</data>
</edge>
<edge source="vLLM" target="the new token">
  <data key="d0">generates</data>
</edge>
<edge source="vLLM" target="the PagedAttention algorithm">
  <data key="d0">uses</data>
</edge>
<edge source="vLLM" target="the first 2 logical KV blocks (0 and 1) to 2 physical KV blocks (7 and 1, respectively)">
  <data key="d0">maps</data>
</edge>
<edge source="vLLM" target="the KV cache of the prompts and the first output token">
  <data key="d0">generates</data>
</edge>
<edge source="vLLM" target="a conventional self-attention algorithm (e.g., 13)">
  <data key="d0">uses</data>
</edge>
<edge source="vLLM" target="the KV cache of the first 4 tokens in logical block 0">
  <data key="d0">stores</data>
</edge>
<edge source="vLLM" target="the KV cache of the following 3 tokens in logical block 1">
  <data key="d0">stores</data>
</edge>
<edge source="vLLM" target="new physical blocks to logical blocks">
  <data key="d0">dynamically assigns</data>
</edge>
<edge source="vLLM" target="free physical blocks for new tokens">
  <data key="d0">exhausts</data>
</edge>
<edge source="vLLM" target="a set of sequences to evict">
  <data key="d0">selects</data>
</edge>
<edge source="vLLM" target="their KV cache to the CPU">
  <data key="d0">transfers</data>
</edge>
<edge source="vLLM" target="a set of candidate sequences for batching">
  <data key="d0">selects</data>
</edge>
<edge source="vLLM" target="the physical blocks for the newly required logical blocks">
  <data key="d0">allocates</data>
</edge>
<edge source="vLLM" target="all the input tokens of the current iteration">
  <data key="d0">concatenates</data>
</edge>
<edge source="vLLM" target="the memory for two sequences">
  <data key="d0">manages</data>
</edge>
<edge source="vLLM" target="a copy-on-write mechanism at the block granularity">
  <data key="d0">implements</data>
</edge>
<edge source="vLLM" target="the reference count of the corresponding physical block (physical block 1) is greater than 1">
  <data key="d0">recognizes</data>
</edge>
<edge source="vLLM" target="a new physical block (physical block 3)">
  <data key="d0">allocates</data>
</edge>
<edge source="vLLM" target="the block engine to copy the information from physical block 1">
  <data key="d0">instructs</data>
</edge>
<edge source="vLLM" target="the reference count to 1">
  <data key="d0">decreases</data>
</edge>
<edge source="vLLM" target="the KV blocks for a beam search example with 4">
  <data key="d0">manages</data>
</edge>
<edge source="vLLM" target="all physical blocks whose reference counts reach 0 (blocks 2, 4, 5, 8)">
  <data key="d0">frees</data>
</edge>
<edge source="vLLM" target="new physical blocks (blocks 9-12)">
  <data key="d0">allocates</data>
</edge>
<edge source="vLLM" target="reserving a set of physical blocks for a set of predefined shared prefixes by the LLM service provider">
  <data key="d0">achieves</data>
</edge>
<edge source="vLLM" target="the simultaneous processing of requests with different decoding preferences">
  <data key="d0">facilitates</data>
</edge>
<edge source="vLLM" target="a subset of requests">
  <data key="d0">must prioritize</data>
</edge>
<edge source="vLLM" target="requests">
  <data key="d0">needs to preempt</data>
</edge>
<edge source="vLLM" target="the earliest arrived requests are served first">
  <data key="d0">ensures</data>
</edge>
<edge source="vLLM" target="the latest requests are preempted first">
  <data key="d0">ensures</data>
</edge>
<edge source="vLLM" target="first-come-first-serve (FCFS) scheduling policy">
  <data key="d0">adopt</data>
</edge>
<edge source="vLLM" target="the GPUs physical blocks to store the newly generated KV cache">
  <data key="d0">can run out of</data>
</edge>
<edge source="vLLM" target="GPUs parallelism for reading and processing KV cache">
  <data key="d0">may not fully utilize</data>
</edge>
<edge source="vLLM" target="two classic questions">
  <data key="d0">needs to answer</data>
</edge>
<edge source="vLLM" target="a sequence">
  <data key="d0">preempts</data>
</edge>
<edge source="vLLM" target="its blocks">
  <data key="d0">evicts</data>
</edge>
<edge source="vLLM" target="new requests">
  <data key="d0">stops accepting</data>
</edge>
<edge source="vLLM" target="all preempted sequences are completed">
  <data key="d0">stops accepting new requests until</data>
</edge>
<edge source="vLLM" target="recomputation">
  <data key="d0">supports</data>
</edge>
<edge source="vLLM" target="swapping">
  <data key="d0">supports</data>
</edge>
<edge source="vLLM" target="distributed settings">
  <data key="d0">is effective in</data>
</edge>
<edge source="vLLM" target="Megatron-LM style tensor model parallelism strategy on Transformers 47">
  <data key="d0">supports</data>
</edge>
<edge source="vLLM" target="a single KV cache manager within the centralized scheduler">
  <data key="d0">features</data>
</edge>
<edge source="vLLM" target="an end-to-end serving system">
  <data key="d0">is</data>
</edge>
<edge source="vLLM" target="a FastAPI 15 frontend">
  <data key="d0">has</data>
</edge>
<edge source="vLLM" target="a GPU-based inference engine">
  <data key="d0">has</data>
</edge>
<edge source="vLLM" target="various decoding algorithms">
  <data key="d0">implements</data>
</edge>
<edge source="vLLM" target="fork, append, and free">
  <data key="d0">uses methods</data>
</edge>
<edge source="vLLM" target="multiple output sequences from the single input sequence using the fork method in parallel sampling">
  <data key="d0">creates</data>
</edge>
<edge source="vLLM" target="1.72.7 higher request rates compared to Orca (Oracle) on the ShareGPT dataset">
  <data key="d0">can sustain</data>
</edge>
<edge source="vLLM" target="2.78 higher request rates compared to Orca (Max) on the ShareGPT dataset">
  <data key="d0">can sustain</data>
</edge>
<edge source="vLLM" target="similar latencies">
  <data key="d0">maintains</data>
</edge>
<edge source="vLLM" target="1.67 higher throughput than Orca (Oracle) when the one-shot prefix is shared">
  <data key="d0">achieves</data>
</edge>
<edge source="vLLM" target="3.58 higher throughput than Orca (Oracle)">
  <data key="d0">achieves</data>
</edge>
<edge source="vLLM" target="increasing memory utilization">
  <data key="d0">achieves increased GPU utilization by</data>
</edge>
<edge source="vLLM" target="more improvement over the Orca baselines with a larger number of sequences to sample">
  <data key="d0">brings</data>
</edge>
<edge source="vLLM" target="2 higher request rates compared to the three Orca baselines">
  <data key="d0">can sustain</data>
</edge>
<edge source="vLLM" target="even greater performance benefits">
  <data key="d0">demonstrates</data>
</edge>
<edge source="vLLM" target="Orca (Oracle) on OPT-13B and the Alpaca dataset">
  <data key="d0">improves over</data>
</edge>
<edge source="vLLM" target="the case a prefix is shared among different input prompts">
  <data key="d0">is effective for</data>
</edge>
<edge source="vLLM" target="64 128 256 context length">
  <data key="d0">can effectively handle</data>
</edge>
<edge source="vLLM" target="its default block size as 16">
  <data key="d0">sets</data>
</edge>
<edge source="vLLM" target="the overhead of memory indirection in paging">
  <data key="d0">mitigates</data>
</edge>
<edge source="vLLM" target="fusing the GPU kernels for memory access operations with those for other operations such as attention">
  <data key="d0">mitigates the overhead by</data>
</edge>
<edge source="vLLM" target="the idea of virtual memory and paging">
  <data key="d0">re-interprets and augments</data>
</edge>
<edge source="vLLM" target="the application-specific semantics">
  <data key="d0">leverages</data>
</edge>
<edge source="flexible sharing of KV cache" target="to further reduce memory usage">
  <data key="d0">purpose</data>
</edge>
<edge source="existing LLM serving systems 31, 60" target="managing the KV cache memory efficiently">
  <data key="d0">fall short of</data>
</edge>
<edge source="we" target="the KV cache in a more flexible way">
  <data key="d0">can manage</data>
</edge>
<edge source="we" target="vLLM">
  <data key="d0">build</data>
</edge>
<edge source="we" target="the performance of vLLM under a variety of workloads">
  <data key="d0">evaluate</data>
</edge>
<edge source="we" target="the design of the KV cache manager in 4.2">
  <data key="d0">show</data>
</edge>
<edge source="we" target="the challenges in memory allocation in serving LLMs">
  <data key="d0">identify</data>
</edge>
<edge source="we" target="their impact on serving performance">
  <data key="d0">quantify</data>
</edge>
<edge source="we" target="a new attention algorithm, PagedAttention">
  <data key="d0">develop</data>
</edge>
<edge source="we" target="an LLM serving engine, vLLM">
  <data key="d0">build</data>
</edge>
<edge source="we" target="the second one">
  <data key="d0">choose</data>
</edge>
<edge source="we" target="one copy of the prompts state at the prompt phase">
  <data key="d0">reserve space for</data>
</edge>
<edge source="we" target="a reference count for each physical block">
  <data key="d0">introduce</data>
</edge>
<edge source="we" target="two techniques: Swapping">
  <data key="d0">consider</data>
</edge>
<edge source="we" target="an all-or-nothing eviction policy">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="evicted blocks to the CPU memory">
  <data key="d0">copy</data>
</edge>
<edge source="we" target="the KV cache">
  <data key="d0">recompute</data>
</edge>
<edge source="we" target="popular LLMs such as GPT 5, OPT 62, and LLaMA 52">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="several GPU kernels for optimizing PagedAttention">
  <data key="d0">develop</data>
</edge>
<edge source="we" target="them into a single kernel">
  <data key="d0">fuse</data>
</edge>
<edge source="we" target="a kernel that batches the copy operations for different blocks into a single kernel launch">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="a GPU warp to read each block">
  <data key="d0">assign</data>
</edge>
<edge source="we" target="variable sequence lengths within a request batch">
  <data key="d0">add support for</data>
</edge>
<edge source="we" target="A2 instances with NVIDIA A100 GPUs on Google Cloud Platform">
  <data key="d0">use</data>
</edge>
<edge source="we" target="request arrival times">
  <data key="d0">generate</data>
</edge>
<edge source="we" target="a custom scheduler with a dynamic batching mechanism">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="a maximum batch size as large as possible for each experiment">
  <data key="d0">set</data>
</edge>
<edge source="we" target="our own version of Orca">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="normalized latency of the systems">
  <data key="d0">measure</data>
</edge>
<edge source="we" target="the systems with 1-hour traces">
  <data key="d0">evaluate</data>
</edge>
<edge source="we" target="15-minute traces for the OPT-175B model">
  <data key="d0">use</data>
</edge>
<edge source="we" target="the WMT16 4 English-to-German translation dataset">
  <data key="d0">use</data>
</edge>
<edge source="we" target="two prefixes that include an instruction and a few translation examples">
  <data key="d0">synthesize</data>
</edge>
<edge source="we" target="the model generate a response">
  <data key="d0">let</data>
</edge>
<edge source="we" target="the prompt to the last 1024 tokens">
  <data key="d0">cut</data>
</edge>
<edge source="we" target="the model generate at most 1024 tokens">
  <data key="d0">let</data>
</edge>
<edge source="we" target="various aspects of vLLM">
  <data key="d0">study</data>
</edge>
<edge source="we" target="the design choices we make with ablation experiments">
  <data key="d0">evaluate</data>
</edge>
<edge source="we" target="the performance of vLLM with different block sizes">
  <data key="d0">evaluate</data>
</edge>
<edge source="we" target="the ShareGPT and Alpaca traces with basic sampling under fixed request rates">
  <data key="d0">use</data>
</edge>
<edge source="we" target="their end-to-end performance">
  <data key="d0">evaluate</data>
</edge>
<edge source="we" target="their overheads">
  <data key="d0">microbenchmark</data>
</edge>
<edge source="we" target="vLLMs techniques being applied to other workloads with similar properties to LLM serving">
  <data key="d0">would be excited to see</data>
</edge>
<edge source="blocks" target="pages">
  <data key="d0">can be thought of as</data>
</edge>
<edge source="blocks" target="a memory layout optimized for block read">
  <data key="d0">are reshaped to</data>
</edge>
<edge source="blocks" target="positions specified by the block table">
  <data key="d0">are saved at</data>
</edge>
<edge source="tokens" target="bytes">
  <data key="d0">can be thought of as</data>
</edge>
<edge source="requests" target="processes">
  <data key="d0">can be thought of as</data>
</edge>
<edge source="requests" target="the same model weights">
  <data key="d0">share</data>
</edge>
<edge source="requests" target="different times">
  <data key="d0">may arrive at</data>
</edge>
<edge source="requests" target="vastly different input and output lengths">
  <data key="d0">may have</data>
</edge>
<edge source="KV cache" target="memory management in existing systems">
  <data key="d0">is related to</data>
</edge>
<edge source="KV cache" target="unique characteristics">
  <data key="d0">has</data>
</edge>
<edge source="KV cache" target="as the model generates new tokens">
  <data key="d0">dynamically grows and shrinks over time</data>
</edge>
<edge source="KV cache" target="fixed-size KV blocks">
  <data key="d0">is organized as</data>
</edge>
<edge source="KV cache" target="unshared during the autoregressive generation phase">
  <data key="d0">should remain</data>
</edge>
<edge source="established techniques" target="operating systems">
  <data key="d0">are inspired by</data>
</edge>
<edge source="established techniques" target="virtual memory">
  <data key="d0">include</data>
</edge>
<edge source="established techniques" target="copy-on-write">
  <data key="d0">include</data>
</edge>
<edge source="established techniques" target="efficiently manage KV cache">
  <data key="d0">can be adapted to</data>
</edge>
<edge source="established techniques" target="handle various decoding algorithms in LLM serving">
  <data key="d0">can be adapted to</data>
</edge>
<edge source="state-of-the-art systems" target="FasterTransformer and Orca">
  <data key="d0">include</data>
</edge>
<edge source="The improvement" target="longer sequences">
  <data key="d0">is more pronounced with</data>
</edge>
<edge source="The improvement" target="larger models">
  <data key="d0">is more pronounced with</data>
</edge>
<edge source="The improvement" target="more complex decoding algorithms">
  <data key="d0">is more pronounced with</data>
</edge>
<edge source="The improvements" target="longer sequences">
  <data key="d0">are more pronounced with</data>
</edge>
<edge source="The improvements" target="larger models">
  <data key="d0">are more pronounced with</data>
</edge>
<edge source="The improvements" target="more complex decoding algorithms">
  <data key="d0">are more pronounced with</data>
</edge>
<edge source="vLLMs source code" target="https://github.com/vllm-project/vllm">
  <data key="d0">is publicly available at</data>
</edge>
<edge source="large language models (LLMs) like GPT 5, 37 and PaLM 9" target="new applications such as programming assistants 6, 18 and universal chatbots 19, 35">
  <data key="d0">have enabled</data>
</edge>
<edge source="programming assistants 6, 18 and universal chatbots 19, 35" target="our work and daily routines">
  <data key="d0">are starting to impact</data>
</edge>
<edge source="Many cloud companies 34, 44" target="these applications as hosted services">
  <data key="d0">are racing to provide</data>
</edge>
<edge source="running these applications" target="very expensive">
  <data key="d0">is</data>
</edge>
<edge source="running these applications" target="a large number of hardware accelerators such as GPUs">
  <data key="d0">requires</data>
</edge>
<edge source="processing an LLM request" target="10 more expensive than a traditional keyword query">
  <data key="d0">can be</data>
</edge>
<edge source="SOSP 23" target="October 23-26, 2023">
  <data key="d0">date</data>
</edge>
<edge source="SOSP 23" target="Koblenz, Germany">
  <data key="d0">location</data>
</edge>
<edge source="Copyright" target="the ownerauthor(s)">
  <data key="d0">held by</data>
</edge>
<edge source="Copyright" target="2023">
  <data key="d0">year</data>
</edge>
<edge source="2023" target="https:bard.google.com">
  <data key="d0">is associated with</data>
</edge>
<edge source="2023" target="https://aws.amazon.com/bedrock">
  <data key="d0">is associated with</data>
</edge>
<edge source="ACM" target="979-8-4007-0229-72310">
  <data key="d0">has ISBN</data>
</edge>
<edge source="DOI" target="https://doi.org/10.1145/3600006.3613165">
  <data key="d0">is</data>
</edge>
<edge source="NVIDIA A100" target="40GB">
  <data key="d0">has memory size</data>
</edge>
<edge source="Parameters" target="26GB">
  <data key="d0">size</data>
</edge>
<edge source="KV Cache" target="30">
  <data key="d0">size</data>
</edge>
<edge source="Others" target="20">
  <data key="d0">size</data>
</edge>
<edge source="Others" target="30">
  <data key="d0">size</data>
</edge>
<edge source="Others" target="40">
  <data key="d0">size</data>
</edge>
<edge source="Memory usage" target="GB">
  <data key="d0">unit</data>
</edge>
<edge source="Batch size" target="requests">
  <data key="d0">unit</data>
</edge>
<edge source="Throughput" target="tokens">
  <data key="d0">unit</data>
</edge>
<edge source="Existing systems" target="vLLM">
  <data key="d0">include</data>
</edge>
<edge source="Memory layout" target="an LLM with 13B parameters on NVIDIA A100">
  <data key="d0">is used when serving</data>
</edge>
<edge source="1 (left)" target="the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM">
  <data key="d0">illustrates</data>
</edge>
<edge source="The parameters (gray)" target="GPU memory throughout serving">
  <data key="d0">persist in</data>
</edge>
<edge source="The memory for the KV cache (red)" target="per serving request">
  <data key="d0">is (de)allocated</data>
</edge>
<edge source="the contiguous chunk of memory" target="storing the KV cache of a request in contiguous space">
  <data key="d0">is for</data>
</edge>
<edge source="all available memory" target="KV cache">
  <data key="d0">was allocated to</data>
</edge>
<edge source="output length of a request" target="decoding">
  <data key="d0">grows at</data>
</edge>
<edge source="memory required for its KV cache" target="as output length of a request grows at decoding">
  <data key="d0">expands</data>
</edge>
<edge source="memory required for its KV cache" target="available memory for incoming requests or ongoing generation for existing prompts">
  <data key="d0">may exhaust</data>
</edge>
<edge source="A small amount of memory (yellow)" target="ephemerally for activation">
  <data key="d0">is used</data>
</edge>
<edge source="The key idea behind vLLMs memory manager" target="the virtual memory 25 in operating systems">
  <data key="d0">is analogous to</data>
</edge>
<edge source="cost per request" target="more important">
  <data key="d0">is becoming</data>
</edge>
<edge source="cost per request" target="LLM serving systems">
  <data key="d0">of</data>
</edge>
<edge source="LLMs" target="an autoregressive Transformer model">
  <data key="d0">have at their core</data>
</edge>
<edge source="LLMs" target="a conditional generation service">
  <data key="d0">are deployed as</data>
</edge>
<edge source="This model" target="words (tokens)">
  <data key="d0">generates</data>
</edge>
<edge source="This model" target="one at a time">
  <data key="d0">generates words</data>
</edge>
<edge source="This model" target="the input (prompt)">
  <data key="d0">generates words based on</data>
</edge>
<edge source="This model" target="the previous sequence of the outputs tokens it has generated so far">
  <data key="d0">generates words based on</data>
</edge>
<edge source="this expensive process" target="for each request">
  <data key="d0">is repeated</data>
</edge>
<edge source="the model" target="a termination token">
  <data key="d0">outputs</data>
</edge>
<edge source="the model" target="a response">
  <data key="d0">generate</data>
</edge>
<edge source="This sequential generation process" target="the workload memory-bound">
  <data key="d0">makes</data>
</edge>
<edge source="This sequential generation process" target="the computation power of GPUs">
  <data key="d0">underutilizes</data>
</edge>
<edge source="This sequential generation process" target="the serving throughput">
  <data key="d0">limits</data>
</edge>
<edge source="Improving the throughput" target="batching multiple requests together">
  <data key="d0">is possible by</data>
</edge>
<edge source="memory space for each request" target="efficiently managed">
  <data key="d0">should be</data>
</edge>
<edge source="One exception" target="Fig.">
  <data key="d0">is</data>
</edge>
<edge source="Approximately 65 of the memory" target="the model weights">
  <data key="d0">is allocated for</data>
</edge>
<edge source="the model weights" target="static during serving">
  <data key="d0">remain</data>
</edge>
<edge source="Close to 30 of the memory" target="the dynamic states of the requests">
  <data key="d0">is used to store</data>
</edge>
<edge source="these states" target="the key and value tensors associated with the attention mechanism">
  <data key="d0">consist of</data>
</edge>
<edge source="the key and value tensors" target="KV cache 41">
  <data key="d0">are commonly referred to as</data>
</edge>
<edge source="KV cache 41" target="the context from earlier tokens">
  <data key="d0">represent</data>
</edge>
<edge source="the context from earlier tokens" target="generate new output tokens in sequence">
  <data key="d0">is used to</data>
</edge>
<edge source="611 Orca (Max)" target="20.4">
  <data key="d0">has KV cache usage</data>
</edge>
<edge source="611 Orca (Max)" target="26.8">
  <data key="d0">has Token states</data>
</edge>
<edge source="611 Orca (Max)" target="38.2">
  <data key="d0">has Reservation</data>
</edge>
<edge source="Orca (Pow2)" target="13.3">
  <data key="d0">has KV cache usage</data>
</edge>
<edge source="Orca (Pow2)" target="17.9">
  <data key="d0">has Token states</data>
</edge>
<edge source="Orca (Pow2)" target="25.2">
  <data key="d0">has Reservation</data>
</edge>
<edge source="Orca (Oracle)" target="57.3">
  <data key="d0">has KV cache usage</data>
</edge>
<edge source="Orca (Oracle)" target="13.6">
  <data key="d0">has Token states</data>
</edge>
<edge source="Orca (Oracle)" target="36.6">
  <data key="d0">has Reservation</data>
</edge>
<edge source="Average percentage of memory wastes" target="different LLM serving systems during the experiment in 6.2">
  <data key="d0">is measured in</data>
</edge>
<edge source="Percentage of memory" target="other data, including activations">
  <data key="d0">is used for</data>
</edge>
<edge source="Activations" target="the ephemeral tensors created when evaluating the LLM">
  <data key="d0">are</data>
</edge>
<edge source="model weights" target="constant">
  <data key="d0">are</data>
</edge>
<edge source="activations" target="a small fraction of the GPU memory">
  <data key="d0">occupy</data>
</edge>
<edge source="the way the KV cache is managed" target="critical in determining the maximum batch size">
  <data key="d0">is</data>
</edge>
<edge source="KV cache memory" target="batch size">
  <data key="d0">can limit</data>
</edge>
<edge source="KV cache memory" target="throughput of the LLM">
  <data key="d0">can limit</data>
</edge>
<edge source="KV cache memory" target="the actual token states">
  <data key="d0">is used to store</data>
</edge>
<edge source="limiting batch size and throughput" target="KV cache memory is managed inefficiently">
  <data key="d0">occurs when</data>
</edge>
<edge source="fine-grained batching" target="the waste of computing">
  <data key="d0">reduces</data>
</edge>
<edge source="fine-grained batching" target="requests to be batched in a more flexible way">
  <data key="d0">enables</data>
</edge>
<edge source="the number of requests that can be batched together" target="GPU memory capacity">
  <data key="d0">is constrained by</data>
</edge>
<edge source="GPU memory capacity" target="the space allocated to store the KV cache">
  <data key="d0">particularly constrains</data>
</edge>
<edge source="The idea of virtual memory and paging" target="managing the KV cache in LLM serving">
  <data key="d0">is effective for</data>
</edge>
<edge source="The workload" target="dynamic memory allocation">
  <data key="d0">requires</data>
</edge>
<edge source="The output length" target="not known a priori">
  <data key="d0">is</data>
</edge>
<edge source="Performance" target="the GPU memory capacity">
  <data key="d0">is bound by</data>
</edge>
<edge source="Performance" target="OPT-13B with the ShareGPT traces at the same request rate">
  <data key="d0">serves</data>
</edge>
<edge source="most deep learning frameworks 33, 39" target="tensors to be stored in contiguous memory">
  <data key="d0">require</data>
</edge>
<edge source="most operators in current deep learning frameworks" target="tensors to be stored in contiguous memory">
  <data key="d0">require</data>
</edge>
<edge source="previous LLM serving systems" target="the KV cache of one request as a contiguous tensor across the different positions">
  <data key="d0">store</data>
</edge>
<edge source="lifetime and length of KV cache" target="not known a priori">
  <data key="d0">are</data>
</edge>
<edge source="existing systems approach" target="significantly inefficient in two ways">
  <data key="d0">is</data>
</edge>
<edge source="existing systems 31, 60" target="internal and external memory fragmentation">
  <data key="d0">suffer from</data>
</edge>
<edge source="the requests actual length" target="much shorter than its maximum length">
  <data key="d0">can be</data>
</edge>
<edge source="pre-allocation" target="inefficient">
  <data key="d0">is</data>
</edge>
<edge source="entire chunk" target="requests lifetime">
  <data key="d0">is reserved during</data>
</edge>
<edge source="other shorter requests" target="any part of the chunk that is currently unused">
  <data key="d0">cannot utilize</data>
</edge>
<edge source="external memory fragmentation" target="significant">
  <data key="d0">can be</data>
</edge>
<edge source="pre-allocated size" target="each request">
  <data key="d0">can be different for</data>
</edge>
<edge source="the actual token states" target="20.4 - 38.2 of the KV cache memory">
  <data key="d0">occupy</data>
</edge>
<edge source="KV cache of one token" target="all its previous tokens">
  <data key="d0">depends on</data>
</edge>
<edge source="the KV cache of the same token appearing at different positions in a sequence" target="different">
  <data key="d0">will be</data>
</edge>
<edge source="The token in each memory slot" target="its KV cache">
  <data key="d0">represents</data>
</edge>
<edge source="the same tokens" target="different KV cache">
  <data key="d0">can have</data>
</edge>
<edge source="different KV cache" target="at different positions">
  <data key="d0">occur</data>
</edge>
<edge source="the existing systems" target="the opportunities for memory sharing">
  <data key="d0">cannot exploit</data>
</edge>
<edge source="LLM services" target="advanced decoding algorithms">
  <data key="d0">use</data>
</edge>
<edge source="LLM services" target="a range of decoding algorithms">
  <data key="d0">offer</data>
</edge>
<edge source="LLM services" target="a unique challenge">
  <data key="d0">face</data>
</edge>
<edge source="advanced decoding algorithms" target="parallel sampling">
  <data key="d0">include</data>
</edge>
<edge source="advanced decoding algorithms" target="beam search">
  <data key="d0">include</data>
</edge>
<edge source="advanced decoding algorithms" target="multiple outputs per request">
  <data key="d0">generate</data>
</edge>
<edge source="beam search" target="each candidate sequence in the beam during decoding">
  <data key="d0">expands</data>
</edge>
<edge source="beam search" target="all possible tokens">
  <data key="d0">considers</data>
</edge>
<edge source="beam search" target="respective probabilities using the LLM">
  <data key="d0">computes</data>
</edge>
<edge source="beam search" target="the top-most probable sequences out of candidates">
  <data key="d0">retains</data>
</edge>
<edge source="beam search" target="initial prompt blocks">
  <data key="d0">facilitates sharing</data>
</edge>
<edge source="beam search" target="other blocks across different candidates">
  <data key="d0">facilitates sharing</data>
</edge>
<edge source="beam search" target="2">
  <data key="d0">has beam width</data>
</edge>
<edge source="beam search" target="more sharing">
  <data key="d0">allows for</data>
</edge>
<edge source="users" target="a range of decoding algorithms">
  <data key="d0">select from</data>
</edge>
<edge source="users" target="multiple random samples from a single input prompt">
  <data key="d0">request</data>
</edge>
<edge source="users" target="a favorite output from various candidates">
  <data key="d0">can choose</data>
</edge>
<edge source="users" target="top-most appropriate translations output by the LLM in LLM tasks like machine translation 59">
  <data key="d0">expect</data>
</edge>
<edge source="decoding algorithms" target="varying implications for memory management complexity">
  <data key="d0">have</data>
</edge>
<edge source="an LLM service" target="more complex decoding scenarios">
  <data key="d0">must offer</data>
</edge>
<edge source="more complex decoding scenarios" target="complex accessing patterns">
  <data key="d0">exhibit</data>
</edge>
<edge source="more complex decoding scenarios" target="more opportunities for memory sharing">
  <data key="d0">exhibit</data>
</edge>
<edge source="the request" target="multiple sequences">
  <data key="d0">consists of</data>
</edge>
<edge source="multiple sequences" target="their KV cache">
  <data key="d0">can partially share</data>
</edge>
<edge source="a request" target="its generation">
  <data key="d0">finishes</data>
</edge>
<edge source="a request" target="multiple sequences">
  <data key="d0">generates</data>
</edge>
<edge source="a request" target="once">
  <data key="d0">completes</data>
</edge>
<edge source="its KV blocks" target="the KV cache of other requests">
  <data key="d0">can be freed to store</data>
</edge>
<edge source="memory sharing" target="the existing systems">
  <data key="d0">is not possible in</data>
</edge>
<edge source="memory sharing" target="the different sequences associated with the same request">
  <data key="d0">occurs across</data>
</edge>
<edge source="memory sharing" target="the different requests">
  <data key="d0">occurs across</data>
</edge>
<edge source="the KV cache of the sequences" target="separate contiguous spaces">
  <data key="d0">is stored in</data>
</edge>
<edge source="each block" target="the attention keys and values of a fixed number of tokens">
  <data key="d0">can contain</data>
</edge>
<edge source="PagedAttention kernel" target="different KV blocks">
  <data key="d0">fetches</data>
</edge>
<edge source="blocks for the KV cache in PagedAttention" target="contiguous space">
  <data key="d0">are not necessarily stored in</data>
</edge>
<edge source="The KV cache manager" target="the KV cache">
  <data key="d0">manages</data>
</edge>
<edge source="The KV cache manager" target="paged fashion">
  <data key="d0">manages in a</data>
</edge>
<edge source="the KV cache" target="the attention layers">
  <data key="d0">is read in</data>
</edge>
<edge source="the KV cache" target="when the preempted sequences are rescheduled">
  <data key="d0">is recomputed</data>
</edge>
<edge source="the design of the KV cache manager" target="PagedAttention in 4.3">
  <data key="d0">facilitates</data>
</edge>
<edge source="fixed-size KV blocks" target="pages in virtual memory">
  <data key="d0">are like</data>
</edge>
<edge source="This design" target="internal fragmentation">
  <data key="d0">alleviates</data>
</edge>
<edge source="This design" target="relatively small blocks">
  <data key="d0">uses</data>
</edge>
<edge source="This design" target="relatively small blocks on demand">
  <data key="d0">allocates</data>
</edge>
<edge source="all blocks" target="the same size">
  <data key="d0">have</data>
</edge>
<edge source="storing KV blocks in non-contiguous physical memory" target="more flexible paged memory management in vLLM">
  <data key="d0">enables</data>
</edge>
<edge source="previous KV cache" target="logical KV blocks">
  <data key="d0">is stored in the form of</data>
</edge>
<edge source="logical KV blocks" target="left to right">
  <data key="d0">are filled from</data>
</edge>
<edge source="logical KV blocks" target="new tokens and their KV cache are generated">
  <data key="d0">are filled as</data>
</edge>
<edge source="popular LLMs such as GPT 5, OPT 62, and LLaMA 52" target="varying sizes">
  <data key="d0">have</data>
</edge>
<edge source="varying sizes" target="ones exceeding the memory capacity of a single GPU">
  <data key="d0">include</data>
</edge>
<edge source="We" target="vLLM">
  <data key="d0">design and implement</data>
</edge>
<edge source="We" target="the concatenation of the prompt and output lists as sequence">
  <data key="d0">refer to</data>
</edge>
<edge source="We" target="the average percentage of memory wastes in our experiments in Fig.">
  <data key="d0">visualize</data>
</edge>
<edge source="We" target="the general applicability of vLLM on them">
  <data key="d0">show</data>
</edge>
<edge source="We" target="the PagedAttention algorithm">
  <data key="d0">describe</data>
</edge>
<edge source="We" target="an example of PagedAttention in Fig.">
  <data key="d0">show</data>
</edge>
<edge source="We" target="the key and value vectors for a fixed number of tokens as KV">
  <data key="d0">denote</data>
</edge>
<edge source="We" target="the effect of block size in 7.2">
  <data key="d0">study</data>
</edge>
<edge source="We" target="an example">
  <data key="d0">walk through</data>
</edge>
<edge source="We" target="the speeds of swapping and recomputation in 7.3">
  <data key="d0">examine</data>
</edge>
<edge source="We" target="control-related components including the scheduler and the block manager in Python">
  <data key="d0">develop</data>
</edge>
<edge source="We" target="custom CUDA kernels for key operations such as PagedAttention">
  <data key="d0">develop</data>
</edge>
<edge source="We" target="NCCL 32 for tensor communication across the distributed GPU workers">
  <data key="d0">use</data>
</edge>
<edge source="We" target="the attention kernel in FasterTransformer 31">
  <data key="d0">adapt</data>
</edge>
<edge source="We" target="the chatting history and user query using the ShareGPT dataset">
  <data key="d0">synthesize</data>
</edge>
<edge source="We" target="the datasets">
  <data key="d0">tokenize</data>
</edge>
<edge source="We" target="their input and output lengths">
  <data key="d0">use</data>
</edge>
<edge source="We" target="three versions of Orca">
  <data key="d0">implement</data>
</edge>
<edge source="We" target="serving throughput">
  <data key="d0">focus on</data>
</edge>
<edge source="We" target="the effectiveness of memory sharing in PagedAttention with two popular sampling methods: parallel sampling and beam search">
  <data key="d0">evaluate</data>
</edge>
<edge source="We" target="6.1 - 9.8 memory saving on parallel sampling">
  <data key="d0">show</data>
</edge>
<edge source="We" target="37.6 - 55.2 memory saving on beam search">
  <data key="d0">show</data>
</edge>
<edge source="We" target="the KV cache between different conversation rounds">
  <data key="d0">do not store</data>
</edge>
<edge source="We" target="Xiaoxuan Liu">
  <data key="d0">would like to thank</data>
</edge>
<edge source="We" target="Zhifeng Chen">
  <data key="d0">would like to thank</data>
</edge>
<edge source="We" target="Yan-ping Huang">
  <data key="d0">would like to thank</data>
</edge>
<edge source="We" target="anonymous SOSP reviewers">
  <data key="d0">would like to thank</data>
</edge>
<edge source="We" target="our shepherd, Lidong Zhou">
  <data key="d0">would like to thank</data>
</edge>
<edge source="FasterTransformer" target="a fine-grained scheduling mechanism">
  <data key="d0">does not utilize</data>
</edge>
<edge source="FasterTransformer" target="inefficiently like Orca (Max)">
  <data key="d0">manages memory</data>
</edge>
<edge source="FasterTransformer" target="its own scheduler">
  <data key="d0">does not have</data>
</edge>
<edge source="FasterTransformer" target="https://github.com/NVIDIA/FasterTransformer">
  <data key="d0">is hosted at</data>
</edge>
<edge source="this section" target="the generation and serving procedures of typical LLMs">
  <data key="d0">describes</data>
</edge>
<edge source="this section" target="the iteration-level scheduling used in LLM serving">
  <data key="d0">describes</data>
</edge>
<edge source="language modeling" target="the probability of a list of tokens">
  <data key="d0">is to model</data>
</edge>
<edge source="language" target="a natural sequential ordering">
  <data key="d0">has</data>
</edge>
<edge source="Transformers 53" target="the de facto standard architecture for modeling the probability above at a large scale">
  <data key="d0">have become</data>
</edge>
<edge source="The most important component of a Transformer-based language model" target="its self-attention layers">
  <data key="d0">is</data>
</edge>
<edge source="a self-attention layer" target="linear transformations on each position to get the query, key, and value vectors">
  <data key="d0">applies</data>
</edge>
<edge source="the self-attention layer" target="the attention score">
  <data key="d0">computes</data>
</edge>
<edge source="the self-attention layer" target="multiplying the query vector at one position with all the key vectors before it">
  <data key="d0">computes the attention score by</data>
</edge>
<edge source="the self-attention layer" target="the output as the weighted average over the value vectors">
  <data key="d0">computes</data>
</edge>
<edge source="all other components in the Transformer model" target="embedding layer">
  <data key="d0">include</data>
</edge>
<edge source="all other components in the Transformer model" target="feed-forward layer">
  <data key="d0">include</data>
</edge>
<edge source="all other components in the Transformer model" target="layer normalization 2">
  <data key="d0">include</data>
</edge>
<edge source="all other components in the Transformer model" target="residual connection 22">
  <data key="d0">include</data>
</edge>
<edge source="all other components in the Transformer model" target="output logit computation">
  <data key="d0">include</data>
</edge>
<edge source="all other components in the Transformer model" target="query, key, and value transformation in Eq.">
  <data key="d0">include</data>
</edge>
<edge source="A request to an LLM service" target="a list of input prompt tokens">
  <data key="d0">provides</data>
</edge>
<edge source="the LLM" target="new tokens one by one">
  <data key="d0">can only sample and generate</data>
</edge>
<edge source="the generation process of each new token" target="all the previous tokens in that sequence">
  <data key="d0">depends on</data>
</edge>
<edge source="all the previous tokens in that sequence" target="their key and value vectors">
  <data key="d0">specifically have</data>
</edge>
<edge source="key and value vectors of existing tokens" target="generating future tokens">
  <data key="d0">are often cached for</data>
</edge>
<edge source="generating future tokens" target="KV cache">
  <data key="d0">is known as</data>
</edge>
<edge source="A requests KV cache" target="a series of logical KV blocks">
  <data key="d0">is represented as</data>
</edge>
<edge source="generation computation in the LLM service" target="two phases">
  <data key="d0">can be decomposed into</data>
</edge>
<edge source="The prompt phase" target="the whole user prompt">
  <data key="d0">takes</data>
</edge>
<edge source="the computation of the prompt phase" target="matrix-matrix multiplication operations">
  <data key="d0">can be parallelized using</data>
</edge>
<edge source="this phase" target="the parallelism inherent in GPUs">
  <data key="d0">can efficiently use</data>
</edge>
<edge source="this phase" target="GPU computation">
  <data key="d0">underutilizes</data>
</edge>
<edge source="this phase" target="memory-bound">
  <data key="d0">becomes</data>
</edge>
<edge source="this phase" target="most portion of the latency of a single request">
  <data key="d0">is responsible for</data>
</edge>
<edge source="The autoregressive generation phase" target="the remaining new tokens sequentially">
  <data key="d0">generates</data>
</edge>
<edge source="key and value vectors at positions 1 to 1" target="previous iterations">
  <data key="d0">are cached at</data>
</edge>
<edge source="new key and value vector" target="this iteration">
  <data key="d0">are computed at</data>
</edge>
<edge source="This phase" target="the sequence reaches a maximum length">
  <data key="d0">completes when</data>
</edge>
<edge source="This phase" target="an end-of-sequence (eos) token is emitted">
  <data key="d0">completes when</data>
</edge>
<edge source="maximum length" target="users">
  <data key="d0">is specified by</data>
</edge>
<edge source="maximum length" target="LLMs">
  <data key="d0">is limited by</data>
</edge>
<edge source="The computation at different iterations" target="parallelized">
  <data key="d0">cannot be</data>
</edge>
<edge source="The computation at different iterations" target="the data dependency">
  <data key="d0">cannot be parallelized due to</data>
</edge>
<edge source="The computation at different iterations" target="matrix-vector multiplication">
  <data key="d0">often uses</data>
</edge>
<edge source="matrix-vector multiplication" target="less efficient">
  <data key="d0">is</data>
</edge>
<edge source="compute utilization in serving LLMs" target="batching multiple requests">
  <data key="d0">can be improved by</data>
</edge>
<edge source="batching the requests to an LLM service" target="non-trivial">
  <data key="d0">is</data>
</edge>
<edge source="the overhead of moving weights" target="the requests in a batch">
  <data key="d0">is amortized across</data>
</edge>
<edge source="the overhead of moving weights" target="the computational overhead">
  <data key="d0">can be overwhelmed by</data>
</edge>
<edge source="the computational overhead" target="the batch size being sufficiently large">
  <data key="d0">is related to</data>
</edge>
<edge source="A naive batching strategy" target="earlier requests wait for later ones">
  <data key="d0">would either make</data>
</edge>
<edge source="A naive batching strategy" target="delay the incoming requests until earlier ones finish">
  <data key="d0">would either</data>
</edge>
<edge source="delaying the incoming requests until earlier ones finish" target="significant queueing delays">
  <data key="d0">leads to</data>
</edge>
<edge source="A straightforward batching technique" target="the inputs and outputs of the requests">
  <data key="d0">would pad</data>
</edge>
<edge source="padding the inputs and outputs of the requests" target="equalize their lengths">
  <data key="d0">to</data>
</edge>
<edge source="padding the inputs and outputs of the requests" target="GPU computation and memory">
  <data key="d0">wastes</data>
</edge>
<edge source="fine-grained batching mechanisms" target="cellular batching">
  <data key="d0">include</data>
</edge>
<edge source="fine-grained batching mechanisms" target="iteration-level scheduling">
  <data key="d0">include</data>
</edge>
<edge source="fine-grained batching mechanisms" target="to address this problem">
  <data key="d0">have been proposed</data>
</edge>
<edge source="fine-grained batching mechanisms" target="queueing delay and the inefficiencies from padding">
  <data key="d0">reduce</data>
</edge>
<edge source="fine-grained batching mechanisms" target="the throughput of LLM serving">
  <data key="d0">significantly increase</data>
</edge>
<edge source="these techniques" target="the iteration level">
  <data key="d0">operate at</data>
</edge>
<edge source="these techniques" target="the need to pad the inputs and outputs">
  <data key="d0">eliminate</data>
</edge>
<edge source="traditional methods" target="the request level">
  <data key="d0">work at</data>
</edge>
<edge source="completed requests" target="the batch">
  <data key="d0">are removed from</data>
</edge>
<edge source="new ones" target="the batch">
  <data key="d0">are added</data>
</edge>
<edge source="a new request" target="waiting for a single iteration">
  <data key="d0">can be processed after</data>
</edge>
<edge source="a new request" target="waiting for the entire batch to complete">
  <data key="d0">cannot be processed after</data>
</edge>
<edge source="special GPU kernels" target="these techniques">
  <data key="d0">are used by</data>
</edge>
<edge source="our fathers" target="613 Four score and seven years ago">
  <data key="d0">brought forth</data>
</edge>
<edge source="our fathers" target="Four score and seven">
  <data key="d0">brought forth</data>
</edge>
<edge source="You" target="once">
  <data key="d0">only live</data>
</edge>
<edge source="2038 slots" target="never used (internal fragmentation)">
  <data key="d0">are</data>
</edge>
<edge source="2 slots" target="future used (reserved)">
  <data key="d0">are</data>
</edge>
<edge source="External fragmentation" target="memory">
  <data key="d0">exists in</data>
</edge>
<edge source="memory" target="an increasingly significant bottleneck">
  <data key="d0">will become</data>
</edge>
<edge source="7 KV cache states" target="request As prompt">
  <data key="d0">are for</data>
</edge>
<edge source="3 KV cache states" target="request Bs prompt">
  <data key="d0">are for</data>
</edge>
<edge source="1 slot" target="future used (reserved)">
  <data key="d0">is</data>
</edge>
<edge source="1 slot" target="generated token">
  <data key="d0">is for</data>
</edge>
<edge source="507 slots" target="never used (Internal fragmentation)">
  <data key="d0">are</data>
</edge>
<edge source="Request B" target="current iteration">
  <data key="d0">is</data>
</edge>
<edge source="Figure 3" target="illustration">
  <data key="d0">is</data>
</edge>
<edge source="Three types of memory wastes" target="reserved, internal fragmentation, and external fragmentation">
  <data key="d0">are</data>
</edge>
<edge source="Three types of memory wastes" target="to prevent other requests from fitting into the memory">
  <data key="d0">exist</data>
</edge>
<edge source="serving systems throughput" target="memory-bound">
  <data key="d0">is</data>
</edge>
<edge source="the performance of the systems" target="compute-bound rather than memory-bound">
  <data key="d0">becomes</data>
</edge>
<edge source="Overcoming this memory-bound" target="the following challenges in the memory management">
  <data key="d0">requires addressing</data>
</edge>
<edge source="the challenges in the memory management" target="Large KV cache">
  <data key="d0">include</data>
</edge>
<edge source="KV cache of a single token" target="800 KB of space">
  <data key="d0">demands</data>
</edge>
<edge source="800 KB of space" target="2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16)">
  <data key="d0">is calculated as</data>
</edge>
<edge source="model" target="13B parameter OPT model 62">
  <data key="d0">is</data>
</edge>
<edge source="OPT" target="2048 tokens">
  <data key="d0">can generate sequences up to</data>
</edge>
<edge source="the memory required to store the KV cache of one request" target="1.6 GB">
  <data key="d0">can be as much as</data>
</edge>
<edge source="Concurrent GPUs" target="memory capacities in the tens of GBs">
  <data key="d0">have</data>
</edge>
<edge source="GPUs computation speed" target="memory capacity">
  <data key="d0">grows faster than</data>
</edge>
<edge source="FLOPS" target="more than 2x from NVIDIA A100 to H100">
  <data key="d0">increases by</data>
</edge>
<edge source="GPU memory" target="80GB maximum">
  <data key="d0">stays at</data>
</edge>
<edge source="multiple random samples from a single input prompt" target="a typical use case in program suggestion 18">
  <data key="d0">is</data>
</edge>
<edge source="the KV cache of the prompt part" target="12 of the total KV cache memory in our experiment (6.3)">
  <data key="d0">accounts for</data>
</edge>
<edge source="the KV cache of the prompt part" target="minimize memory usage">
  <data key="d0">can be shared to</data>
</edge>
<edge source="different sample results and their dependence on context and position" target="KV cache to remain unshared">
  <data key="d0">cause</data>
</edge>
<edge source="The extent of KV cache sharing" target="the specific decoding algorithm employed">
  <data key="d0">depends on</data>
</edge>
<edge source="different request beams" target="larger portions of their KV cache">
  <data key="d0">can share</data>
</edge>
<edge source="larger portions" target="55 memory saving">
  <data key="d0">up to</data>
</edge>
<edge source="sharing pattern" target="as the decoding process advances">
  <data key="d0">evolves</data>
</edge>
<edge source="Scheduling" target="unknown input output lengths">
  <data key="d0">is for</data>
</edge>
<edge source="The requests to an LLM service" target="variability in their input and output lengths">
  <data key="d0">exhibit</data>
</edge>
<edge source="input prompts for an LLM" target="significantly in length">
  <data key="d0">can vary</data>
</edge>
<edge source="resulting output lengths" target="a priori">
  <data key="d0">are not known</data>
</edge>
<edge source="resulting output lengths" target="both the input prompt and the model">
  <data key="d0">are contingent on</data>
</edge>
<edge source="the memory management system" target="a wide range of prompt lengths">
  <data key="d0">to accommodate</data>
</edge>
<edge source="The system" target="scheduling decisions">
  <data key="d0">needs to make</data>
</edge>
<edge source="scheduling decisions" target="deleting or swapping out the KV cache of some requests from GPU memory">
  <data key="d0">include</data>
</edge>
<edge source="allocation" target="the request's maximum possible sequence length">
  <data key="d0">is based on</data>
</edge>
<edge source="allocation" target="the actual input or eventual output length of the request">
  <data key="d0">is irrespective of</data>
</edge>
<edge source="request A" target="2048">
  <data key="d0">has maximum possible sequence length</data>
</edge>
<edge source="request B" target="512">
  <data key="d0">has maximum possible sequence length</data>
</edge>
<edge source="The chunk pre-allocation scheme in existing systems" target="three primary sources of memory wastes">
  <data key="d0">has</data>
</edge>
<edge source="three primary sources of memory wastes" target="reserved slots for future tokens">
  <data key="d0">include</data>
</edge>
<edge source="three primary sources of memory wastes" target="internal fragmentation due to over-provisioning for potential maximum sequence lengths">
  <data key="d0">include</data>
</edge>
<edge source="three primary sources of memory wastes" target="external fragmentation from the memory allocator like the buddy allocator">
  <data key="d0">include</data>
</edge>
<edge source="The external fragmentation" target="generated tokens">
  <data key="d0">will never be used for</data>
</edge>
<edge source="The external fragmentation" target="serving a request">
  <data key="d0">is known before</data>
</edge>
<edge source="Internal fragmentation" target="unused">
  <data key="d0">remains</data>
</edge>
<edge source="reserving this space for the entire requests duration" target="the space that could otherwise be used to process other requests">
  <data key="d0">occupies</data>
</edge>
<edge source="actual effective memory in previous systems" target="20.4">
  <data key="d0">can be as low as</data>
</edge>
<edge source="614 KV Cache Manager" target="vLLM system">
  <data key="d0">is part of</data>
</edge>
<edge source="Scheduler" target="vLLM system">
  <data key="d0">is part of</data>
</edge>
<edge source="CPU Block Allocator" target="vLLM system">
  <data key="d0">is part of</data>
</edge>
<edge source="GPU Block Allocator" target="vLLM system">
  <data key="d0">is part of</data>
</edge>
<edge source="Block tables" target="vLLM system">
  <data key="d0">are part of</data>
</edge>
<edge source="Worker 0" target="Model Shard 0 Cache Engine">
  <data key="d0">runs</data>
</edge>
<edge source="Figure 4" target="vLLM system overview">
  <data key="d0">shows</data>
</edge>
<edge source="CPU block allocator" target="the physical blocks swapped to CPU RAM">
  <data key="d0">manages</data>
</edge>
<edge source="GPU block allocator" target="vLLM">
  <data key="d0">is included in</data>
</edge>
<edge source="compaction 54" target="a potential solution to fragmentation">
  <data key="d0">has been proposed as</data>
</edge>
<edge source="performing compaction in a performance-sensitive LLM serving system" target="impractical">
  <data key="d0">is</data>
</edge>
<edge source="performing compaction in a performance-sensitive LLM serving system" target="the massive KV cache">
  <data key="d0">is impractical due to</data>
</edge>
<edge source="pre-allocated chunk space for each request" target="memory sharing specific to decoding algorithms in existing memory management systems">
  <data key="d0">prevents</data>
</edge>
<edge source="The architecture of vLLM" target="Fig.">
  <data key="d0">is shown in</data>
</edge>
<edge source="a centralized scheduler" target="the execution of distributed GPU workers">
  <data key="d0">coordinates</data>
</edge>
<edge source="the KV cache manager" target="the physical KV cache memory on the GPU workers">
  <data key="d0">manages</data>
</edge>
<edge source="the KV cache manager" target="the instructions sent by the centralized scheduler">
  <data key="d0">manages through</data>
</edge>
<edge source="each GPU worker" target="the same physical block IDs">
  <data key="d0">has</data>
</edge>
<edge source="a worker" target="a portion of the KV cache for its corresponding attention heads">
  <data key="d0">stores</data>
</edge>
<edge source="GPU workers" target="the KV cache">
  <data key="d0">read</data>
</edge>
<edge source="GPU workers" target="the block table in the control message">
  <data key="d0">read according to</data>
</edge>
<edge source="GPU workers" target="the model with the input token IDs">
  <data key="d0">start to execute</data>
</edge>
<edge source="GPU workers" target="intermediate results">
  <data key="d0">synchronize</data>
</edge>
<edge source="GPU workers" target="all-reduce communication primitive">
  <data key="d0">use</data>
</edge>
<edge source="GPU workers" target="coordination of the scheduler">
  <data key="d0">synchronize without</data>
</edge>
<edge source="GPU workers" target="memory management">
  <data key="d0">do not need to synchronize on</data>
</edge>
<edge source="GPU workers" target="all the memory management information at the beginning of each decoding iteration along with the step inputs">
  <data key="d0">need to receive</data>
</edge>
<edge source="the block table" target="the control message">
  <data key="d0">is in</data>
</edge>
<edge source="the PagedAttention algorithm" target="4.1">
  <data key="d0">is described in</data>
</edge>
<edge source="the PagedAttention algorithm" target="physical blocks 7 and 1">
  <data key="d0">operates on</data>
</edge>
<edge source="this design" target="effective memory management for various decoding methods (4.4)">
  <data key="d0">facilitates</data>
</edge>
<edge source="this design" target="the variable length input and output sequences (4.5)">
  <data key="d0">handles</data>
</edge>
<edge source="the system design of vLLM" target="a distributed setting">
  <data key="d0">works in</data>
</edge>
<edge source="Each block" target="the key and value vectors for a fixed number of tokens">
  <data key="d0">contains</data>
</edge>
<edge source="Each token" target="a set of key and value vectors across layers and attention heads within a layer">
  <data key="d0">has</data>
</edge>
<edge source="All the key and value vectors" target="a single KV block">
  <data key="d0">can be managed together within</data>
</edge>
<edge source="the key and value vectors at different heads and layers" target="a separate block">
  <data key="d0">can each have</data>
</edge>
<edge source="the key and value vectors at different heads and layers" target="separate block tables">
  <data key="d0">can be managed in</data>
</edge>
<edge source="The two designs" target="no performance difference">
  <data key="d0">have</data>
</edge>
<edge source="the second one" target="easy implementation">
  <data key="d0">is chosen for</data>
</edge>
<edge source="key block" target="((1)1">
  <data key="d0">denote</data>
</edge>
<edge source="The attention computation" target="Eq.">
  <data key="d0">is in</data>
</edge>
<edge source="4" target="the following block-wise computation">
  <data key="d0">can be transformed into</data>
</edge>
<edge source="The key and value vectors" target="three blocks">
  <data key="d0">are spread across</data>
</edge>
<edge source="The three blocks" target="contiguous on the physical memory">
  <data key="d0">are not</data>
</edge>
<edge source="the kernel" target="the query vector of the query token (forth) and the key vectors in a block">
  <data key="d0">multiplies</data>
</edge>
<edge source="the kernel" target="the attention score">
  <data key="d0">computes</data>
</edge>
<edge source="the kernel" target="the value vectors in a block">
  <data key="d0">multiplies with</data>
</edge>
<edge source="the kernel" target="the final attention output">
  <data key="d0">derives</data>
</edge>
<edge source="OS" target="memory into fixed-sized pages">
  <data key="d0">partitions</data>
</edge>
<edge source="OS" target="user programs logical pages to physical pages">
  <data key="d0">maps</data>
</edge>
<edge source="OS" target="shared library across processes">
  <data key="d0">handles</data>
</edge>
<edge source="Contiguous logical pages" target="non-contiguous physical memory pages">
  <data key="d0">can correspond to</data>
</edge>
<edge source="user programs" target="memory as though it were contiguous">
  <data key="d0">can access</data>
</edge>
<edge source="physical memory space" target="fully reserved in advance">
  <data key="d0">needs not to be</data>
</edge>
<edge source="the OS" target="to dynamically allocate physical pages as needed">
  <data key="d0">enables</data>
</edge>
<edge source="The last KV blocks unfilled positions" target="future generations">
  <data key="d0">are reserved for</data>
</edge>
<edge source="block engine" target="a contiguous chunk of GPU DRAM">
  <data key="d0">allocates</data>
</edge>
<edge source="fathers" target="Four score and seven years ago">
  <data key="d0">brought</data>
</edge>
<edge source="Physical KV blocks" target="GPU DRAM">
  <data key="d0">are located on</data>
</edge>
<edge source="Physical KV blocks" target="Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8">
  <data key="d0">include</data>
</edge>
<edge source="Block table translation" target="vLLM">
  <data key="d0">is in</data>
</edge>
<edge source="The KV block manager" target="block tables">
  <data key="d0">maintains</data>
</edge>
<edge source="block tables" target="the mapping between logical and physical KV blocks of each request">
  <data key="d0">are</data>
</edge>
<edge source="Each block table entry" target="the corresponding physical blocks of a logical block">
  <data key="d0">records</data>
</edge>
<edge source="Each block table entry" target="the number of filled positions">
  <data key="d0">records</data>
</edge>
<edge source="Separating logical and physical KV blocks" target="vLLM to dynamically grow the KV cache memory without reserving it for all positions in advance">
  <data key="d0">allows</data>
</edge>
<edge source="Separating logical and physical KV blocks" target="most memory waste in existing systems">
  <data key="d0">eliminates</data>
</edge>
<edge source="all the blocks" target="from left to right">
  <data key="d0">are filled</data>
</edge>
<edge source="a new physical block" target="only when all previous blocks are full">
  <data key="d0">is allocated</data>
</edge>
<edge source="the final logical block" target="a copy-on-write mechanism">
  <data key="d0">is managed by</data>
</edge>
<edge source="vLLMs physical block sharing" target="frequent memory copy overhead">
  <data key="d0">reduces</data>
</edge>
<edge source="a common mapping layer" target="logical blocks to physical blocks">
  <data key="d0">translates</data>
</edge>
<edge source="introducing the vLLMs techniques" target="the performance">
  <data key="d0">may degrade</data>
</edge>
<edge source="the degradation of performance" target="the extra overhead of memory indirection and non-contiguous block memory">
  <data key="d0">is due to</data>
</edge>
<edge source="The example" target="Fig.">
  <data key="d0">is in</data>
</edge>
<edge source="the new token" target="the first autoregressive decoding step">
  <data key="d0">is generated in</data>
</edge>
<edge source="4.3" target="PagedAttention and vLLM handle basic decoding algorithms">
  <data key="d0">shows how</data>
</edge>
<edge source="basic decoding algorithms" target="greedy decoding and sampling">
  <data key="d0">include</data>
</edge>
<edge source="basic decoding algorithms" target="one user prompt">
  <data key="d0">take as input</data>
</edge>
<edge source="basic decoding algorithms" target="a single output sequence">
  <data key="d0">generate</data>
</edge>
<edge source="The prompt" target="7 tokens">
  <data key="d0">has</data>
</edge>
<edge source="the prefill step" target="vLLM generating the KV cache of the prompts and the first output token">
  <data key="d0">involves</data>
</edge>
<edge source="The remaining slot" target="the subsequent autoregressive generation phase">
  <data key="d0">is reserved for</data>
</edge>
<edge source="one slot" target="available in the last logical block">
  <data key="d0">remains</data>
</edge>
<edge source="newly generated KV cache" target="in the last logical block">
  <data key="d0">is stored</data>
</edge>
<edge source="all tokens" target="prompt phase">
  <data key="d0">are for</data>
</edge>
<edge source="prompt phase" target="Four score and seven years ago our fathers brought">
  <data key="d0">includes</data>
</edge>
<edge source="Logical KV blocks" target="Block 0 Block 1 Block 2">
  <data key="d0">include</data>
</edge>
<edge source="Logical KV blocks" target="Block 0">
  <data key="d0">include</data>
</edge>
<edge source="Logical KV blocks" target="Four score and seven years ago our mothers">
  <data key="d0">include</data>
</edge>
<edge source="Storing multiple tokens within a KV block (block size 1)" target="the PagedAttention kernel to process the KV cache across more positions in parallel">
  <data key="d0">enables</data>
</edge>
<edge source="Processing the KV cache across more positions in parallel" target="hardware utilization">
  <data key="d0">increases</data>
</edge>
<edge source="Processing the KV cache across more positions in parallel" target="latency">
  <data key="d0">reduces</data>
</edge>
<edge source="latency" target="at a gradual pace initially">
  <data key="d0">increases</data>
</edge>
<edge source="latency" target="suddenly">
  <data key="d0">explodes</data>
</edge>
<edge source="a larger block size" target="memory fragmentation">
  <data key="d0">increases</data>
</edge>
<edge source="batching" target="the throughput">
  <data key="d0">improves</data>
</edge>
<edge source="batching" target="serving single or multiple models">
  <data key="d0">is for</data>
</edge>
<edge source="The logical blocks of the two sequences" target="different physical blocks within the space reserved by the block engine in GPU workers">
  <data key="d0">are mapped to</data>
</edge>
<edge source="The neighboring logical blocks of both sequences" target="contiguous in physical GPU memory">
  <data key="d0">do not need to be</data>
</edge>
<edge source="The space of physical blocks" target="both sequences">
  <data key="d0">can be effectively utilized by</data>
</edge>
<edge source="an LLM" target="multiple sampled outputs for a single input prompt">
  <data key="d0">generates</data>
</edge>
<edge source="request" target="616 Sample A1">
  <data key="d0">is</data>
</edge>
<edge source="Sample A1" target="Four score and seven years ago our fathers">
  <data key="d0">contains</data>
</edge>
<edge source="Block 0" target="Physical KV blocks">
  <data key="d0">is part of</data>
</edge>
<edge source="Sample A2" target="Copy-on-write">
  <data key="d0">is</data>
</edge>
<edge source="Sample A2" target="Ref count: 2">
  <data key="d0">has</data>
</edge>
<edge source="Figure 8" target="in the text">
  <data key="d0">is referenced</data>
</edge>
<edge source="one request" target="multiple samples sharing the same input prompt">
  <data key="d0">includes</data>
</edge>
<edge source="one request" target="the KV cache of the prompt to be shared">
  <data key="d0">allows</data>
</edge>
<edge source="multiple samples" target="the same input prompt">
  <data key="d0">share</data>
</edge>
<edge source="all parallel sequences in a request" target="the KV cache for the prompt">
  <data key="d0">can share</data>
</edge>
<edge source="8" target="an example of parallel decoding for two outputs">
  <data key="d0">shows</data>
</edge>
<edge source="both outputs" target="the same prompt">
  <data key="d0">share</data>
</edge>
<edge source="the logical blocks for the prompts of both sequences" target="the same physical blocks">
  <data key="d0">are mapped to</data>
</edge>
<edge source="logical block 0 of both sequences" target="physical block 7">
  <data key="d0">is mapped to</data>
</edge>
<edge source="a single physical block" target="multiple logical blocks">
  <data key="d0">can be mapped to</data>
</edge>
<edge source="reference counts for physical block 7" target="2">
  <data key="d0">are</data>
</edge>
<edge source="the two outputs" target="different output tokens">
  <data key="d0">sample</data>
</edge>
<edge source="the two outputs" target="separate storage for KV cache">
  <data key="d0">need</data>
</edge>
<edge source="copy-on-write mechanism" target="physical blocks that need modification by multiple sequences">
  <data key="d0">applies to</data>
</edge>
<edge source="copy-on-write mechanism" target="the copy-on-write technique in OS virtual memory">
  <data key="d0">is similar to</data>
</edge>
<edge source="copy-on-write technique in OS virtual memory" target="when forking a process">
  <data key="d0">is used</data>
</edge>
<edge source="sample A1" target="its last logical block (logical block 1)">
  <data key="d0">needs to write to</data>
</edge>
<edge source="sample A2" target="physical block 1">
  <data key="d0">writes to</data>
</edge>
<edge source="reference count" target="1">
  <data key="d0">is reduced to</data>
</edge>
<edge source="A2" target="newly generated KV cache to physical block 1">
  <data key="d0">writes</data>
</edge>
<edge source="sharing physical blocks across multiple samples" target="memory usage">
  <data key="d0">can reduce</data>
</edge>
<edge source="memory usage" target="greatly">
  <data key="d0">can be reduced</data>
</edge>
<edge source="memory usage reduction" target="long input prompts">
  <data key="d0">is especially effective for</data>
</edge>
<edge source="Beam search 49" target="the most probable output sequence from an LLM">
  <data key="d0">is widely used to decode</data>
</edge>
<edge source="Beam search 49" target="the computational complexity of fully traversing the Block 10 Block 11 Block 1 Block 3 Block 6 Block 7 Block 5 Block 0 Block 2 Block 4 Block 8 Block 9 Block 12">
  <data key="d0">mitigates</data>
</edge>
<edge source="candidates" target="vocabulary size">
  <data key="d0">is related to</data>
</edge>
<edge source="The algorithm" target="the beam width parameter">
  <data key="d0">relies on</data>
</edge>
<edge source="the beam width parameter" target="the number of top candidates retained at every step">
  <data key="d0">determines</data>
</edge>
<edge source="sharing patterns" target="as the decoding process advances">
  <data key="d0">dynamically change</data>
</edge>
<edge source="sharing patterns" target="the process tree in the OS created by compound forks">
  <data key="d0">are similar to</data>
</edge>
<edge source="each candidate sequence" target="4 full logical blocks">
  <data key="d0">has used</data>
</edge>
<edge source="the iteration" target="the dotted line">
  <data key="d0">is illustrated as</data>
</edge>
<edge source="All beam candidates" target="the first block 0 (i.e., prompt)">
  <data key="d0">share</data>
</edge>
<edge source="Candidate 3" target="others from the second block">
  <data key="d0">digresses from</data>
</edge>
<edge source="Candidates 0-2" target="the first 3 blocks">
  <data key="d0">share</data>
</edge>
<edge source="Candidates 0-2" target="at the fourth block">
  <data key="d0">diverge</data>
</edge>
<edge source="all candidates" target="blocks 0, 1, 3">
  <data key="d0">share</data>
</edge>
<edge source="candidates 0 and 1" target="block 6">
  <data key="d0">share</data>
</edge>
<edge source="top-4 probable candidates" target="candidates 1 and 2">
  <data key="d0">originate from</data>
</edge>
<edge source="original candidates 0 and 3" target="the top candidates">
  <data key="d0">are no longer among</data>
</edge>
<edge source="their logical blocks" target="freed">
  <data key="d0">are</data>
</edge>
<edge source="the reference counts of corresponding physical blocks" target="reduced">
  <data key="d0">are</data>
</edge>
<edge source="new physical blocks (blocks 9-12)" target="the new KV cache from the new candidates">
  <data key="d0">store</data>
</edge>
<edge source="Previous LLM serving systems" target="frequent memory copies of the KV cache across the beam candidates">
  <data key="d0">require</data>
</edge>
<edge source="candidate 3" target="a large portion of candidate 2's KV cache">
  <data key="d0">would need to copy</data>
</edge>
<edge source="candidate 3" target="to continue generation">
  <data key="d0">would need to copy</data>
</edge>
<edge source="most blocks of different beam candidates" target="in vLLM">
  <data key="d0">can be shared</data>
</edge>
<edge source="The same strategy" target="beam search">
  <data key="d0">is applied in</data>
</edge>
<edge source="The same strategy" target="prefix sharing">
  <data key="d0">is applied in</data>
</edge>
<edge source="beam search and prefix sharing" target="vLLM">
  <data key="d0">are applied by</data>
</edge>
<edge source="LLM user" target="a (long) description of the task including instructions and example inputs and outputs">
  <data key="d0">provides</data>
</edge>
<edge source="a (long) description of the task including instructions and example inputs and outputs" target="system prompt 36">
  <data key="d0">is also known as</data>
</edge>
<edge source="The description" target="the actual task input">
  <data key="d0">is concatenated with</data>
</edge>
<edge source="The description and the actual task input" target="the prompt of the request">
  <data key="d0">form</data>
</edge>
<edge source="sea otter" target="loutre de mer">
  <data key="d0">translates to</data>
</edge>
<edge source="peppermint" target="menthe poivre">
  <data key="d0">translates to</data>
</edge>
<edge source="plush girafe" target="girafe en peluche">
  <data key="d0">translates to</data>
</edge>
<edge source="cheese" target="fromage">
  <data key="d0">translates to</data>
</edge>
<edge source="I love you" target="Je tamie">
  <data key="d0">translates to</data>
</edge>
<edge source="Shared prompt" target="machine translation">
  <data key="d0">is example for</data>
</edge>
<edge source="The examples" target="5. on the full prompt">
  <data key="d0">are adopted from</data>
</edge>
<edge source="10" target="an example">
  <data key="d0">shows</data>
</edge>
<edge source="the shared prefix" target="prompt engineering">
  <data key="d0">can be tuned via</data>
</edge>
<edge source="tuning the shared prefix via prompt engineering" target="the accuracy of the downstream tasks 26, 27">
  <data key="d0">improves</data>
</edge>
<edge source="many user prompts" target="a prefix">
  <data key="d0">share</data>
</edge>
<edge source="the LLM service provider" target="the KV cache of the prefix in advance">
  <data key="d0">can store</data>
</edge>
<edge source="storing the KV cache of the prefix in advance" target="the redundant computation spent on the prefix">
  <data key="d0">reduces</data>
</edge>
<edge source="LLM service provider" target="a set of physical blocks for a set of predefined shared prefixes">
  <data key="d0">reserves</data>
</edge>
<edge source="A user input prompt with the shared prefix" target="its logical blocks to the cached physical blocks">
  <data key="d0">can map</data>
</edge>
<edge source="the last block" target="copy-on-write">
  <data key="d0">is marked</data>
</edge>
<edge source="The prompt phase computation" target="the users task input">
  <data key="d0">needs to execute on</data>
</edge>
<edge source="The decoding methods discussed earlier" target="diverse memory sharing and accessing patterns">
  <data key="d0">exhibit</data>
</edge>
<edge source="The LLM and its execution kernel" target="a list of physical block IDs for each sequence">
  <data key="d0">see</data>
</edge>
<edge source="The LLM and its execution kernel" target="sharing patterns across sequences">
  <data key="d0">do not need to handle</data>
</edge>
<edge source="this approach" target="the batching opportunities for requests with different sampling requirements">
  <data key="d0">broadens</data>
</edge>
<edge source="this approach" target="the systems overall throughput">
  <data key="d0">increases</data>
</edge>
<edge source="request traffic" target="systems capacity">
  <data key="d0">surpasses</data>
</edge>
<edge source="first-come-first-serve (FCFS) scheduling policy" target="all requests">
  <data key="d0">applies to</data>
</edge>
<edge source="first-come-first-serve (FCFS) scheduling policy" target="fairness">
  <data key="d0">ensures</data>
</edge>
<edge source="first-come-first-serve (FCFS) scheduling policy" target="starvation">
  <data key="d0">prevents</data>
</edge>
<edge source="block size" target="too small">
  <data key="d0">is</data>
</edge>
<edge source="block size" target="too large">
  <data key="d0">is</data>
</edge>
<edge source="eviction policies" target="heuristics">
  <data key="d0">use</data>
</edge>
<edge source="eviction policies" target="that block">
  <data key="d0">evict</data>
</edge>
<edge source="heuristics" target="which block will be accessed furthest in the future">
  <data key="d0">predict</data>
</edge>
<edge source="all blocks of a sequence" target="together">
  <data key="d0">are accessed</data>
</edge>
<edge source="all-or-nothing eviction policy" target="either evict all or none of the blocks of a sequence">
  <data key="d0">means</data>
</edge>
<edge source="multiple sequences within one request" target="a sequence group">
  <data key="d0">are gang-scheduled as</data>
</edge>
<edge source="The sequences within one sequence group" target="due to potential memory sharing across those sequences">
  <data key="d0">are preempted or rescheduled together</data>
</edge>
<edge source="classic technique" target="most virtual memory implementations">
  <data key="d0">is used by</data>
</edge>
<edge source="most virtual memory implementations" target="evicted pages to a swap space on the disk">
  <data key="d0">copy</data>
</edge>
<edge source="its blocks" target="memory">
  <data key="d0">are freed from</data>
</edge>
<edge source="the blocks of a preempted sequence" target="to continue the processing of that sequence">
  <data key="d0">are brought back in</data>
</edge>
<edge source="the number of blocks swapped to the CPU RAM" target="the number of total physical blocks in the GPU RAM">
  <data key="d0">never exceeds</data>
</edge>
<edge source="the swap space on the CPU RAM" target="the GPU memory allocated for the KV cache">
  <data key="d0">is bounded by</data>
</edge>
<edge source="recomputation latency" target="significantly lower than the original latency">
  <data key="d0">can be</data>
</edge>
<edge source="tokens generated at decoding" target="the original user prompt as a new prompt">
  <data key="d0">can be concatenated with</data>
</edge>
<edge source="their KV cache at all positions" target="one prompt phase iteration">
  <data key="d0">can be generated in</data>
</edge>
<edge source="performances of swapping and recomputation" target="the bandwidth between CPU RAM and GPU memory">
  <data key="d0">depend on</data>
</edge>
<edge source="performances of swapping and recomputation" target="the computation power of the GPU">
  <data key="d0">depend on</data>
</edge>
<edge source="recomputation" target="the KV blocks">
  <data key="d0">does not utilize</data>
</edge>
<edge source="recomputation" target="the block size is small">
  <data key="d0">is more efficient when</data>
</edge>
<edge source="swapping" target="the block size is large">
  <data key="d0">is more efficient when</data>
</edge>
<edge source="swapping" target="excessive overhead with small block sizes">
  <data key="d0">incurs</data>
</edge>
<edge source="recomputation and swapping" target="recovery mechanisms">
  <data key="d0">are</data>
</edge>
<edge source="Many LLMs" target="parameter sizes exceeding the capacity of a single GPU">
  <data key="d0">have</data>
</edge>
<edge source="This strategy" target="an SPMD (Single Program Multiple Data) execution schedule">
  <data key="d0">adheres to</data>
</edge>
<edge source="The detailed model sizes and server configurations" target="Table 1">
  <data key="d0">are shown in</data>
</edge>
<edge source="Model size 13B" target="A100 4A100 8A100-80GB">
  <data key="d0">uses GPUs</data>
</edge>
<edge source="Model size 13B" target="40 GB">
  <data key="d0">has total GPU memory</data>
</edge>
<edge source="Model size 13B" target="26 GB">
  <data key="d0">has parameter size</data>
</edge>
<edge source="Model size 13B" target="12 GB">
  <data key="d0">has memory for KV cache</data>
</edge>
<edge source="Model size 66B" target="160 GB">
  <data key="d0">has total GPU memory</data>
</edge>
<edge source="Model size 66B" target="132 GB">
  <data key="d0">has parameter size</data>
</edge>
<edge source="Model size 66B" target="21 GB">
  <data key="d0">has memory for KV cache</data>
</edge>
<edge source="Model size 175B" target="640 GB">
  <data key="d0">has total GPU memory</data>
</edge>
<edge source="Model size 175B" target="346 GB">
  <data key="d0">has parameter size</data>
</edge>
<edge source="Model size 175B" target="264 GB">
  <data key="d0">has memory for KV cache</data>
</edge>
<edge source="GPUs" target="intermediate results">
  <data key="d0">synchronize</data>
</edge>
<edge source="synchronize" target="all-reduce operation">
  <data key="d0">method</data>
</edge>
<edge source="KV cache slots" target="15.7K, 9.7K, 60.1K">
  <data key="d0">values</data>
</edge>
<edge source="KV cache slots" target="block-wise matrix multiplication">
  <data key="d0">used for</data>
</edge>
<edge source="the attention operator" target="the attention head dimension">
  <data key="d0">is split on</data>
</edge>
<edge source="each SPMD process" target="a subset of attention heads in multi-head attention">
  <data key="d0">takes care of</data>
</edge>
<edge source="each model shard" target="the same set of input tokens">
  <data key="d0">processes</data>
</edge>
<edge source="each model shard" target="the KV Cache for the same positions">
  <data key="d0">requires</data>
</edge>
<edge source="model parallel execution" target="each model shard processing the same set of input tokens">
  <data key="d0">is used with</data>
</edge>
<edge source="Different GPU workers" target="the manager">
  <data key="d0">share</data>
</edge>
<edge source="Different GPU workers" target="the mapping from logical blocks to physical blocks">
  <data key="d0">share</data>
</edge>
<edge source="This common mapping" target="GPU workers to execute the model with the physical blocks provided by the scheduler for each input request">
  <data key="d0">allows</data>
</edge>
<edge source="the scheduler" target="the message with input token IDs for each request in the batch">
  <data key="d0">prepares</data>
</edge>
<edge source="the scheduler" target="the block table for each request">
  <data key="d0">prepares</data>
</edge>
<edge source="the scheduler" target="this control message to the GPU workers">
  <data key="d0">broadcasts</data>
</edge>
<edge source="the GPU workers" target="the sampled tokens of this iteration back to the scheduler">
  <data key="d0">send</data>
</edge>
<edge source="The frontend" target="the OpenAI API 34 interface">
  <data key="d0">extends</data>
</edge>
<edge source="The frontend" target="users to customize sampling parameters for each request">
  <data key="d0">allows</data>
</edge>
<edge source="sampling parameters" target="the maximum sequence length">
  <data key="d0">include</data>
</edge>
<edge source="sampling parameters" target="the beam width">
  <data key="d0">include</data>
</edge>
<edge source="vLLM engine" target="8.5K lines of Python">
  <data key="d0">is written in</data>
</edge>
<edge source="vLLM engine" target="2K lines of CCUDA code">
  <data key="d0">is written in</data>
</edge>
<edge source="popular LLMs" target="the model executor">
  <data key="d0">are used for</data>
</edge>
<edge source="Input and output length distributions" target="ShareGPT dataset">
  <data key="d0">are of</data>
</edge>
<edge source="Input and output length distributions" target="Alpaca dataset">
  <data key="d0">are of</data>
</edge>
<edge source="ShareGPT dataset" target="many long conversations">
  <data key="d0">contains</data>
</edge>
<edge source="Alpaca dataset" target="short sequences">
  <data key="d0">has</data>
</edge>
<edge source="the ShareGPT dataset" target="8.4 longer input prompts on average than the Alpaca dataset">
  <data key="d0">has</data>
</edge>
<edge source="the ShareGPT dataset" target="5.8 longer outputs on average than the Alpaca dataset">
  <data key="d0">has</data>
</edge>
<edge source="the ShareGPT dataset" target="higher variance than the Alpaca dataset">
  <data key="d0">has</data>
</edge>
<edge source="PyTorch" target="39">
  <data key="d0">version</data>
</edge>
<edge source="39" target="Adam Paszke">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="Sam Gross">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="Francisco Massa">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="Adam Lerer">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="James Bradbury">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="Gregory Chanan">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="Trevor Killeen">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="Zeming Lin">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="Natalia Gimelshein">
  <data key="d0">is associated with</data>
</edge>
<edge source="39" target="Luca Antiga">
  <data key="d0">is associated with</data>
</edge>
<edge source="Transformers" target="58">
  <data key="d0">version</data>
</edge>
<edge source="Transformers" target="state-of-the-art natural language processing">
  <data key="d0">are</data>
</edge>
<edge source="The dynamic block mapping in PagedAttention" target="the performance of the GPU operations involving the stored KV cache">
  <data key="d0">affects</data>
</edge>
<edge source="the GPU operations involving the stored KV cache" target="block readwrites and attention">
  <data key="d0">include</data>
</edge>
<edge source="Fused re-shape" target="block write">
  <data key="d0">is combined with</data>
</edge>
<edge source="Fused block copy" target="3">
  <data key="d0">is</data>
</edge>
<edge source="new KV cache" target="blocks">
  <data key="d0">are split into</data>
</edge>
<edge source="new KV cache" target="every Transformer layer">
  <data key="d0">are split in</data>
</edge>
<edge source="kernel launch overheads" target="fusing them into a single kernel">
  <data key="d0">are minimized by</data>
</edge>
<edge source="the attention kernel in FasterTransformer 31" target="KV cache according to the block table">
  <data key="d0">reads</data>
</edge>
<edge source="the attention kernel in FasterTransformer 31" target="attention operations on the fly">
  <data key="d0">performs</data>
</edge>
<edge source="a GPU warp" target="each block">
  <data key="d0">read</data>
</edge>
<edge source="we assign a GPU warp to read each block" target="to ensure coalesced memory access">
  <data key="d0">purpose</data>
</edge>
<edge source="Block copy operations" target="the copy-on-write mechanism">
  <data key="d0">are issued by</data>
</edge>
<edge source="Block copy operations" target="discontinuous blocks">
  <data key="d0">may operate on</data>
</edge>
<edge source="use of the cudaMemcpyAsync API" target="numerous invocations of small data movements">
  <data key="d0">can lead to</data>
</edge>
<edge source="The fork method" target="a new sequence from an existing one">
  <data key="d0">creates</data>
</edge>
<edge source="The append method" target="a new token to the sequence">
  <data key="d0">appends</data>
</edge>
<edge source="the free method" target="the sequence">
  <data key="d0">deletes</data>
</edge>
<edge source="future decoding algorithms" target="combining these methods">
  <data key="d0">can be supported by</data>
</edge>
<edge source="OPT-13B" target="1 GPU">
  <data key="d0">uses</data>
</edge>
<edge source="OPT-13B" target="ShareGPT">
  <data key="d0">runs</data>
</edge>
<edge source="OPT-13B" target="Alpaca">
  <data key="d0">runs</data>
</edge>
<edge source="OPT-13B" target="Alpaca dataset">
  <data key="d0">applied on</data>
</edge>
<edge source="OPT-66B" target="4 GPUs">
  <data key="d0">uses</data>
</edge>
<edge source="OPT-66B" target="ShareGPT">
  <data key="d0">runs</data>
</edge>
<edge source="OPT-66B" target="Alpaca">
  <data key="d0">runs</data>
</edge>
<edge source="OPT-175B" target="8 GPUs">
  <data key="d0">uses</data>
</edge>
<edge source="OPT-175B" target="ShareGPT">
  <data key="d0">runs</data>
</edge>
<edge source="OPT-175B" target="Alpaca">
  <data key="d0">runs</data>
</edge>
<edge source="Normalized latency" target="stoken">
  <data key="d0">measured in</data>
</edge>
<edge source="Request rate" target="reqs">
  <data key="d0">measured in</data>
</edge>
<edge source="Figure 12" target="Normalized latency and Request rate for OPT models with ShareGPT and Alpaca">
  <data key="d0">illustrates</data>
</edge>
<edge source="parallel generation" target="2">
  <data key="d0">has parallel size</data>
</edge>
<edge source="Orca" target="Max">
  <data key="d0">has name</data>
</edge>
<edge source="Orca" target="Pow2">
  <data key="d0">also known as</data>
</edge>
<edge source="Orca" target="Oracle">
  <data key="d0">has variant</data>
</edge>
<edge source="Orca" target="publicly available for use">
  <data key="d0">is not</data>
</edge>
<edge source="Orca" target="the buddy allocation algorithm">
  <data key="d0">uses</data>
</edge>
<edge source="Orca" target="scheduling and interleaving the requests">
  <data key="d0">achieves increased GPU utilization by</data>
</edge>
<edge source="Orca" target="A Distributed Serving System for Transformer-Based Generative Models">
  <data key="d0">is</data>
</edge>
<edge source="Figure 14" target="request rate and normalized latency for parallel generation and beam search">
  <data key="d0">shows</data>
</edge>
<edge source="Single sequence generation" target="OPT models">
  <data key="d0">is performed with</data>
</edge>
<edge source="OPT models" target="ShareGPT dataset">
  <data key="d0">are applied on</data>
</edge>
<edge source="OPT models" target="Alpaca dataset">
  <data key="d0">are applied on</data>
</edge>
<edge source="Orca (Max), Orca (Pow2), Orca (Oracle), vLLM" target="ShareGPT dataset">
  <data key="d0">are compared on</data>
</edge>
<edge source="Orca (Max), Orca (Pow2), Orca (Oracle), vLLM" target="Alpaca dataset">
  <data key="d0">are compared on</data>
</edge>
<edge source="Batched requests" target="0, 5, 10, 15, 20, 25, 30, 35">
  <data key="d0">have values on ShareGPT dataset</data>
</edge>
<edge source="Batched requests" target="0, 25, 50, 75, 100, 125, 150">
  <data key="d0">have values on Alpaca dataset</data>
</edge>
<edge source="Performance values on ShareGPT dataset" target="7.00, 9.81, 13.62, 30.42">
  <data key="d0">are</data>
</edge>
<edge source="Performance values on Alpaca dataset" target="7.00, 43.24, 72.75, 132.44">
  <data key="d0">are</data>
</edge>
<edge source="Average number of batched requests" target="OPT-13B for the ShareGPT (2 reqss) and Alpaca (30 reqss) traces">
  <data key="d0">is measured when serving</data>
</edge>
<edge source="Experimental Setup" target="Model and server configurations">
  <data key="d0">includes</data>
</edge>
<edge source="13B" target="popular sizes for LLMs">
  <data key="d0">are</data>
</edge>
<edge source="66B" target="popular sizes for LLMs">
  <data key="d0">are</data>
</edge>
<edge source="13B and 66B" target="an LLM leaderboard 38">
  <data key="d0">are shown in</data>
</edge>
<edge source="175B" target="the size of the famous GPT-3 5 model">
  <data key="d0">is</data>
</edge>
<edge source="The ShareGPT dataset" target="a collection of user-shared conversations with ChatGPT">
  <data key="d0">is</data>
</edge>
<edge source="The Alpaca dataset" target="an instruction dataset">
  <data key="d0">is</data>
</edge>
<edge source="The Alpaca dataset" target="GPT-3.5 with self-instruct 57">
  <data key="d0">is generated by</data>
</edge>
<edge source="their input and output lengths" target="client requests">
  <data key="d0">to synthesize</data>
</edge>
<edge source="these datasets" target="timestamps">
  <data key="d0">do not include</data>
</edge>
<edge source="request arrival times" target="Poisson distribution with different request rates">
  <data key="d0">are generated using</data>
</edge>
<edge source="Baseline 1" target="FasterTransformer">
  <data key="d0">is</data>
</edge>
<edge source="FasterTransformer 31" target="a distributed inference engine highly optimized for latency">
  <data key="d0">is</data>
</edge>
<edge source="dynamic batching mechanism" target="the existing serving systems such as Triton 30">
  <data key="d0">is similar to</data>
</edge>
<edge source="maximum batch size" target="the GPU memory capacity">
  <data key="d0">is set according to</data>
</edge>
<edge source="The scheduler" target="number of earliest arrived requests">
  <data key="d0">takes up to</data>
</edge>
<edge source="The scheduler" target="FasterTransformer for processing">
  <data key="d0">sends the batch to</data>
</edge>
<edge source="Baseline 2" target="Orca">
  <data key="d0">is</data>
</edge>
<edge source="the three Orca baselines" target="similarly">
  <data key="d0">behave</data>
</edge>
<edge source="Orca 60" target="a state-of-the-art LLM serving system">
  <data key="d0">is</data>
</edge>
<edge source="Orca 60" target="throughput">
  <data key="d0">is optimized for</data>
</edge>
<edge source="Orca 60" target="most relevant to our approach">
  <data key="d0">is</data>
</edge>
<edge source="three versions of Orca" target="how much it over-reserves the space for request outputs">
  <data key="d0">are based on</data>
</edge>
<edge source="One version" target="Orca (Oracle)">
  <data key="d0">is</data>
</edge>
<edge source="the buddy allocation algorithm" target="the memory address to store KV cache">
  <data key="d0">determines</data>
</edge>
<edge source="the system" target="the knowledge of the lengths of the outputs that will be actually generated for the requests">
  <data key="d0">has</data>
</edge>
<edge source="the system" target="the space for outputs">
  <data key="d0">over-reserves</data>
</edge>
<edge source="the system" target="2">
  <data key="d0">over-reserves by at most</data>
</edge>
<edge source="the system" target="the space up to the maximum sequence length of the model">
  <data key="d0">reserves</data>
</edge>
<edge source="the upper-bound performance of Orca" target="infeasible to achieve in practice">
  <data key="d0">is</data>
</edge>
<edge source="true output length" target="25">
  <data key="d0">is</data>
</edge>
<edge source="25" target="Tom Kilburn">
  <data key="d0">includes authors</data>
</edge>
<edge source="25" target="David BG Edwards">
  <data key="d0">includes authors</data>
</edge>
<edge source="25" target="Michael J Lanigan">
  <data key="d0">includes authors</data>
</edge>
<edge source="25" target="Frank H Sumner">
  <data key="d0">includes authors</data>
</edge>
<edge source="the maximum sequence length of the model" target="2048 tokens">
  <data key="d0">is</data>
</edge>
<edge source="normalized latency of the systems" target="the mean of every request's end-to-end latency divided by its output length">
  <data key="d0">is</data>
</edge>
<edge source="normalized latency measurement" target="workloads with different request rates">
  <data key="d0">uses</data>
</edge>
<edge source="normalized latency measurement" target="Orca 60">
  <data key="d0">is done as in</data>
</edge>
<edge source="A high-throughput serving system" target="low normalized latency against high request rates">
  <data key="d0">should retain</data>
</edge>
<edge source="15-minute traces" target="the OPT-175B model">
  <data key="d0">are used for</data>
</edge>
<edge source="use of 15-minute traces" target="the cost limit">
  <data key="d0">is due to</data>
</edge>
<edge source="Parallel generation and beam search" target="OPT-13B">
  <data key="d0">used with</data>
</edge>
<edge source="12" target="the results on the ShareGPT dataset">
  <data key="d0">shows</data>
</edge>
<edge source="13b" target="the results on the Alpaca dataset">
  <data key="d0">shows</data>
</edge>
<edge source="the Alpaca dataset" target="a similar trend to the ShareGPT dataset">
  <data key="d0">follows</data>
</edge>
<edge source="The curves" target="that as the request rate increases, the latency initially increases at a gradual pace but then suddenly explodes">
  <data key="d0">illustrate</data>
</edge>
<edge source="request rate" target="latency initially increases at a gradual pace">
  <data key="d0">increases</data>
</edge>
<edge source="request rate" target="capacity of the serving system">
  <data key="d0">surpasses</data>
</edge>
<edge source="queue length" target="infinitely">
  <data key="d0">continues to grow</data>
</edge>
<edge source="latency of the requests" target="infinitely">
  <data key="d0">continues to grow</data>
</edge>
<edge source="OPT-13B vLLM" target="2.2 more requests at the same time than Orca (Oracle)">
  <data key="d0">processes</data>
</edge>
<edge source="OPT-13B vLLM" target="4.3 more requests at the same time than Orca (Max)">
  <data key="d0">processes</data>
</edge>
<edge source="vLLMs PagedAttention" target="memory usage efficiently">
  <data key="d0">can manage</data>
</edge>
<edge source="vLLMs PagedAttention" target="batching more requests than Orca">
  <data key="d0">enable</data>
</edge>
<edge source="The iteration-level scheduling in Orca 60" target="a complementary technique to PagedAttention in vLLM">
  <data key="d0">is</data>
</edge>
<edge source="Both systems" target="the GPU utilization">
  <data key="d0">aim to increase</data>
</edge>
<edge source="Both systems" target="the throughput of LLM serving">
  <data key="d0">aim to increase</data>
</edge>
<edge source="Scheduling and interleaving the requests" target="more requests to be processed in parallel">
  <data key="d0">allows</data>
</edge>
<edge source="Increasing memory utilization" target="the working sets of more requests to fit into memory">
  <data key="d0">allows</data>
</edge>
<edge source="vLLMs" target="Orca (Oracle) and Orca (Pow2)">
  <data key="d0">have advantage over</data>
</edge>
<edge source="Orca (Oracle) and Orca (Pow2)" target="a large number of requests">
  <data key="d0">can batch</data>
</edge>
<edge source="Orca (Oracle) and Orca (Pow2)" target="inefficiencies in their memory management">
  <data key="d0">can batch requests despite</data>
</edge>
<edge source="advantage of vLLMs over Orca (Oracle) and Orca (Pow2)" target="less pronounced in 12 (f)">
  <data key="d0">is</data>
</edge>
<edge source="model and server configuration for OPT-175B" target="large GPU memory space available to store KV cache">
  <data key="d0">allows for</data>
</edge>
<edge source="Output sequences" target="0 4 8 12">
  <data key="d0">are</data>
</edge>
<edge source="Memory saving (a) Parallel sampling" target="6.09 8.53 9.79">
  <data key="d0">values are</data>
</edge>
<edge source="Beam width" target="0 20 40 60">
  <data key="d0">are</data>
</edge>
<edge source="Memory saving (b) Beam search" target="37.56 53.13 55.16">
  <data key="d0">values are</data>
</edge>
<edge source="Figure 15" target="Memory saving for Parallel sampling and Beam search">
  <data key="d0">illustrates</data>
</edge>
<edge source="Average amount of memory saving" target="sharing KV blocks">
  <data key="d0">is from</data>
</edge>
<edge source="sharing KV blocks" target="OPT-13B for the Alpaca trace">
  <data key="d0">occurs when serving</data>
</edge>
<edge source="14" target="the results for beam search with different beam widths">
  <data key="d0">shows</data>
</edge>
<edge source="improvement of vLLM over Orca (Oracle) on OPT-13B and the Alpaca dataset" target="1.3 in basic sampling">
  <data key="d0">goes from</data>
</edge>
<edge source="improvement of vLLM over Orca (Oracle) on OPT-13B and the Alpaca dataset" target="2.3 in beam search with a width of 6">
  <data key="d0">goes to</data>
</edge>
<edge source="input prompts" target="a common prefix">
  <data key="d0">share</data>
</edge>
<edge source="The prefix" target="1 example with 80 tokens">
  <data key="d0">includes</data>
</edge>
<edge source="The prefix" target="5 examples with 341 tokens">
  <data key="d0">includes</data>
</edge>
<edge source="LLaMA-13B 52" target="the model">
  <data key="d0">is used for</data>
</edge>
<edge source="LLaMA-13B 52" target="multilingual">
  <data key="d0">is</data>
</edge>
<edge source="The first prefix" target="a single example">
  <data key="d0">includes</data>
</edge>
<edge source="a single example" target="one-shot">
  <data key="d0">is</data>
</edge>
<edge source="the other prefix" target="5 examples">
  <data key="d0">includes</data>
</edge>
<edge source="5 examples" target="few-shot">
  <data key="d0">are</data>
</edge>
<edge source="Chatbot" target="LLMs">
  <data key="d0">is one of the most important applications of</data>
</edge>
<edge source="a response" target="the chatting history and the last user query into a prompt">
  <data key="d0">is generated by concatenating</data>
</edge>
<edge source="OPT-13B model" target="limited context length">
  <data key="d0">has</data>
</edge>
<edge source="storing the KV cache between different conversation rounds" target="the space for other requests between the conversation rounds">
  <data key="d0">would occupy</data>
</edge>
<edge source="input prompts for most requests" target="1024 tokens">
  <data key="d0">have</data>
</edge>
<edge source="Orca baselines" target="space for 1024 tokens for the request outputs">
  <data key="d0">reserve</data>
</edge>
<edge source="Orca baselines" target="buddy allocation algorithm">
  <data key="d0">reserve space due to</data>
</edge>
<edge source="Orca baselines" target="how they predict the output lengths">
  <data key="d0">reserve space regardless of</data>
</edge>
<edge source="Latency of attention kernels" target="vLLM (bs 8), FT (bs 8), vLLM (bs 32), FT (bs 32)">
  <data key="d0">is shown for</data>
</edge>
<edge source="Block size" target="1, 2, 4, 8, 16, 32, 64, 128, 256">
  <data key="d0">includes values</data>
</edge>
<edge source="Normalized latency (stoken)" target="0.0, 2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5">
  <data key="d0">has values</data>
</edge>
<edge source="ShareGPT Alpaca (b)" target="End-to-end latency with different block sizes">
  <data key="d0">measures</data>
</edge>
<edge source="our GPU kernels (5)" target="extra overheads of accessing the block table">
  <data key="d0">involve</data>
</edge>
<edge source="our GPU kernels (5)" target="executing extra branches">
  <data key="d0">involve</data>
</edge>
<edge source="our GPU kernels (5)" target="handling variable sequence lengths">
  <data key="d0">involve</data>
</edge>
<edge source="18a" target="2026 higher attention kernel latency">
  <data key="d0">leads to</data>
</edge>
<edge source="2026 higher attention kernel latency" target="highly-optimized FasterTransformer implementation">
  <data key="d0">is compared to</data>
</edge>
<edge source="the overhead" target="small">
  <data key="d0">is</data>
</edge>
<edge source="the overhead" target="the attention operator">
  <data key="d0">affects</data>
</edge>
<edge source="the overhead" target="the other operators in the model">
  <data key="d0">does not affect</data>
</edge>
<edge source="the other operators in the model" target="Linear">
  <data key="d0">include</data>
</edge>
<edge source="The choice of block size" target="a substantial impact on the performance of vLLM">
  <data key="d0">can have</data>
</edge>
<edge source="block sizes from 16 to 128" target="the best performance">
  <data key="d0">lead to</data>
</edge>
<edge source="block sizes" target="the ShareGPT trace">
  <data key="d0">are in</data>
</edge>
<edge source="block size 16 and 32" target="in the Alpaca trace">
  <data key="d0">work well</data>
</edge>
<edge source="larger block sizes" target="the performance">
  <data key="d0">significantly degrade</data>
</edge>
<edge source="sequences" target="shorter than the block sizes">
  <data key="d0">become</data>
</edge>
<edge source="block size 16" target="efficiently utilize the GPU">
  <data key="d0">is large enough to</data>
</edge>
<edge source="block size 16" target="avoid significant internal fragmentation in most workloads">
  <data key="d0">is small enough to</data>
</edge>
<edge source="Time" target="ms">
  <data key="d0">measured in</data>
</edge>
<edge source="Microbenchmark" target="Recompute, Swap in, Swap out, Swap in out">
  <data key="d0">has operations</data>
</edge>
<edge source="End-to-end performance" target="Recompute, Swap">
  <data key="d0">includes operations</data>
</edge>
<edge source="Figure 19" target="Microbenchmark and End-to-end performance">
  <data key="d0">illustrates</data>
</edge>
<edge source="Overhead" target="recomputation and swapping for different block sizes">
  <data key="d0">is for</data>
</edge>
<edge source="the overhead of recomputation" target="constant across different block sizes">
  <data key="d0">remains</data>
</edge>
<edge source="recomputation overhead" target="20 of swappings latency">
  <data key="d0">is never higher than</data>
</edge>
<edge source="small block sizes" target="numerous small data transfers between CPU and GPU">
  <data key="d0">result in</data>
</edge>
<edge source="numerous small data transfers between CPU and GPU" target="the effective PCIe bandwidth">
  <data key="d0">limit</data>
</edge>
<edge source="the two methods" target="comparable end-to-end performance for medium block sizes from 16 to 64">
  <data key="d0">exhibit</data>
</edge>
<edge source="virtual memory and paging technique" target="other GPU workloads">
  <data key="d0">applied to</data>
</edge>
<edge source="tensor shapes" target="typically static in DNN training">
  <data key="d0">are</data>
</edge>
<edge source="memory allocation" target="optimized ahead of time">
  <data key="d0">can be</data>
</edge>
<edge source="an increase in memory efficiency" target="any performance improvement">
  <data key="d0">may not result in</data>
</edge>
<edge source="performance" target="primarily compute-bound">
  <data key="d0">is</data>
</edge>
<edge source="LLM-specific optimizations" target="virtual memory and paging">
  <data key="d0">are applied in</data>
</edge>
<edge source="vLLMs all-or-nothing swap-out policy" target="the fact that processing a request requires all of its corresponding token states to be stored in GPU memory">
  <data key="d0">exploits</data>
</edge>
<edge source="recomputation method" target="recover the evicted blocks">
  <data key="d0">is used to</data>
</edge>
<edge source="recomputation method" target="OS">
  <data key="d0">is not feasible in</data>
</edge>
<edge source="9" target="Related Work General model serving systems">
  <data key="d0">is related to</data>
</edge>
<edge source="Model serving" target="an active area of research in recent years">
  <data key="d0">has been</data>
</edge>
<edge source="numerous systems" target="to tackle diverse aspects of deep learning model deployment">
  <data key="d0">have been proposed</data>
</edge>
<edge source="Clipper" target="general model serving system">
  <data key="d0">is a</data>
</edge>
<edge source="Clipper" target="A Low-Latency Online Prediction Serving System">
  <data key="d0">is</data>
</edge>
<edge source="TensorFlow Serving" target="general model serving system">
  <data key="d0">is a</data>
</edge>
<edge source="Nexus" target="general model serving system">
  <data key="d0">is a</data>
</edge>
<edge source="Nexus" target="a GPU cluster engine">
  <data key="d0">is</data>
</edge>
<edge source="Nexus" target="accelerating DNN-based video analysis">
  <data key="d0">is used for</data>
</edge>
<edge source="InferLine" target="general model serving system">
  <data key="d0">is a</data>
</edge>
<edge source="InferLine" target="latency-aware provisioning and scaling for prediction serving pipelines">
  <data key="d0">is</data>
</edge>
<edge source="Clockwork" target="general model serving system">
  <data key="d0">is a</data>
</edge>
<edge source="caching" target="serving single or multiple models">
  <data key="d0">is for</data>
</edge>
<edge source="placement" target="serving single or multiple models">
  <data key="d0">is for</data>
</edge>
<edge source="scheduling" target="serving single or multiple models">
  <data key="d0">is for</data>
</edge>
<edge source="DVABatch 12" target="multi-entry multi-exit batching">
  <data key="d0">introduces</data>
</edge>
<edge source="REEF 21 and Shep- herd 61" target="preemption for serving">
  <data key="d0">propose</data>
</edge>
<edge source="AlpaServe 28" target="model parallelism for statistical multiplexing">
  <data key="d0">utilizes</data>
</edge>
<edge source="AlpaServe" target="Statistical Multiplexing with Model Parallelism for Deep Learning Serving">
  <data key="d0">is</data>
</edge>
<edge source="these general systems" target="the auto-regressive property and token state of LLM inference">
  <data key="d0">fail to take into account</data>
</edge>
<edge source="Specialized serving systems" target="transformers">
  <data key="d0">are for</data>
</edge>
<edge source="numerous specialized serving systems" target="the transformer architecture">
  <data key="d0">have been developed for</data>
</edge>
<edge source="These systems" target="GPU kernel optimizations 1, 29, 31, 56">
  <data key="d0">utilize</data>
</edge>
<edge source="These systems" target="advanced batching mechanisms 14, 60">
  <data key="d0">utilize</data>
</edge>
<edge source="These systems" target="model parallelism 1, 41, 60">
  <data key="d0">utilize</data>
</edge>
<edge source="These systems" target="parameter sharing 64">
  <data key="d0">utilize</data>
</edge>
<edge source="These systems" target="GPU kernel optimizations, advanced batching mechanisms, model parallelism, and parameter sharing for efficient serving">
  <data key="d0">utilize</data>
</edge>
<edge source="the fine-grained scheduling and interleaving of the requests like in Orca" target="memory management more challenging">
  <data key="d0">makes</data>
</edge>
<edge source="the techniques proposed in vLLM" target="even more crucial">
  <data key="d0">are</data>
</edge>
<edge source="The widening gap between the compute capability and memory capacity of accelerators" target="memory to become a bottleneck for both training and inference">
  <data key="d0">has caused</data>
</edge>
<edge source="Swapping 23, 42, 55, recomputation 7, 24 and their combination 40" target="the peak memory of training">
  <data key="d0">have been utilized to reduce</data>
</edge>
<edge source="FlexGen 46" target="how to swap weights and token states for LLM inference with 623 limited GPU memory">
  <data key="d0">studies</data>
</edge>
<edge source="FlexGen 46" target="the online serving settings">
  <data key="d0">does not target</data>
</edge>
<edge source="OLLA 48" target="the lifetime and location of tensors">
  <data key="d0">optimizes</data>
</edge>
<edge source="OLLA 48" target="fragmentation">
  <data key="d0">reduces</data>
</edge>
<edge source="OLLA 48" target="fine-grained block-level management">
  <data key="d0">does not do</data>
</edge>
<edge source="OLLA 48" target="online serving">
  <data key="d0">does not do</data>
</edge>
<edge source="FlashAttention 13" target="tiling and kernel optimizations">
  <data key="d0">applies</data>
</edge>
<edge source="tiling and kernel optimizations" target="the peak memory of attention computation">
  <data key="d0">reduce</data>
</edge>
<edge source="tiling and kernel optimizations" target="IO costs">
  <data key="d0">reduce</data>
</edge>
<edge source="Zhifeng Chen" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Xiaoxuan Liu, Zhifeng Chen, Yan-ping Huang, anonymous SOSP reviewers, and our shepherd, Lidong Zhou" target="insightful feedback">
  <data key="d0">provided</data>
</edge>
<edge source="This research" target="gifts from Andreessen Horowitz">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Anyscale">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Astronomer">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Google">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from IBM">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Intel">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Lacework">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Microsoft">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Mohamed Bin Zayed University of Artificial Intelligence">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Samsung SDS">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from Uber">
  <data key="d0">is supported by</data>
</edge>
<edge source="This research" target="gifts from VMware">
  <data key="d0">is supported by</data>
</edge>
<edge source="DeepSpeed Inference" target="efficient inference of transformer models at unprecedented scale">
  <data key="d0">enables</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2207.00032">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:1607.06450">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2107.03374">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:1604.06174">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2204.02311">
  <data key="d0">identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2104.08691">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2101.00190">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2302.11665">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:1712.06139">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2211.05102">
  <data key="d0">identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2303.06865">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:1909.08053">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2302.13971">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2212.10560">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:1609.08144">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint" target="arXiv:2205.01068">
  <data key="d0">has identifier</data>
</edge>
<edge source="arXiv preprint arXiv:2207.00032" target="2022">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:1607.06450" target="2016">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:2107.03374" target="2021">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:1604.06174" target="2016">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:2204.02311" target="2022">
  <data key="d0">year</data>
</edge>
<edge source="arXiv preprint arXiv:2104.08691" target="2021">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:2101.00190" target="2021">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:2302.11665" target="2023">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:1712.06139" target="2017">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:2211.05102" target="2022">
  <data key="d0">year</data>
</edge>
<edge source="arXiv preprint arXiv:2303.06865" target="2023">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:1909.08053" target="2019">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:2302.13971" target="2023">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:2212.10560" target="2022">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:1609.08144" target="2016">
  <data key="d0">was published in</data>
</edge>
<edge source="arXiv preprint arXiv:2205.01068" target="2022">
  <data key="d0">was published in</data>
</edge>
<edge source="Jimmy Lei Ba" target="Jamie Ryan Kiros and Geoffrey E Hinton">
  <data key="d0">is an author</data>
</edge>
<edge source="Layer normalization" target="a technique">
  <data key="d0">is</data>
</edge>
<edge source="Yoshua Bengio" target="Rjean Ducharme and Pascal Vincent">
  <data key="d0">is mentioned with</data>
</edge>
<edge source="A neural probabilistic language model" target="a language model">
  <data key="d0">is</data>
</edge>
<edge source="Advances in neural information processing systems" target="13">
  <data key="d0">is volume</data>
</edge>
<edge source="Advances in neural information processing systems" target="30">
  <data key="d0">is volume</data>
</edge>
<edge source="Advances in neural information processing systems 13" target="2000">
  <data key="d0">published in</data>
</edge>
<edge source="Findings" target="the 2016 Conference on Machine Translation">
  <data key="d0">are from</data>
</edge>
<edge source="Association for Computational Linguistics" target="Berlin, Germany">
  <data key="d0">location</data>
</edge>
<edge source="Language models" target="few-shot learners">
  <data key="d0">are</data>
</edge>
<edge source="Advances in Neural Information Processing Systems" target="35">
  <data key="d0">volume</data>
</edge>
<edge source="Advances in Neural Information Processing Systems 35" target="2022">
  <data key="d0">year</data>
</edge>
<edge source="Advances in Neural Information Processing Systems 35 (2022)" target="16344-16359">
  <data key="d0">pages</data>
</edge>
<edge source="Advances in neural information processing systems 27" target="2014">
  <data key="d0">is published in</data>
</edge>
<edge source="Advances in neural information processing systems 30" target="2017">
  <data key="d0">was published in</data>
</edge>
<edge source="6 Mark Chen" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Jerry Tworek" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Heewoo Jun" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Qiming Yuan" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Henrique Ponde de Oliveira Pinto" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Jared Kaplan" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Harri Edwards" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Yuri Burda" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Nicholas Joseph" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="Greg Brockman" target="in the text">
  <data key="d0">is a person mentioned</data>
</edge>
<edge source="large language models" target="code">
  <data key="d0">are trained on</data>
</edge>
<edge source="7" target="Tianqi Chen">
  <data key="d0">is associated with</data>
</edge>
<edge source="7" target="Bing Xu">
  <data key="d0">is associated with</data>
</edge>
<edge source="7" target="Chiyuan Zhang">
  <data key="d0">is associated with</data>
</edge>
<edge source="7" target="Carlos Guestrin">
  <data key="d0">is associated with</data>
</edge>
<edge source="22" target="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun">
  <data key="d0">refers to</data>
</edge>
<edge source="64" target="Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun">
  <data key="d0">is associated with</data>
</edge>
<edge source="Training deep nets" target="sublinear memory cost">
  <data key="d0">has</data>
</edge>
<edge source="28 Zhuohan Li" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Yinmin Zhong" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Vincent Liu" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Xin Jin" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Yanping Huang" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Joseph E Gonzalez" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="46 Ying Sheng" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Binhang Yuan" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Max Ryabinin" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Daniel Y Fu" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Zhiqiang Xie" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Beidi Chen" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Clark Barrett" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="59" target="Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.">
  <data key="d0">is associated with authors</data>
</edge>
<edge source="Vicuna" target="an open-source chatbot">
  <data key="d0">is</data>
</edge>
<edge source="Vicuna" target="GPT-4">
  <data key="d0">is impressing</data>
</edge>
<edge source="Vicuna" target="90 ChatGPT quality">
  <data key="d0">has</data>
</edge>
<edge source="orgblog2023-03-30-vicuna" target="Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.">
  <data key="d0">has authors</data>
</edge>
<edge source="Palm" target="Scaling language modeling with pathways">
  <data key="d0">is about</data>
</edge>
<edge source="10 Daniel Crankshaw, Gur-Eyal Sela, Xiangxi Mo, Corey Zumar, Ion Stoica, Joseph Gonzalez, and Alexey Tumanov" target="persons">
  <data key="d0">are</data>
</edge>
<edge source="Daniel Crankshaw" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Xin Wang" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Guilio Zhou" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Michael J Franklin" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Proceedings" target="the 11th ACM Symposium on Cloud Computing">
  <data key="d0">of</data>
</edge>
<edge source="Proceedings" target="the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming">
  <data key="d0">of</data>
</edge>
<edge source="Proceedings" target="the 14th USENIX Conference on Operating Systems Design and Implementation">
  <data key="d0">of</data>
</edge>
<edge source="Proceedings" target="the IEEE conference on computer vision and pattern recognition">
  <data key="d0">are of</data>
</edge>
<edge source="Proceedings" target="the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems">
  <data key="d0">of</data>
</edge>
<edge source="Proceedings" target="the 27th ACM Symposium on Operating Systems Principles">
  <data key="d0">of</data>
</edge>
<edge source="Proceedings" target="the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers">
  <data key="d0">of</data>
</edge>
<edge source="20th USENIX Symposium on Networked Systems Design and Implementation" target="NSDI 23">
  <data key="d0">abbreviation</data>
</edge>
<edge source="DVABatch" target="Diversity-aware Multi-Entry Multi-Exit Batching">
  <data key="d0">is</data>
</edge>
<edge source="DVABatch" target="Efficient Processing of DNN Services on GPUs">
  <data key="d0">is used for</data>
</edge>
<edge source="USENIX Annual Technical Conference" target="2022">
  <data key="d0">occurred in</data>
</edge>
<edge source="USENIX Annual Technical Conference" target="USENIX ATC 22">
  <data key="d0">abbreviated as</data>
</edge>
<edge source="Flashattention" target="Fast and memory-efficient exact attention with io-awareness">
  <data key="d0">is</data>
</edge>
<edge source="TurboTransformers" target="an efficient GPU serving system for transformer models">
  <data key="d0">is</data>
</edge>
<edge source="Proceedings of the 23rd ACM SIGPLAN symposium" target="principles and practice of parallel programming">
  <data key="d0">is about</data>
</edge>
<edge source="FastAPI" target="15">
  <data key="d0">is</data>
</edge>
<edge source="rnn inference" target="low latency">
  <data key="d0">has characteristic</data>
</edge>
<edge source="rnn inference" target="cellular batching">
  <data key="d0">uses method</data>
</edge>
<edge source="17" target="Amir Gholami">
  <data key="d0">is associated with</data>
</edge>
<edge source="17" target="Zhewei Yao">
  <data key="d0">is associated with</data>
</edge>
<edge source="17" target="Sehoon Kim">
  <data key="d0">is associated with</data>
</edge>
<edge source="17" target="Michael W Mahoney">
  <data key="d0">is associated with</data>
</edge>
<edge source="17" target="Kurt Keutzer">
  <data key="d0">is associated with</data>
</edge>
<edge source="Amir Gholami" target="Amir Gholami">
  <data key="d0">is a person</data>
</edge>
<edge source="Kurt Keutzer" target="Kurt Keutzer">
  <data key="d0">is a person</data>
</edge>
<edge source="18" target="Github">
  <data key="d0">is associated with</data>
</edge>
<edge source="Arpan Gujarati" target="2023 document">
  <data key="d0">is an author</data>
</edge>
<edge source="Reza Karimi" target="2023 document">
  <data key="d0">is an author</data>
</edge>
<edge source="Safya Alzayat" target="2023 document">
  <data key="d0">is an author</data>
</edge>
<edge source="Wei Hao" target="2023 document">
  <data key="d0">is an author</data>
</edge>
<edge source="Antoine Kaufmann" target="2023 document">
  <data key="d0">is an author</data>
</edge>
<edge source="Ymir Vigfusson" target="2023 document">
  <data key="d0">is an author</data>
</edge>
<edge source="Jonathan Mace" target="2023 document">
  <data key="d0">is an author</data>
</edge>
<edge source="Serving DNNs" target="Clockwork">
  <data key="d0">is like</data>
</edge>
<edge source="Serving DNNs" target="Performance Predictability from the Bottom Up">
  <data key="d0">has</data>
</edge>
<edge source="14th USENIX Symposium on Operating Systems Design and Implementation" target="OSDI 20">
  <data key="d0">abbreviation</data>
</edge>
<edge source="16th USENIX Symposium on Operating Systems Design and Implementation" target="OSDI 22">
  <data key="d0">abbreviation</data>
</edge>
<edge source="Microsecond-scale Preemption" target="Concurrent GPU-accelerated DNN Inferences">
  <data key="d0">is for</data>
</edge>
<edge source="Deep residual learning" target="image recognition">
  <data key="d0">is used for</data>
</edge>
<edge source="Swapadvisor" target="deep learning beyond the GPU memory limit via smart swapping">
  <data key="d0">pushes</data>
</edge>
<edge source="24 Paras Jain" target="Paras Jain">
  <data key="d0">is a person</data>
</edge>
<edge source="Ajay Jain" target="Ajay Jain">
  <data key="d0">is a person</data>
</edge>
<edge source="Aniruddha Nrusimha" target="Aniruddha Nrusimha">
  <data key="d0">is a person</data>
</edge>
<edge source="Pieter Abbeel" target="Pieter Abbeel">
  <data key="d0">is a person</data>
</edge>
<edge source="Joseph Gonzalez" target="Joseph Gonzalez">
  <data key="d0">is a person</data>
</edge>
<edge source="Check-mate" target="breaking the memory wall with optimal tensor rematerialization">
  <data key="d0">is about</data>
</edge>
<edge source="One-level storage system" target="a storage system">
  <data key="d0">is</data>
</edge>
<edge source="Brian Lester" target="26">
  <data key="d0">is an author</data>
</edge>
<edge source="Rami Al-Rfou" target="26">
  <data key="d0">is an author</data>
</edge>
<edge source="Noah Constant" target="26">
  <data key="d0">is an author</data>
</edge>
<edge source="The power of scale" target="parameter-efficient prompt tuning">
  <data key="d0">is for</data>
</edge>
<edge source="Prefix-tuning" target="optimizing continuous prompts for generation">
  <data key="d0">is</data>
</edge>
<edge source="Rammer" target="holistic deep learning compiler optimizations with rtasks">
  <data key="d0">enables</data>
</edge>
<edge source="NVIDIA" target="30">
  <data key="d0">is</data>
</edge>
<edge source="NVIDIA" target="31">
  <data key="d0">is</data>
</edge>
<edge source="32" target="NVIDIA">
  <data key="d0">is associated with</data>
</edge>
<edge source="Triton Inference Server" target="n. d.">
  <data key="d0">has no date</data>
</edge>
<edge source="nvidia-triton-inference-server" target="https://developer.nvidia.com">
  <data key="d0">is hosted at</data>
</edge>
<edge source="NCCL" target="The NVIDIA Collective Communication Library">
  <data key="d0">is</data>
</edge>
<edge source="Tensorflow-serving" target="flexible, high-performance ml serving">
  <data key="d0">is</data>
</edge>
<edge source="OpenAI" target="34">
  <data key="d0">is</data>
</edge>
<edge source="OpenAI" target="OpenAI API">
  <data key="d0">is associated with</data>
</edge>
<edge source="OpenAI" target="ChatGPT blog post in 2022">
  <data key="d0">published</data>
</edge>
<edge source="OpenAI API" target="https://openai.com/blog/openai-api in 2020">
  <data key="d0">was referenced in</data>
</edge>
<edge source="ChatGPT blog post" target="https://openai.com/blog/chatgpt">
  <data key="d0">URL</data>
</edge>
<edge source="arXiv:2303.08774" target="cs.CL">
  <data key="d0">has category</data>
</edge>
<edge source="arXiv:2303.08774" target="38">
  <data key="d0">has number of pages</data>
</edge>
<edge source="arXiv:2303.08774" target="LMSYS ORG">
  <data key="d0">is associated with</data>
</edge>
<edge source="Chatbot Arena Leaderboard Week 8" target="MT-Bench and Vicuna-33B">
  <data key="d0">introduces</data>
</edge>
<edge source="Pytorch" target="an imperative style, high-performance deep learning library">
  <data key="d0">is</data>
</edge>
<edge source="Advances in neural information processing systems 32" target="2019">
  <data key="d0">is published in</data>
</edge>
<edge source="POET" target="Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging">
  <data key="d0">is about</data>
</edge>
<edge source="PMLR" target="1757317583">
  <data key="d0">has identifier</data>
</edge>
<edge source="41" target="Reiner Pope">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="Sholto Douglas">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="Aakanksha Chowdhery">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="Jacob Devlin">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="James Bradbury">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="Anselm Levskaya">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="Jonathan Heek">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="Kefan Xiao">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="Shivani Agrawal">
  <data key="d0">includes</data>
</edge>
<edge source="41" target="Jeff Dean">
  <data key="d0">includes</data>
</edge>
<edge source="ZeRO-Offload" target="a method for democratizing billion-scale model training">
  <data key="d0">is</data>
</edge>
<edge source="Amazon Web Services" target="2023. https:www.reuters.comtechnologytech-giants-ai-like-bing-bard-poses-billion-dollar-search-problem-2023-02-22">
  <data key="d0">is mentioned in</data>
</edge>
<edge source="Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram" target="a work referenced as 45">
  <data key="d0">are authors of</data>
</edge>
<edge source="High-throughput Generative Inference" target="Large Language Models">
  <data key="d0">is applied to</data>
</edge>
<edge source="High-throughput Generative Inference of Large Language Models" target="a Single GPU">
  <data key="d0">is performed with</data>
</edge>
<edge source="47" target="Mohammad Shoeybi">
  <data key="d0">includes authors</data>
</edge>
<edge source="47" target="Mostofa Patwary">
  <data key="d0">includes authors</data>
</edge>
<edge source="47" target="Raul Puri">
  <data key="d0">includes authors</data>
</edge>
<edge source="47" target="Patrick LeGresley">
  <data key="d0">includes authors</data>
</edge>
<edge source="47" target="Jared Casper">
  <data key="d0">includes authors</data>
</edge>
<edge source="47" target="Bryan Catanzaro">
  <data key="d0">includes authors</data>
</edge>
<edge source="Megatron-lm" target="training multi-billion parameter language models">
  <data key="d0">is used for</data>
</edge>
<edge source="Megatron-lm" target="model parallelism">
  <data key="d0">uses</data>
</edge>
<edge source="48" target="Benoit Steiner, Mostafa Elhoushi, Jacob Kahn, and James Hegarty">
  <data key="d0">includes authors</data>
</edge>
<edge source="OLLA" target="Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks">
  <data key="d0">stands for</data>
</edge>
<edge source="Ilya Sutskever, Oriol Vinyals, and Quoc V Le" target="arXiv.2210.12924">
  <data key="d0">are authors of</data>
</edge>
<edge source="Sequence to sequence learning" target="neural networks">
  <data key="d0">is done with</data>
</edge>
<edge source="50" target="Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto">
  <data key="d0">includes authors</data>
</edge>
<edge source="57 Yizhong Wang" target="Yeganeh Kordi">
  <data key="d0">is an author</data>
</edge>
<edge source="57 Yizhong Wang" target="Swaroop Mishra">
  <data key="d0">is an author</data>
</edge>
<edge source="57 Yizhong Wang" target="Alisa Liu">
  <data key="d0">is an author</data>
</edge>
<edge source="57 Yizhong Wang" target="Noah A Smith">
  <data key="d0">is an author</data>
</edge>
<edge source="57 Yizhong Wang" target="Daniel Khashabi">
  <data key="d0">is an author</data>
</edge>
<edge source="57 Yizhong Wang" target="Hannaneh Hajishirzi">
  <data key="d0">is an author</data>
</edge>
<edge source="Stanford Alpaca" target="an Instruction-following LLaMA model">
  <data key="d0">is</data>
</edge>
<edge source="https://github.com/tatsu-lab/stanford-alpaca" target="a URL">
  <data key="d0">is</data>
</edge>
<edge source="Hugo Touvron" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Thibaut Lavril" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Gautier Izacard" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Xavier Martinet" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Marie-Anne Lachaux" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Timothe Lacroix" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Baptiste Rozire" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Naman Goyal" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Eric Hambro" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Faisal Azhar" target="2023 document on https:sharegpt.com">
  <data key="d0">is an author of</data>
</edge>
<edge source="Llama" target="Open and efficient foundation language models">
  <data key="d0">is</data>
</edge>
<edge source="53" target="Ashish Vaswani">
  <data key="d0">is associated with</data>
</edge>
<edge source="53" target="Noam Shazeer">
  <data key="d0">is associated with</data>
</edge>
<edge source="53" target="Niki Parmar">
  <data key="d0">is associated with</data>
</edge>
<edge source="53" target="Jakob Uszkoreit">
  <data key="d0">is associated with</data>
</edge>
<edge source="53" target="Llion Jones">
  <data key="d0">is associated with</data>
</edge>
<edge source="53" target="Aidan N Gomez">
  <data key="d0">is associated with</data>
</edge>
<edge source="53" target="ukasz Kaiser">
  <data key="d0">is associated with</data>
</edge>
<edge source="53" target="Illia Polosukhin">
  <data key="d0">is associated with</data>
</edge>
<edge source="Attention" target="all you need">
  <data key="d0">is</data>
</edge>
<edge source="Pacman" target="An Efficient Compaction Approach for Log-Structured Key-Value Store on Persistent Memory">
  <data key="d0">is</data>
</edge>
<edge source="Superneurons" target="Dynamic GPU memory management for training deep neural networks">
  <data key="d0">is about</data>
</edge>
<edge source="LightSeq" target="A High Performance Inference Library for Transformers">
  <data key="d0">is</data>
</edge>
<edge source="Self-Instruct" target="Aligning Language Model with Self Generated Instructions">
  <data key="d0">is about</data>
</edge>
<edge source="Thomas Wolf" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Lysandre Debut" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Victor Sanh" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Julien Chaumond" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Clement Delangue" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Anthony Moi" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Pierric Cistac" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Tim Rault" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Rmi Louf" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Morgan Funtowicz" target="the text">
  <data key="d0">is an author</data>
</edge>
<edge source="Proceedings of the 2020 conference on empirical methods in natural language processing" target="system demonstrations">
  <data key="d0">include</data>
</edge>
<edge source="Google's neural machine translation system" target="the gap between human and machine translation">
  <data key="d0">bridges</data>
</edge>
<edge source="60" target="Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun">
  <data key="d0">is associated with</data>
</edge>
<edge source="SHEPHERD" target="Serving DNNs in the Wild">
  <data key="d0">is</data>
</edge>
<edge source="USENIX Association" target="Boston, MA">
  <data key="d0">is located in</data>
</edge>
<edge source="USENIX Association" target="https://www.usenix.org/conference/nsdi23/presentation/zhang-hong">
  <data key="d0">website</data>
</edge>
<edge source="Susan Zhang" target="Stephen Roller">
  <data key="d0">is an author with</data>
</edge>
<edge source="Susan Zhang" target="Naman Goyal">
  <data key="d0">is an author with</data>
</edge>
<edge source="Susan Zhang" target="Mikel Artetxe">
  <data key="d0">is an author with</data>
</edge>
<edge source="Susan Zhang" target="Moya Chen">
  <data key="d0">is an author with</data>
</edge>
<edge source="Susan Zhang" target="Shuohui Chen">
  <data key="d0">is an author with</data>
</edge>
<edge source="Susan Zhang" target="Christopher Dewan">
  <data key="d0">is an author with</data>
</edge>
<edge source="Susan Zhang" target="Mona Diab">
  <data key="d0">is an author with</data>
</edge>
<edge source="Susan Zhang" target="Xian Li">
  <data key="d0">is an author with</data>
</edge>
<edge source="Susan Zhang" target="Xi Victoria Lin">
  <data key="d0">is an author with</data>
</edge>
<edge source="Opt" target="Open pre-trained transformer language models">
  <data key="d0">is</data>
</edge>
<edge source="Alpa" target="Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning">
  <data key="d0">is</data>
</edge>
<edge source="PetS" target="A Unified Framework for Parameter-Efficient Transformers Serving">
  <data key="d0">is</data>
</edge>
</graph></graphml>