<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d0" for="edge" attr.name="relationship" attr.type="string"/>
<graph edgedefault="directed"><node id="EXISTING SYSTEMS"/>
<node id="KEY-VALUE CACHE (KV CACHE) MEMORY FOR EACH REQUEST IS HUGE AND GROWS AND SHRINKS DYNAMICALLY"/>
<node id="THE KV CACHE SIZE"/>
<node id="THE NUMBER OF REQUESTS"/>
<node id="THIS MEMORY"/>
<node id="FRAGMENTATION AND REDUNDANT DUPLICATION"/>
<node id="THE BATCH SIZE"/>
<node id="INEFFICIENT MEMORY MANAGEMENT"/>
<node id="BATCH SIZE"/>
<node id="WE"/>
<node id="PAGEDATTENTION"/>
<node id="AN ATTENTION ALGORITHM"/>
<node id="THE CLASSICAL VIRTUAL MEMORY AND PAGING TECHNIQUES IN OPERATING SYSTEMS"/>
<node id="PAGEDAT-TENTION"/>
<node id="OPERATING SYSTEMS (OS) SOLUTION TO MEMORY FRAGMENTATION AND SHARING"/>
<node id="VIRTUAL MEMORY WITH PAGING"/>
<node id="KV CACHE STORED IN NON-CONTIGUOUS PAGED MEMORY"/>
<node id="VIRTUAL MEMORY AND PAGING IN OS"/>
<node id="PAGE-DATTENTION"/>
<node id="THE CLASSIC IDEA OF PAGING IN OPERATING SYSTEMS"/>
<node id="STORING CONTINUOUS KEYS AND VALUES IN NON-CONTIGUOUS MEMORY SPACE"/>
<node id="TRADITIONAL ATTENTION ALGORITHMS"/>
<node id="ATTENTION KEY AND VALUES VECTORS"/>
<node id="NON-CONTIGUOUS BLOCKS IN THE MEMORY"/>
<node id="THIS PAPER"/>
<node id="A NEW ATTENTION ALGORITHM"/>
<node id="ATTENTION KEYS AND VALUES TO BE STORED IN NON-CONTIGUOUS PAGED MEMORY"/>
<node id="VLLM"/>
<node id="A HIGH-THROUGHPUT LLM SERVING SYSTEM"/>
<node id="EFFICIENT MEMORY MANAGEMENT ENABLED BY PAGEDATTENTION"/>
<node id="AN LLM SERVING SYSTEM"/>
<node id="NEAR-ZERO WASTE IN KV CACHE MEMORY"/>
<node id="FLEXIBLE SHARING OF KV CACHE WITHIN AND ACROSS REQUESTS"/>
<node id="FLEXIBLE SHARING OF KV CACHE"/>
<node id="MEMORY USAGE"/>
<node id="EXISTING LLM SERVING SYSTEMS 31, 60"/>
<node id="MANAGING THE KV CACHE MEMORY EFFICIENTLY"/>
<node id="THE KV CACHE IN A MORE FLEXIBLE WAY"/>
<node id="BLOCKS"/>
<node id="PAGES"/>
<node id="TOKENS"/>
<node id="BYTES"/>
<node id="REQUESTS"/>
<node id="PROCESSES"/>
<node id="A HIGH-THROUGHPUT DISTRIBUTED LLM SERVING ENGINE"/>
<node id="KV CACHE MEMORY"/>
<node id="ESTABLISHED TECHNIQUES"/>
<node id="OPERATING SYSTEMS"/>
<node id="VIRTUAL MEMORY"/>
<node id="COPY-ON-WRITE"/>
<node id="EFFICIENTLY MANAGE KV CACHE"/>
<node id="HANDLE VARIOUS DECODING ALGORITHMS IN LLM SERVING"/>
<node id="THROUGHPUT OF POPULAR LLMS BY 2-4"/>
<node id="THE SAME LEVEL OF LATENCY"/>
<node id="STATE-OF-THE-ART SYSTEMS"/>
<node id="FASTERTRANSFORMER AND ORCA"/>
<node id="THE PERFORMANCE OF VLLM UNDER A VARIETY OF WORKLOADS"/>
<node id="2-4 THROUGHPUT IMPROVEMENTS OVER THE STATE-OF-THE-ART SYSTEMS"/>
<node id="THE IMPROVEMENT"/>
<node id="LONGER SEQUENCES"/>
<node id="LARGER MODELS"/>
<node id="MORE COMPLEX DECODING ALGORITHMS"/>
<node id="IMPROVEMENTS"/>
<node id="VLLMS SOURCE CODE"/>
<node id="PUBLICLY AVAILABLE"/>
<node id="HTTPS:GITHUB.COMVLLM-PROJECTVLLM"/>
<node id="LARGE LANGUAGE MODELS (LLMS) LIKE GPT 5, 37 AND PALM 9"/>
<node id="NEW APPLICATIONS SUCH AS PROGRAMMING ASSISTANTS 6, 18 AND UNIVERSAL CHATBOTS 19, 35"/>
<node id="OUR WORK AND DAILY ROUTINES"/>
<node id="MANY CLOUD COMPANIES 34, 44"/>
<node id="THESE APPLICATIONS AS HOSTED SERVICES"/>
<node id="RUNNING THESE APPLICATIONS"/>
<node id="VERY EXPENSIVE"/>
<node id="A LARGE NUMBER OF HARDWARE ACCELERATORS SUCH AS GPUS"/>
<node id="PROCESSING AN LLM REQUEST"/>
<node id="10 MORE EXPENSIVE THAN A TRADITIONAL KEYWORD QUERY"/>
<node id="THIS WORK"/>
<node id="A CREATIVE COMMONS ATTRIBUTION INTERNATIONAL 4.0 LICENSE"/>
<node id="SOSP 23"/>
<node id="OCTOBER 23 26 2023"/>
<node id="KOBLENZ GERMANY"/>
<node id="COPYRIGHT"/>
<node id="2023"/>
<node id="OWNERAUTHOR(S)"/>
<node id="ACM"/>
<node id="ISBN 979-8-4007-0229-72310"/>
<node id="NVIDIA A100"/>
<node id="40GB PARAMETERS"/>
<node id="KV CACHE"/>
<node id="26GB"/>
<node id="65"/>
<node id="OTHERS"/>
<node id="20"/>
<node id="30"/>
<node id="40"/>
<node id="GB"/>
<node id="PARAMETER SIZE"/>
<node id="EXISTING SYSTEMS VLLM"/>
<node id="THROUGHPUT"/>
<node id="MEMORY LAYOUT"/>
<node id="LEFT WHEN SERVING AN LLM WITH 13B PARAMETERS ON NVIDIA A100"/>
<node id="MEMORY DISTRIBUTION"/>
<node id="1 (LEFT)"/>
<node id="13B-PARAMETER LLM"/>
<node id="NVIDIA A100 GPU"/>
<node id="40GB RAM"/>
<node id="THE PARAMETERS (GRAY)"/>
<node id="GPU MEMORY"/>
<node id="SERVING"/>
<node id="THE MEMORY FOR THE KV CACHE (RED)"/>
<node id="PER SERVING REQUEST"/>
<node id="A CONTIGUOUS CHUNK OF MEMORY"/>
<node id="THE REQUESTS MAXIMUM LENGTH"/>
<node id="2048 TOKENS"/>
<node id="ALL AVAILABLE MEMORY"/>
<node id="OUTPUT LENGTH OF A REQUEST"/>
<node id="DECODING"/>
<node id="MEMORY REQUIRED FOR ITS KV CACHE"/>
<node id="AS THE OUTPUT LENGTH OF A REQUEST GROWS AT DECODING"/>
<node id="AVAILABLE MEMORY FOR INCOMING REQUESTS OR ONGOING GENERATION FOR EXISTING PROMPTS"/>
<node id="A SMALL AMOUNT OF MEMORY (YELLOW)"/>
<node id="EPHEMERALLY FOR ACTIVATION"/>
<node id="THE RAPID GROWTH CURVE OF KV CACHE MEMORY SEEN IN EXISTING SYSTEMS 31, 60"/>
<node id="A NOTABLE BOOST IN SERVING THROUGHPUT"/>
<node id="KEY IDEA BEHIND VLLMS MEMORY MANAGER"/>
<node id="VIRTUAL MEMORY 25 IN OPERATING SYSTEMS"/>
<node id="THE IDEAS BEHIND VIRTUAL MEMORY"/>
<node id="THE KV CACHE IN AN LLM SERVICE"/>
<node id="COST PER REQUEST OF LLM SERVING SYSTEMS"/>
<node id="MORE IMPORTANT"/>
<node id="LLMS"/>
<node id="AN AUTOREGRESSIVE TRANSFORMER MODEL 53"/>
<node id="THIS MODEL"/>
<node id="WORDS (TOKENS)"/>
<node id="ONE AT A TIME"/>
<node id="BASED ON THE INPUT (PROMPT)"/>
<node id="BASED ON THE PREVIOUS SEQUENCE OF THE OUTPUTS TOKENS IT HAS GENERATED SO FAR"/>
<node id="THIS EXPENSIVE PROCESS"/>
<node id="FOR EACH REQUEST"/>
<node id="THE MODEL OUTPUTS A TERMINATION TOKEN"/>
<node id="SEQUENTIAL GENERATION PROCESS"/>
<node id="WORKLOAD MEMORY-BOUND"/>
<node id="COMPUTATION POWER OF GPUS"/>
<node id="SERVING THROUGHPUT"/>
<node id="IMPROVING THE THROUGHPUT"/>
<node id="BATCHING MULTIPLE REQUESTS TOGETHER"/>
<node id="MEMORY SPACE FOR EACH REQUEST"/>
<node id="EFFICIENTLY MANAGED"/>
<node id="APPROXIMATELY 65 OF THE MEMORY"/>
<node id="THE MODEL WEIGHTS"/>
<node id="STATIC DURING SERVING"/>
<node id="CLOSE TO 30 OF THE MEMORY"/>
<node id="THE DYNAMIC STATES OF THE REQUESTS"/>
<node id="STATES"/>
<node id="KEY AND VALUE TENSORS"/>
<node id="ATTENTION MECHANISM"/>
<node id="KV CACHE 41"/>
<node id="CONTEXT FROM EARLIER TOKENS"/>
<node id="NEW OUTPUT TOKENS IN SEQUENCE"/>
<node id="THE REMAINING SMALL EQUAL CONTRIBUTION"/>
<node id="UNKNOWN"/>
<node id="ORCA (MAX)"/>
<node id="20.4"/>
<node id="ORCA (POW2)"/>
<node id="13.3"/>
<node id="ORCA (ORACLE)"/>
<node id="57.3"/>
<node id="8.9"/>
<node id="26.8"/>
<node id="17.9"/>
<node id="13.6"/>
<node id="41.6"/>
<node id="38.2"/>
<node id="25.2"/>
<node id="36.6"/>
<node id="96.3"/>
<node id="AVERAGE PERCENTAGE OF MEMORY"/>
<node id="DIFFERENT LLM SERVING SYSTEMS"/>
<node id="EXPERIMENT"/>
<node id="6.2"/>
<node id="CONTRIBUTIONS"/>
<node id="CHALLENGES IN MEMORY ALLOCATION IN SERVING LLMS"/>
<node id="IMPACT ON SERVING PERFORMANCE"/>
<node id="PERCENTAGE OF MEMORY"/>
<node id="OTHER DATA"/>
<node id="ACTIVATIONS"/>
<node id="EPHEMERAL TENSORS"/>
<node id="EVALUATING THE LLM"/>
<node id="MODEL WEIGHTS"/>
<node id="CONSTANT"/>
<node id="A SMALL FRACTION OF THE GPU MEMORY"/>
<node id="THE WAY THE KV CACHE IS MANAGED"/>
<node id="CRITICAL IN DETERMINING THE MAXIMUM BATCH SIZE"/>
<node id="THROUGHPUT OF THE LLM"/>
<node id="LIMITATION OF BATCH SIZE AND THROUGHPUT"/>
<node id="INEFFICIENT MANAGEMENT OF KV CACHE MEMORY"/>
<node id="FINE-GRAINED BATCHING"/>
<node id="THE WASTE OF COMPUTING"/>
<node id="REQUESTS TO BE BATCHED IN A MORE FLEXIBLE WAY"/>
<node id="THE NUMBER OF REQUESTS THAT CAN BE BATCHED TOGETHER"/>
<node id="GPU MEMORY CAPACITY"/>
<node id="THE SPACE TO STORE THE KV CACHE"/>
<node id="THE IDEA OF VIRTUAL MEMORY AND PAGING"/>
<node id="MANAGING THE KV CACHE IN LLM SERVING"/>
<node id="THE WORKLOAD"/>
<node id="DYNAMIC MEMORY ALLOCATION"/>
<node id="THE OUTPUT LENGTH"/>
<node id="NOT KNOWN A PRIORI"/>
<node id="ITS PERFORMANCE"/>
<node id="THE GPU MEMORY CAPACITY"/>
<node id="THE KV CACHE OF A REQUEST"/>
<node id="CONTIGUOUS MEMORY SPACE"/>
<node id="MOST DEEP LEARNING FRAMEWORKS 33, 39"/>
<node id="TENSORS TO BE STORED IN CONTIGUOUS MEMORY"/>
<node id="OPERATORS IN CURRENT DEEP LEARNING FRAMEWORKS"/>
<node id="PREVIOUS LLM SERVING SYSTEMS"/>
<node id="THE KV CACHE OF ONE REQUEST AS A CONTIGUOUS TENSOR ACROSS THE DIFFERENT POSITIONS"/>
<node id="UNIQUE CHARACTERISTICS"/>
<node id="OVER TIME"/>
<node id="MODEL"/>
<node id="NEW TOKENS"/>
<node id="LIFETIME AND LENGTH OF KV CACHE"/>
<node id="THE EXISTING SYSTEMS"/>
<node id="INTERNAL AND EXTERNAL MEMORY FRAGMENTATION"/>
<node id="THE REQUESTS ACTUAL LENGTH"/>
<node id="MUCH SHORTER THAN ITS MAXIMUM LENGTH"/>
<node id="PRE-ALLOCATION"/>
<node id="INEFFICIENT"/>
<node id="ENTIRE CHUNK"/>
<node id="REQUESTS LIFETIME"/>
<node id="OTHER SHORTER REQUESTS"/>
<node id="ANY PART OF THE CHUNK THAT IS CURRENTLY UNUSED"/>
<node id="ONLY 20.4 - 38.2 OF THE KV CACHE MEMORY"/>
<node id="THE ACTUAL TOKEN STATES"/>
<node id="KV CACHE OF ONE TOKEN"/>
<node id="ALL ITS PREVIOUS TOKENS"/>
<node id="THE KV CACHE OF THE SAME TOKEN"/>
<node id="DIFFERENT"/>
<node id="THE SAME TOKEN"/>
<node id="DIFFERENT POSITIONS IN A SEQUENCE"/>
<node id="THE TOKEN IN EACH MEMORY SLOT"/>
<node id="ITS KV CACHE"/>
<node id="THE SAME TOKENS"/>
<node id="DIFFERENT KV CACHE"/>
<node id="AT DIFFERENT POSITIONS"/>
<node id="THE OPPORTUNITIES FOR MEMORY SHARING"/>
<node id="LLM SERVICES"/>
<node id="ADVANCED DECODING ALGORITHMS"/>
<node id="PARALLEL SAMPLING"/>
<node id="BEAM SEARCH"/>
<node id="MULTIPLE OUTPUTS PER REQUEST"/>
<node id="A RANGE OF DECODING ALGORITHMS"/>
<node id="DECODING ALGORITHMS"/>
<node id="USERS TO SELECT FROM"/>
<node id="VARYING IMPLICATIONS FOR MEMORY MANAGEMENT COMPLEXITY"/>
<node id="LLM SERVICE"/>
<node id="MORE COMPLEX DECODING SCENARIOS"/>
<node id="COMPLEX ACCESSING PATTERNS"/>
<node id="MORE OPPORTUNITIES FOR MEMORY SHARING"/>
<node id="THE REQUEST"/>
<node id="MULTIPLE SEQUENCES"/>
<node id="THEIR KV CACHE"/>
<node id="OF TWO REQUESTS"/>
<node id="TWO REQUESTS"/>
<node id="AT THE SAME TIME"/>
<node id="IN VLLM"/>
<node id="REQUEST"/>
<node id="ITS GENERATION"/>
<node id="ITS KV BLOCKS"/>
<node id="THE KV CACHE OF OTHER REQUESTS"/>
<node id="MEMORY SHARING"/>
<node id="THE KV CACHE OF THE SEQUENCES"/>
<node id="SEPARATE CONTIGUOUS SPACES"/>
<node id="THE REQUESTS KV CACHE INTO BLOCKS"/>
<node id="EACH BLOCK"/>
<node id="THE ATTENTION KEYS AND VALUES OF A FIXED NUMBER OF TOKENS"/>
<node id="PAGEDATTENTION KERNEL"/>
<node id="DIFFERENT KV BLOCKS"/>
<node id="BLOCKS FOR THE KV CACHE"/>
<node id="CONTIGUOUS SPACE"/>
<node id="THE KV CACHE MANAGER"/>
<node id="THE KV CACHE"/>
<node id="A PAGED FASHION"/>
<node id="THE DESIGN OF THE KV CACHE MANAGER IN 4.2"/>
<node id="THE DESIGN OF THE KV CACHE MANAGER"/>
<node id="PAGEDATTENTION IN 4.3"/>
<node id="THE KV CACHE OF EACH SEQUENCE INTO KV BLOCKS"/>
<node id="THE KV CACHE AS FIXED-SIZE KV BLOCKS"/>
<node id="KV BLOCKS"/>
<node id="PAGES IN VIRTUAL MEMORY"/>
<node id="WE TO ORGANIZE THE KV CACHE"/>
<node id="THIS DESIGN"/>
<node id="INTERNAL FRAGMENTATION"/>
<node id="RELATIVELY SMALL BLOCKS"/>
<node id="THEM ON DEMAND"/>
<node id="ALL BLOCKS"/>
<node id="THE SAME SIZE"/>
<node id="THE DIFFERENT SEQUENCES ASSOCIATED WITH THE SAME REQUEST"/>
<node id="THE DIFFERENT REQUESTS"/>
<node id="BLOCK-LEVEL MEMORY MANAGEMENT"/>
<node id="PREEMPTIVE REQUEST SCHEDULING"/>
<node id="PAGEDATTENTION ALGORITHM"/>
<node id="KV BLOCKS TO BE STORED IN NON-CONTIGUOUS PHYSICAL MEMORY"/>
<node id="NON-CONTIGUOUS PHYSICAL MEMORY"/>
<node id="MORE FLEXIBLE PAGED MEMORY MANAGEMENT IN VLLM"/>
<node id="THE MEMORY DURING THE DECODING PROCESS OF A SINGLE INPUT SEQUENCE"/>
<node id="RESERVING THE MEMORY FOR THE MAXIMUM POSSIBLE GENERATED SEQUENCE LENGTH INITIALLY"/>
<node id="OSS VIRTUAL MEMORY"/>
<node id="PREVIOUS KV CACHE"/>
<node id="LOGICAL KV BLOCKS"/>
<node id="NEWLY GENERATED KV CACHE"/>
<node id="PHYSICAL KV BLOCKS"/>
<node id="THIS SHARING EASILY"/>
<node id="MEMORY"/>
<node id="PAGEDATTENTION AND PAGED MEMORY MANAGEMENT"/>
<node id="POPULAR LLMS SUCH AS GPT 5, OPT 62, AND LLAMA 52"/>
<node id="POPULAR LLMS"/>
<node id="GPT 5"/>
<node id="OPT 62"/>
<node id="LLAMA 52"/>
<node id="VARYING SIZES"/>
<node id="ONES EXCEEDING THE MEMORY CAPACITY OF A SINGLE GPU"/>
<node id="A DISTRIBUTED LLM SERVING ENGINE"/>
<node id="VLLM ON VARIOUS SCENARIOS"/>
<node id="PREVIOUS STATE-OF-THE-ART SOLUTIONS"/>
<node id="FASTERTRANSFORMER 31"/>
<node id="ORCA 60"/>
<node id="UP TO 22 HIGHER REQUEST RATES"/>
<node id="FASTERTRANSFORMER"/>
<node id="A FINE-GRAINED SCHEDULING MECHANISM"/>
<node id="THE MEMORY LIKE ORCA (MAX)"/>
<node id="MEMORY FRAGMENTATION"/>
<node id="SHARING"/>
<node id="MORE REQUESTS IN A BATCH IN PARALLEL"/>
<node id="2-4 SPEEDUP"/>
<node id="ORCA"/>
<node id="THIS SECTION"/>
<node id="THE GENERATION AND SERVING PROCEDURES OF TYPICAL LLMS"/>
<node id="THE ITERATION-LEVEL SCHEDULING USED IN LLM SERVING"/>
<node id="THE TASK OF LANGUAGE MODELING"/>
<node id="THE PROBABILITY OF A LIST OF TOKENS"/>
<node id="LANGUAGE"/>
<node id="A NATURAL SEQUENTIAL ORDERING"/>
<node id="TRANSFORMERS 53"/>
<node id="THE DE FACTO STANDARD ARCHITECTURE FOR MODELING THE PROBABILITY ABOVE AT A LARGE SCALE"/>
<node id="THE MOST IMPORTANT COMPONENT OF A TRANSFORMER-BASED LANGUAGE MODEL"/>
<node id="ITS SELF-ATTENTION LAYERS"/>
<node id="A SELF-ATTENTION LAYER"/>
<node id="LINEAR TRANSFORMATIONS ON EACH POSITION"/>
<node id="THE QUERY VECTOR"/>
<node id="THE KEY VECTOR"/>
<node id="THE VALUE VECTOR"/>
<node id="THE SELF-ATTENTION LAYER"/>
<node id="THE ATTENTION SCORE"/>
<node id="MULTIPLYING THE QUERY VECTOR AT ONE POSITION WITH ALL THE KEY VECTORS BEFORE IT"/>
<node id="THE OUTPUT"/>
<node id="THE WEIGHTED AVERAGE OVER THE VALUE VECTORS"/>
<node id="ALL OTHER COMPONENTS"/>
<node id="THE TRANSFORMER MODEL"/>
<node id="THE EMBEDDING LAYER"/>
<node id="FEED-FORWARD LAYER"/>
<node id="LAYER NORMALIZATION 2"/>
<node id="RESIDUAL CONNECTION 22"/>
<node id="OUTPUT LOGIT COMPUTATION"/>
<node id="THE QUERY TRANSFORMATION"/>
<node id="THE KEY TRANSFORMATION"/>
<node id="THE VALUE TRANSFORMATION"/>
<node id="CONDITIONAL GENERATION SERVICE"/>
<node id="COMPLETION API 34"/>
<node id="CHATBOT 19, 35"/>
<node id="A REQUEST TO AN LLM SERVICE"/>
<node id="A LIST OF INPUT PROMPT TOKENS"/>
<node id="THE CONCATENATION OF THE PROMPT AND OUTPUT LISTS AS SEQUENCE"/>
<node id="DECOMPOSITION"/>
<node id="IN EQ"/>
<node id="THE LLM"/>
<node id="NEW TOKENS ONE BY ONE"/>
<node id="THE GENERATION PROCESS OF EACH NEW TOKEN"/>
<node id="ALL THE PREVIOUS TOKENS IN THAT SEQUENCE"/>
<node id="KEY AND VALUE VECTORS"/>
<node id="KEY AND VALUE VECTORS OF EXISTING TOKENS"/>
<node id="GENERATING FUTURE TOKENS"/>
<node id="A REQUESTS KV CACHE"/>
<node id="A SERIES OF LOGICAL KV BLOCKS"/>
<node id="LEFT TO RIGHT"/>
<node id="NEW TOKENS AND THEIR KV CACHE ARE GENERATED"/>
<node id="GENERATION COMPUTATION IN THE LLM SERVICE"/>
<node id="TWO PHASES"/>
<node id="THE PROMPT PHASE"/>
<node id="THE WHOLE USER PROMPT"/>
<node id="REQUESTS AND THE LATEST TOKENS FOR GENERATION PHASE REQUESTS"/>
<node id="THE LLM AS ONE SEQUENCE"/>
<node id="THIS PROCESS"/>
<node id="THE KEY VECTORS 1"/>
<node id="THE COMPUTATION OF THE PROMPT PHASE"/>
<node id="MATRIX-MATRIX MULTIPLICATION OPERATIONS"/>
<node id="THIS PHASE"/>
<node id="PARALLELISM INHERENT IN GPUS"/>
<node id="THE AUTOREGRESSIVE GENERATION PHASE"/>
<node id="THE REMAINING NEW TOKENS SEQUENTIALLY"/>
<node id="THE MODEL"/>
<node id="ONE TOKEN AS INPUT"/>
<node id="KEY AND VALUE VECTORS AT POSITIONS 1 TO 1"/>
<node id="CACHED AT PREVIOUS ITERATIONS"/>
<node id="NEW KEY AND VALUE VECTOR"/>
<node id="COMPUTED AT THIS ITERATION"/>
<node id="WHEN THE SEQUENCE REACHES A MAXIMUM LENGTH"/>
<node id="MAXIMUM LENGTH"/>
<node id="USERS"/>
<node id="WHEN AN END-OF-SEQUENCE (EOS) TOKEN IS EMITTED"/>
<node id="THE COMPUTATION AT DIFFERENT ITERATIONS"/>
<node id="DUE TO THE DATA DEPENDENCY"/>
<node id="MATRIX-VECTOR MULTIPLICATION"/>
<node id="LESS EFFICIENT"/>
<node id="GPU COMPUTATION"/>
<node id="MEMORY-BOUND"/>
<node id="MOST PORTION OF THE LATENCY OF A SINGLE REQUEST"/>
<node id="COMPUTE UTILIZATION IN SERVING LLMS"/>
<node id="BATCHING MULTIPLE REQUESTS"/>
<node id="BATCHING THE REQUESTS TO AN LLM SERVICE"/>
<node id="NON-TRIVIAL"/>
<node id="OVERHEAD OF MOVING WEIGHTS"/>
<node id="REQUESTS IN A BATCH"/>
<node id="COMPUTATIONAL OVERHEAD"/>
<node id="SUFFICIENTLY LARGE"/>
<node id="DIFFERENT TIMES"/>
<node id="A NAIVE BATCHING STRATEGY"/>
<node id="EARLIER REQUESTS WAIT FOR LATER ONES"/>
<node id="THE INCOMING REQUESTS UNTIL EARLIER ONES FINISH"/>
<node id="SIGNIFICANT QUEUEING DELAYS"/>
<node id="VASTLY DIFFERENT INPUT AND OUTPUT LENGTHS"/>
<node id="A STRAIGHTFORWARD BATCHING TECHNIQUE"/>
<node id="THE INPUTS AND OUTPUTS OF THE REQUESTS"/>
<node id="EQUALIZE THEIR LENGTHS"/>
<node id="GPU COMPUTATION AND MEMORY"/>
<node id="FINE-GRAINED BATCHING MECHANISMS"/>
<node id="CELLULAR BATCHING 16"/>
<node id="ITERATION-LEVEL SCHEDULING 60"/>
<node id="PROPOSED"/>
<node id="THESE TECHNIQUES"/>
<node id="THE ITERATION LEVEL"/>
<node id="TRADITIONAL METHODS"/>
<node id="THE REQUEST LEVEL"/>
<node id="COMPLETED REQUESTS"/>
<node id="THE BATCH"/>
<node id="A NEW REQUEST"/>
<node id="WAITING FOR A SINGLE ITERATION"/>
<node id="WAITING FOR THE ENTIRE BATCH TO COMPLETE"/>
<node id="SPECIAL GPU KERNELS"/>
<node id="THE NEED TO PAD THE INPUTS AND OUTPUTS"/>
<node id="QUEUEING DELAY"/>
<node id="INEFFICIENCIES FROM PADDING"/>
<node id="THROUGHPUT OF LLM SERVING"/>
<node id="THREE TYPES OF MEMORY WASTES"/>
<node id="RESERVED, INTERNAL FRAGMENTATION, AND EXTERNAL FRAGMENTATION"/>
<node id="THAT PREVENT OTHER REQUESTS FROM FITTING INTO THE MEMORY"/>
<node id="THE SERVING SYSTEMS THROUGHPUT"/>
<node id="THE PERFORMANCE OF THE SYSTEMS"/>
<node id="COMPUTE-BOUND RATHER THAN MEMORY-BOUND"/>
<node id="OVERCOMING THIS MEMORY-BOUND"/>
<node id="ADDRESSING THE FOLLOWING CHALLENGES IN THE MEMORY MANAGEMENT"/>
<node id="CHALLENGES"/>
<node id="LARGE KV CACHE"/>
<node id="OPT"/>
<node id="SEQUENCES UP TO 2048 TOKENS"/>
<node id="MEMORY REQUIRED TO STORE THE KV CACHE OF ONE REQUEST"/>
<node id="1.6 GB"/>
<node id="CONCURRENT GPUS"/>
<node id="MEMORY CAPACITIES IN THE TENS OF GBS"/>
<node id="GPU'S COMPUTATION SPEED"/>
<node id="MEMORY CAPACITY"/>
<node id="FLOPS"/>
<node id="NVIDIA A100 TO H100"/>
<node id="MORE THAN 2X"/>
<node id="80GB MAXIMUM"/>
<node id="AN INCREASINGLY SIGNIFICANT BOTTLENECK"/>
<node id="COMPLEX DECODING ALGORITHMS"/>
<node id="ALGORITHMS"/>
<node id="MULTIPLE RANDOM SAMPLES FROM A SINGLE INPUT PROMPT"/>
<node id="KV CACHE OF THE PROMPT PART"/>
<node id="12 OF THE TOTAL KV CACHE MEMORY IN OUR EXPERIMENT"/>
<node id="MINIMIZE MEMORY USAGE"/>
<node id="UNSHARED DURING THE AUTOREGRESSIVE GENERATION PHASE"/>
<node id="DIFFERENT SAMPLE RESULTS"/>
<node id="CONTEXT AND POSITION"/>
<node id="THE EXTENT OF KV CACHE SHARING"/>
<node id="THE SPECIFIC DECODING ALGORITHM EMPLOYED"/>
<node id="DIFFERENT REQUEST BEAMS"/>
<node id="LARGER PORTIONS OF THEIR KV CACHE"/>
<node id="LARGER PORTIONS"/>
<node id="55 MEMORY SAVING"/>
<node id="SHARING PATTERN"/>
<node id="THE DECODING PROCESS ADVANCES"/>
<node id="BEAM SEARCH 49"/>
<node id="MORE SOPHISTICATED ALGORITHMS"/>
<node id="SCHEDULING"/>
<node id="UNKNOWN INPUT OUTPUT LENGTHS"/>
<node id="THE REQUESTS TO AN LLM SERVICE"/>
<node id="VARIABILITY IN THEIR INPUT AND OUTPUT LENGTHS"/>
<node id="A UNIQUE CHALLENGE"/>
<node id="INPUT PROMPTS FOR AN LLM"/>
<node id="SIGNIFICANTLY IN LENGTH"/>
<node id="RESULTING OUTPUT LENGTHS"/>
<node id="A PRIORI"/>
<node id="BOTH THE INPUT PROMPT AND THE MODEL"/>
<node id="THE MEMORY MANAGEMENT SYSTEM"/>
<node id="A WIDE RANGE OF PROMPT LENGTHS"/>
<node id="THE SYSTEM"/>
<node id="SCHEDULING DECISIONS"/>
<node id="DELETING OR SWAPPING OUT THE KV CACHE OF SOME REQUESTS FROM GPU MEMORY"/>
<node id="THE ALLOCATION"/>
<node id="THE ACTUAL INPUT OR EVENTUAL OUTPUT LENGTH OF THE REQUEST"/>
<node id="REQUEST A"/>
<node id="2048"/>
<node id="REQUEST B"/>
<node id="512"/>
<node id="THE EXTERNAL FRAGMENTATION"/>
<node id="GENERATED TOKENS"/>
<node id="BEFORE SERVING A REQUEST"/>
<node id="UNUSED"/>
<node id="RESERVING THIS SPACE"/>
<node id="THE SPACE THAT COULD OTHERWISE BE USED TO PROCESS OTHER REQUESTS"/>
<node id="THE RESERVED MEMORY"/>
<node id="EVENTUALLY"/>
<node id="THE ENTIRE REQUESTS DURATION"/>
<node id="THE RESERVED SPACE"/>
<node id="LARGE"/>
<node id="THE AVERAGE PERCENTAGE OF MEMORY WASTES IN OUR EXPERIMENTS IN FIG."/>
<node id="THE ACTUAL EFFECTIVE MEMORY IN PREVIOUS SYSTEMS"/>
<node id="THE ARCHITECTURE OF VLLM"/>
<node id="FIG."/>
<node id="SYSTEM DESIGN OF VLLM"/>
<node id="DISTRIBUTED SETTING"/>
<node id="THE GENERAL APPLICABILITY OF VLLM ON THEM"/>
<node id="COMPACTION 54"/>
<node id="A POTENTIAL SOLUTION TO FRAGMENTATION"/>
<node id="PERFORMING COMPACTION IN A PERFORMANCE-SENSITIVE LLM SERVING SYSTEM"/>
<node id="IMPRATICAL DUE TO THE MASSIVE KV CACHE"/>
<node id="THE PRE-ALLOCATED CHUNK SPACE FOR EACH REQUEST"/>
<node id="MEMORY SHARING SPECIFIC TO DECODING ALGORITHMS IN EXISTING MEMORY MANAGEMENT SYSTEMS"/>
<node id="A NEW ATTENTION ALGORITHM PAGE-DATTENTION"/>
<node id="AN LLM SERVING ENGINE VLLM"/>
<node id="THE CHALLENGES OUTLINED IN 3"/>
<node id="A CENTRALIZED SCHEDULER"/>
<node id="THE EXECUTION OF DISTRIBUTED GPU WORKERS"/>
<node id="THE PHYSICAL KV CACHE MEMORY ON THE GPU WORKERS"/>
<node id="THE INSTRUCTIONS SENT BY THE CENTRALIZED SCHEDULER"/>
<node id="EACH GPU WORKER"/>
<node id="THE SAME PHYSICAL BLOCK IDS"/>
<node id="A WORKER"/>
<node id="A PORTION OF THE KV CACHE FOR ITS CORRESPONDING ATTENTION HEADS"/>
<node id="GPU WORKERS"/>
<node id="BLOCK TABLE"/>
<node id="CONTROL MESSAGE"/>
<node id="THE PAGEDATTENTION ALGORITHM"/>
<node id="4.1"/>
<node id="AN EXAMPLE OF PAGEDATTENTION IN FIG."/>
<node id="EFFECTIVE MEMORY MANAGEMENT FOR VARIOUS DECODING METHODS"/>
<node id="VARIABLE LENGTH INPUT AND OUTPUT SEQUENCES"/>
<node id="THE KEY AND VALUE VECTORS FOR A FIXED NUMBER OF TOKENS"/>
<node id="THE FIXED NUMBER OF TOKENS"/>
<node id="KV 1 IN TRANSFORMER"/>
<node id="EACH TOKEN"/>
<node id="A SET OF KEY AND VALUE VECTORS ACROSS LAYERS AND ATTENTION HEADS WITHIN A LAYER"/>
<node id="ALL THE KEY AND VALUE VECTORS"/>
<node id="TOGETHER WITHIN A SINGLE KV BLOCK"/>
<node id="THE KEY AND VALUE VECTORS AT DIFFERENT HEADS AND LAYERS"/>
<node id="A SEPARATE BLOCK"/>
<node id="IN SEPARATE BLOCK TABLES"/>
<node id="THE TWO DESIGNS"/>
<node id="NO PERFORMANCE DIFFERENCE"/>
<node id="THE SECOND ONE"/>
<node id="EASY IMPLEMENTATION"/>
<node id="OUR FATHERS"/>
<node id="FOUR SCORE AND SEVEN KEY AND VALUE VECTORS"/>
<node id="THE EFFECT OF BLOCK SIZE IN 7.2"/>
<node id="KEY BLOCK"/>
<node id="((1)1"/>
<node id="THE ATTENTION COMPUTATION"/>
<node id="EQ."/>
<node id="4"/>
<node id="THE FOLLOWING BLOCK-WISE COMPUTATION"/>
<node id="ROW VECTOR"/>
<node id="ATTENTION SCORE ON -TH KV BLOCK"/>
<node id="THE KEY AND VALUE VECTORS"/>
<node id="THREE BLOCKS"/>
<node id="THE THREE BLOCKS"/>
<node id="THE PHYSICAL MEMORY"/>
<node id="THE KERNEL"/>
<node id="THE QUERY VECTOR OF THE QUERY TOKEN (FORTH) AND THE KEY VECTORS IN A BLOCK"/>
<node id="THE VALUE VECTORS IN A BLOCK"/>
<node id="THE FINAL ATTENTION OUTPUT"/>
<node id="OS"/>
<node id="MEMORY INTO FIXED-SIZED PAGES"/>
<node id="USER PROGRAMS LOGICAL PAGES TO PHYSICAL PAGES"/>
<node id="CONTIGUOUS LOGICAL PAGES"/>
<node id="NON-CONTIGUOUS PHYSICAL MEMORY PAGES"/>
<node id="USER PROGRAMS"/>
<node id="MEMORY AS THOUGH IT WERE CONTIGUOUS"/>
<node id="PHYSICAL MEMORY SPACE"/>
<node id="FULLY RESERVED IN ADVANCE"/>
<node id="PHYSICAL PAGES DYNAMICALLY"/>
<node id="THE LAST KV BLOCKS UNFILLED POSITIONS"/>
<node id="FUTURE GENERATIONS"/>
<node id="THE KV BLOCK MANAGER"/>
<node id="BLOCK TABLES"/>
<node id="THE MAPPING BETWEEN LOGICAL AND PHYSICAL KV BLOCKS OF EACH REQUEST"/>
<node id="EACH BLOCK TABLE ENTRY"/>
<node id="THE CORRESPONDING PHYSICAL BLOCKS OF A LOGICAL BLOCK"/>
<node id="THE NUMBER OF FILLED POSITIONS"/>
<node id="SEPARATING LOGICAL AND PHYSICAL KV BLOCKS"/>
<node id="VLLM TO DYNAMICALLY GROW THE KV CACHE MEMORY WITHOUT RESERVING IT FOR ALL POSITIONS IN ADVANCE"/>
<node id="MOST MEMORY WASTE IN EXISTING SYSTEMS"/>
<node id="ALL THE BLOCKS"/>
<node id="A NEW PHYSICAL BLOCK"/>
<node id="ALL PREVIOUS BLOCKS ARE FULL"/>
<node id="ALL THE MEMORY WASTES FOR A REQUEST WITHIN ONE BLOCK"/>
<node id="ALL THE MEMORY"/>
<node id="THE SHARING OF MOST OF THE SPACE USED TO STORE THE PROMPTS KV CACHE ACROSS MULTIPLE OUTPUT SAMPLES"/>
<node id="THE FINAL LOGICAL BLOCK"/>
<node id="A COPY-ON-WRITE MECHANISM"/>
<node id="FREQUENT MEMORY COPY OVERHEAD"/>
<node id="VLLMS PHYSICAL BLOCK SHARING"/>
<node id="THE COMPLEX MEMORY SHARING BETWEEN DIFFERENT SEQUENCES"/>
<node id="A COMMON MAPPING LAYER"/>
<node id="LOGICAL BLOCKS TO PHYSICAL BLOCKS"/>
<node id="INTRODUCING THE VLLMS TECHNIQUES"/>
<node id="THE PERFORMANCE"/>
<node id="THE DEGRADATION"/>
<node id="THE EXTRA OVERHEAD OF MEMORY INDIRECTION AND NON-CONTIGUOUS BLOCK MEMORY"/>
<node id="PAGEDATTENTION AND VLLM"/>
<node id="EXAMPLE"/>
<node id="IN FIG"/>
<node id="THE NEW TOKEN"/>
<node id="PHYSICAL BLOCKS 7 AND 1"/>
<node id="THE FIRST AUTOREGRESSIVE DECODING STEP"/>
<node id="VLLM GENERATING THE NEW TOKEN"/>
<node id="4.3"/>
<node id="HOW PAGEDATTENTION AND VLLM HANDLE BASIC DECODING ALGORITHMS"/>
<node id="BASIC DECODING ALGORITHMS"/>
<node id="GREEDY DECODING AND SAMPLING"/>
<node id="ONE USER PROMPT AS INPUT"/>
<node id="A SINGLE OUTPUT SEQUENCE"/>
<node id="THE NECESSARY KV BLOCKS"/>
<node id="THE KV CACHE GENERATED DURING PROMPT COMPUTATION"/>
<node id="THE PROMPT"/>
<node id="7 TOKENS"/>
<node id="THE FIRST 2 LOGICAL KV BLOCKS (0 AND 1) TO 2 PHYSICAL KV BLOCKS (7 AND 1, RESPECTIVELY)"/>
<node id="KV CACHE OF THE PROMPTS"/>
<node id="FIRST OUTPUT TOKEN"/>
<node id="CONVENTIONAL SELF-ATTENTION ALGORITHM"/>
<node id="THE KV CACHE OF THE FIRST 4 TOKENS IN LOGICAL BLOCK 0"/>
<node id="THE FOLLOWING 3 TOKENS IN LOGICAL BLOCK 1"/>
<node id="NEW PHYSICAL BLOCKS TO LOGICAL BLOCKS"/>
<node id="TOKENS AND THEIR KV CACHE"/>
<node id="MORE"/>
<node id="FREE PHYSICAL BLOCKS FOR NEW TOKENS"/>
<node id="A SET OF SEQUENCES TO EVICT"/>
<node id="THEIR KV CACHE TO THE CPU"/>
<node id="THE REMAINING SLOT"/>
<node id="THE SUBSEQUENT AUTOREGRESSIVE GENERATION PHASE"/>
<node id="ONE SLOT"/>
<node id="AVAILABLE IN THE LAST LOGICAL BLOCK"/>
<node id="THE NEWLY GENERATED KV CACHE"/>
<node id="THERE"/>
<node id="A SET OF CANDIDATE SEQUENCES FOR BATCHING"/>
<node id="THE PHYSICAL BLOCKS FOR THE NEWLY REQUIRED LOGICAL BLOCKS"/>
<node id="STORING MULTIPLE TOKENS WITHIN A KV BLOCK (BLOCK SIZE 1)"/>
<node id="THE PAGEDATTENTION KERNEL TO PROCESS THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL"/>
<node id="PROCESSING THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL"/>
<node id="THE HARDWARE UTILIZATION"/>
<node id="LATENCY"/>
<node id="A LARGER BLOCK SIZE"/>
<node id="THE MEMORY FOR TWO SEQUENCES"/>
<node id="THE LOGICAL BLOCKS OF THE TWO SEQUENCES"/>
<node id="DIFFERENT PHYSICAL BLOCKS"/>
<node id="THE SPACE RESERVED BY THE BLOCK ENGINE IN GPU WORKERS"/>
<node id="THE NEIGHBORING LOGICAL BLOCKS OF BOTH SEQUENCES"/>
<node id="CONTIGUOUS IN PHYSICAL GPU MEMORY"/>
<node id="THE SPACE OF PHYSICAL BLOCKS"/>
<node id="BOTH SEQUENCES"/>
<node id="AN LLM"/>
<node id="MULTIPLE SAMPLED OUTPUTS FOR A SINGLE INPUT PROMPT"/>
<node id="A FAVORITE OUTPUT FROM VARIOUS CANDIDATES"/>
<node id="GENERATES"/>
<node id="A SINGLE SEQUENCE"/>
<node id="THE MORE GENERAL CASE"/>
<node id="A REQUEST"/>
<node id="ONE REQUEST"/>
<node id="MULTIPLE SAMPLES"/>
<node id="THE SAME INPUT PROMPT"/>
<node id="THE KV CACHE OF THE PROMPT"/>
<node id="SHARED"/>
<node id="ALL PARALLEL SEQUENCES IN A REQUEST"/>
<node id="THE KV CACHE FOR THE PROMPT"/>
<node id="8"/>
<node id="AN EXAMPLE OF PARALLEL DECODING FOR TWO OUTPUTS"/>
<node id="OUTPUTS"/>
<node id="THE SAME PROMPT"/>
<node id="ONE COPY OF THE PROMPTS STATE AT THE PROMPT PHASE"/>
<node id="THE LOGICAL BLOCKS FOR THE PROMPTS OF BOTH SEQUENCES"/>
<node id="THE SAME PHYSICAL BLOCKS"/>
<node id="THE LOGICAL BLOCK 0 AND 1 OF BOTH SEQUENCES"/>
<node id="A SINGLE PHYSICAL BLOCK"/>
<node id="MULTIPLE LOGICAL BLOCKS"/>
<node id="A REFERENCE COUNT FOR EACH PHYSICAL BLOCK"/>
<node id="REFERENCE COUNTS FOR PHYSICAL BLOCK 7"/>
<node id="2"/>
<node id="THE TWO OUTPUTS"/>
<node id="DIFFERENT OUTPUT TOKENS"/>
<node id="SEPARATE STORAGE FOR KV CACHE"/>
<node id="AT THE GENERATION PHASE"/>
<node id="THE TWO OUTPUTS SAMPLE DIFFERENT OUTPUT TOKENS"/>
<node id="THE TWO OUTPUTS NEED SEPARATE STORAGE FOR KV CACHE"/>
<node id="SAMPLE A2"/>
<node id="PHYSICAL BLOCK 1"/>
<node id="REFERENCE COUNT"/>
<node id="1"/>
<node id="A2"/>
<node id="NEWLY GENERATED KV CACHE TO PHYSICAL BLOCK 1"/>
<node id="SHARING PHYSICAL BLOCKS ACROSS MULTIPLE SAMPLES"/>
<node id="GREATLY REDUCED"/>
<node id="ESPECIALLY FOR LONG INPUT PROMPTS"/>
<node id="TOP-MOST APPROPRIATE TRANSLATIONS OUTPUT BY THE LLM"/>
<node id="LLM TASKS LIKE MACHINE TRANSLATION 59"/>
<node id="MACHINE TRANSLATION 59"/>
<node id="EACH CANDIDATE SEQUENCE IN THE BEAM"/>
<node id="ALL POSSIBLE TOKENS"/>
<node id="THEIR RESPECTIVE PROBABILITIES USING THE LLM"/>
<node id="THE TOP-MOST PROBABLE SEQUENCES"/>
<node id="THE ALGORITHM"/>
<node id="THE BEAM WIDTH PARAMETER"/>
<node id="THE NUMBER OF TOP CANDIDATES RETAINED AT EVERY STEP"/>
<node id="SHARING NOT ONLY THE INITIAL PROMPT BLOCKS BUT ALSO OTHER BLOCKS ACROSS DIFFERENT CANDIDATES"/>
<node id="SHARING PATTERNS"/>
<node id="AS THE DECODING PROCESS ADVANCES"/>
<node id="THE PROCESS TREE IN THE OS CREATED BY COMPOUND FORKS"/>
<node id="THE KV BLOCKS"/>
<node id="A BEAM SEARCH EXAMPLE"/>
<node id="EACH CANDIDATE SEQUENCE"/>
<node id="4 FULL LOGICAL BLOCKS"/>
<node id="THE ITERATION ILLUSTRATED AS THE DOTTED LINE"/>
<node id="EACH CANDIDATE SEQUENCE USING 4 FULL LOGICAL BLOCKS"/>
<node id="ALL BEAM CANDIDATES"/>
<node id="THE FIRST BLOCK 0 (I.E., PROMPT)"/>
<node id="CANDIDATE 3"/>
<node id="OTHERS FROM THE SECOND BLOCK"/>
<node id="CANDIDATES 0-2"/>
<node id="THE FIRST 3 BLOCKS"/>
<node id="AT THE FOURTH BLOCK"/>
<node id="ALL CANDIDATES"/>
<node id="BLOCKS 0, 1, 3"/>
<node id="CANDIDATES 0 AND 1"/>
<node id="BLOCK 6"/>
<node id="ORIGINAL CANDIDATES 0 AND 3"/>
<node id="TOP CANDIDATES"/>
<node id="THEIR LOGICAL BLOCKS"/>
<node id="FREED"/>
<node id="REFERENCE COUNTS OF CORRESPONDING PHYSICAL BLOCKS"/>
<node id="REDUCED"/>
<node id="ALL PHYSICAL BLOCKS WHOSE REFERENCE COUNTS REACH 0"/>
<node id="2, 4, 5, 8"/>
<node id="NEW PHYSICAL BLOCKS (BLOCKS 9-12)"/>
<node id="THE NEW KV CACHE"/>
<node id="THE NEW CANDIDATES"/>
<node id="FREQUENT MEMORY COPIES OF THE KV CACHE ACROSS THE BEAM CANDIDATES"/>
<node id="A LARGE PORTION OF CANDIDATE 2S KV CACHE"/>
<node id="A LARGE PORTION OF CANDIDATE 2S KV CACHE TO CONTINUE GENERATION"/>
<node id="MOST BLOCKS OF DIFFERENT BEAM CANDIDATES"/>
<node id="MOST BLOCKS"/>
<node id="DIFFERENT BEAM CANDIDATES"/>
<node id="THE SAME STRATEGY"/>
<node id="PREFIX SHARING"/>
<node id="THE COPY-ON-WRITE MECHANISM"/>
<node id="THE NEWLY GENERATED TOKENS ARE WITHIN AN OLD SHARED BLOCK"/>
<node id="PARALLEL DECODING"/>
<node id="LLM USER"/>
<node id="DESCRIPTION OF THE TASK"/>
<node id="INSTRUCTIONS"/>
<node id="EXAMPLE INPUTS AND OUTPUTS"/>
<node id="SYSTEM PROMPT 36"/>
<node id="THE DESCRIPTION"/>
<node id="THE ACTUAL TASK INPUT"/>
<node id="THE DESCRIPTION AND THE ACTUAL TASK INPUT"/>
<node id="THE PROMPT OF THE REQUEST"/>
<node id="SHARED PROMPT EXAMPLE"/>
<node id="MACHINE TRANSLATION"/>
<node id="THE EXAMPLES"/>
<node id="5"/>
<node id="10"/>
<node id="AN EXAMPLE"/>
<node id="SHARED PREFIX"/>
<node id="PROMPT ENGINEERING"/>
<node id="TUNING SHARED PREFIX"/>
<node id="ACCURACY OF DOWNSTREAM TASKS"/>
<node id="MANY USER PROMPTS"/>
<node id="A PREFIX"/>
<node id="THE LLM SERVICE PROVIDER"/>
<node id="THE KV CACHE OF THE PREFIX"/>
<node id="STORING THE KV CACHE OF THE PREFIX"/>
<node id="THE REDUNDANT COMPUTATION SPENT ON THE PREFIX"/>
<node id="RESERVING A SET OF PHYSICAL BLOCKS FOR A SET OF PREDEFINED SHARED PREFIXES BY THE LLM SERVICE PROVIDER"/>
<node id="SHARED LIBRARY ACROSS PROCESSES"/>
<node id="A USER INPUT PROMPT WITH THE SHARED PREFIX"/>
<node id="ITS LOGICAL BLOCKS TO THE CACHED PHYSICAL BLOCKS"/>
<node id="THE LAST BLOCK"/>
<node id="THE PROMPT PHASE COMPUTATION"/>
<node id="THE USERS TASK INPUT"/>
<node id="THE DECODING METHODS DISCUSSED EARLIER"/>
<node id="DIVERSE MEMORY SHARING AND ACCESSING PATTERNS"/>
<node id="SIMULTANEOUS PROCESSING OF REQUESTS WITH DIFFERENT DECODING PREFERENCES"/>
<node id="SIMULTANEOUS PROCESSING OF REQUESTS WITH DIFFERENT DECODING PREFERENCES EFFICIENTLY"/>
<node id="THE LLM AND ITS EXECUTION KERNEL"/>
<node id="A LIST OF PHYSICAL BLOCK IDS FOR EACH SEQUENCE"/>
<node id="SHARING PATTERNS ACROSS SEQUENCES"/>
<node id="THIS APPROACH"/>
<node id="THE BATCHING OPPORTUNITIES FOR REQUESTS WITH DIFFERENT SAMPLING REQUIREMENTS"/>
<node id="THE SYSTEMS OVERALL THROUGHPUT"/>
<node id="A SUBSET OF REQUESTS"/>
<node id="REQUEST TRAFFIC"/>
<node id="THE SYSTEMS CAPACITY"/>
<node id="EARLIEST ARRIVED REQUESTS ARE SERVED FIRST"/>
<node id="LATEST REQUESTS ARE PREEMPTED FIRST"/>
<node id="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY"/>
<node id="ALL REQUESTS"/>
<node id="FAIRNESS"/>
<node id="STARVATION"/>
<node id="THE GPUS PHYSICAL BLOCKS TO STORE THE NEWLY GENERATED KV CACHE"/>
<node id="BLOCK SIZE"/>
<node id="TOO SMALL"/>
<node id="GPUS PARALLELISM FOR READING AND PROCESSING KV CACHE"/>
<node id="TWO CLASSIC QUESTIONS"/>
<node id="WHICH BLOCKS SHOULD IT EVICT"/>
<node id="EVICTED BLOCKS"/>
<node id="IF NEEDED AGAIN"/>
<node id="TWO TECHNIQUES"/>
<node id="SWAPPING"/>
<node id="EVICTION POLICIES"/>
<node id="HEURISTICS"/>
<node id="WHICH BLOCK WILL BE ACCESSED FURTHEST IN THE FUTURE"/>
<node id="THAT BLOCK"/>
<node id="ALL BLOCKS OF A SEQUENCE"/>
<node id="TOGETHER"/>
<node id="AN ALL-OR-NOTHING EVICTION POLICY"/>
<node id="ALL-OR-NOTHING EVICTION POLICY"/>
<node id="EITHER EVICT ALL OR NONE OF THE BLOCKS OF A SEQUENCE"/>
<node id="MULTIPLE SEQUENCES WITHIN ONE REQUEST"/>
<node id="A SEQUENCE GROUP"/>
<node id="THE SEQUENCES WITHIN ONE SEQUENCE GROUP"/>
<node id="TRUE"/>
<node id="DUE TO POTENTIAL MEMORY SHARING ACROSS THOSE SEQUENCES"/>
<node id="CLASSIC TECHNIQUE"/>
<node id="MOST VIRTUAL MEMORY IMPLEMENTATIONS"/>
<node id="EVICTED PAGES TO A SWAP SPACE ON THE DISK"/>
<node id="EVICTED BLOCKS TO THE CPU MEMORY"/>
<node id="GPU BLOCK ALLOCATOR"/>
<node id="CPU BLOCK ALLOCATOR"/>
<node id="PHYSICAL BLOCKS SWAPPED TO CPU RAM"/>
<node id="A SEQUENCE"/>
<node id="ITS BLOCKS"/>
<node id="NEW REQUESTS"/>
<node id="ALL PREEMPTED SEQUENCES ARE COMPLETED"/>
<node id="ONCE"/>
<node id="THE BLOCKS OF A PREEMPTED SEQUENCE"/>
<node id="CONTINUE THE PROCESSING OF THAT SEQUENCE"/>
<node id="NUMBER OF BLOCKS SWAPPED TO THE CPU RAM"/>
<node id="NUMBER OF TOTAL PHYSICAL BLOCKS IN THE GPU RAM"/>
<node id="SWAP SPACE ON THE CPU RAM"/>
<node id="GPU MEMORY ALLOCATED FOR THE KV CACHE"/>
<node id="WHEN THE PREEMPTED SEQUENCES ARE RESCHEDULED"/>
<node id="RECOMPUTATION LATENCY"/>
<node id="SIGNIFICANTLY LOWER THAN THE ORIGINAL LATENCY"/>
<node id="TOKENS GENERATED AT DECODING"/>
<node id="THE ORIGINAL USER PROMPT AS A NEW PROMPT"/>
<node id="THEIR KV CACHE AT ALL POSITIONS"/>
<node id="ONE PROMPT PHASE ITERATION"/>
<node id="PERFORMANCES OF SWAPPING AND RECOMPUTATION"/>
<node id="BANDWIDTH BETWEEN CPU RAM AND GPU MEMORY"/>
<node id="COMPUTATION POWER OF THE GPU"/>
<node id="THE SPEEDS OF SWAPPING AND RECOMPUTATION IN 7.3"/>
<node id="RECOMPUTATION"/>
<node id="RECOMPUTATION AND SWAPPING"/>
<node id="RECOVERY MECHANISMS"/>
<node id="MANY LLMS"/>
<node id="PARAMETER SIZES EXCEEDING THE CAPACITY OF A SINGLE GPU"/>
<node id="DISTRIBUTED SETTINGS"/>
<node id="MEGATRON-LM STYLE TENSOR MODEL PARALLELISM STRATEGY"/>
<node id="TRANSFORMERS 47"/>
<node id="THIS STRATEGY"/>
<node id="AN SPMD (SINGLE PROGRAM MULTIPLE DATA) EXECUTION SCHEDULE"/>
<node id="THE LINEAR LAYERS"/>
<node id="618 TABLE 1"/>
<node id="MODEL SIZES"/>
<node id="SERVER CONFIGURATIONS"/>
<node id="THE DETAILED MODEL SIZES AND SERVER CONFIGURATIONS"/>
<node id="TABLE 1"/>
<node id="KV CACHE SLOTS"/>
<node id="15.7K 9.7K 60.1K"/>
<node id="GPUS"/>
<node id="INTERMEDIATE RESULTS"/>
<node id="AN ALL-REDUCE OPERATION"/>
<node id="BLOCK-WISE MATRIX MULTIPLICATION"/>
<node id="ATTENTION OPERATOR"/>
<node id="ATTENTION HEAD DIMENSION"/>
<node id="EACH SPMD PROCESS"/>
<node id="A SUBSET OF ATTENTION HEADS IN MULTI-HEAD ATTENTION"/>
<node id="MODEL SHARD"/>
<node id="THE SAME SET OF INPUT TOKENS"/>
<node id="THE KV CACHE FOR THE SAME POSITIONS"/>
<node id="MODEL PARALLEL EXECUTION"/>
<node id="EACH MODEL SHARD PROCESSING THE SAME SET OF INPUT TOKENS"/>
<node id="A SINGLE KV CACHE MANAGER"/>
<node id="KV CACHE MANAGER"/>
<node id="THE CENTRALIZED SCHEDULER"/>
<node id="DIFFERENT GPU WORKERS"/>
<node id="THE MANAGER"/>
<node id="THE MAPPING FROM LOGICAL BLOCKS TO PHYSICAL BLOCKS"/>
<node id="THIS COMMON MAPPING"/>
<node id="GPU WORKERS TO EXECUTE THE MODEL WITH THE PHYSICAL BLOCKS PROVIDED BY THE SCHEDULER FOR EACH INPUT REQUEST"/>
<node id="THE SCHEDULER"/>
<node id="THE MESSAGE WITH INPUT TOKEN IDS FOR EACH REQUEST IN THE BATCH"/>
<node id="THE BLOCK TABLE FOR EACH REQUEST"/>
<node id="THIS CONTROL MESSAGE TO THE GPU WORKERS"/>
<node id="SAMPLED TOKENS OF THIS ITERATION BACK TO THE SCHEDULER"/>
<node id="THE INPUT TOKEN IDS"/>
<node id="ALL-REDUCE COMMUNICATION PRIMITIVE"/>
<node id="SYNCHRONIZATION"/>
<node id="COORDINATION OF THE SCHEDULER"/>
<node id="MEMORY MANAGEMENT"/>
<node id="ALL THE MEMORY MANAGEMENT INFORMATION AT THE BEGINNING OF EACH DECODING ITERATION"/>
<node id="STEP INPUTS"/>
<node id="AN END-TO-END SERVING SYSTEM"/>
<node id="A FASTAPI 15 FRONTEND"/>
<node id="A GPU-BASED INFERENCE ENGINE"/>
<node id="THE FRONTEND"/>
<node id="THE OPENAI API 34 INTERFACE"/>
<node id="USERS TO CUSTOMIZE SAMPLING PARAMETERS FOR EACH REQUEST"/>
<node id="SAMPLING PARAMETERS"/>
<node id="THE MAXIMUM SEQUENCE LENGTH"/>
<node id="THE BEAM WIDTH"/>
<node id="THE VLLM ENGINE"/>
<node id="8.5K LINES OF PYTHON"/>
<node id="2K LINES OF CCUDA CODE"/>
<node id="CONTROL-RELATED COMPONENTS"/>
<node id="THE BLOCK MANAGER"/>
<node id="CUSTOM CUDA KERNELS"/>
<node id="KEY OPERATIONS"/>
<node id="INPUT AND OUTPUT LENGTH DISTRIBUTIONS"/>
<node id="THE (A) SHAREGPT DATASET"/>
<node id="THE (B) ALPACA DATASET"/>
<node id="THE SHAREGPT DATASET"/>
<node id="THE ALPACA DATASET"/>
<node id="PYTORCH"/>
<node id="39"/>
<node id="TRANSFORMERS"/>
<node id="58"/>
<node id="NCCL 32 FOR TENSOR COMMUNICATION ACROSS THE DISTRIBUTED GPU WORKERS"/>
<node id="MEMORY ACCESS PATTERNS"/>
<node id="SEVERAL GPU KERNELS"/>
<node id="OPTIMIZING PAGEDATTENTION"/>
<node id="THE DYNAMIC BLOCK MAPPING IN PAGEDATTENTION"/>
<node id="THE PERFORMANCE OF THE GPU OPERATIONS INVOLVING THE STORED KV CACHE"/>
<node id="THE GPU OPERATIONS"/>
<node id="THE STORED KV CACHE"/>
<node id="BLOCK READWRITES AND ATTENTION"/>
<node id="FUSED RE-SHAPE"/>
<node id="BLOCK WRITE"/>
<node id="FUSED BLOCK COPY"/>
<node id="3"/>
<node id="NEW KV CACHE"/>
<node id="A MEMORY LAYOUT OPTIMIZED FOR BLOCK READ"/>
<node id="POSITIONS SPECIFIED BY THE BLOCK TABLE"/>
<node id="THEM INTO A SINGLE KERNEL"/>
<node id="FUSING THEM INTO A SINGLE KERNEL"/>
<node id="KERNEL LAUNCH OVERHEADS"/>
<node id="A KERNEL"/>
<node id="THE COPY OPERATIONS FOR DIFFERENT BLOCKS"/>
<node id="A SINGLE KERNEL LAUNCH"/>
<node id="THE ATTENTION KERNEL IN FASTERTRANSFORMER 31"/>
<node id="KV CACHE ACCORDING TO THE BLOCK TABLE"/>
<node id="ATTENTION OPERATIONS ON THE FLY"/>
<node id="A GPU WARP TO READ EACH BLOCK"/>
<node id="SUPPORT FOR VARIABLE SEQUENCE LENGTHS WITHIN A REQUEST BATCH"/>
<node id="BLOCK COPY OPERATIONS"/>
<node id="DISCONTINUOUS BLOCKS"/>
<node id="THE CUDAMEMCPYASYNC API"/>
<node id="THE FORK METHOD"/>
<node id="A NEW SEQUENCE FROM AN EXISTING ONE"/>
<node id="THE APPEND METHOD"/>
<node id="A NEW TOKEN TO THE SEQUENCE"/>
<node id="THE FREE METHOD"/>
<node id="THE SEQUENCE"/>
<node id="MULTIPLE OUTPUT SEQUENCES"/>
<node id="THE SINGLE INPUT SEQUENCE"/>
<node id="FUTURE DECODING ALGORITHMS"/>
<node id="COMBINING THESE METHODS"/>
<node id="PARALLEL GENERATION"/>
<node id="MAX"/>
<node id="POW2"/>
<node id="ORACLE"/>
<node id="NORMALIZED LATENCY"/>
<node id="STOKEN"/>
<node id="REQUEST RATE"/>
<node id="REQS"/>
<node id="FIGURE"/>
<node id="14"/>
<node id="FIGURE 17"/>
<node id="REQUEST RATE (REQS)"/>
<node id="NORMALIZED LATENCY (STOKEN)"/>
<node id="SINGLE SEQUENCE GENERATION"/>
<node id="OPT MODELS"/>
<node id="SHAREGPT DATASET"/>
<node id="ALPACA DATASET"/>
<node id="OPT MODEL"/>
<node id="BATCHED REQUESTS"/>
<node id="0 5 10 15 20 25 30 35 FOR SHAREGPT"/>
<node id="0 25 50 75 100 125 150 FOR ALPACA"/>
<node id="7.00 FOR SHAREGPT"/>
<node id="9.81 FOR SHAREGPT"/>
<node id="13.62 FOR SHAREGPT"/>
<node id="30.42 FOR SHAREGPT"/>
<node id="43.24 FOR ALPACA"/>
<node id="72.75 FOR ALPACA"/>
<node id="132.44 FOR ALPACA"/>
<node id="AVERAGE NUMBER OF BATCHED REQUESTS"/>
<node id="WHEN SERVING OPT-13B FOR THE SHAREGPT (2 REQSS) AND ALPACA (30 REQSS) TRACES"/>
<node id="6.1"/>
<node id="EXPERIMENTAL SETUP MODEL AND SERVER CONFIGURATIONS"/>
<node id="OPT 62 MODELS WITH 13B PARAMETERS"/>
<node id="OPT 62 MODELS WITH 66B PARAMETERS"/>
<node id="OPT 62 MODELS WITH 175B PARAMETERS"/>
<node id="LLAMA 52 WITH 13B PARAMETERS"/>
<node id="OPT 62 MODELS"/>
<node id="13B PARAMETERS"/>
<node id="66B PARAMETERS"/>
<node id="175B PARAMETERS"/>
<node id="13B"/>
<node id="POPULAR SIZES FOR LLMS"/>
<node id="66B"/>
<node id="13B AND 66B"/>
<node id="LLM LEADERBOARD 38"/>
<node id="175B"/>
<node id="THE SIZE OF THE FAMOUS GPT-3 5 MODEL"/>
<node id="A2 INSTANCES WITH NVIDIA A100 GPUS ON GOOGLE CLOUD PLATFORM"/>
<node id="WORKLOADS BASED ON SHAREGPT 51 AND ALPACA 50 DATASETS"/>
<node id="SHAREGPT 51 AND ALPACA 50 DATASETS"/>
<node id="INPUT AND OUTPUT TEXTS OF REAL LLM SERVICES"/>
<node id="A COLLECTION OF USER-SHARED CONVERSATIONS WITH CHATGPT 35"/>
<node id="THE CHATTING HISTORY AND USER QUERY USING THE SHAREGPT DATASET"/>
<node id="THE DATASETS"/>
<node id="THEIR INPUT AND OUTPUT LENGTHS"/>
<node id="CLIENT REQUESTS"/>
<node id="REQUEST ARRIVAL TIMES"/>
<node id="POISSON DISTRIBUTION"/>
<node id="DIFFERENT REQUEST RATES"/>
<node id="THESE DATASETS"/>
<node id="TIMESTAMPS"/>
<node id="BASELINE 1"/>
<node id="A DISTRIBUTED INFERENCE ENGINE"/>
<node id="HIGHLY OPTIMIZED FOR LATENCY"/>
<node id="ITS OWN SCHEDULER"/>
<node id="A CUSTOM SCHEDULER"/>
<node id="CUSTOM SCHEDULER"/>
<node id="A DYNAMIC BATCHING MECHANISM"/>
<node id="DYNAMIC BATCHING MECHANISM"/>
<node id="EXISTING SERVING SYSTEMS SUCH AS TRITON 30"/>
<node id="A MAXIMUM BATCH SIZE AS LARGE AS POSSIBLE FOR EACH EXPERIMENT"/>
<node id="MAXIMUM BATCH SIZE"/>
<node id="EARLIEST ARRIVED REQUESTS"/>
<node id="THE BATCH TO FASTERTRANS-FORMER FOR PROCESSING"/>
<node id="BASELINE 2"/>
<node id="THE THREE ORCA BASELINES"/>
<node id="SIMILARLY"/>
<node id="STATE-OF-THE-ART LLM SERVING SYSTEM"/>
<node id="PUBLICLY AVAILABLE FOR USE"/>
<node id="OUR OWN VERSION OF ORCA"/>
<node id="BUDDY ALLOCATION ALGORITHM"/>
<node id="MEMORY ADDRESS TO STORE KV CACHE"/>
<node id="THE SYSTEM HAS THE KNOWLEDGE OF THE LENGTHS OF THE OUTPUTS THAT WILL BE ACTUALLY GENERATED FOR THE REQUESTS"/>
<node id="THE UPPER-BOUND PERFORMANCE OF ORCA"/>
<node id="INFEASIBLE TO ACHIEVE IN PRACTICE"/>
<node id="THE SPACE FOR OUTPUTS"/>
<node id="AT MOST 2"/>
<node id="TRUE OUTPUT LENGTH"/>
<node id="25"/>
<node id="NORMALIZED LATENCY OF THE SYSTEMS"/>
<node id="THE MEAN OF EVERY REQUESTS END-TO-END LATENCY DIVIDED BY ITS OUTPUT LENGTH"/>
<node id="USING THE WORKLOADS WITH DIFFERENT REQUEST RATES"/>
<node id="SPECIFICALLY"/>
<node id="A HIGH-THROUGHPUT SERVING SYSTEM"/>
<node id="LOW NORMALIZED LATENCY"/>
<node id="HIGH REQUEST RATES"/>
<node id="THE SYSTEMS WITH 1-HOUR TRACES"/>
<node id="15-MINUTE TRACES FOR THE OPT-175B MODEL"/>
<node id="15-MINUTE TRACES"/>
<node id="AS AN EXCEPTION"/>
<node id="DUE TO THE COST LIMIT"/>
<node id="PARALLEL GENERATION AND BEAM SEARCH"/>
<node id="OPT-13B"/>
<node id="THE PERFORMANCE OF VLLM WITH BASIC SAMPLING"/>
<node id="BASIC SAMPLING"/>
<node id="ONE SAMPLE PER REQUEST"/>
<node id="THREE MODELS AND TWO DATASETS"/>
<node id="12"/>
<node id="THE RESULTS ON THE SHAREGPT DATASET"/>
<node id="THE RESULTS ON THE ALPACA DATASET"/>
<node id="A SIMILAR TREND TO THE SHAREGPT DATASET"/>
<node id="THE CURVES"/>
<node id="AS THE REQUEST RATE INCREASES, THE LATENCY INITIALLY INCREASES AT A GRADUAL PACE BUT THEN SUDDENLY EXPLODES"/>
<node id="CAPACITY OF THE SERVING SYSTEM"/>
<node id="QUEUE LENGTH"/>
<node id="INFINITELY"/>
<node id="LATENCY OF THE REQUESTS"/>
<node id="SO"/>
<node id="1.72.7 HIGHER REQUEST RATES COMPARED TO ORCA (ORACLE)"/>
<node id="2.78 HIGHER REQUEST RATES COMPARED TO ORCA (MAX)"/>
<node id="SIMILAR LATENCIES"/>
<node id="13A, FOR OPT-13B VLLM"/>
<node id="2.2 MORE REQUESTS AT THE SAME TIME THAN ORCA (ORACLE)"/>
<node id="4.3 MORE REQUESTS THAN ORCA (MAX)"/>
<node id="3.58 HIGHER THROUGHPUT THAN ORCA (ORACLE)"/>
<node id="VLLMS PAGEDATTENTION"/>
<node id="MEMORY USAGE EFFICIENTLY"/>
<node id="BATCHING MORE REQUESTS THAN ORCA"/>
<node id="ONE EXCEPTION"/>
<node id="VLLMS"/>
<node id="ADVANTAGE OF VLLMS OVER ORCA (ORACLE) AND ORCA (POW2)"/>
<node id="LESS PRONOUNCED"/>
<node id="MODEL AND SERVER CONFIGURATION FOR OPT-175B"/>
<node id="LARGE GPU MEMORY SPACE AVAILABLE TO STORE KV CACHE"/>
<node id="SHORT SEQUENCES"/>
<node id="ORCA (ORACLE) AND ORCA (POW2)"/>
<node id="A LARGE NUMBER OF REQUESTS"/>
<node id="REQUESTS DESPITE THE INEFFICIENCIES IN THEIR MEMORY MANAGEMENT"/>
<node id="OUTPUT SEQUENCES"/>
<node id="2 4 6"/>
<node id="0 4 8 12"/>
<node id="MEMORY SAVING"/>
<node id="6.09"/>
<node id="8.53"/>
<node id="9.79"/>
<node id="BEAM WIDTH"/>
<node id="0 20 40 60"/>
<node id="37.56"/>
<node id="53.13"/>
<node id="55.16"/>
<node id="FIGURE 15"/>
<node id="AVERAGE AMOUNT OF MEMORY SAVING"/>
<node id="SHARING KV BLOCKS"/>
<node id="SERVING OPT-13B FOR THE ALPACA TRACE"/>
<node id="THE EFFECTIVENESS OF MEMORY SHARING IN PAGE-DATTENTION"/>
<node id="PARALLEL SAMPLING AND BEAM SEARCH"/>
<node id="TWO POPULAR SAMPLING METHODS"/>
<node id="6.1 - 9.8 MEMORY SAVING ON PARALLEL SAMPLING"/>
<node id="37.6 - 55.2 MEMORY SAVING ON BEAM SEARCH"/>
<node id="16.2 - 30.5"/>
<node id="44.3 - 66.3"/>
<node id="MORE IMPROVEMENT OVER THE ORCA BASELINES"/>
<node id="2 HIGHER REQUEST RATES"/>
<node id="RESULTS FOR BEAM SEARCH WITH DIFFERENT BEAM WIDTHS"/>
<node id="15"/>
<node id="THE AMOUNT OF MEMORY SAVING"/>
<node id="THE NUMBER OF BLOCKS WE SAVED BY SHARING DIVIDED BY THE NUMBER OF TOTAL BLOCKS WITHOUT SHARING"/>
<node id="INPUT PROMPTS"/>
<node id="A COMMON PREFIX"/>
<node id="THE PREFIX"/>
<node id="(A) 1 EXAMPLE WITH 80 TOKENS"/>
<node id="(B) 5 EXAMPLES WITH 341 TOKENS"/>
<node id="PERFORMANCE"/>
<node id="CHATBOT WORKLOAD"/>
<node id="LLAMA-13B 52"/>
<node id="MULTILINGUAL"/>
<node id="WMT16 4 ENGLISH-TO-GERMAN TRANSLATION DATASET"/>
<node id="TWO PREFIXES"/>
<node id="AN INSTRUCTION AND A FEW TRANSLATION EXAMPLES"/>
<node id="THE FIRST PREFIX"/>
<node id="A SINGLE EXAMPLE (I.E., ONE-SHOT)"/>
<node id="THE OTHER PREFIX"/>
<node id="5 EXAMPLES (I.E., FEW-SHOT)"/>
<node id="CHATBOT"/>
<node id="ONE OF THE MOST IMPORTANT APPLICATIONS OF LLMS"/>
<node id="THE MODEL GENERATE A RESPONSE"/>
<node id="A RESPONSE"/>
<node id="CONCATENATING THE CHATTING HISTORY AND THE LAST USER QUERY INTO A PROMPT"/>
<node id="THE CONTEXT LENGTH OF THE OPT-13B MODEL"/>
<node id="LIMITED"/>
<node id="THE PROMPT TO THE LAST 1024 TOKENS"/>
<node id="THE MODEL GENERATE AT MOST 1024 TOKENS"/>
<node id="THE KV CACHE BETWEEN DIFFERENT CONVERSATION ROUNDS"/>
<node id="DOING THIS"/>
<node id="THE SPACE FOR OTHER REQUESTS BETWEEN THE CONVERSATION ROUNDS"/>
<node id="MANY LONG CONVERSATIONS"/>
<node id="INPUT PROMPTS FOR MOST REQUESTS"/>
<node id="1024 TOKENS"/>
<node id="ORCA BASELINES"/>
<node id="SPACE FOR 1024 TOKENS FOR THE REQUEST OUTPUTS"/>
<node id="ORCA BASELINES TO RESERVE THE SPACE FOR 1024 TOKENS FOR THE REQUEST OUTPUTS"/>
<node id="REGARDLESS OF HOW THEY PREDICT THE OUTPUT LENGTHS"/>
<node id="64 128 256 CONTEXT LENGTH"/>
<node id="1 2 4 8 16 32 64 128 256"/>
<node id="0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5"/>
<node id="SHAREGPT ALPACA (B)"/>
<node id="END-TO-END LATENCY"/>
<node id="DIFFERENT BLOCK SIZES"/>
<node id="VARIOUS ASPECTS OF VLLM"/>
<node id="THE DESIGN CHOICES"/>
<node id="ABLATION EXPERIMENTS"/>
<node id="THE PROBLEM OF MEMORY FRAGMENTATION AND RESERVATION"/>
<node id="OUR GPU KERNELS (5)"/>
<node id="EXTRA OVERHEADS OF ACCESSING THE BLOCK TABLE"/>
<node id="EXECUTING EXTRA BRANCHES"/>
<node id="HANDLING VARIABLE SEQUENCE LENGTHS"/>
<node id="18A"/>
<node id="2026 HIGHER ATTENTION KERNEL LATENCY"/>
<node id="HIGHLY-OPTIMIZED FASTERTRANSFORMER IMPLEMENTATION"/>
<node id="THE OVERHEAD"/>
<node id="SMALL"/>
<node id="THE ATTENTION OPERATOR"/>
<node id="THE OTHER OPERATORS IN THE MODEL"/>
<node id="LINEAR"/>
<node id="VLLM SIGNIFICANTLY OUTPERFORM FASTERTRANSFORMER IN END-TO-END PERFORMANCE"/>
<node id="CHOICE OF BLOCK SIZE"/>
<node id="SUBSTANTIAL IMPACT ON THE PERFORMANCE OF VLLM"/>
<node id="THE PERFORMANCE OF VLLM WITH DIFFERENT BLOCK SIZES"/>
<node id="THE SHAREGPT AND ALPACA TRACES WITH BASIC SAMPLING"/>
<node id="THE PERFORMANCE OF VLLM"/>
<node id="FIXED REQUEST RATES"/>
<node id="BLOCK SIZES FROM 16 TO 128"/>
<node id="THE BEST PERFORMANCE"/>
<node id="BLOCK SIZE 16 AND 32"/>
<node id="WELL IN THE ALPACA TRACE"/>
<node id="LARGER BLOCK SIZES"/>
<node id="SEQUENCES"/>
<node id="SHORTER THAN THE BLOCK SIZES"/>
<node id="BLOCK SIZE 16"/>
<node id="EFFICIENTLY UTILIZE THE GPU"/>
<node id="AVOID SIGNIFICANT INTERNAL FRAGMENTATION IN MOST WORKLOADS"/>
<node id="DEFAULT BLOCK SIZE AS 16"/>
<node id="TIME"/>
<node id="MS"/>
<node id="MICROBENCHMARK"/>
<node id="RECOMPUTE SWAP IN SWAP OUT SWAP IN OUT"/>
<node id="19"/>
<node id="END-TO-END PERFORMANCE"/>
<node id="(B)"/>
<node id="(A)"/>
<node id="OVERHEAD"/>
<node id="THE OVERHEAD OF RECOMPUTATION"/>
<node id="CONSTANT ACROSS DIFFERENT BLOCK SIZES"/>
<node id="BLOCK SIZE IS SMALL"/>
<node id="BLOCK SIZE IS LARGE"/>
<node id="RECOMPUTATION OVERHEAD"/>
<node id="20 OF SWAPPINGS LATENCY"/>
<node id="OPT-13B WITH THE SHAREGPT TRACES AT THE SAME REQUEST RATE"/>
<node id="THEIR END-TO-END PERFORMANCE"/>
<node id="THEIR OVERHEADS"/>
<node id="EXCESSIVE OVERHEAD WITH SMALL BLOCK SIZES"/>
<node id="SMALL BLOCK SIZES"/>
<node id="NUMEROUS SMALL DATA TRANSFERS BETWEEN CPU AND GPU"/>
<node id="EFFECTIVE PCIE BANDWIDTH"/>
<node id="THE TWO METHODS"/>
<node id="COMPARABLE END-TO-END PERFORMANCE FOR MEDIUM BLOCK SIZES FROM 16 TO 64"/>
<node id="VIRTUAL MEMORY AND PAGING TECHNIQUE"/>
<node id="OTHER GPU WORKLOADS"/>
<node id="THE OVERHEAD OF MEMORY INDIRECTION IN PAGING"/>
<node id="THE GPU KERNELS FOR MEMORY ACCESS OPERATIONS WITH THOSE FOR OTHER OPERATIONS SUCH AS ATTENTION"/>
<node id="TENSOR SHAPES"/>
<node id="TYPICALLY STATIC"/>
<node id="MEMORY ALLOCATION"/>
<node id="OPTIMIZED AHEAD OF TIME"/>
<node id="AN INCREASE IN MEMORY EFFICIENCY"/>
<node id="ANY PERFORMANCE IMPROVEMENT"/>
<node id="PRIMARILY COMPUTE-BOUND"/>
<node id="VLLMS TECHNIQUES BEING APPLIED TO OTHER WORKLOADS WITH SIMILAR PROPERTIES TO LLM SERVING"/>
<node id="LLM-SPECIFIC OPTIMIZATIONS"/>
<node id="VIRTUAL MEMORY AND PAGING"/>
<node id="THE APPLICATION-SPECIFIC SEMANTICS"/>
<node id="VLLMS ALL-OR-NOTHING SWAP-OUT POLICY"/>
<node id="ONE EXAMPLE"/>
<node id="THE FACT THAT PROCESSING A REQUEST REQUIRES ALL OF ITS CORRESPONDING TOKEN STATES TO BE STORED IN GPU MEMORY"/>
<node id="RECOMPUTATION METHOD"/>
<node id="ANOTHER EXAMPLE"/>
<node id="FEASIBLE IN OS"/>
<node id="RELATED WORK"/>
<node id="GENERAL MODEL SERVING SYSTEMS"/>
<node id="MODEL SERVING"/>
<node id="AN ACTIVE AREA OF RESEARCH"/>
<node id="NUMEROUS SYSTEMS"/>
<node id="DIVERSE ASPECTS OF DEEP LEARNING MODEL DEPLOYMENT"/>
<node id="CLIPPER 11"/>
<node id="EARLIER GENERAL MODEL SERVING SYSTEMS"/>
<node id="TENSORFLOW SERVING 33"/>
<node id="NEXUS 45"/>
<node id="INFERLINE 10"/>
<node id="CLOCKWORK 20"/>
<node id="BATCH-ING"/>
<node id="SERVING SINGLE OR MULTIPLE MODELS"/>
<node id="CACHING"/>
<node id="PLACEMENT"/>
<node id="DVABATCH 12"/>
<node id="MULTI-ENTRY MULTI-EXIT BATCHING"/>
<node id="REEF 21"/>
<node id="PREEMPTION FOR SERVING"/>
<node id="SHEP-HERD 61"/>
<node id="ALPASERVE 28"/>
<node id="MODEL PARALLELISM"/>
<node id="STATISTICAL MULTIPLEXING"/>
<node id="ALPASERVE"/>
<node id="STATISTICAL MULTIPLEXING WITH MODEL PARALLELISM FOR DEEP LEARNING SERVING"/>
<node id="GENERAL SYSTEMS"/>
<node id="AUTO-REGRESSIVE PROPERTY AND TOKEN STATE OF LLM INFERENCE"/>
<node id="SPECIALIZED SERVING SYSTEMS"/>
<node id="NUMEROUS SPECIALIZED SERVING SYSTEMS"/>
<node id="THE TRANSFORMER ARCHITECTURE"/>
<node id="THESE SYSTEMS"/>
<node id="GPU KERNEL OPTIMIZATIONS"/>
<node id="ADVANCED BATCHING MECHANISMS"/>
<node id="PARAMETER SHARING"/>
<node id="EFFICIENT SERVING"/>
<node id="MOST RELEVANT TO OUR APPROACH"/>
<node id="FINE-GRAINED SCHEDULING AND INTERLEAVING OF THE REQUESTS LIKE IN ORCA"/>
<node id="MEMORY MANAGEMENT MORE CHALLENGING"/>
<node id="TECHNIQUES PROPOSED IN VLLM"/>
<node id="MORE CRUCIAL"/>
<node id="THE WIDENING GAP BETWEEN THE COMPUTE CAPABILITY AND MEMORY CAPACITY OF ACCELERATORS"/>
<node id="MEMORY TO BECOME A BOTTLENECK FOR BOTH TRAINING AND INFERENCE"/>
<node id="SWAPPING 23, 42, 55, RECOMPUTATION 7, 24 AND THEIR COMBINATION 40"/>
<node id="REDUCE THE PEAK MEMORY OF TRAINING"/>
<node id="FLEXGEN 46 STUDIES"/>
<node id="HOW TO SWAP WEIGHTS AND TOKEN STATES FOR LLM INFERENCE WITH 623 LIMITED GPU MEMORY"/>
<node id="OLLA 48"/>
<node id="THE LIFETIME AND LOCATION OF TENSORS"/>
<node id="FRAGMENTATION"/>
<node id="FINE-GRAINED BLOCK-LEVEL MANAGEMENT"/>
<node id="ONLINE SERVING"/>
<node id="FLASHAT-TENTION 13"/>
<node id="TILING AND KERNEL OPTIMIZATIONS"/>
<node id="THE PEAK MEMORY OF ATTENTION COMPUTATION"/>
<node id="IO COSTS"/>
<node id="A NEW IDEA OF BLOCK-LEVEL MEMORY MANAGEMENT"/>
<node id="XIAOXUAN LIU"/>
<node id="ZHIFENG CHEN"/>
<node id="YAN-PING HUANG"/>
<node id="ANONYMOUS SOSP REVIEWERS"/>
<node id="OUR SHEPHERD"/>
<node id="LIDONG ZHOU"/>
<node id="XIAOXUAN LIU, ZHIFENG CHEN, YAN-PING HUANG, ANONYMOUS SOSP REVIEWERS, AND LIDONG ZHOU"/>
<node id="INSIGHTFUL FEEDBACK"/>
<node id="THIS RESEARCH"/>
<node id="GIFTS FROM ANDREESSEN HOROWITZ"/>
<node id="GIFTS FROM ANYSCALE"/>
<node id="GIFTS FROM ASTRONOMER"/>
<node id="GIFTS FROM GOOGLE"/>
<node id="GIFTS FROM IBM"/>
<node id="GIFTS FROM INTEL"/>
<node id="GIFTS FROM LACEWORK"/>
<node id="GIFTS FROM MICROSOFT"/>
<node id="GIFTS FROM MOHAMED BIN ZAYED UNIVERSITY OF ARTIFICIAL INTELLIGENCE"/>
<node id="GIFTS FROM SAMSUNG SDS"/>
<node id="GIFTS FROM UBER"/>
<node id="GIFTS FROM VMWARE"/>
<node id="REFERENCES"/>
<node id="REZA YAZDANI AMINABADI"/>
<node id="SAMYAM RAJBHANDARI"/>
<node id="MINJIA ZHANG"/>
<node id="AMMAR AHMAD AWAN"/>
<node id="CHENG LI"/>
<node id="DU LI"/>
<node id="ELTON ZHENG"/>
<node id="JEFF RASLEY"/>
<node id="SHADEN SMITH"/>
<node id="OLATUNJI RUWASE"/>
<node id="DEEPSPEED INFERENCE"/>
<node id="EFFICIENT INFERENCE OF TRANSFORMER MODELS AT UNPRECEDENTED SCALE"/>
<node id="ARXIV PREPRINT"/>
<node id="ARXIV:2207.00032"/>
<node id="2022"/>
<node id="ARXIV:1607.06450"/>
<node id="2016"/>
<node id="ARXIV:2107.03374"/>
<node id="2021"/>
<node id="ARXIV:1604.06174"/>
<node id="ARXIV:2204.02311"/>
<node id="ARXIV:2104.08691"/>
<node id="ARXIV:2101.00190"/>
<node id="ARXIV:2302.11665"/>
<node id="ARXIV:1712.06139"/>
<node id="2017"/>
<node id="ARXIV:2211.05102"/>
<node id="ARXIV:2303.06865"/>
<node id="ARXIV:1909.08053"/>
<node id="2019"/>
<node id="ARXIV:2302.13971"/>
<node id="ARXIV:2212.10560"/>
<node id="ARXIV:1609.08144"/>
<node id="ARXIV:2205.01068"/>
<node id="JIMMY LEI BA"/>
<node id="AUTHOR"/>
<node id="JAMIE RYAN KIROS"/>
<node id="GEOFFREY E HINTON"/>
<node id="LAYER NORMALIZATION"/>
<node id="A TECHNIQUE"/>
<node id="YOSHUA BENGIO"/>
<node id="RJEAN DUCHARME"/>
<node id="PASCAL VINCENT"/>
<node id="NEURAL PROBABILISTIC LANGUAGE MODEL"/>
<node id="A MODEL"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS"/>
<node id="13"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13"/>
<node id="2000"/>
<node id="4 OND REJ BOJAR"/>
<node id="PERSON"/>
<node id="RAJEN CHATTERJEE"/>
<node id="CHRISTIAN FEDERMANN"/>
<node id="YVETTE GRAHAM"/>
<node id="BARRY HADDOW"/>
<node id="MATTHIAS HUCK"/>
<node id="ANTONIO JIMENO YEPES"/>
<node id="PHILIPP KOEHN"/>
<node id="VARVARA LOGACHEVA"/>
<node id="CHRISTOF MONZ"/>
<node id="MATTEO NEGRI"/>
<node id="AURELIE NEVEOL"/>
<node id="MARIANA NEVES"/>
<node id="MARTIN POPEL"/>
<node id="MATT POST"/>
<node id="RAPHAEL RUBINO"/>
<node id="CAROLINA SCARTON"/>
<node id="LUCIA SPECIA"/>
<node id="MARCO TURCHI"/>
<node id="KARIN VERSPOOR"/>
<node id="MARCOS ZAMPIERI"/>
<node id="2016 CONFERENCE"/>
<node id="PROCEEDINGS"/>
<node id="THE FIRST CONFERENCE ON MACHINE TRANSLATION"/>
<node id="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS"/>
<node id="BERLIN, GERMANY"/>
<node id="131198"/>
<node id="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301"/>
<node id="TOM BROWN"/>
<node id="BENJAMIN MANN"/>
<node id="NICK RYDER"/>
<node id="MELANIE SUBBIAH"/>
<node id="JARED D KAPLAN"/>
<node id="PRAFULLA DHARIWAL"/>
<node id="ARVIND NEELAKANTAN"/>
<node id="PRANAV SHYAM"/>
<node id="GIRISH SASTRY"/>
<node id="AMANDA ASKELL"/>
<node id="LANGUAGE MODELS"/>
<node id="FEW-SHOT LEARNERS"/>
<node id="33"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 33"/>
<node id="2020"/>
<node id="1877-1901"/>
<node id="35"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35"/>
<node id="16344-16359"/>
<node id="27TH EDITION"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27"/>
<node id="2014"/>
<node id="30TH EDITION"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30"/>
<node id="6 MARK CHEN"/>
<node id="JERRY TWOREK"/>
<node id="HEEWOO JUN"/>
<node id="QIMING YUAN"/>
<node id="HENRIQUE PONDE DE OLIVEIRA PINTO"/>
<node id="JARED KAPLAN"/>
<node id="HARRI EDWARDS"/>
<node id="YURI BURDA"/>
<node id="NICHOLAS JOSEPH"/>
<node id="GREG BROCKMAN"/>
<node id="LARGE LANGUAGE MODELS"/>
<node id="CODE"/>
<node id="7 TIANQI CHEN"/>
<node id="BING XU"/>
<node id="CHIYUAN ZHANG"/>
<node id="CARLOS GUESTRIN"/>
<node id="14 JIARUI FANG"/>
<node id="YANG YU"/>
<node id="CHENGDUO ZHAO"/>
<node id="JIE ZHOU"/>
<node id="21 MINGCONG HAN"/>
<node id="HANZE ZHANG"/>
<node id="RONG CHEN"/>
<node id="HAIBO CHEN"/>
<node id="22"/>
<node id="KAIMING HE"/>
<node id="XIANGYU ZHANG"/>
<node id="SHAOQING REN"/>
<node id="JIAN SUN"/>
<node id="23"/>
<node id="CHIEN-CHIN HUANG, GU JIN, AND JINYANG LI"/>
<node id="54 JING WANG"/>
<node id="YOUYOU LU"/>
<node id="QING WANG"/>
<node id="MINHUI XIE"/>
<node id="KEJI HUANG"/>
<node id="JIWU SHU"/>
<node id="56"/>
<node id="XIAOHUI WANG"/>
<node id="YING XIONG"/>
<node id="YANG WEI"/>
<node id="MINGXUAN WANG"/>
<node id="LEI LI"/>
<node id="64"/>
<node id="ZHE ZHOU"/>
<node id="XUECHAO WEI"/>
<node id="JIEJING ZHANG"/>
<node id="GUANGYU SUN"/>
<node id="TRAINING DEEP NETS"/>
<node id="SUBLINEAR MEMORY COST"/>
<node id="8 WEI-LIN CHIANG"/>
<node id="TEXT"/>
<node id="ZHUOHAN LI"/>
<node id="ZI LIN"/>
<node id="YING SHENG"/>
<node id="ZHANGHAO WU"/>
<node id="HAO ZHANG"/>
<node id="LIANMIN ZHENG"/>
<node id="SIYUAN ZHUANG"/>
<node id="YONGHAO ZHUANG"/>
<node id="JOSEPH E. GONZALEZ"/>
<node id="ION STOICA"/>
<node id="ERIC P. XING"/>
<node id="12 WEIHAO CUI"/>
<node id="HAN ZHAO"/>
<node id="QUAN CHEN"/>
<node id="HAO WEI"/>
<node id="ZIRUI LI"/>
<node id="DEZE ZENG"/>
<node id="CHAO LI"/>
<node id="MINYI GUO"/>
<node id="28"/>
<node id="YINMIN ZHONG"/>
<node id="VINCENT LIU"/>
<node id="XIN JIN"/>
<node id="YANPING HUANG"/>
<node id="JOSEPH E GONZALEZ"/>
<node id="ET AL"/>
<node id="59"/>
<node id="NUMBER"/>
<node id="YONGHUI WU"/>
<node id="MIKE SCHUSTER"/>
<node id="QUOC V LE"/>
<node id="MOHAMMAD NOROUZI"/>
<node id="WOLFGANG MACHEREY"/>
<node id="MAXIM KRIKUN"/>
<node id="YUAN CAO"/>
<node id="QIN GAO"/>
<node id="KLAUS MACHEREY"/>
<node id="63 LIANMIN ZHENG, ZHUOHAN LI, HAO ZHANG, YONGHAO ZHUANG, ZHIFENG CHEN, YANPING HUANG, YIDA WANG, YUANZHONG XU, DANYANG ZHUO, ERIC P XING"/>
<node id="ET AL."/>
<node id="VICUNA"/>
<node id="AN OPEN-SOURCE CHATBOT"/>
<node id="GPT-4"/>
<node id="90 CHATGPT QUALITY"/>
<node id="ORGBLOG2023-03-30-VICUNA 9"/>
<node id="AAKANKSHA CHOWDHERY, SHARAN NARANG, JACOB DEVLIN, MAARTEN BOSMA, GAURAV MISHRA, ADAM ROBERTS, PAUL BARHAM, HYUNG WON CHUNG, CHARLES SUTTON, SEBASTIAN GEHRMANN, ET AL."/>
<node id="PALM"/>
<node id="SCALING LANGUAGE MODELING WITH PATHWAYS"/>
<node id="DANIEL CRANKSHAW"/>
<node id="GUR-EYAL SELA"/>
<node id="XIANGXI MO"/>
<node id="COREY ZUMAR"/>
<node id="JOSEPH GONZALEZ"/>
<node id="ALEXEY TUMANOV"/>
<node id="XIN WANG"/>
<node id="GUILIO ZHOU"/>
<node id="MICHAEL J FRANKLIN"/>
<node id="INFERLINE"/>
<node id="LATENCY-AWARE PROVISIONING AND SCALING FOR PREDICTION SERVING PIPELINES"/>
<node id="11TH ACM SYMPOSIUM"/>
<node id="CLOUD COMPUTING"/>
<node id="CLIPPER"/>
<node id="A LOW-LATENCY ONLINE PREDICTION SERVING SYSTEM"/>
<node id="14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="NSDI 17"/>
<node id="20TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="NSDI 23"/>
<node id="DVABATCH"/>
<node id="DIVERSITY-AWARE MULTI-ENTRY MULTI-EXIT BATCHING"/>
<node id="EFFICIENT PROCESSING OF DNN SERVICES ON GPUS"/>
<node id="USENIX ANNUAL TECHNICAL CONFERENCE"/>
<node id="USENIX ATC 22"/>
<node id="13 TRI DAO, DAN FU, STEFANO ERMON, ATRI RUDRA, AND CHRISTOPHER R."/>
<node id="IN 2022"/>
<node id="FLASHATTENTION"/>
<node id="FAST AND MEMORY-EFFICIENT EXACT ATTENTION WITH IO-AWARENESS"/>
<node id="TURBOTRANSFORMERS"/>
<node id="AN EFFICIENT GPU SERVING SYSTEM"/>
<node id="TRANSFORMER MODELS"/>
<node id="26TH ACM SIGPLAN SYMPOSIUM"/>
<node id="PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING"/>
<node id="THE 23RD ACM SIGPLAN SYMPOSIUM"/>
<node id="FASTAPI"/>
<node id="A WEB FRAMEWORK"/>
<node id="HTTPS:GITHUB.COM"/>
<node id="TIANGOLOFASTAPI"/>
<node id="16"/>
<node id="PIN GAO"/>
<node id="LINGFAN YU"/>
<node id="YONGWEI WU"/>
<node id="JINYANG LI"/>
<node id="LOW LATENCY RNN INFERENCE"/>
<node id="CELLULAR BATCHING"/>
<node id="THIRTEENTH EUROSYS CONFERENCE"/>
<node id="17 AMIR GHOLAMI"/>
<node id="ZHEWEI YAO"/>
<node id="SEHOON KIM"/>
<node id="MICHAEL W MAHONEY"/>
<node id="KURT KEUTZER"/>
<node id="AI"/>
<node id="MEMORY WALL"/>
<node id="RISELAB MEDIUM POST 1"/>
<node id="6"/>
<node id="GITHUB"/>
<node id="18"/>
<node id="COPILOT"/>
<node id="GOOGLE"/>
<node id="HTTPS:BARD.GOOGLE.COM"/>
<node id="ARPAN GUJARATI"/>
<node id="REZA KARIMI"/>
<node id="SAFYA ALZAYAT"/>
<node id="WEI HAO"/>
<node id="ANTOINE KAUFMANN"/>
<node id="YMIR VIGFUSSON"/>
<node id="JONATHAN MACE"/>
<node id="SERVING DNNS"/>
<node id="LIKE CLOCKWORK"/>
<node id="PERFORMANCE PREDICTABILITY"/>
<node id="FROM THE BOTTOM UP"/>
<node id="OSDI 20"/>
<node id="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="14TH USENIX CONFERENCE"/>
<node id="OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="OSDI 22"/>
<node id="16TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="MICROSECOND-SCALE PREEMPTION"/>
<node id="CONCURRENT GPU-ACCELERATED DNN INFERENCES"/>
<node id="DEEP RESIDUAL LEARNING"/>
<node id="IMAGE RECOGNITION"/>
<node id="THE IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"/>
<node id="SWAPADVISOR"/>
<node id="DEEP LEARNING BEYOND THE GPU MEMORY LIMIT VIA SMART SWAPPING"/>
<node id="THE TWENTY-FIFTH INTERNATIONAL CONFERENCE"/>
<node id="ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS"/>
<node id="24 PARAS JAIN"/>
<node id="AJAY JAIN"/>
<node id="ANIRUDDHA NRUSIMHA"/>
<node id="AMIR GHOLAMI"/>
<node id="PIETER ABBEEL"/>
<node id="40 SHISHIR G PATIL"/>
<node id="PARAS JAIN"/>
<node id="PRABAL DUTTA"/>
<node id="CHECK-MATE"/>
<node id="BREAKING THE MEMORY WALL WITH OPTIMAL TENSOR REMATERIALIZATION"/>
<node id="PROCEEDINGS OF MACHINE LEARNING AND SYSTEMS"/>
<node id="497-511"/>
<node id="TOM KILBURN"/>
<node id="DAVID BG EDWARDS"/>
<node id="MICHAEL J LANIGAN"/>
<node id="FRANK H SUMNER"/>
<node id="1962"/>
<node id="YEAR"/>
<node id="ONE-LEVEL STORAGE SYSTEM"/>
<node id="SYSTEM"/>
<node id="IRE TRANSACTIONS ON ELECTRONIC COMPUTERS"/>
<node id="223-235"/>
<node id="26"/>
<node id="BRIAN LESTER"/>
<node id="RAMI AL-RFOU"/>
<node id="NOAH CONSTANT"/>
<node id="POWER OF SCALE"/>
<node id="PARAMETER-EFFICIENT PROMPT TUNING"/>
<node id="XIANG LISA LI"/>
<node id="PERCY LIANG"/>
<node id="PREFIX-TUNING"/>
<node id="OPTIMIZING CONTINUOUS PROMPTS FOR GENERATION"/>
<node id="29 LINGXIAO MA"/>
<node id="ZHIQIANG XIE"/>
<node id="ZHI YANG"/>
<node id="JILONG XUE"/>
<node id="YOUSHAN MIAO"/>
<node id="WEI CUI"/>
<node id="WENXIANG HU"/>
<node id="FAN YANG"/>
<node id="LINTAO ZHANG"/>
<node id="RAMMER"/>
<node id="HOLISTIC DEEP LEARNING COMPILER OPTIMIZATIONS WITH RTASKS"/>
<node id="NVIDIA"/>
<node id="31"/>
<node id="32"/>
<node id="N. D."/>
<node id="TRITON INFERENCE SERVER"/>
<node id="HTTPS://DEVELOPER.NVIDIA.COM"/>
<node id="NVIDIA-TRITON-INFERENCE-SERVER"/>
<node id="HTTPS"/>
<node id="DEVELOPER.NVIDIA.COMNCCL"/>
<node id="NVIDIA FASTERTRANSFORMER"/>
<node id="NCCL"/>
<node id="THE NVIDIA COLLECTIVE COMMUNICATION LIBRARY"/>
<node id="CHRISTOPHER OLSTON"/>
<node id="NOAH FIEDEL"/>
<node id="KIRIL GOROVOY"/>
<node id="JEREMIAH HARMSEN"/>
<node id="LI LAO"/>
<node id="FANGWEI LI"/>
<node id="VINU RAJASHEKHAR"/>
<node id="SUKRITI RAMESH"/>
<node id="JORDAN SOYKE"/>
<node id="TENSORFLOW-SERVING"/>
<node id="FLEXIBLE ML SERVING"/>
<node id="HIGH-PERFORMANCE ML SERVING"/>
<node id="OPENAI"/>
<node id="34"/>
<node id="HTTPS://OPENAI.COM/BLOG/OPENAI-API"/>
<node id="HTTPS://OPENAI.COM/BLOG/CHATGPT"/>
<node id="OPENAI.COM/BLOG/CUSTOM-INSTRUCTIONS-FOR-CHATGPT"/>
<node id="BLOG CUSTOM INSTRUCTIONS FOR CHATGPT"/>
<node id="TECHNICAL REPORT"/>
<node id="CHATBOT ARENA LEADERBOARD"/>
<node id="WEEK 8"/>
<node id="MT-BENCH"/>
<node id="VICUNA-33B"/>
<node id="HTTPS:LMSYS.ORG"/>
<node id="BLOG POST DATE 2023-06-22"/>
<node id="LEADERBOARD"/>
<node id="AN IMPERATIVE STYLE HIGH-PERFORMANCE DEEP LEARNING LIBRARY"/>
<node id="32ND EDITION"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32"/>
<node id="POET"/>
<node id="NEURAL NETWORKS"/>
<node id="TINY DEVICES"/>
<node id="INTEGRATED REMATERIALIZATION AND PAGING"/>
<node id="INTERNATIONAL CONFERENCE ON MACHINE LEARNING"/>
<node id="AN EVENT"/>
<node id="PMLR"/>
<node id="1757317583"/>
<node id="41"/>
<node id="REINER POPE"/>
<node id="SHOLTO DOUGLAS"/>
<node id="AAKANKSHA CHOWDHERY"/>
<node id="JACOB DEVLIN"/>
<node id="JAMES BRADBURY"/>
<node id="ANSELM LEVSKAYA"/>
<node id="JONATHAN HEEK"/>
<node id="KEFAN XIAO"/>
<node id="SHIVANI AGRAWAL"/>
<node id="JEFF DEAN"/>
<node id="ZERO-OFFLOAD"/>
<node id="DEMOCRATIZING BILLION-SCALE MODEL TRAINING"/>
<node id="AMAZON WEB SERVICES"/>
<node id="HTTPS://WWW.REUTERS.COM/TECHNOLOGY/TECH-GIANTS-AI-LIKE-BING-BARD-POSES-BILLION-DOLLAR-SEARCH-PROBLEM-2023-02-22"/>
<node id="HTTPS:AWS.AMAZON.COMBEDROCK"/>
<node id="HAICHEN SHEN"/>
<node id="LEQUN CHEN"/>
<node id="YUCHEN JIN"/>
<node id="LIANGYU ZHAO"/>
<node id="BINGYU KONG"/>
<node id="MATTHAI PHILIPOSE"/>
<node id="ARVIND KRISHNAMURTHY"/>
<node id="RAVI SUNDARAM"/>
<node id="NEXUS"/>
<node id="A GPU CLUSTER ENGINE"/>
<node id="ACCELERATING DNN-BASED VIDEO ANALYSIS"/>
<node id="27TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES"/>
<node id="HIGH-THROUGHPUT GENERATIVE INFERENCE"/>
<node id="A SINGLE GPU"/>
<node id="47"/>
<node id="MOHAMMAD SHOEYBI"/>
<node id="MOSTOFA PATWARY"/>
<node id="RAUL PURI"/>
<node id="PATRICK LEGRESLEY"/>
<node id="JARED CASPER"/>
<node id="BRYAN CATANZARO"/>
<node id="MEGATRON-LM"/>
<node id="TRAINING MULTI-BILLION PARAMETER LANGUAGE MODELS"/>
<node id="48"/>
<node id="BENOIT STEINER"/>
<node id="MOSTAFA ELHOUSHI"/>
<node id="JACOB KAHN"/>
<node id="JAMES HEGARTY"/>
<node id="OLLA"/>
<node id="THE LIFETIME OF ARRAYS"/>
<node id="THE LOCATION OF ARRAYS"/>
<node id="THE MEMORY USAGE OF NEURAL NETWORKS"/>
<node id="DOI.ORG/10.48550/ARXIV.2210.12924"/>
<node id="ILYA SUTSKEVER"/>
<node id="ORIOL VINYALS"/>
<node id="SEQUENCE TO SEQUENCE LEARNING"/>
<node id="50"/>
<node id="ROHAN TAORI"/>
<node id="ISHAAN GULRAJANI"/>
<node id="TIANYI ZHANG"/>
<node id="YANN DUBOIS"/>
<node id="XUECHEN LI"/>
<node id="TATSUNORI B. HASHIMOTO"/>
<node id="55"/>
<node id="LINNAN WANG"/>
<node id="JINMIAN YE"/>
<node id="YIYANG ZHAO"/>
<node id="WEI WU"/>
<node id="ANG LI"/>
<node id="SHUAI-WEN LEON SONG"/>
<node id="ZENGLIN XU"/>
<node id="TIM KRASKA"/>
<node id="57 YIZHONG WANG"/>
<node id="YEGANEH KORDI"/>
<node id="SWAROOP MISHRA"/>
<node id="ALISA LIU"/>
<node id="NOAH A SMITH"/>
<node id="DANIEL KHASHABI"/>
<node id="HANNANEH HAJISHIRZI"/>
<node id="STANFORD ALPACA"/>
<node id="AN INSTRUCTION-FOLLOWING LLAMA MODEL"/>
<node id="GITHUB.COM/TATSU-LABS"/>
<node id="SHAREGPT TEAM"/>
<node id="51"/>
<node id="HTTPS:SHAREGPT.COM"/>
<node id="HUGO TOUVRON, THIBAUT LAVRIL, GAUTIER IZACARD, XAVIER MARTINET, MARIE-ANNE LACHAUX, TIMOTHE LACROIX, BAPTISTE ROZIRE, NAMAN GOYAL, ERIC HAMBRO, FAISAL AZHAR, ET AL."/>
<node id="LLAMA"/>
<node id="OPEN AND EFFICIENT FOUNDATION LANGUAGE MODELS"/>
<node id="53"/>
<node id="ASHISH VASWANI"/>
<node id="NOAM SHAZEER"/>
<node id="NIKI PARMAR"/>
<node id="JAKOB USZKOREIT"/>
<node id="LLION JONES"/>
<node id="AIDAN N GOMEZ"/>
<node id="UKASZ KAISER"/>
<node id="ILLIA POLOSUKHIN"/>
<node id="ATTENTION"/>
<node id="ALL YOU NEED"/>
<node id="PACMAN"/>
<node id="AN EFFICIENT COMPACTION APPROACH"/>
<node id="LOG-STRUCTURED KEY-VALUE STORE"/>
<node id="PERSISTENT MEMORY"/>
<node id="SUPERNEURONS"/>
<node id="DYNAMIC GPU MEMORY MANAGEMENT FOR TRAINING DEEP NEURAL NETWORKS"/>
<node id="LIGHTSEQ"/>
<node id="A HIGH PERFORMANCE INFERENCE LIBRARY FOR TRANSFORMERS"/>
<node id="2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS"/>
<node id="2021 CONFERENCE"/>
<node id="NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS"/>
<node id="NORTH AMERICAN CHAPTER"/>
<node id="HUMAN LANGUAGE TECHNOLOGIES: INDUSTRY PAPERS"/>
<node id="SELF-INSTRUCT"/>
<node id="ALIGNING LANGUAGE MODEL WITH SELF GENERATED INSTRUCTIONS"/>
<node id="STATE-OF-THE-ART NATURAL LANGUAGE PROCESSING"/>
<node id="THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS"/>
<node id="GOOGLE'S NEURAL MACHINE TRANSLATION SYSTEM"/>
<node id="THE GAP BETWEEN HUMAN AND MACHINE TRANSLATION"/>
<node id="60 GYEONG-IN YU"/>
<node id="JOO SEONG JEONG"/>
<node id="GEON-WOO KIM"/>
<node id="SOOJEONG KIM"/>
<node id="BYUNG-GON CHUN"/>
<node id="A DISTRIBUTED SERVING SYSTEM"/>
<node id="TRANSFORMER-BASED GENERATIVE MODELS"/>
<node id="61"/>
<node id="HONG ZHANG"/>
<node id="YUPENG TANG"/>
<node id="ANURAG KHANDELWAL"/>
<node id="SHEPHERD"/>
<node id="DNNS IN THE WILD"/>
<node id="USENIX ASSOCIATION"/>
<node id="BOSTON, MA"/>
<node id="787808"/>
<node id="SUSAN ZHANG"/>
<node id="PRESENTER AT NSDI23"/>
<node id="STEPHEN ROLLER"/>
<node id="NAMAN GOYAL"/>
<node id="MIKEL ARTETXE"/>
<node id="MOYA CHEN"/>
<node id="SHUOHUI CHEN"/>
<node id="CHRISTOPHER DEWAN"/>
<node id="MONA DIAB"/>
<node id="XIAN LI"/>
<node id="XI VICTORIA LIN"/>
<node id="NSDI23"/>
<node id="CONFERENCE"/>
<node id="HTTPS://WWW.USENIX.ORG/CONFERENCE/NSDI23/PRESENTATION/ZHANG-HONG"/>
<node id="URL"/>
<node id="OPEN PRE-TRAINED TRANSFORMER LANGUAGE MODELS"/>
<node id="ALPA"/>
<node id="AUTOMATING INTER-AND INTRA-OPERATOR PARALLELISM FOR DISTRIBUTED DEEP LEARNING"/>
<node id="PETS"/>
<node id="A UNIFIED FRAMEWORK FOR PARAMETER-EFFICIENT TRANSFORMERS SERVING"/>
<edge source="EXISTING SYSTEMS" target="KEY-VALUE CACHE (KV CACHE) MEMORY FOR EACH REQUEST IS HUGE AND GROWS AND SHRINKS DYNAMICALLY">
  <data key="d0">STRUGGLE BECAUSE</data>
</edge>
<edge source="EXISTING SYSTEMS" target="SIMULTANEOUS PROCESSING OF REQUESTS WITH DIFFERENT DECODING PREFERENCES EFFICIENTLY">
  <data key="d0">CANNOT DO</data>
</edge>
<edge source="THE KV CACHE SIZE" target="THE NUMBER OF REQUESTS">
  <data key="d0">GROWS QUICKLY WITH</data>
</edge>
<edge source="THIS MEMORY" target="FRAGMENTATION AND REDUNDANT DUPLICATION">
  <data key="d0">CAN BE WASTED BY</data>
</edge>
<edge source="FRAGMENTATION AND REDUNDANT DUPLICATION" target="THE BATCH SIZE">
  <data key="d0">LIMIT</data>
</edge>
<edge source="INEFFICIENT MEMORY MANAGEMENT" target="BATCH SIZE">
  <data key="d0">CAN DECREASE</data>
</edge>
<edge source="BATCH SIZE" target="REQUESTS">
  <data key="d0">IS MEASURED IN</data>
</edge>
<edge source="BATCH SIZE" target="SUFFICIENTLY LARGE">
  <data key="d0">IS</data>
</edge>
<edge source="WE" target="PAGEDATTENTION">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="WE" target="VLLM">
  <data key="d0">DESIGN AND IMPLEMENT</data>
</edge>
<edge source="WE" target="THE KV CACHE IN A MORE FLEXIBLE WAY">
  <data key="d0">CAN MANAGE</data>
</edge>
<edge source="WE" target="THE PERFORMANCE OF VLLM UNDER A VARIETY OF WORKLOADS">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="CONTRIBUTIONS">
  <data key="d0">MAKE</data>
</edge>
<edge source="WE" target="CHALLENGES IN MEMORY ALLOCATION IN SERVING LLMS">
  <data key="d0">IDENTIFY</data>
</edge>
<edge source="WE" target="IMPACT ON SERVING PERFORMANCE">
  <data key="d0">QUANTIFY</data>
</edge>
<edge source="WE" target="THE DESIGN OF THE KV CACHE MANAGER IN 4.2">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="THE KV CACHE AS FIXED-SIZE KV BLOCKS">
  <data key="d0">ORGANIZE</data>
</edge>
<edge source="WE" target="VLLM ON VARIOUS SCENARIOS">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THE CONCATENATION OF THE PROMPT AND OUTPUT LISTS AS SEQUENCE">
  <data key="d0">REFER TO</data>
</edge>
<edge source="WE" target="THE AVERAGE PERCENTAGE OF MEMORY WASTES IN OUR EXPERIMENTS IN FIG.">
  <data key="d0">VISUALIZE</data>
</edge>
<edge source="WE" target="THE GENERAL APPLICABILITY OF VLLM ON THEM">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="A NEW ATTENTION ALGORITHM PAGE-DATTENTION">
  <data key="d0">DEVELOP</data>
</edge>
<edge source="WE" target="AN LLM SERVING ENGINE VLLM">
  <data key="d0">BUILD</data>
</edge>
<edge source="WE" target="THE PAGEDATTENTION ALGORITHM">
  <data key="d0">DESCRIBE</data>
</edge>
<edge source="WE" target="AN EXAMPLE OF PAGEDATTENTION IN FIG.">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="THE SECOND ONE">
  <data key="d0">CHOOSE</data>
</edge>
<edge source="WE" target="THE EFFECT OF BLOCK SIZE IN 7.2">
  <data key="d0">STUDY</data>
</edge>
<edge source="WE" target="THE MORE GENERAL CASE">
  <data key="d0">ASSUME</data>
</edge>
<edge source="WE" target="ONE COPY OF THE PROMPTS STATE AT THE PROMPT PHASE">
  <data key="d0">RESERVE SPACE FOR</data>
</edge>
<edge source="WE" target="A REFERENCE COUNT FOR EACH PHYSICAL BLOCK">
  <data key="d0">INTRODUCE</data>
</edge>
<edge source="WE" target="TWO TECHNIQUES">
  <data key="d0">CONSIDER</data>
</edge>
<edge source="WE" target="AN ALL-OR-NOTHING EVICTION POLICY">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="EVICTED BLOCKS TO THE CPU MEMORY">
  <data key="d0">COPY</data>
</edge>
<edge source="WE" target="THE KV CACHE">
  <data key="d0">RECOMPUTE</data>
</edge>
<edge source="WE" target="THE SPEEDS OF SWAPPING AND RECOMPUTATION IN 7.3">
  <data key="d0">EXAMINE</data>
</edge>
<edge source="WE" target="CONTROL-RELATED COMPONENTS">
  <data key="d0">DEVELOP</data>
</edge>
<edge source="WE" target="CUSTOM CUDA KERNELS">
  <data key="d0">DEVELOP</data>
</edge>
<edge source="WE" target="NCCL 32 FOR TENSOR COMMUNICATION ACROSS THE DISTRIBUTED GPU WORKERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="SEVERAL GPU KERNELS">
  <data key="d0">DEVELOP</data>
</edge>
<edge source="WE" target="THEM INTO A SINGLE KERNEL">
  <data key="d0">FUSE</data>
</edge>
<edge source="WE" target="A KERNEL">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="THE ATTENTION KERNEL IN FASTERTRANSFORMER 31">
  <data key="d0">ADAPT</data>
</edge>
<edge source="WE" target="KV CACHE ACCORDING TO THE BLOCK TABLE">
  <data key="d0">READ</data>
</edge>
<edge source="WE" target="ATTENTION OPERATIONS ON THE FLY">
  <data key="d0">PERFORM</data>
</edge>
<edge source="WE" target="A GPU WARP TO READ EACH BLOCK">
  <data key="d0">ASSIGN</data>
</edge>
<edge source="WE" target="SUPPORT FOR VARIABLE SEQUENCE LENGTHS WITHIN A REQUEST BATCH">
  <data key="d0">ADD</data>
</edge>
<edge source="WE" target="THE CUDAMEMCPYASYNC API">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="OPT 62 MODELS WITH 13B PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="OPT 62 MODELS WITH 66B PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="OPT 62 MODELS WITH 175B PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="LLAMA 52 WITH 13B PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="OPT 62 MODELS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="A2 INSTANCES WITH NVIDIA A100 GPUS ON GOOGLE CLOUD PLATFORM">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="WORKLOADS BASED ON SHAREGPT 51 AND ALPACA 50 DATASETS">
  <data key="d0">SYNTHESIZE</data>
</edge>
<edge source="WE" target="THE CHATTING HISTORY AND USER QUERY USING THE SHAREGPT DATASET">
  <data key="d0">SYNTHESIZE</data>
</edge>
<edge source="WE" target="THE DATASETS">
  <data key="d0">TOKENIZE</data>
</edge>
<edge source="WE" target="THEIR INPUT AND OUTPUT LENGTHS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="REQUEST ARRIVAL TIMES">
  <data key="d0">GENERATE</data>
</edge>
<edge source="WE" target="A CUSTOM SCHEDULER">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="A MAXIMUM BATCH SIZE AS LARGE AS POSSIBLE FOR EACH EXPERIMENT">
  <data key="d0">SET</data>
</edge>
<edge source="WE" target="OUR OWN VERSION OF ORCA">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="THE SYSTEM HAS THE KNOWLEDGE OF THE LENGTHS OF THE OUTPUTS THAT WILL BE ACTUALLY GENERATED FOR THE REQUESTS">
  <data key="d0">ASSUME</data>
</edge>
<edge source="WE" target="SERVING THROUGHPUT">
  <data key="d0">FOCUS ON</data>
</edge>
<edge source="WE" target="NORMALIZED LATENCY OF THE SYSTEMS">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="THE SYSTEMS WITH 1-HOUR TRACES">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="15-MINUTE TRACES FOR THE OPT-175B MODEL">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="THE PERFORMANCE OF VLLM WITH BASIC SAMPLING">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THREE MODELS AND TWO DATASETS">
  <data key="d0">EVALUATE ON</data>
</edge>
<edge source="WE" target="THE EFFECTIVENESS OF MEMORY SHARING IN PAGE-DATTENTION">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="6.1 - 9.8 MEMORY SAVING ON PARALLEL SAMPLING">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="37.6 - 55.2 MEMORY SAVING ON BEAM SEARCH">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="WMT16 4 ENGLISH-TO-GERMAN TRANSLATION DATASET">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="TWO PREFIXES">
  <data key="d0">SYNTHESIZE</data>
</edge>
<edge source="WE" target="THE MODEL GENERATE A RESPONSE">
  <data key="d0">LET</data>
</edge>
<edge source="WE" target="THE PROMPT TO THE LAST 1024 TOKENS">
  <data key="d0">CUT</data>
</edge>
<edge source="WE" target="THE MODEL GENERATE AT MOST 1024 TOKENS">
  <data key="d0">LET</data>
</edge>
<edge source="WE" target="THE KV CACHE BETWEEN DIFFERENT CONVERSATION ROUNDS">
  <data key="d0">DO NOT STORE</data>
</edge>
<edge source="WE" target="VARIOUS ASPECTS OF VLLM">
  <data key="d0">STUDY</data>
</edge>
<edge source="WE" target="THE DESIGN CHOICES">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THE PERFORMANCE OF VLLM WITH DIFFERENT BLOCK SIZES">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THE SHAREGPT AND ALPACA TRACES WITH BASIC SAMPLING">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="THEIR END-TO-END PERFORMANCE">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THEIR OVERHEADS">
  <data key="d0">MICROBENCHMARK</data>
</edge>
<edge source="WE" target="VLLMS TECHNIQUES BEING APPLIED TO OTHER WORKLOADS WITH SIMILAR PROPERTIES TO LLM SERVING">
  <data key="d0">WOULD BE EXCITED TO SEE</data>
</edge>
<edge source="WE" target="XIAOXUAN LIU">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="ZHIFENG CHEN">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="YAN-PING HUANG">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="ANONYMOUS SOSP REVIEWERS">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="OUR SHEPHERD">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="PAGEDATTENTION" target="AN ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGEDATTENTION" target="THE CLASSICAL VIRTUAL MEMORY AND PAGING TECHNIQUES IN OPERATING SYSTEMS">
  <data key="d0">IS INSPIRED BY</data>
</edge>
<edge source="PAGEDATTENTION" target="KV CACHE STORED IN NON-CONTIGUOUS PAGED MEMORY">
  <data key="d0">OPERATES ON</data>
</edge>
<edge source="PAGEDATTENTION" target="VIRTUAL MEMORY AND PAGING IN OS">
  <data key="d0">IS INSPIRED BY</data>
</edge>
<edge source="PAGEDATTENTION" target="STORING CONTINUOUS KEYS AND VALUES IN NON-CONTIGUOUS MEMORY SPACE">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="PAGEDATTENTION" target="A NEW ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGEDATTENTION" target="ATTENTION KEYS AND VALUES TO BE STORED IN NON-CONTIGUOUS PAGED MEMORY">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="PAGEDATTENTION" target="THE REQUESTS KV CACHE INTO BLOCKS">
  <data key="d0">DIVIDES</data>
</edge>
<edge source="PAGEDATTENTION" target="THE KV CACHE OF EACH SEQUENCE INTO KV BLOCKS">
  <data key="d0">PARTITIONS</data>
</edge>
<edge source="PAGEDATTENTION" target="WE TO ORGANIZE THE KV CACHE">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PAGEDATTENTION" target="MEMORY ACCESS PATTERNS">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="PAGEDATTENTION" target="THE PROBLEM OF MEMORY FRAGMENTATION AND RESERVATION">
  <data key="d0">RESOLVES</data>
</edge>
<edge source="PAGEDATTENTION" target="VLLM SIGNIFICANTLY OUTPERFORM FASTERTRANSFORMER IN END-TO-END PERFORMANCE">
  <data key="d0">MAKES</data>
</edge>
<edge source="PAGEDAT-TENTION" target="AN ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGEDAT-TENTION" target="OPERATING SYSTEMS (OS) SOLUTION TO MEMORY FRAGMENTATION AND SHARING">
  <data key="d0">IS INSPIRED BY</data>
</edge>
<edge source="OPERATING SYSTEMS (OS) SOLUTION TO MEMORY FRAGMENTATION AND SHARING" target="VIRTUAL MEMORY WITH PAGING">
  <data key="d0">IS</data>
</edge>
<edge source="PAGE-DATTENTION" target="AN ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGE-DATTENTION" target="THE CLASSIC IDEA OF PAGING IN OPERATING SYSTEMS">
  <data key="d0">IS INSPIRED BY</data>
</edge>
<edge source="TRADITIONAL ATTENTION ALGORITHMS" target="PAGEDATTENTION">
  <data key="d0">ARE UNLIKE</data>
</edge>
<edge source="ATTENTION KEY AND VALUES VECTORS" target="NON-CONTIGUOUS BLOCKS IN THE MEMORY">
  <data key="d0">ARE STORED AS</data>
</edge>
<edge source="THIS PAPER" target="PAGEDATTENTION">
  <data key="d0">PROPOSES</data>
</edge>
<edge source="THIS PAPER" target="VLLM">
  <data key="d0">PRESENTS</data>
</edge>
<edge source="THIS PAPER" target="A NEW IDEA OF BLOCK-LEVEL MEMORY MANAGEMENT">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="VLLM" target="A HIGH-THROUGHPUT LLM SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="EFFICIENT MEMORY MANAGEMENT ENABLED BY PAGEDATTENTION">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="AN LLM SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="NEAR-ZERO WASTE IN KV CACHE MEMORY">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="FLEXIBLE SHARING OF KV CACHE WITHIN AND ACROSS REQUESTS">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="A HIGH-THROUGHPUT DISTRIBUTED LLM SERVING ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="PAGEDATTENTION">
  <data key="d0">EXECUTES</data>
</edge>
<edge source="VLLM" target="THROUGHPUT OF POPULAR LLMS BY 2-4">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="VLLM" target="THE SAME LEVEL OF LATENCY">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="STATE-OF-THE-ART SYSTEMS">
  <data key="d0">IS COMPARED TO</data>
</edge>
<edge source="VLLM" target="2-4 THROUGHPUT IMPROVEMENTS OVER THE STATE-OF-THE-ART SYSTEMS">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="THE RAPID GROWTH CURVE OF KV CACHE MEMORY SEEN IN EXISTING SYSTEMS 31, 60">
  <data key="d0">SMOOTHS OUT</data>
</edge>
<edge source="VLLM" target="A NOTABLE BOOST IN SERVING THROUGHPUT">
  <data key="d0">LEADS TO</data>
</edge>
<edge source="VLLM" target="THE IDEAS BEHIND VIRTUAL MEMORY">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE KV CACHE IN AN LLM SERVICE">
  <data key="d0">MANAGES</data>
</edge>
<edge source="VLLM" target="8.9">
  <data key="d0">HAS KV CACHE USAGE</data>
</edge>
<edge source="VLLM" target="41.6">
  <data key="d0">HAS TOKEN STATES RESERVATION</data>
</edge>
<edge source="VLLM" target="96.3">
  <data key="d0">HAS INTERNAL FRAGMENTATION</data>
</edge>
<edge source="VLLM" target="BLOCK-LEVEL MEMORY MANAGEMENT">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="PREEMPTIVE REQUEST SCHEDULING">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE MEMORY DURING THE DECODING PROCESS OF A SINGLE INPUT SEQUENCE">
  <data key="d0">MANAGES</data>
</edge>
<edge source="VLLM" target="RESERVING THE MEMORY FOR THE MAXIMUM POSSIBLE GENERATED SEQUENCE LENGTH INITIALLY">
  <data key="d0">DOES NOT REQUIRE</data>
</edge>
<edge source="VLLM" target="OSS VIRTUAL MEMORY">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="VLLM" target="PAGEDATTENTION KERNEL">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="NEWLY GENERATED KV CACHE">
  <data key="d0">SAVES</data>
</edge>
<edge source="VLLM" target="THIS SHARING EASILY">
  <data key="d0">CAN REALIZE</data>
</edge>
<edge source="VLLM" target="MEMORY">
  <data key="d0">CAN SAVE</data>
</edge>
<edge source="VLLM" target="PAGEDATTENTION AND PAGED MEMORY MANAGEMENT">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="POPULAR LLMS SUCH AS GPT 5, OPT 62, AND LLAMA 52">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VLLM" target="A DISTRIBUTED LLM SERVING ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="PREVIOUS STATE-OF-THE-ART SOLUTIONS">
  <data key="d0">OUTPERFORMS</data>
</edge>
<edge source="VLLM" target="UP TO 22 HIGHER REQUEST RATES">
  <data key="d0">CAN SUSTAIN</data>
</edge>
<edge source="VLLM" target="MEMORY FRAGMENTATION">
  <data key="d0">REDUCES</data>
</edge>
<edge source="VLLM" target="SHARING">
  <data key="d0">ENABLES</data>
</edge>
<edge source="VLLM" target="MORE REQUESTS IN A BATCH IN PARALLEL">
  <data key="d0">RUNS</data>
</edge>
<edge source="VLLM" target="2-4 SPEEDUP">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="THE CHALLENGES OUTLINED IN 3">
  <data key="d0">TACKLE</data>
</edge>
<edge source="VLLM" target="A CENTRALIZED SCHEDULER">
  <data key="d0">ADOPTS</data>
</edge>
<edge source="VLLM" target="ALL THE MEMORY WASTES FOR A REQUEST WITHIN ONE BLOCK">
  <data key="d0">LIMITS</data>
</edge>
<edge source="VLLM" target="ALL THE MEMORY">
  <data key="d0">CAN EFFECTIVELY UTILIZE</data>
</edge>
<edge source="VLLM" target="THE SHARING OF MOST OF THE SPACE USED TO STORE THE PROMPTS KV CACHE ACROSS MULTIPLE OUTPUT SAMPLES">
  <data key="d0">ENABLES</data>
</edge>
<edge source="VLLM" target="THE COMPLEX MEMORY SHARING BETWEEN DIFFERENT SEQUENCES">
  <data key="d0">CONCEALS</data>
</edge>
<edge source="VLLM" target="A COMMON MAPPING LAYER">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE NEW TOKEN">
  <data key="d0">GENERATES</data>
</edge>
<edge source="VLLM" target="THE PAGEDATTENTION ALGORITHM">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE FIRST 2 LOGICAL KV BLOCKS (0 AND 1) TO 2 PHYSICAL KV BLOCKS (7 AND 1, RESPECTIVELY)">
  <data key="d0">MAPS</data>
</edge>
<edge source="VLLM" target="KV CACHE OF THE PROMPTS">
  <data key="d0">GENERATES</data>
</edge>
<edge source="VLLM" target="FIRST OUTPUT TOKEN">
  <data key="d0">GENERATES</data>
</edge>
<edge source="VLLM" target="CONVENTIONAL SELF-ATTENTION ALGORITHM">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE KV CACHE OF THE FIRST 4 TOKENS IN LOGICAL BLOCK 0">
  <data key="d0">STORES</data>
</edge>
<edge source="VLLM" target="THE FOLLOWING 3 TOKENS IN LOGICAL BLOCK 1">
  <data key="d0">STORES</data>
</edge>
<edge source="VLLM" target="NEW PHYSICAL BLOCKS TO LOGICAL BLOCKS">
  <data key="d0">DYNAMICALLY ASSIGNS</data>
</edge>
<edge source="VLLM" target="FREE PHYSICAL BLOCKS FOR NEW TOKENS">
  <data key="d0">EXHAUSTS</data>
</edge>
<edge source="VLLM" target="A SET OF SEQUENCES TO EVICT">
  <data key="d0">SELECTS</data>
</edge>
<edge source="VLLM" target="THEIR KV CACHE TO THE CPU">
  <data key="d0">TRANSFERS</data>
</edge>
<edge source="VLLM" target="A SET OF CANDIDATE SEQUENCES FOR BATCHING">
  <data key="d0">SELECTS</data>
</edge>
<edge source="VLLM" target="THE PHYSICAL BLOCKS FOR THE NEWLY REQUIRED LOGICAL BLOCKS">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="VLLM" target="THE MEMORY FOR TWO SEQUENCES">
  <data key="d0">MANAGES</data>
</edge>
<edge source="VLLM" target="THE KV BLOCKS">
  <data key="d0">MANAGES</data>
</edge>
<edge source="VLLM" target="ALL PHYSICAL BLOCKS WHOSE REFERENCE COUNTS REACH 0">
  <data key="d0">FREES</data>
</edge>
<edge source="VLLM" target="NEW PHYSICAL BLOCKS (BLOCKS 9-12)">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="VLLM" target="RESERVING A SET OF PHYSICAL BLOCKS FOR A SET OF PREDEFINED SHARED PREFIXES BY THE LLM SERVICE PROVIDER">
  <data key="d0">CAN ACHIEVE</data>
</edge>
<edge source="VLLM" target="SIMULTANEOUS PROCESSING OF REQUESTS WITH DIFFERENT DECODING PREFERENCES">
  <data key="d0">FACILITATES</data>
</edge>
<edge source="VLLM" target="A SUBSET OF REQUESTS">
  <data key="d0">MUST PRIORITIZE</data>
</edge>
<edge source="VLLM" target="REQUESTS">
  <data key="d0">NEEDS TO PREEMPT</data>
</edge>
<edge source="VLLM" target="EARLIEST ARRIVED REQUESTS ARE SERVED FIRST">
  <data key="d0">ENSURES</data>
</edge>
<edge source="VLLM" target="LATEST REQUESTS ARE PREEMPTED FIRST">
  <data key="d0">ENSURES</data>
</edge>
<edge source="VLLM" target="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY">
  <data key="d0">ADOPTS</data>
</edge>
<edge source="VLLM" target="THE GPUS PHYSICAL BLOCKS TO STORE THE NEWLY GENERATED KV CACHE">
  <data key="d0">CAN RUN OUT OF</data>
</edge>
<edge source="VLLM" target="GPUS PARALLELISM FOR READING AND PROCESSING KV CACHE">
  <data key="d0">MAY NOT FULLY UTILIZE</data>
</edge>
<edge source="VLLM" target="TWO CLASSIC QUESTIONS">
  <data key="d0">NEEDS TO ANSWER</data>
</edge>
<edge source="VLLM" target="GPU BLOCK ALLOCATOR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="VLLM" target="CPU BLOCK ALLOCATOR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="VLLM" target="A SEQUENCE">
  <data key="d0">PREEMPTS</data>
</edge>
<edge source="VLLM" target="ITS BLOCKS">
  <data key="d0">EVICTS</data>
</edge>
<edge source="VLLM" target="NEW REQUESTS">
  <data key="d0">STOPS ACCEPTING</data>
</edge>
<edge source="VLLM" target="ALL PREEMPTED SEQUENCES ARE COMPLETED">
  <data key="d0">STOPS ACCEPTING NEW REQUESTS UNTIL</data>
</edge>
<edge source="VLLM" target="RECOMPUTATION">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VLLM" target="SWAPPING">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VLLM" target="DISTRIBUTED SETTINGS">
  <data key="d0">IS EFFECTIVE IN</data>
</edge>
<edge source="VLLM" target="MEGATRON-LM STYLE TENSOR MODEL PARALLELISM STRATEGY">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VLLM" target="A SINGLE KV CACHE MANAGER">
  <data key="d0">FEATURES</data>
</edge>
<edge source="VLLM" target="AN END-TO-END SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="A FASTAPI 15 FRONTEND">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="A GPU-BASED INFERENCE ENGINE">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="MULTIPLE OUTPUT SEQUENCES">
  <data key="d0">CREATES</data>
</edge>
<edge source="VLLM" target="THE SINGLE INPUT SEQUENCE">
  <data key="d0">CREATES MULTIPLE OUTPUT SEQUENCES FROM</data>
</edge>
<edge source="VLLM" target="THE FORK METHOD">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="OPT MODEL">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="VLLM" target="30.42 FOR SHAREGPT">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="VLLM" target="1.72.7 HIGHER REQUEST RATES COMPARED TO ORCA (ORACLE)">
  <data key="d0">CAN SUSTAIN</data>
</edge>
<edge source="VLLM" target="2.78 HIGHER REQUEST RATES COMPARED TO ORCA (MAX)">
  <data key="d0">CAN SUSTAIN</data>
</edge>
<edge source="VLLM" target="SIMILAR LATENCIES">
  <data key="d0">MAINTAINS</data>
</edge>
<edge source="VLLM" target="3.58 HIGHER THROUGHPUT THAN ORCA (ORACLE)">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="MORE IMPROVEMENT OVER THE ORCA BASELINES">
  <data key="d0">BRINGS</data>
</edge>
<edge source="VLLM" target="2 HIGHER REQUEST RATES">
  <data key="d0">CAN SUSTAIN</data>
</edge>
<edge source="VLLM" target="64 128 256 CONTEXT LENGTH">
  <data key="d0">CAN EFFECTIVELY HANDLE</data>
</edge>
<edge source="VLLM" target="DEFAULT BLOCK SIZE AS 16">
  <data key="d0">SETS</data>
</edge>
<edge source="VLLM" target="THE OVERHEAD OF MEMORY INDIRECTION IN PAGING">
  <data key="d0">MITIGATES</data>
</edge>
<edge source="VLLM" target="THE GPU KERNELS FOR MEMORY ACCESS OPERATIONS WITH THOSE FOR OTHER OPERATIONS SUCH AS ATTENTION">
  <data key="d0">FUSES</data>
</edge>
<edge source="VLLM" target="THE IDEA OF VIRTUAL MEMORY AND PAGING">
  <data key="d0">AUGMENTS</data>
</edge>
<edge source="VLLM" target="THE APPLICATION-SPECIFIC SEMANTICS">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="FLEXIBLE SHARING OF KV CACHE" target="MEMORY USAGE">
  <data key="d0">REDUCES</data>
</edge>
<edge source="MEMORY USAGE" target="GB">
  <data key="d0">IS MEASURED IN</data>
</edge>
<edge source="MEMORY USAGE" target="GREATLY REDUCED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="MEMORY USAGE" target="ESPECIALLY FOR LONG INPUT PROMPTS">
  <data key="d0">IS REDUCED</data>
</edge>
<edge source="EXISTING LLM SERVING SYSTEMS 31, 60" target="MANAGING THE KV CACHE MEMORY EFFICIENTLY">
  <data key="d0">FALL SHORT OF</data>
</edge>
<edge source="BLOCKS" target="PAGES">
  <data key="d0">ARE</data>
</edge>
<edge source="BLOCKS" target="2, 4, 5, 8">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="TOKENS" target="BYTES">
  <data key="d0">ARE</data>
</edge>
<edge source="REQUESTS" target="PROCESSES">
  <data key="d0">ARE</data>
</edge>
<edge source="REQUESTS" target="MODEL WEIGHTS">
  <data key="d0">SHARE</data>
</edge>
<edge source="REQUESTS" target="DIFFERENT TIMES">
  <data key="d0">MAY ARRIVE AT</data>
</edge>
<edge source="REQUESTS" target="VASTLY DIFFERENT INPUT AND OUTPUT LENGTHS">
  <data key="d0">MAY HAVE</data>
</edge>
<edge source="KV CACHE MEMORY" target="EXISTING SYSTEMS">
  <data key="d0">IS MANAGED IN</data>
</edge>
<edge source="KV CACHE MEMORY" target="BATCH SIZE">
  <data key="d0">CAN LIMIT</data>
</edge>
<edge source="KV CACHE MEMORY" target="THROUGHPUT OF THE LLM">
  <data key="d0">CAN LIMIT</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="OPERATING SYSTEMS">
  <data key="d0">ARE INSPIRED BY</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="VIRTUAL MEMORY">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="COPY-ON-WRITE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="EFFICIENTLY MANAGE KV CACHE">
  <data key="d0">CAN BE ADAPTED TO</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="HANDLE VARIOUS DECODING ALGORITHMS IN LLM SERVING">
  <data key="d0">CAN BE ADAPTED TO</data>
</edge>
<edge source="STATE-OF-THE-ART SYSTEMS" target="FASTERTRANSFORMER AND ORCA">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE IMPROVEMENT" target="LONGER SEQUENCES">
  <data key="d0">IS MORE PRONOUNCED WITH</data>
</edge>
<edge source="THE IMPROVEMENT" target="LARGER MODELS">
  <data key="d0">IS MORE PRONOUNCED WITH</data>
</edge>
<edge source="THE IMPROVEMENT" target="MORE COMPLEX DECODING ALGORITHMS">
  <data key="d0">IS MORE PRONOUNCED WITH</data>
</edge>
<edge source="IMPROVEMENTS" target="LONGER SEQUENCES">
  <data key="d0">ARE MORE PRONOUNCED WITH</data>
</edge>
<edge source="IMPROVEMENTS" target="LARGER MODELS">
  <data key="d0">ARE MORE PRONOUNCED WITH</data>
</edge>
<edge source="IMPROVEMENTS" target="MORE COMPLEX DECODING ALGORITHMS">
  <data key="d0">ARE MORE PRONOUNCED WITH</data>
</edge>
<edge source="VLLMS SOURCE CODE" target="PUBLICLY AVAILABLE">
  <data key="d0">IS</data>
</edge>
<edge source="VLLMS SOURCE CODE" target="HTTPS:GITHUB.COMVLLM-PROJECTVLLM">
  <data key="d0">IS AVAILABLE AT</data>
</edge>
<edge source="LARGE LANGUAGE MODELS (LLMS) LIKE GPT 5, 37 AND PALM 9" target="NEW APPLICATIONS SUCH AS PROGRAMMING ASSISTANTS 6, 18 AND UNIVERSAL CHATBOTS 19, 35">
  <data key="d0">HAVE ENABLED</data>
</edge>
<edge source="NEW APPLICATIONS SUCH AS PROGRAMMING ASSISTANTS 6, 18 AND UNIVERSAL CHATBOTS 19, 35" target="OUR WORK AND DAILY ROUTINES">
  <data key="d0">ARE STARTING TO IMPACT</data>
</edge>
<edge source="MANY CLOUD COMPANIES 34, 44" target="THESE APPLICATIONS AS HOSTED SERVICES">
  <data key="d0">ARE RACING TO PROVIDE</data>
</edge>
<edge source="RUNNING THESE APPLICATIONS" target="VERY EXPENSIVE">
  <data key="d0">IS</data>
</edge>
<edge source="RUNNING THESE APPLICATIONS" target="A LARGE NUMBER OF HARDWARE ACCELERATORS SUCH AS GPUS">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="PROCESSING AN LLM REQUEST" target="10 MORE EXPENSIVE THAN A TRADITIONAL KEYWORD QUERY">
  <data key="d0">CAN BE</data>
</edge>
<edge source="THIS WORK" target="A CREATIVE COMMONS ATTRIBUTION INTERNATIONAL 4.0 LICENSE">
  <data key="d0">IS LICENSED UNDER</data>
</edge>
<edge source="SOSP 23" target="OCTOBER 23 26 2023">
  <data key="d0">DATE</data>
</edge>
<edge source="SOSP 23" target="KOBLENZ GERMANY">
  <data key="d0">LOCATION</data>
</edge>
<edge source="COPYRIGHT" target="2023">
  <data key="d0">YEAR</data>
</edge>
<edge source="COPYRIGHT" target="OWNERAUTHOR(S)">
  <data key="d0">HELD BY</data>
</edge>
<edge source="ACM" target="ISBN 979-8-4007-0229-72310">
  <data key="d0">HAS</data>
</edge>
<edge source="NVIDIA A100" target="40GB PARAMETERS">
  <data key="d0">HAS</data>
</edge>
<edge source="KV CACHE" target="26GB">
  <data key="d0">HAS SIZE</data>
</edge>
<edge source="KV CACHE" target="65">
  <data key="d0">HAS SIZE</data>
</edge>
<edge source="KV CACHE" target="UNIQUE CHARACTERISTICS">
  <data key="d0">HAS</data>
</edge>
<edge source="KV CACHE" target="OVER TIME">
  <data key="d0">DYNAMICALLY GROWS AND SHRINKS</data>
</edge>
<edge source="KV CACHE" target="OF TWO REQUESTS">
  <data key="d0">IS STORED</data>
</edge>
<edge source="KV CACHE" target="IN VLLM">
  <data key="d0">IS STORED</data>
</edge>
<edge source="KV CACHE" target="UNSHARED DURING THE AUTOREGRESSIVE GENERATION PHASE">
  <data key="d0">SHOULD REMAIN</data>
</edge>
<edge source="OTHERS" target="20">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="OTHERS" target="30">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="OTHERS" target="40">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="PARAMETER SIZE" target="EXISTING SYSTEMS VLLM">
  <data key="d0">IS ATTRIBUTE OF</data>
</edge>
<edge source="THROUGHPUT" target="TOKENS">
  <data key="d0">IS MEASURED IN</data>
</edge>
<edge source="MEMORY LAYOUT" target="LEFT WHEN SERVING AN LLM WITH 13B PARAMETERS ON NVIDIA A100">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY DISTRIBUTION" target="1 (LEFT)">
  <data key="d0">IS ILLUSTRATED BY</data>
</edge>
<edge source="13B-PARAMETER LLM" target="NVIDIA A100 GPU">
  <data key="d0">RUNS ON</data>
</edge>
<edge source="NVIDIA A100 GPU" target="40GB RAM">
  <data key="d0">HAS</data>
</edge>
<edge source="THE PARAMETERS (GRAY)" target="GPU MEMORY">
  <data key="d0">PERSIST IN</data>
</edge>
<edge source="THE PARAMETERS (GRAY)" target="SERVING">
  <data key="d0">PERSIST THROUGHOUT</data>
</edge>
<edge source="GPU MEMORY" target="80GB MAXIMUM">
  <data key="d0">STAYS AT</data>
</edge>
<edge source="THE MEMORY FOR THE KV CACHE (RED)" target="PER SERVING REQUEST">
  <data key="d0">IS (DE)ALLOCATED</data>
</edge>
<edge source="A CONTIGUOUS CHUNK OF MEMORY" target="THE REQUESTS MAXIMUM LENGTH">
  <data key="d0">HAS</data>
</edge>
<edge source="THE REQUESTS MAXIMUM LENGTH" target="2048 TOKENS">
  <data key="d0">IS</data>
</edge>
<edge source="ALL AVAILABLE MEMORY" target="KV CACHE">
  <data key="d0">WAS ALLOCATED TO</data>
</edge>
<edge source="OUTPUT LENGTH OF A REQUEST" target="DECODING">
  <data key="d0">GROWS AT</data>
</edge>
<edge source="DECODING" target="PAGEDATTENTION AND VLLM">
  <data key="d0">USES</data>
</edge>
<edge source="MEMORY REQUIRED FOR ITS KV CACHE" target="AS THE OUTPUT LENGTH OF A REQUEST GROWS AT DECODING">
  <data key="d0">EXPANDS</data>
</edge>
<edge source="MEMORY REQUIRED FOR ITS KV CACHE" target="AVAILABLE MEMORY FOR INCOMING REQUESTS OR ONGOING GENERATION FOR EXISTING PROMPTS">
  <data key="d0">MAY EXHAUST</data>
</edge>
<edge source="A SMALL AMOUNT OF MEMORY (YELLOW)" target="EPHEMERALLY FOR ACTIVATION">
  <data key="d0">IS USED</data>
</edge>
<edge source="KEY IDEA BEHIND VLLMS MEMORY MANAGER" target="VIRTUAL MEMORY 25 IN OPERATING SYSTEMS">
  <data key="d0">IS ANALOGOUS TO</data>
</edge>
<edge source="COST PER REQUEST OF LLM SERVING SYSTEMS" target="MORE IMPORTANT">
  <data key="d0">IS BECOMING</data>
</edge>
<edge source="LLMS" target="AN AUTOREGRESSIVE TRANSFORMER MODEL 53">
  <data key="d0">LIES AT THE CORE OF</data>
</edge>
<edge source="LLMS" target="CONDITIONAL GENERATION SERVICE">
  <data key="d0">ARE DEPLOYED AS</data>
</edge>
<edge source="THIS MODEL" target="WORDS (TOKENS)">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THIS MODEL" target="ONE AT A TIME">
  <data key="d0">GENERATES WORDS</data>
</edge>
<edge source="THIS MODEL" target="BASED ON THE INPUT (PROMPT)">
  <data key="d0">GENERATES WORDS</data>
</edge>
<edge source="THIS MODEL" target="BASED ON THE PREVIOUS SEQUENCE OF THE OUTPUTS TOKENS IT HAS GENERATED SO FAR">
  <data key="d0">GENERATES WORDS</data>
</edge>
<edge source="THIS EXPENSIVE PROCESS" target="FOR EACH REQUEST">
  <data key="d0">IS REPEATED</data>
</edge>
<edge source="THIS EXPENSIVE PROCESS" target="THE MODEL OUTPUTS A TERMINATION TOKEN">
  <data key="d0">IS REPEATED UNTIL</data>
</edge>
<edge source="SEQUENTIAL GENERATION PROCESS" target="WORKLOAD MEMORY-BOUND">
  <data key="d0">MAKES</data>
</edge>
<edge source="SEQUENTIAL GENERATION PROCESS" target="COMPUTATION POWER OF GPUS">
  <data key="d0">UNDERUTILIZES</data>
</edge>
<edge source="SEQUENTIAL GENERATION PROCESS" target="SERVING THROUGHPUT">
  <data key="d0">LIMITS</data>
</edge>
<edge source="IMPROVING THE THROUGHPUT" target="BATCHING MULTIPLE REQUESTS TOGETHER">
  <data key="d0">IS POSSIBLE BY</data>
</edge>
<edge source="MEMORY SPACE FOR EACH REQUEST" target="EFFICIENTLY MANAGED">
  <data key="d0">SHOULD BE</data>
</edge>
<edge source="APPROXIMATELY 65 OF THE MEMORY" target="THE MODEL WEIGHTS">
  <data key="d0">IS ALLOCATED FOR</data>
</edge>
<edge source="THE MODEL WEIGHTS" target="STATIC DURING SERVING">
  <data key="d0">REMAIN</data>
</edge>
<edge source="CLOSE TO 30 OF THE MEMORY" target="THE DYNAMIC STATES OF THE REQUESTS">
  <data key="d0">IS USED TO STORE</data>
</edge>
<edge source="STATES" target="KEY AND VALUE TENSORS">
  <data key="d0">CONSIST OF</data>
</edge>
<edge source="KEY AND VALUE TENSORS" target="ATTENTION MECHANISM">
  <data key="d0">ARE ASSOCIATED WITH</data>
</edge>
<edge source="KEY AND VALUE TENSORS" target="KV CACHE 41">
  <data key="d0">ARE COMMONLY REFERRED TO AS</data>
</edge>
<edge source="KV CACHE 41" target="CONTEXT FROM EARLIER TOKENS">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="CONTEXT FROM EARLIER TOKENS" target="NEW OUTPUT TOKENS IN SEQUENCE">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THE REMAINING SMALL EQUAL CONTRIBUTION" target="UNKNOWN">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA (MAX)" target="20.4">
  <data key="d0">HAS KV CACHE USAGE</data>
</edge>
<edge source="ORCA (MAX)" target="26.8">
  <data key="d0">HAS TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (MAX)" target="38.2">
  <data key="d0">HAS INTERNAL FRAGMENTATION</data>
</edge>
<edge source="ORCA (MAX)" target="OPT MODEL">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="ORCA (MAX)" target="7.00 FOR SHAREGPT">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (MAX)" target="43.24 FOR ALPACA">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (POW2)" target="13.3">
  <data key="d0">HAS KV CACHE USAGE</data>
</edge>
<edge source="ORCA (POW2)" target="17.9">
  <data key="d0">HAS TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (POW2)" target="25.2">
  <data key="d0">HAS INTERNAL FRAGMENTATION</data>
</edge>
<edge source="ORCA (POW2)" target="OPT MODEL">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="ORCA (POW2)" target="9.81 FOR SHAREGPT">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (POW2)" target="72.75 FOR ALPACA">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (ORACLE)" target="57.3">
  <data key="d0">HAS KV CACHE USAGE</data>
</edge>
<edge source="ORCA (ORACLE)" target="13.6">
  <data key="d0">HAS TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (ORACLE)" target="36.6">
  <data key="d0">HAS INTERNAL FRAGMENTATION</data>
</edge>
<edge source="ORCA (ORACLE)" target="OPT MODEL">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="ORCA (ORACLE)" target="13.62 FOR SHAREGPT">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (ORACLE)" target="132.44 FOR ALPACA">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="AVERAGE PERCENTAGE OF MEMORY" target="DIFFERENT LLM SERVING SYSTEMS">
  <data key="d0">WASTES IN</data>
</edge>
<edge source="EXPERIMENT" target="6.2">
  <data key="d0">IS IN</data>
</edge>
<edge source="PERCENTAGE OF MEMORY" target="OTHER DATA">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="OTHER DATA" target="ACTIVATIONS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ACTIVATIONS" target="EPHEMERAL TENSORS">
  <data key="d0">ARE</data>
</edge>
<edge source="ACTIVATIONS" target="A SMALL FRACTION OF THE GPU MEMORY">
  <data key="d0">OCCUPY</data>
</edge>
<edge source="EPHEMERAL TENSORS" target="EVALUATING THE LLM">
  <data key="d0">ARE CREATED WHEN</data>
</edge>
<edge source="MODEL WEIGHTS" target="CONSTANT">
  <data key="d0">ARE</data>
</edge>
<edge source="THE WAY THE KV CACHE IS MANAGED" target="CRITICAL IN DETERMINING THE MAXIMUM BATCH SIZE">
  <data key="d0">IS</data>
</edge>
<edge source="LIMITATION OF BATCH SIZE AND THROUGHPUT" target="INEFFICIENT MANAGEMENT OF KV CACHE MEMORY">
  <data key="d0">IS CAUSED BY</data>
</edge>
<edge source="FINE-GRAINED BATCHING" target="THE WASTE OF COMPUTING">
  <data key="d0">REDUCES</data>
</edge>
<edge source="FINE-GRAINED BATCHING" target="REQUESTS TO BE BATCHED IN A MORE FLEXIBLE WAY">
  <data key="d0">ENABLES</data>
</edge>
<edge source="THE NUMBER OF REQUESTS THAT CAN BE BATCHED TOGETHER" target="GPU MEMORY CAPACITY">
  <data key="d0">IS CONSTRAINED BY</data>
</edge>
<edge source="GPU MEMORY CAPACITY" target="THE SPACE TO STORE THE KV CACHE">
  <data key="d0">PARTICULARLY ALLOCATES</data>
</edge>
<edge source="THE IDEA OF VIRTUAL MEMORY AND PAGING" target="MANAGING THE KV CACHE IN LLM SERVING">
  <data key="d0">IS EFFECTIVE FOR</data>
</edge>
<edge source="THE WORKLOAD" target="DYNAMIC MEMORY ALLOCATION">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="THE OUTPUT LENGTH" target="NOT KNOWN A PRIORI">
  <data key="d0">IS</data>
</edge>
<edge source="ITS PERFORMANCE" target="THE GPU MEMORY CAPACITY">
  <data key="d0">IS BOUND BY</data>
</edge>
<edge source="THE KV CACHE OF A REQUEST" target="CONTIGUOUS MEMORY SPACE">
  <data key="d0">IS STORED IN</data>
</edge>
<edge source="MOST DEEP LEARNING FRAMEWORKS 33, 39" target="TENSORS TO BE STORED IN CONTIGUOUS MEMORY">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="OPERATORS IN CURRENT DEEP LEARNING FRAMEWORKS" target="TENSORS TO BE STORED IN CONTIGUOUS MEMORY">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="PREVIOUS LLM SERVING SYSTEMS" target="THE KV CACHE OF ONE REQUEST AS A CONTIGUOUS TENSOR ACROSS THE DIFFERENT POSITIONS">
  <data key="d0">STORE</data>
</edge>
<edge source="PREVIOUS LLM SERVING SYSTEMS" target="FREQUENT MEMORY COPIES OF THE KV CACHE ACROSS THE BEAM CANDIDATES">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="MODEL" target="NEW TOKENS">
  <data key="d0">GENERATES</data>
</edge>
<edge source="LIFETIME AND LENGTH OF KV CACHE" target="NOT KNOWN A PRIORI">
  <data key="d0">ARE</data>
</edge>
<edge source="THE EXISTING SYSTEMS" target="INTERNAL AND EXTERNAL MEMORY FRAGMENTATION">
  <data key="d0">SUFFER FROM</data>
</edge>
<edge source="THE EXISTING SYSTEMS" target="THE OPPORTUNITIES FOR MEMORY SHARING">
  <data key="d0">CANNOT EXPLOIT</data>
</edge>
<edge source="THE REQUESTS ACTUAL LENGTH" target="MUCH SHORTER THAN ITS MAXIMUM LENGTH">
  <data key="d0">CAN BE</data>
</edge>
<edge source="PRE-ALLOCATION" target="INEFFICIENT">
  <data key="d0">IS</data>
</edge>
<edge source="ENTIRE CHUNK" target="REQUESTS LIFETIME">
  <data key="d0">IS RESERVED DURING</data>
</edge>
<edge source="OTHER SHORTER REQUESTS" target="ANY PART OF THE CHUNK THAT IS CURRENTLY UNUSED">
  <data key="d0">CANNOT UTILIZE</data>
</edge>
<edge source="ONLY 20.4 - 38.2 OF THE KV CACHE MEMORY" target="THE ACTUAL TOKEN STATES">
  <data key="d0">IS USED TO STORE</data>
</edge>
<edge source="THE ACTUAL TOKEN STATES" target="THE EXISTING SYSTEMS">
  <data key="d0">ARE STORED IN</data>
</edge>
<edge source="KV CACHE OF ONE TOKEN" target="ALL ITS PREVIOUS TOKENS">
  <data key="d0">DEPENDS ON</data>
</edge>
<edge source="THE KV CACHE OF THE SAME TOKEN" target="DIFFERENT">
  <data key="d0">WILL BE</data>
</edge>
<edge source="THE SAME TOKEN" target="DIFFERENT POSITIONS IN A SEQUENCE">
  <data key="d0">APPEARS AT</data>
</edge>
<edge source="THE TOKEN IN EACH MEMORY SLOT" target="ITS KV CACHE">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="THE SAME TOKENS" target="DIFFERENT KV CACHE">
  <data key="d0">CAN HAVE</data>
</edge>
<edge source="THE SAME TOKENS" target="AT DIFFERENT POSITIONS">
  <data key="d0">ARE</data>
</edge>
<edge source="LLM SERVICES" target="ADVANCED DECODING ALGORITHMS">
  <data key="d0">USE</data>
</edge>
<edge source="LLM SERVICES" target="A RANGE OF DECODING ALGORITHMS">
  <data key="d0">OFFER</data>
</edge>
<edge source="LLM SERVICES" target="A UNIQUE CHALLENGE">
  <data key="d0">FACE</data>
</edge>
<edge source="ADVANCED DECODING ALGORITHMS" target="PARALLEL SAMPLING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ADVANCED DECODING ALGORITHMS" target="BEAM SEARCH">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ADVANCED DECODING ALGORITHMS" target="MULTIPLE OUTPUTS PER REQUEST">
  <data key="d0">GENERATE</data>
</edge>
<edge source="PARALLEL SAMPLING" target="EXAMPLE">
  <data key="d0">IS</data>
</edge>
<edge source="PARALLEL SAMPLING" target="OUTPUT SEQUENCES">
  <data key="d0">ASSOCIATED WITH</data>
</edge>
<edge source="BEAM SEARCH" target="EXAMPLE">
  <data key="d0">IS</data>
</edge>
<edge source="BEAM SEARCH" target="EACH CANDIDATE SEQUENCE IN THE BEAM">
  <data key="d0">EXPANDS</data>
</edge>
<edge source="BEAM SEARCH" target="ALL POSSIBLE TOKENS">
  <data key="d0">CONSIDERS</data>
</edge>
<edge source="BEAM SEARCH" target="THEIR RESPECTIVE PROBABILITIES USING THE LLM">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="BEAM SEARCH" target="THE TOP-MOST PROBABLE SEQUENCES">
  <data key="d0">RETAINS</data>
</edge>
<edge source="BEAM SEARCH" target="SHARING NOT ONLY THE INITIAL PROMPT BLOCKS BUT ALSO OTHER BLOCKS ACROSS DIFFERENT CANDIDATES">
  <data key="d0">FACILITIES</data>
</edge>
<edge source="BEAM SEARCH" target="VLLM">
  <data key="d0">IS APPLIED BY</data>
</edge>
<edge source="BEAM SEARCH" target="2">
  <data key="d0">HAS BEAM WIDTH</data>
</edge>
<edge source="BEAM SEARCH" target="FIGURE 15">
  <data key="d0">REFERENCED IN</data>
</edge>
<edge source="DECODING ALGORITHMS" target="USERS TO SELECT FROM">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="DECODING ALGORITHMS" target="VARYING IMPLICATIONS FOR MEMORY MANAGEMENT COMPLEXITY">
  <data key="d0">HAVE</data>
</edge>
<edge source="LLM SERVICE" target="MORE COMPLEX DECODING SCENARIOS">
  <data key="d0">MUST OFFER</data>
</edge>
<edge source="LLM SERVICE" target="MORE OPPORTUNITIES FOR MEMORY SHARING">
  <data key="d0">MUST OFFER</data>
</edge>
<edge source="MORE COMPLEX DECODING SCENARIOS" target="COMPLEX ACCESSING PATTERNS">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="THE REQUEST" target="MULTIPLE SEQUENCES">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="MULTIPLE SEQUENCES" target="THEIR KV CACHE">
  <data key="d0">CAN PARTIALLY SHARE</data>
</edge>
<edge source="TWO REQUESTS" target="AT THE SAME TIME">
  <data key="d0">ARE STORED</data>
</edge>
<edge source="REQUEST" target="ITS GENERATION">
  <data key="d0">FINISHES</data>
</edge>
<edge source="ITS KV BLOCKS" target="THE KV CACHE OF OTHER REQUESTS">
  <data key="d0">CAN BE FREED TO STORE</data>
</edge>
<edge source="MEMORY SHARING" target="THE EXISTING SYSTEMS">
  <data key="d0">IS NOT POSSIBLE IN</data>
</edge>
<edge source="MEMORY SHARING" target="THE DIFFERENT SEQUENCES ASSOCIATED WITH THE SAME REQUEST">
  <data key="d0">OCCURS ACROSS</data>
</edge>
<edge source="MEMORY SHARING" target="THE DIFFERENT REQUESTS">
  <data key="d0">OCCURS ACROSS</data>
</edge>
<edge source="MEMORY SHARING" target="PAGE-DATTENTION">
  <data key="d0">IS USED IN</data>
</edge>
<edge source="THE KV CACHE OF THE SEQUENCES" target="SEPARATE CONTIGUOUS SPACES">
  <data key="d0">IS STORED IN</data>
</edge>
<edge source="EACH BLOCK" target="THE ATTENTION KEYS AND VALUES OF A FIXED NUMBER OF TOKENS">
  <data key="d0">CAN CONTAIN</data>
</edge>
<edge source="EACH BLOCK" target="THE KEY AND VALUE VECTORS FOR A FIXED NUMBER OF TOKENS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="PAGEDATTENTION KERNEL" target="DIFFERENT KV BLOCKS">
  <data key="d0">FETCHES</data>
</edge>
<edge source="PAGEDATTENTION KERNEL" target="PREVIOUS KV CACHE">
  <data key="d0">ACCESSES</data>
</edge>
<edge source="BLOCKS FOR THE KV CACHE" target="CONTIGUOUS SPACE">
  <data key="d0">ARE NOT NECESSARILY STORED IN</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="THE KV CACHE">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="A PAGED FASHION">
  <data key="d0">MANAGES IN</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="THE PHYSICAL KV CACHE MEMORY ON THE GPU WORKERS">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="THE INSTRUCTIONS SENT BY THE CENTRALIZED SCHEDULER">
  <data key="d0">MANAGES THROUGH</data>
</edge>
<edge source="THE KV CACHE" target="PAGEDATTENTION">
  <data key="d0">IS ENABLED BY</data>
</edge>
<edge source="THE KV CACHE" target="WHEN THE PREEMPTED SEQUENCES ARE RESCHEDULED">
  <data key="d0">IS RECOMPUTED</data>
</edge>
<edge source="THE DESIGN OF THE KV CACHE MANAGER" target="PAGEDATTENTION IN 4.3">
  <data key="d0">FACILITATES</data>
</edge>
<edge source="KV BLOCKS" target="PAGES IN VIRTUAL MEMORY">
  <data key="d0">ARE LIKE</data>
</edge>
<edge source="THIS DESIGN" target="INTERNAL FRAGMENTATION">
  <data key="d0">ALLEVIATES</data>
</edge>
<edge source="THIS DESIGN" target="RELATIVELY SMALL BLOCKS">
  <data key="d0">USES</data>
</edge>
<edge source="THIS DESIGN" target="THEM ON DEMAND">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THIS DESIGN" target="EFFECTIVE MEMORY MANAGEMENT FOR VARIOUS DECODING METHODS">
  <data key="d0">FACILITATES</data>
</edge>
<edge source="THIS DESIGN" target="VARIABLE LENGTH INPUT AND OUTPUT SEQUENCES">
  <data key="d0">HANDLES</data>
</edge>
<edge source="INTERNAL FRAGMENTATION" target="UNUSED">
  <data key="d0">REMAINS</data>
</edge>
<edge source="ALL BLOCKS" target="THE SAME SIZE">
  <data key="d0">HAVE</data>
</edge>
<edge source="BLOCK-LEVEL MEMORY MANAGEMENT" target="PAGEDATTENTION">
  <data key="d0">ARE CO-DESIGNED WITH</data>
</edge>
<edge source="BLOCK-LEVEL MEMORY MANAGEMENT" target="ONLINE SERVING">
  <data key="d0">IS IN THE CONTEXT OF</data>
</edge>
<edge source="PREEMPTIVE REQUEST SCHEDULING" target="PAGEDATTENTION">
  <data key="d0">ARE CO-DESIGNED WITH</data>
</edge>
<edge source="PAGEDATTENTION ALGORITHM" target="KV BLOCKS TO BE STORED IN NON-CONTIGUOUS PHYSICAL MEMORY">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="NON-CONTIGUOUS PHYSICAL MEMORY" target="MORE FLEXIBLE PAGED MEMORY MANAGEMENT IN VLLM">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PREVIOUS KV CACHE" target="LOGICAL KV BLOCKS">
  <data key="d0">IS STORED IN</data>
</edge>
<edge source="LOGICAL KV BLOCKS" target="LEFT TO RIGHT">
  <data key="d0">ARE FILLED FROM</data>
</edge>
<edge source="LOGICAL KV BLOCKS" target="NEW TOKENS AND THEIR KV CACHE ARE GENERATED">
  <data key="d0">ARE FILLED AS</data>
</edge>
<edge source="NEWLY GENERATED KV CACHE" target="PHYSICAL KV BLOCKS">
  <data key="d0">IS SAVED INTO</data>
</edge>
<edge source="MEMORY" target="AN INCREASINGLY SIGNIFICANT BOTTLENECK">
  <data key="d0">WILL BECOME</data>
</edge>
<edge source="POPULAR LLMS" target="GPT 5">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="POPULAR LLMS" target="OPT 62">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="POPULAR LLMS" target="LLAMA 52">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="POPULAR LLMS" target="VARYING SIZES">
  <data key="d0">HAVE</data>
</edge>
<edge source="LLAMA 52" target="13B PARAMETERS">
  <data key="d0">HAS</data>
</edge>
<edge source="VARYING SIZES" target="ONES EXCEEDING THE MEMORY CAPACITY OF A SINGLE GPU">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="PREVIOUS STATE-OF-THE-ART SOLUTIONS" target="FASTERTRANSFORMER 31">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="PREVIOUS STATE-OF-THE-ART SOLUTIONS" target="ORCA 60">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FASTERTRANSFORMER 31" target="A DISTRIBUTED INFERENCE ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="FASTERTRANSFORMER 31" target="HIGHLY OPTIMIZED FOR LATENCY">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA 60" target="STATE-OF-THE-ART LLM SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA 60" target="THROUGHPUT">
  <data key="d0">IS OPTIMIZED FOR</data>
</edge>
<edge source="ORCA 60" target="MOST RELEVANT TO OUR APPROACH">
  <data key="d0">IS</data>
</edge>
<edge source="FASTERTRANSFORMER" target="A FINE-GRAINED SCHEDULING MECHANISM">
  <data key="d0">DOES NOT UTILIZE</data>
</edge>
<edge source="FASTERTRANSFORMER" target="THE MEMORY LIKE ORCA (MAX)">
  <data key="d0">INEFFICIENTLY MANAGES</data>
</edge>
<edge source="FASTERTRANSFORMER" target="ITS OWN SCHEDULER">
  <data key="d0">DOES NOT HAVE</data>
</edge>
<edge source="SHARING" target="VLLM">
  <data key="d0">OCCURS IN</data>
</edge>
<edge source="2-4 SPEEDUP" target="ORCA">
  <data key="d0">IS COMPARED TO</data>
</edge>
<edge source="ORCA" target="MAX">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA" target="POW2">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA" target="ORACLE">
  <data key="d0">HAS VARIANTS</data>
</edge>
<edge source="ORCA" target="PUBLICLY AVAILABLE FOR USE">
  <data key="d0">IS NOT</data>
</edge>
<edge source="ORCA" target="BUDDY ALLOCATION ALGORITHM">
  <data key="d0">USES</data>
</edge>
<edge source="ORCA" target="A DISTRIBUTED SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA" target="TRANSFORMER-BASED GENERATIVE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THIS SECTION" target="THE GENERATION AND SERVING PROCEDURES OF TYPICAL LLMS">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="THIS SECTION" target="THE ITERATION-LEVEL SCHEDULING USED IN LLM SERVING">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="THE TASK OF LANGUAGE MODELING" target="THE PROBABILITY OF A LIST OF TOKENS">
  <data key="d0">IS TO MODEL</data>
</edge>
<edge source="LANGUAGE" target="A NATURAL SEQUENTIAL ORDERING">
  <data key="d0">HAS</data>
</edge>
<edge source="TRANSFORMERS 53" target="THE DE FACTO STANDARD ARCHITECTURE FOR MODELING THE PROBABILITY ABOVE AT A LARGE SCALE">
  <data key="d0">HAVE BECOME</data>
</edge>
<edge source="THE MOST IMPORTANT COMPONENT OF A TRANSFORMER-BASED LANGUAGE MODEL" target="ITS SELF-ATTENTION LAYERS">
  <data key="d0">IS</data>
</edge>
<edge source="A SELF-ATTENTION LAYER" target="LINEAR TRANSFORMATIONS ON EACH POSITION">
  <data key="d0">APPLIES</data>
</edge>
<edge source="A SELF-ATTENTION LAYER" target="THE QUERY VECTOR">
  <data key="d0">GETS</data>
</edge>
<edge source="A SELF-ATTENTION LAYER" target="THE KEY VECTOR">
  <data key="d0">GETS</data>
</edge>
<edge source="A SELF-ATTENTION LAYER" target="THE VALUE VECTOR">
  <data key="d0">GETS</data>
</edge>
<edge source="THE SELF-ATTENTION LAYER" target="THE ATTENTION SCORE">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="THE SELF-ATTENTION LAYER" target="THE OUTPUT">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="THE ATTENTION SCORE" target="MULTIPLYING THE QUERY VECTOR AT ONE POSITION WITH ALL THE KEY VECTORS BEFORE IT">
  <data key="d0">IS COMPUTED BY</data>
</edge>
<edge source="THE OUTPUT" target="THE WEIGHTED AVERAGE OVER THE VALUE VECTORS">
  <data key="d0">IS</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE TRANSFORMER MODEL">
  <data key="d0">ARE IN</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE EMBEDDING LAYER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="FEED-FORWARD LAYER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="LAYER NORMALIZATION 2">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="RESIDUAL CONNECTION 22">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="OUTPUT LOGIT COMPUTATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE QUERY TRANSFORMATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE KEY TRANSFORMATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE VALUE TRANSFORMATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CONDITIONAL GENERATION SERVICE" target="COMPLETION API 34">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="CONDITIONAL GENERATION SERVICE" target="CHATBOT 19, 35">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="A REQUEST TO AN LLM SERVICE" target="A LIST OF INPUT PROMPT TOKENS">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="DECOMPOSITION" target="IN EQ">
  <data key="d0">OCCURS</data>
</edge>
<edge source="THE LLM" target="NEW TOKENS ONE BY ONE">
  <data key="d0">CAN ONLY SAMPLE AND GENERATE</data>
</edge>
<edge source="THE GENERATION PROCESS OF EACH NEW TOKEN" target="ALL THE PREVIOUS TOKENS IN THAT SEQUENCE">
  <data key="d0">DEPENDS ON</data>
</edge>
<edge source="ALL THE PREVIOUS TOKENS IN THAT SEQUENCE" target="KEY AND VALUE VECTORS">
  <data key="d0">HAVE</data>
</edge>
<edge source="KEY AND VALUE VECTORS OF EXISTING TOKENS" target="GENERATING FUTURE TOKENS">
  <data key="d0">ARE OFTEN CACHED FOR</data>
</edge>
<edge source="GENERATING FUTURE TOKENS" target="KV CACHE">
  <data key="d0">IS KNOWN AS</data>
</edge>
<edge source="A REQUESTS KV CACHE" target="A SERIES OF LOGICAL KV BLOCKS">
  <data key="d0">IS REPRESENTED AS</data>
</edge>
<edge source="GENERATION COMPUTATION IN THE LLM SERVICE" target="TWO PHASES">
  <data key="d0">CAN BE DECOMPOSED INTO</data>
</edge>
<edge source="THE PROMPT PHASE" target="THE WHOLE USER PROMPT">
  <data key="d0">TAKES</data>
</edge>
<edge source="REQUESTS AND THE LATEST TOKENS FOR GENERATION PHASE REQUESTS" target="THE LLM AS ONE SEQUENCE">
  <data key="d0">ARE FED INTO</data>
</edge>
<edge source="THIS PROCESS" target="THE KEY VECTORS 1">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THE COMPUTATION OF THE PROMPT PHASE" target="MATRIX-MATRIX MULTIPLICATION OPERATIONS">
  <data key="d0">CAN BE PARALLELIZED USING</data>
</edge>
<edge source="THIS PHASE" target="PARALLELISM INHERENT IN GPUS">
  <data key="d0">CAN USE</data>
</edge>
<edge source="THIS PHASE" target="WHEN THE SEQUENCE REACHES A MAXIMUM LENGTH">
  <data key="d0">COMPLETES</data>
</edge>
<edge source="THIS PHASE" target="WHEN AN END-OF-SEQUENCE (EOS) TOKEN IS EMITTED">
  <data key="d0">COMPLETES</data>
</edge>
<edge source="THIS PHASE" target="GPU COMPUTATION">
  <data key="d0">UNDERUTILIZES</data>
</edge>
<edge source="THIS PHASE" target="MEMORY-BOUND">
  <data key="d0">BECOMES</data>
</edge>
<edge source="THIS PHASE" target="MOST PORTION OF THE LATENCY OF A SINGLE REQUEST">
  <data key="d0">IS RESPONSIBLE FOR</data>
</edge>
<edge source="THE AUTOREGRESSIVE GENERATION PHASE" target="THE REMAINING NEW TOKENS SEQUENTIALLY">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THE MODEL" target="ONE TOKEN AS INPUT">
  <data key="d0">TAKES</data>
</edge>
<edge source="THE MODEL" target="A RESPONSE">
  <data key="d0">GENERATE</data>
</edge>
<edge source="KEY AND VALUE VECTORS AT POSITIONS 1 TO 1" target="CACHED AT PREVIOUS ITERATIONS">
  <data key="d0">ARE</data>
</edge>
<edge source="NEW KEY AND VALUE VECTOR" target="COMPUTED AT THIS ITERATION">
  <data key="d0">ARE</data>
</edge>
<edge source="MAXIMUM LENGTH" target="USERS">
  <data key="d0">IS SPECIFIED BY</data>
</edge>
<edge source="MAXIMUM LENGTH" target="LLMS">
  <data key="d0">IS LIMITED BY</data>
</edge>
<edge source="USERS" target="MULTIPLE RANDOM SAMPLES FROM A SINGLE INPUT PROMPT">
  <data key="d0">REQUEST</data>
</edge>
<edge source="USERS" target="A FAVORITE OUTPUT FROM VARIOUS CANDIDATES">
  <data key="d0">CAN CHOOSE</data>
</edge>
<edge source="USERS" target="TOP-MOST APPROPRIATE TRANSLATIONS OUTPUT BY THE LLM">
  <data key="d0">EXPECT</data>
</edge>
<edge source="THE COMPUTATION AT DIFFERENT ITERATIONS" target="DUE TO THE DATA DEPENDENCY">
  <data key="d0">CANNOT BE PARALLELIZED</data>
</edge>
<edge source="THE COMPUTATION AT DIFFERENT ITERATIONS" target="MATRIX-VECTOR MULTIPLICATION">
  <data key="d0">OFTEN USES</data>
</edge>
<edge source="MATRIX-VECTOR MULTIPLICATION" target="LESS EFFICIENT">
  <data key="d0">IS</data>
</edge>
<edge source="COMPUTE UTILIZATION IN SERVING LLMS" target="BATCHING MULTIPLE REQUESTS">
  <data key="d0">CAN BE IMPROVED BY</data>
</edge>
<edge source="BATCHING THE REQUESTS TO AN LLM SERVICE" target="NON-TRIVIAL">
  <data key="d0">IS</data>
</edge>
<edge source="OVERHEAD OF MOVING WEIGHTS" target="REQUESTS IN A BATCH">
  <data key="d0">IS AMORTIZED ACROSS</data>
</edge>
<edge source="OVERHEAD OF MOVING WEIGHTS" target="COMPUTATIONAL OVERHEAD">
  <data key="d0">CAN BE OVERWHELMED BY</data>
</edge>
<edge source="A NAIVE BATCHING STRATEGY" target="EARLIER REQUESTS WAIT FOR LATER ONES">
  <data key="d0">WOULD MAKE</data>
</edge>
<edge source="A NAIVE BATCHING STRATEGY" target="THE INCOMING REQUESTS UNTIL EARLIER ONES FINISH">
  <data key="d0">WOULD DELAY</data>
</edge>
<edge source="A NAIVE BATCHING STRATEGY" target="SIGNIFICANT QUEUEING DELAYS">
  <data key="d0">LEADS TO</data>
</edge>
<edge source="A STRAIGHTFORWARD BATCHING TECHNIQUE" target="THE INPUTS AND OUTPUTS OF THE REQUESTS">
  <data key="d0">WOULD PAD</data>
</edge>
<edge source="A STRAIGHTFORWARD BATCHING TECHNIQUE" target="EQUALIZE THEIR LENGTHS">
  <data key="d0">WOULD PAD TO</data>
</edge>
<edge source="A STRAIGHTFORWARD BATCHING TECHNIQUE" target="GPU COMPUTATION AND MEMORY">
  <data key="d0">WASTES</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="CELLULAR BATCHING 16">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="ITERATION-LEVEL SCHEDULING 60">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="PROPOSED">
  <data key="d0">HAVE BEEN</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="QUEUEING DELAY">
  <data key="d0">REDUCE</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="INEFFICIENCIES FROM PADDING">
  <data key="d0">REDUCE</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="THROUGHPUT OF LLM SERVING">
  <data key="d0">INCREASE</data>
</edge>
<edge source="THESE TECHNIQUES" target="THE ITERATION LEVEL">
  <data key="d0">OPERATE AT</data>
</edge>
<edge source="TRADITIONAL METHODS" target="THE REQUEST LEVEL">
  <data key="d0">WORK AT</data>
</edge>
<edge source="COMPLETED REQUESTS" target="THE BATCH">
  <data key="d0">ARE REMOVED FROM</data>
</edge>
<edge source="A NEW REQUEST" target="WAITING FOR A SINGLE ITERATION">
  <data key="d0">CAN BE PROCESSED AFTER</data>
</edge>
<edge source="A NEW REQUEST" target="WAITING FOR THE ENTIRE BATCH TO COMPLETE">
  <data key="d0">CANNOT BE PROCESSED AFTER</data>
</edge>
<edge source="SPECIAL GPU KERNELS" target="THE NEED TO PAD THE INPUTS AND OUTPUTS">
  <data key="d0">ELIMINATE</data>
</edge>
<edge source="THREE TYPES OF MEMORY WASTES" target="RESERVED, INTERNAL FRAGMENTATION, AND EXTERNAL FRAGMENTATION">
  <data key="d0">ARE</data>
</edge>
<edge source="THREE TYPES OF MEMORY WASTES" target="THAT PREVENT OTHER REQUESTS FROM FITTING INTO THE MEMORY">
  <data key="d0">EXIST</data>
</edge>
<edge source="THE SERVING SYSTEMS THROUGHPUT" target="MEMORY-BOUND">
  <data key="d0">IS</data>
</edge>
<edge source="THE PERFORMANCE OF THE SYSTEMS" target="COMPUTE-BOUND RATHER THAN MEMORY-BOUND">
  <data key="d0">BECOMES</data>
</edge>
<edge source="OVERCOMING THIS MEMORY-BOUND" target="ADDRESSING THE FOLLOWING CHALLENGES IN THE MEMORY MANAGEMENT">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="CHALLENGES" target="LARGE KV CACHE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="OPT" target="SEQUENCES UP TO 2048 TOKENS">
  <data key="d0">CAN GENERATE</data>
</edge>
<edge source="OPT" target="OPEN PRE-TRAINED TRANSFORMER LANGUAGE MODELS">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY REQUIRED TO STORE THE KV CACHE OF ONE REQUEST" target="1.6 GB">
  <data key="d0">CAN BE AS MUCH AS</data>
</edge>
<edge source="CONCURRENT GPUS" target="MEMORY CAPACITIES IN THE TENS OF GBS">
  <data key="d0">HAVE</data>
</edge>
<edge source="GPU'S COMPUTATION SPEED" target="MEMORY CAPACITY">
  <data key="d0">GROWS FASTER THAN</data>
</edge>
<edge source="FLOPS" target="NVIDIA A100 TO H100">
  <data key="d0">INCREASES FROM</data>
</edge>
<edge source="FLOPS" target="MORE THAN 2X">
  <data key="d0">INCREASES BY</data>
</edge>
<edge source="COMPLEX DECODING ALGORITHMS" target="ALGORITHMS">
  <data key="d0">IS</data>
</edge>
<edge source="KV CACHE OF THE PROMPT PART" target="12 OF THE TOTAL KV CACHE MEMORY IN OUR EXPERIMENT">
  <data key="d0">ACCOUNTS FOR</data>
</edge>
<edge source="KV CACHE OF THE PROMPT PART" target="MINIMIZE MEMORY USAGE">
  <data key="d0">CAN BE SHARED TO</data>
</edge>
<edge source="DIFFERENT SAMPLE RESULTS" target="CONTEXT AND POSITION">
  <data key="d0">DEPEND ON</data>
</edge>
<edge source="THE EXTENT OF KV CACHE SHARING" target="THE SPECIFIC DECODING ALGORITHM EMPLOYED">
  <data key="d0">DEPENDS ON</data>
</edge>
<edge source="DIFFERENT REQUEST BEAMS" target="LARGER PORTIONS OF THEIR KV CACHE">
  <data key="d0">CAN SHARE</data>
</edge>
<edge source="LARGER PORTIONS" target="55 MEMORY SAVING">
  <data key="d0">CAN BE UP TO</data>
</edge>
<edge source="SHARING PATTERN" target="THE DECODING PROCESS ADVANCES">
  <data key="d0">EVOLVES AS</data>
</edge>
<edge source="BEAM SEARCH 49" target="MORE SOPHISTICATED ALGORITHMS">
  <data key="d0">IS AN EXAMPLE OF</data>
</edge>
<edge source="SCHEDULING" target="UNKNOWN INPUT OUTPUT LENGTHS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="SCHEDULING" target="SERVING SINGLE OR MULTIPLE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE REQUESTS TO AN LLM SERVICE" target="VARIABILITY IN THEIR INPUT AND OUTPUT LENGTHS">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="INPUT PROMPTS FOR AN LLM" target="SIGNIFICANTLY IN LENGTH">
  <data key="d0">CAN VARY</data>
</edge>
<edge source="RESULTING OUTPUT LENGTHS" target="A PRIORI">
  <data key="d0">ARE NOT KNOWN</data>
</edge>
<edge source="RESULTING OUTPUT LENGTHS" target="BOTH THE INPUT PROMPT AND THE MODEL">
  <data key="d0">ARE CONTINGENT ON</data>
</edge>
<edge source="THE MEMORY MANAGEMENT SYSTEM" target="A WIDE RANGE OF PROMPT LENGTHS">
  <data key="d0">REQUIRES TO ACCOMMODATE</data>
</edge>
<edge source="THE SYSTEM" target="SCHEDULING DECISIONS">
  <data key="d0">NEEDS TO MAKE</data>
</edge>
<edge source="THE SYSTEM" target="THE SPACE FOR OUTPUTS">
  <data key="d0">OVER-RESERVES</data>
</edge>
<edge source="THE SYSTEM" target="AT MOST 2">
  <data key="d0">OVER-RESERVES BY</data>
</edge>
<edge source="SCHEDULING DECISIONS" target="DELETING OR SWAPPING OUT THE KV CACHE OF SOME REQUESTS FROM GPU MEMORY">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE ALLOCATION" target="THE ACTUAL INPUT OR EVENTUAL OUTPUT LENGTH OF THE REQUEST">
  <data key="d0">IS IRRESPECTIVE OF</data>
</edge>
<edge source="REQUEST A" target="2048">
  <data key="d0">HAS MAXIMUM POSSIBLE SEQUENCE LENGTH</data>
</edge>
<edge source="REQUEST B" target="512">
  <data key="d0">HAS MAXIMUM POSSIBLE SEQUENCE LENGTH</data>
</edge>
<edge source="THE EXTERNAL FRAGMENTATION" target="GENERATED TOKENS">
  <data key="d0">WILL NEVER BE USED FOR</data>
</edge>
<edge source="THE EXTERNAL FRAGMENTATION" target="BEFORE SERVING A REQUEST">
  <data key="d0">IS KNOWN</data>
</edge>
<edge source="RESERVING THIS SPACE" target="THE SPACE THAT COULD OTHERWISE BE USED TO PROCESS OTHER REQUESTS">
  <data key="d0">OCCUPIES</data>
</edge>
<edge source="RESERVING THIS SPACE" target="THE ENTIRE REQUESTS DURATION">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE RESERVED MEMORY" target="EVENTUALLY">
  <data key="d0">IS USED</data>
</edge>
<edge source="THE RESERVED SPACE" target="LARGE">
  <data key="d0">IS</data>
</edge>
<edge source="THE ACTUAL EFFECTIVE MEMORY IN PREVIOUS SYSTEMS" target="20.4">
  <data key="d0">CAN BE AS LOW AS</data>
</edge>
<edge source="THE ARCHITECTURE OF VLLM" target="FIG.">
  <data key="d0">IS SHOWN IN</data>
</edge>
<edge source="SYSTEM DESIGN OF VLLM" target="DISTRIBUTED SETTING">
  <data key="d0">WORKS IN</data>
</edge>
<edge source="COMPACTION 54" target="A POTENTIAL SOLUTION TO FRAGMENTATION">
  <data key="d0">HAS BEEN PROPOSED AS</data>
</edge>
<edge source="PERFORMING COMPACTION IN A PERFORMANCE-SENSITIVE LLM SERVING SYSTEM" target="IMPRATICAL DUE TO THE MASSIVE KV CACHE">
  <data key="d0">IS</data>
</edge>
<edge source="THE PRE-ALLOCATED CHUNK SPACE FOR EACH REQUEST" target="MEMORY SHARING SPECIFIC TO DECODING ALGORITHMS IN EXISTING MEMORY MANAGEMENT SYSTEMS">
  <data key="d0">PREVENTS</data>
</edge>
<edge source="A CENTRALIZED SCHEDULER" target="THE EXECUTION OF DISTRIBUTED GPU WORKERS">
  <data key="d0">COORDINATES</data>
</edge>
<edge source="EACH GPU WORKER" target="THE SAME PHYSICAL BLOCK IDS">
  <data key="d0">HAS</data>
</edge>
<edge source="A WORKER" target="A PORTION OF THE KV CACHE FOR ITS CORRESPONDING ATTENTION HEADS">
  <data key="d0">ONLY STORES</data>
</edge>
<edge source="GPU WORKERS" target="KV CACHE">
  <data key="d0">READ</data>
</edge>
<edge source="GPU WORKERS" target="BLOCK TABLE">
  <data key="d0">READ ACCORDING TO</data>
</edge>
<edge source="GPU WORKERS" target="SAMPLED TOKENS OF THIS ITERATION BACK TO THE SCHEDULER">
  <data key="d0">SEND</data>
</edge>
<edge source="GPU WORKERS" target="THE MODEL">
  <data key="d0">START TO EXECUTE</data>
</edge>
<edge source="GPU WORKERS" target="THE INPUT TOKEN IDS">
  <data key="d0">EXECUTE WITH</data>
</edge>
<edge source="GPU WORKERS" target="INTERMEDIATE RESULTS">
  <data key="d0">SYNCHRONIZE</data>
</edge>
<edge source="GPU WORKERS" target="ALL-REDUCE COMMUNICATION PRIMITIVE">
  <data key="d0">SYNCHRONIZE WITH</data>
</edge>
<edge source="GPU WORKERS" target="MEMORY MANAGEMENT">
  <data key="d0">DO NOT NEED TO SYNCHRONIZE ON</data>
</edge>
<edge source="GPU WORKERS" target="ALL THE MEMORY MANAGEMENT INFORMATION AT THE BEGINNING OF EACH DECODING ITERATION">
  <data key="d0">NEED TO RECEIVE</data>
</edge>
<edge source="GPU WORKERS" target="STEP INPUTS">
  <data key="d0">NEED TO RECEIVE</data>
</edge>
<edge source="BLOCK TABLE" target="CONTROL MESSAGE">
  <data key="d0">IS IN</data>
</edge>
<edge source="BLOCK TABLE" target="VLLM">
  <data key="d0">IS IN</data>
</edge>
<edge source="THE PAGEDATTENTION ALGORITHM" target="4.1">
  <data key="d0">IS DESCRIBED IN</data>
</edge>
<edge source="THE PAGEDATTENTION ALGORITHM" target="PHYSICAL BLOCKS 7 AND 1">
  <data key="d0">OPERATES ON</data>
</edge>
<edge source="THE FIXED NUMBER OF TOKENS" target="KV 1 IN TRANSFORMER">
  <data key="d0">IS DENOTED AS</data>
</edge>
<edge source="EACH TOKEN" target="A SET OF KEY AND VALUE VECTORS ACROSS LAYERS AND ATTENTION HEADS WITHIN A LAYER">
  <data key="d0">HAS</data>
</edge>
<edge source="ALL THE KEY AND VALUE VECTORS" target="TOGETHER WITHIN A SINGLE KV BLOCK">
  <data key="d0">CAN BE MANAGED</data>
</edge>
<edge source="THE KEY AND VALUE VECTORS AT DIFFERENT HEADS AND LAYERS" target="A SEPARATE BLOCK">
  <data key="d0">CAN EACH HAVE</data>
</edge>
<edge source="THE KEY AND VALUE VECTORS AT DIFFERENT HEADS AND LAYERS" target="IN SEPARATE BLOCK TABLES">
  <data key="d0">CAN BE MANAGED</data>
</edge>
<edge source="THE TWO DESIGNS" target="NO PERFORMANCE DIFFERENCE">
  <data key="d0">HAVE</data>
</edge>
<edge source="THE SECOND ONE" target="EASY IMPLEMENTATION">
  <data key="d0">IS CHOSEN FOR</data>
</edge>
<edge source="OUR FATHERS" target="FOUR SCORE AND SEVEN KEY AND VALUE VECTORS">
  <data key="d0">BROUGHT FORTH</data>
</edge>
<edge source="KEY BLOCK" target="((1)1">
  <data key="d0">IS DENOTED BY</data>
</edge>
<edge source="THE ATTENTION COMPUTATION" target="EQ.">
  <data key="d0">IS IN</data>
</edge>
<edge source="4" target="THE FOLLOWING BLOCK-WISE COMPUTATION">
  <data key="d0">CAN BE TRANSFORMED INTO</data>
</edge>
<edge source="ROW VECTOR" target="ATTENTION SCORE ON -TH KV BLOCK">
  <data key="d0">IS</data>
</edge>
<edge source="THE KEY AND VALUE VECTORS" target="THREE BLOCKS">
  <data key="d0">ARE SPREAD ACROSS</data>
</edge>
<edge source="THE THREE BLOCKS" target="THE PHYSICAL MEMORY">
  <data key="d0">ARE NOT CONTIGUOUS ON</data>
</edge>
<edge source="THE KERNEL" target="THE QUERY VECTOR OF THE QUERY TOKEN (FORTH) AND THE KEY VECTORS IN A BLOCK">
  <data key="d0">MULTIPLIES</data>
</edge>
<edge source="THE KERNEL" target="THE ATTENTION SCORE">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="THE KERNEL" target="THE VALUE VECTORS IN A BLOCK">
  <data key="d0">MULTIPLIES WITH</data>
</edge>
<edge source="THE KERNEL" target="THE FINAL ATTENTION OUTPUT">
  <data key="d0">DERIVES</data>
</edge>
<edge source="OS" target="MEMORY INTO FIXED-SIZED PAGES">
  <data key="d0">PARTITIONS</data>
</edge>
<edge source="OS" target="USER PROGRAMS LOGICAL PAGES TO PHYSICAL PAGES">
  <data key="d0">MAPS</data>
</edge>
<edge source="OS" target="PHYSICAL PAGES DYNAMICALLY">
  <data key="d0">CAN ALLOCATE</data>
</edge>
<edge source="OS" target="SHARED LIBRARY ACROSS PROCESSES">
  <data key="d0">HANDLES</data>
</edge>
<edge source="CONTIGUOUS LOGICAL PAGES" target="NON-CONTIGUOUS PHYSICAL MEMORY PAGES">
  <data key="d0">CAN CORRESPOND TO</data>
</edge>
<edge source="USER PROGRAMS" target="MEMORY AS THOUGH IT WERE CONTIGUOUS">
  <data key="d0">CAN ACCESS</data>
</edge>
<edge source="PHYSICAL MEMORY SPACE" target="FULLY RESERVED IN ADVANCE">
  <data key="d0">NEEDS NOT TO BE</data>
</edge>
<edge source="THE LAST KV BLOCKS UNFILLED POSITIONS" target="FUTURE GENERATIONS">
  <data key="d0">ARE RESERVED FOR</data>
</edge>
<edge source="THE KV BLOCK MANAGER" target="BLOCK TABLES">
  <data key="d0">MAINTAINS</data>
</edge>
<edge source="BLOCK TABLES" target="THE MAPPING BETWEEN LOGICAL AND PHYSICAL KV BLOCKS OF EACH REQUEST">
  <data key="d0">ARE</data>
</edge>
<edge source="EACH BLOCK TABLE ENTRY" target="THE CORRESPONDING PHYSICAL BLOCKS OF A LOGICAL BLOCK">
  <data key="d0">RECORDS</data>
</edge>
<edge source="EACH BLOCK TABLE ENTRY" target="THE NUMBER OF FILLED POSITIONS">
  <data key="d0">RECORDS</data>
</edge>
<edge source="SEPARATING LOGICAL AND PHYSICAL KV BLOCKS" target="VLLM TO DYNAMICALLY GROW THE KV CACHE MEMORY WITHOUT RESERVING IT FOR ALL POSITIONS IN ADVANCE">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="SEPARATING LOGICAL AND PHYSICAL KV BLOCKS" target="MOST MEMORY WASTE IN EXISTING SYSTEMS">
  <data key="d0">ELIMINATES</data>
</edge>
<edge source="ALL THE BLOCKS" target="LEFT TO RIGHT">
  <data key="d0">ARE FILLED FROM</data>
</edge>
<edge source="A NEW PHYSICAL BLOCK" target="ALL PREVIOUS BLOCKS ARE FULL">
  <data key="d0">IS ONLY ALLOCATED WHEN</data>
</edge>
<edge source="THE FINAL LOGICAL BLOCK" target="A COPY-ON-WRITE MECHANISM">
  <data key="d0">IS MANAGED BY</data>
</edge>
<edge source="FREQUENT MEMORY COPY OVERHEAD" target="VLLMS PHYSICAL BLOCK SHARING">
  <data key="d0">IS REDUCED BY</data>
</edge>
<edge source="A COMMON MAPPING LAYER" target="LOGICAL BLOCKS TO PHYSICAL BLOCKS">
  <data key="d0">TRANSLATES</data>
</edge>
<edge source="INTRODUCING THE VLLMS TECHNIQUES" target="THE PERFORMANCE">
  <data key="d0">MAY DEGRADE</data>
</edge>
<edge source="THE PERFORMANCE" target="PRIMARILY COMPUTE-BOUND">
  <data key="d0">IS</data>
</edge>
<edge source="THE DEGRADATION" target="THE EXTRA OVERHEAD OF MEMORY INDIRECTION AND NON-CONTIGUOUS BLOCK MEMORY">
  <data key="d0">IS DUE TO</data>
</edge>
<edge source="EXAMPLE" target="IN FIG">
  <data key="d0">IS SHOWN</data>
</edge>
<edge source="THE FIRST AUTOREGRESSIVE DECODING STEP" target="VLLM GENERATING THE NEW TOKEN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="4.3" target="HOW PAGEDATTENTION AND VLLM HANDLE BASIC DECODING ALGORITHMS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="BASIC DECODING ALGORITHMS" target="GREEDY DECODING AND SAMPLING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="BASIC DECODING ALGORITHMS" target="ONE USER PROMPT AS INPUT">
  <data key="d0">TAKE</data>
</edge>
<edge source="BASIC DECODING ALGORITHMS" target="A SINGLE OUTPUT SEQUENCE">
  <data key="d0">GENERATE</data>
</edge>
<edge source="THE NECESSARY KV BLOCKS" target="THE KV CACHE GENERATED DURING PROMPT COMPUTATION">
  <data key="d0">ACCOMMODATE</data>
</edge>
<edge source="THE PROMPT" target="7 TOKENS">
  <data key="d0">HAS</data>
</edge>
<edge source="TOKENS AND THEIR KV CACHE" target="MORE">
  <data key="d0">ARE GENERATED</data>
</edge>
<edge source="THE REMAINING SLOT" target="THE SUBSEQUENT AUTOREGRESSIVE GENERATION PHASE">
  <data key="d0">IS RESERVED FOR</data>
</edge>
<edge source="ONE SLOT" target="AVAILABLE IN THE LAST LOGICAL BLOCK">
  <data key="d0">REMAINS</data>
</edge>
<edge source="THE NEWLY GENERATED KV CACHE" target="THERE">
  <data key="d0">IS STORED</data>
</edge>
<edge source="STORING MULTIPLE TOKENS WITHIN A KV BLOCK (BLOCK SIZE 1)" target="THE PAGEDATTENTION KERNEL TO PROCESS THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PROCESSING THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL" target="THE HARDWARE UTILIZATION">
  <data key="d0">INCREASES</data>
</edge>
<edge source="PROCESSING THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL" target="LATENCY">
  <data key="d0">REDUCES</data>
</edge>
<edge source="A LARGER BLOCK SIZE" target="MEMORY FRAGMENTATION">
  <data key="d0">INCREASES</data>
</edge>
<edge source="THE LOGICAL BLOCKS OF THE TWO SEQUENCES" target="DIFFERENT PHYSICAL BLOCKS">
  <data key="d0">ARE MAPPED TO</data>
</edge>
<edge source="DIFFERENT PHYSICAL BLOCKS" target="THE SPACE RESERVED BY THE BLOCK ENGINE IN GPU WORKERS">
  <data key="d0">ARE WITHIN</data>
</edge>
<edge source="THE NEIGHBORING LOGICAL BLOCKS OF BOTH SEQUENCES" target="CONTIGUOUS IN PHYSICAL GPU MEMORY">
  <data key="d0">DO NOT NEED TO BE</data>
</edge>
<edge source="THE SPACE OF PHYSICAL BLOCKS" target="BOTH SEQUENCES">
  <data key="d0">CAN BE EFFECTIVELY UTILIZED BY</data>
</edge>
<edge source="AN LLM" target="MULTIPLE SAMPLED OUTPUTS FOR A SINGLE INPUT PROMPT">
  <data key="d0">GENERATES</data>
</edge>
<edge source="GENERATES" target="A SINGLE SEQUENCE">
  <data key="d0">GENERATES</data>
</edge>
<edge source="A REQUEST" target="MULTIPLE SEQUENCES">
  <data key="d0">GENERATES</data>
</edge>
<edge source="A REQUEST" target="ONCE">
  <data key="d0">COMPLETES</data>
</edge>
<edge source="ONE REQUEST" target="MULTIPLE SAMPLES">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MULTIPLE SAMPLES" target="THE SAME INPUT PROMPT">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE KV CACHE OF THE PROMPT" target="SHARED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="ALL PARALLEL SEQUENCES IN A REQUEST" target="THE KV CACHE FOR THE PROMPT">
  <data key="d0">CAN SHARE</data>
</edge>
<edge source="8" target="AN EXAMPLE OF PARALLEL DECODING FOR TWO OUTPUTS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="OUTPUTS" target="THE SAME PROMPT">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE LOGICAL BLOCKS FOR THE PROMPTS OF BOTH SEQUENCES" target="THE SAME PHYSICAL BLOCKS">
  <data key="d0">ARE MAPPED TO</data>
</edge>
<edge source="THE LOGICAL BLOCK 0 AND 1 OF BOTH SEQUENCES" target="PHYSICAL BLOCKS 7 AND 1">
  <data key="d0">ARE MAPPED TO</data>
</edge>
<edge source="A SINGLE PHYSICAL BLOCK" target="MULTIPLE LOGICAL BLOCKS">
  <data key="d0">CAN BE MAPPED TO</data>
</edge>
<edge source="REFERENCE COUNTS FOR PHYSICAL BLOCK 7" target="2">
  <data key="d0">ARE</data>
</edge>
<edge source="THE TWO OUTPUTS" target="DIFFERENT OUTPUT TOKENS">
  <data key="d0">SAMPLE</data>
</edge>
<edge source="THE TWO OUTPUTS" target="SEPARATE STORAGE FOR KV CACHE">
  <data key="d0">NEED</data>
</edge>
<edge source="AT THE GENERATION PHASE" target="THE TWO OUTPUTS SAMPLE DIFFERENT OUTPUT TOKENS">
  <data key="d0">IS CONTEXT FOR</data>
</edge>
<edge source="AT THE GENERATION PHASE" target="THE TWO OUTPUTS NEED SEPARATE STORAGE FOR KV CACHE">
  <data key="d0">IS CONTEXT FOR</data>
</edge>
<edge source="SAMPLE A2" target="PHYSICAL BLOCK 1">
  <data key="d0">WRITES TO</data>
</edge>
<edge source="REFERENCE COUNT" target="1">
  <data key="d0">IS REDUCED TO</data>
</edge>
<edge source="A2" target="NEWLY GENERATED KV CACHE TO PHYSICAL BLOCK 1">
  <data key="d0">WRITES</data>
</edge>
<edge source="SHARING PHYSICAL BLOCKS ACROSS MULTIPLE SAMPLES" target="MEMORY USAGE">
  <data key="d0">CAN REDUCE</data>
</edge>
<edge source="LLM TASKS LIKE MACHINE TRANSLATION 59" target="MACHINE TRANSLATION 59">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE ALGORITHM" target="THE BEAM WIDTH PARAMETER">
  <data key="d0">RELIES ON</data>
</edge>
<edge source="THE BEAM WIDTH PARAMETER" target="THE NUMBER OF TOP CANDIDATES RETAINED AT EVERY STEP">
  <data key="d0">DETERMINES</data>
</edge>
<edge source="SHARING PATTERNS" target="AS THE DECODING PROCESS ADVANCES">
  <data key="d0">DYNAMICALLY CHANGE</data>
</edge>
<edge source="SHARING PATTERNS" target="THE PROCESS TREE IN THE OS CREATED BY COMPOUND FORKS">
  <data key="d0">ARE SIMILAR TO</data>
</edge>
<edge source="THE KV BLOCKS" target="A BEAM SEARCH EXAMPLE">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="A BEAM SEARCH EXAMPLE" target="4">
  <data key="d0">HAS</data>
</edge>
<edge source="EACH CANDIDATE SEQUENCE" target="4 FULL LOGICAL BLOCKS">
  <data key="d0">HAS USED</data>
</edge>
<edge source="THE ITERATION ILLUSTRATED AS THE DOTTED LINE" target="EACH CANDIDATE SEQUENCE USING 4 FULL LOGICAL BLOCKS">
  <data key="d0">IS PRIOR TO</data>
</edge>
<edge source="ALL BEAM CANDIDATES" target="THE FIRST BLOCK 0 (I.E., PROMPT)">
  <data key="d0">SHARE</data>
</edge>
<edge source="CANDIDATE 3" target="OTHERS FROM THE SECOND BLOCK">
  <data key="d0">DIGRESSES FROM</data>
</edge>
<edge source="CANDIDATE 3" target="A LARGE PORTION OF CANDIDATE 2S KV CACHE">
  <data key="d0">WOULD NEED TO COPY</data>
</edge>
<edge source="CANDIDATE 3" target="A LARGE PORTION OF CANDIDATE 2S KV CACHE TO CONTINUE GENERATION">
  <data key="d0">WOULD NEED TO COPY</data>
</edge>
<edge source="CANDIDATES 0-2" target="THE FIRST 3 BLOCKS">
  <data key="d0">SHARE</data>
</edge>
<edge source="CANDIDATES 0-2" target="AT THE FOURTH BLOCK">
  <data key="d0">DIVERGE</data>
</edge>
<edge source="ALL CANDIDATES" target="BLOCKS 0, 1, 3">
  <data key="d0">SHARE</data>
</edge>
<edge source="CANDIDATES 0 AND 1" target="BLOCK 6">
  <data key="d0">SHARE</data>
</edge>
<edge source="ORIGINAL CANDIDATES 0 AND 3" target="TOP CANDIDATES">
  <data key="d0">ARE NO LONGER AMONG</data>
</edge>
<edge source="THEIR LOGICAL BLOCKS" target="FREED">
  <data key="d0">ARE</data>
</edge>
<edge source="REFERENCE COUNTS OF CORRESPONDING PHYSICAL BLOCKS" target="REDUCED">
  <data key="d0">ARE</data>
</edge>
<edge source="NEW PHYSICAL BLOCKS (BLOCKS 9-12)" target="THE NEW KV CACHE">
  <data key="d0">ARE ALLOCATED TO STORE</data>
</edge>
<edge source="THE NEW KV CACHE" target="THE NEW CANDIDATES">
  <data key="d0">IS FROM</data>
</edge>
<edge source="MOST BLOCKS OF DIFFERENT BEAM CANDIDATES" target="SHARED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="MOST BLOCKS" target="DIFFERENT BEAM CANDIDATES">
  <data key="d0">ARE OF</data>
</edge>
<edge source="THE SAME STRATEGY" target="BEAM SEARCH">
  <data key="d0">IS APPLIED IN</data>
</edge>
<edge source="THE SAME STRATEGY" target="PREFIX SHARING">
  <data key="d0">IS APPLIED IN</data>
</edge>
<edge source="PREFIX SHARING" target="VLLM">
  <data key="d0">IS APPLIED BY</data>
</edge>
<edge source="THE COPY-ON-WRITE MECHANISM" target="THE NEWLY GENERATED TOKENS ARE WITHIN AN OLD SHARED BLOCK">
  <data key="d0">IS APPLIED ONLY WHEN</data>
</edge>
<edge source="THE COPY-ON-WRITE MECHANISM" target="PARALLEL DECODING">
  <data key="d0">IS APPLIED IN</data>
</edge>
<edge source="LLM USER" target="DESCRIPTION OF THE TASK">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="DESCRIPTION OF THE TASK" target="INSTRUCTIONS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="DESCRIPTION OF THE TASK" target="EXAMPLE INPUTS AND OUTPUTS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="DESCRIPTION OF THE TASK" target="SYSTEM PROMPT 36">
  <data key="d0">IS ALSO KNOWN AS</data>
</edge>
<edge source="THE DESCRIPTION" target="THE ACTUAL TASK INPUT">
  <data key="d0">IS CONCATENATED WITH</data>
</edge>
<edge source="THE DESCRIPTION AND THE ACTUAL TASK INPUT" target="THE PROMPT OF THE REQUEST">
  <data key="d0">FORM</data>
</edge>
<edge source="SHARED PROMPT EXAMPLE" target="MACHINE TRANSLATION">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE EXAMPLES" target="5">
  <data key="d0">ARE ADOPTED FROM</data>
</edge>
<edge source="10" target="AN EXAMPLE">
  <data key="d0">SHOWS</data>
</edge>
<edge source="10" target="DANIEL CRANKSHAW">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="GUR-EYAL SELA">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="XIANGXI MO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="COREY ZUMAR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="ION STOICA">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="JOSEPH GONZALEZ">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="ALEXEY TUMANOV">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="SHARED PREFIX" target="PROMPT ENGINEERING">
  <data key="d0">CAN BE TUNED VIA</data>
</edge>
<edge source="TUNING SHARED PREFIX" target="ACCURACY OF DOWNSTREAM TASKS">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="MANY USER PROMPTS" target="A PREFIX">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE LLM SERVICE PROVIDER" target="THE KV CACHE OF THE PREFIX">
  <data key="d0">CAN STORE</data>
</edge>
<edge source="STORING THE KV CACHE OF THE PREFIX" target="THE REDUNDANT COMPUTATION SPENT ON THE PREFIX">
  <data key="d0">REDUCES</data>
</edge>
<edge source="A USER INPUT PROMPT WITH THE SHARED PREFIX" target="ITS LOGICAL BLOCKS TO THE CACHED PHYSICAL BLOCKS">
  <data key="d0">CAN MAP</data>
</edge>
<edge source="THE LAST BLOCK" target="COPY-ON-WRITE">
  <data key="d0">IS MARKED</data>
</edge>
<edge source="THE PROMPT PHASE COMPUTATION" target="THE USERS TASK INPUT">
  <data key="d0">ONLY NEEDS TO EXECUTE ON</data>
</edge>
<edge source="THE DECODING METHODS DISCUSSED EARLIER" target="DIVERSE MEMORY SHARING AND ACCESSING PATTERNS">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="THE LLM AND ITS EXECUTION KERNEL" target="A LIST OF PHYSICAL BLOCK IDS FOR EACH SEQUENCE">
  <data key="d0">SEE</data>
</edge>
<edge source="THE LLM AND ITS EXECUTION KERNEL" target="SHARING PATTERNS ACROSS SEQUENCES">
  <data key="d0">DO NOT NEED TO HANDLE</data>
</edge>
<edge source="THIS APPROACH" target="THE BATCHING OPPORTUNITIES FOR REQUESTS WITH DIFFERENT SAMPLING REQUIREMENTS">
  <data key="d0">BROADENS</data>
</edge>
<edge source="THIS APPROACH" target="THE SYSTEMS OVERALL THROUGHPUT">
  <data key="d0">INCREASES</data>
</edge>
<edge source="REQUEST TRAFFIC" target="THE SYSTEMS CAPACITY">
  <data key="d0">SURPASSES</data>
</edge>
<edge source="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY" target="ALL REQUESTS">
  <data key="d0">APPLIES TO</data>
</edge>
<edge source="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY" target="FAIRNESS">
  <data key="d0">ENSURES</data>
</edge>
<edge source="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY" target="STARVATION">
  <data key="d0">PREVENTS</data>
</edge>
<edge source="BLOCK SIZE" target="TOO SMALL">
  <data key="d0">IS</data>
</edge>
<edge source="BLOCK SIZE" target="1 2 4 8 16 32 64 128 256">
  <data key="d0">INCLUDES VALUES</data>
</edge>
<edge source="TWO CLASSIC QUESTIONS" target="WHICH BLOCKS SHOULD IT EVICT">
  <data key="d0">ARE</data>
</edge>
<edge source="EVICTED BLOCKS" target="IF NEEDED AGAIN">
  <data key="d0">CAN BE RECOVERED</data>
</edge>
<edge source="TWO TECHNIQUES" target="SWAPPING">
  <data key="d0">ARE</data>
</edge>
<edge source="SWAPPING" target="BLOCK SIZE IS LARGE">
  <data key="d0">IS MORE EFFICIENT WHEN</data>
</edge>
<edge source="SWAPPING" target="EXCESSIVE OVERHEAD WITH SMALL BLOCK SIZES">
  <data key="d0">INCURS</data>
</edge>
<edge source="EVICTION POLICIES" target="HEURISTICS">
  <data key="d0">USE</data>
</edge>
<edge source="EVICTION POLICIES" target="THAT BLOCK">
  <data key="d0">EVICT</data>
</edge>
<edge source="HEURISTICS" target="WHICH BLOCK WILL BE ACCESSED FURTHEST IN THE FUTURE">
  <data key="d0">PREDICT</data>
</edge>
<edge source="ALL BLOCKS OF A SEQUENCE" target="TOGETHER">
  <data key="d0">ARE ACCESSED</data>
</edge>
<edge source="ALL-OR-NOTHING EVICTION POLICY" target="EITHER EVICT ALL OR NONE OF THE BLOCKS OF A SEQUENCE">
  <data key="d0">MEANS</data>
</edge>
<edge source="MULTIPLE SEQUENCES WITHIN ONE REQUEST" target="A SEQUENCE GROUP">
  <data key="d0">ARE GANG-SCHEDULED AS</data>
</edge>
<edge source="THE SEQUENCES WITHIN ONE SEQUENCE GROUP" target="TRUE">
  <data key="d0">ARE RESCHEDULED TOGETHER</data>
</edge>
<edge source="THE SEQUENCES WITHIN ONE SEQUENCE GROUP" target="DUE TO POTENTIAL MEMORY SHARING ACROSS THOSE SEQUENCES">
  <data key="d0">ARE PREEMPTED OR RESCHEDULED TOGETHER</data>
</edge>
<edge source="CLASSIC TECHNIQUE" target="MOST VIRTUAL MEMORY IMPLEMENTATIONS">
  <data key="d0">IS USED BY</data>
</edge>
<edge source="MOST VIRTUAL MEMORY IMPLEMENTATIONS" target="EVICTED PAGES TO A SWAP SPACE ON THE DISK">
  <data key="d0">COPY</data>
</edge>
<edge source="CPU BLOCK ALLOCATOR" target="PHYSICAL BLOCKS SWAPPED TO CPU RAM">
  <data key="d0">MANAGES</data>
</edge>
<edge source="ITS BLOCKS" target="MEMORY">
  <data key="d0">ARE FREED FROM</data>
</edge>
<edge source="THE BLOCKS OF A PREEMPTED SEQUENCE" target="CONTINUE THE PROCESSING OF THAT SEQUENCE">
  <data key="d0">ARE BROUGHT BACK IN TO</data>
</edge>
<edge source="NUMBER OF BLOCKS SWAPPED TO THE CPU RAM" target="NUMBER OF TOTAL PHYSICAL BLOCKS IN THE GPU RAM">
  <data key="d0">NEVER EXCEEDS</data>
</edge>
<edge source="SWAP SPACE ON THE CPU RAM" target="GPU MEMORY ALLOCATED FOR THE KV CACHE">
  <data key="d0">IS BOUNDED BY</data>
</edge>
<edge source="RECOMPUTATION LATENCY" target="SIGNIFICANTLY LOWER THAN THE ORIGINAL LATENCY">
  <data key="d0">CAN BE</data>
</edge>
<edge source="TOKENS GENERATED AT DECODING" target="THE ORIGINAL USER PROMPT AS A NEW PROMPT">
  <data key="d0">CAN BE CONCATENATED WITH</data>
</edge>
<edge source="THEIR KV CACHE AT ALL POSITIONS" target="ONE PROMPT PHASE ITERATION">
  <data key="d0">CAN BE GENERATED IN</data>
</edge>
<edge source="PERFORMANCES OF SWAPPING AND RECOMPUTATION" target="BANDWIDTH BETWEEN CPU RAM AND GPU MEMORY">
  <data key="d0">DEPEND ON</data>
</edge>
<edge source="PERFORMANCES OF SWAPPING AND RECOMPUTATION" target="COMPUTATION POWER OF THE GPU">
  <data key="d0">DEPEND ON</data>
</edge>
<edge source="RECOMPUTATION" target="THE KV BLOCKS">
  <data key="d0">DOES NOT UTILIZE</data>
</edge>
<edge source="RECOMPUTATION" target="BLOCK SIZE IS SMALL">
  <data key="d0">IS MORE EFFICIENT WHEN</data>
</edge>
<edge source="RECOMPUTATION AND SWAPPING" target="RECOVERY MECHANISMS">
  <data key="d0">ARE</data>
</edge>
<edge source="MANY LLMS" target="PARAMETER SIZES EXCEEDING THE CAPACITY OF A SINGLE GPU">
  <data key="d0">HAVE</data>
</edge>
<edge source="MEGATRON-LM STYLE TENSOR MODEL PARALLELISM STRATEGY" target="TRANSFORMERS 47">
  <data key="d0">IS USED ON</data>
</edge>
<edge source="THIS STRATEGY" target="AN SPMD (SINGLE PROGRAM MULTIPLE DATA) EXECUTION SCHEDULE">
  <data key="d0">ADHERES TO</data>
</edge>
<edge source="THE LINEAR LAYERS" target="618 TABLE 1">
  <data key="d0">ARE PARTITIONED</data>
</edge>
<edge source="MODEL SIZES" target="SERVER CONFIGURATIONS">
  <data key="d0">AND</data>
</edge>
<edge source="THE DETAILED MODEL SIZES AND SERVER CONFIGURATIONS" target="TABLE 1">
  <data key="d0">ARE SHOWN IN</data>
</edge>
<edge source="KV CACHE SLOTS" target="15.7K 9.7K 60.1K">
  <data key="d0">ARE</data>
</edge>
<edge source="GPUS" target="INTERMEDIATE RESULTS">
  <data key="d0">SYNCHRONIZE</data>
</edge>
<edge source="INTERMEDIATE RESULTS" target="AN ALL-REDUCE OPERATION">
  <data key="d0">ARE SYNCHRONIZED VIA</data>
</edge>
<edge source="BLOCK-WISE MATRIX MULTIPLICATION" target="KV CACHE SLOTS">
  <data key="d0">IS PERFORMED BY</data>
</edge>
<edge source="ATTENTION OPERATOR" target="ATTENTION HEAD DIMENSION">
  <data key="d0">IS SPLIT ON</data>
</edge>
<edge source="EACH SPMD PROCESS" target="A SUBSET OF ATTENTION HEADS IN MULTI-HEAD ATTENTION">
  <data key="d0">TAKES CARE OF</data>
</edge>
<edge source="MODEL SHARD" target="THE SAME SET OF INPUT TOKENS">
  <data key="d0">PROCESSES</data>
</edge>
<edge source="MODEL SHARD" target="THE KV CACHE FOR THE SAME POSITIONS">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="MODEL PARALLEL EXECUTION" target="EACH MODEL SHARD PROCESSING THE SAME SET OF INPUT TOKENS">
  <data key="d0">DOES NOT PREVENT</data>
</edge>
<edge source="KV CACHE MANAGER" target="THE CENTRALIZED SCHEDULER">
  <data key="d0">IS WITHIN</data>
</edge>
<edge source="DIFFERENT GPU WORKERS" target="THE MANAGER">
  <data key="d0">SHARE</data>
</edge>
<edge source="DIFFERENT GPU WORKERS" target="THE MAPPING FROM LOGICAL BLOCKS TO PHYSICAL BLOCKS">
  <data key="d0">SHARE</data>
</edge>
<edge source="THIS COMMON MAPPING" target="GPU WORKERS TO EXECUTE THE MODEL WITH THE PHYSICAL BLOCKS PROVIDED BY THE SCHEDULER FOR EACH INPUT REQUEST">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THE SCHEDULER" target="THE MESSAGE WITH INPUT TOKEN IDS FOR EACH REQUEST IN THE BATCH">
  <data key="d0">PREPARES</data>
</edge>
<edge source="THE SCHEDULER" target="THE BLOCK TABLE FOR EACH REQUEST">
  <data key="d0">PREPARES</data>
</edge>
<edge source="THE SCHEDULER" target="THIS CONTROL MESSAGE TO THE GPU WORKERS">
  <data key="d0">BROADCASTS</data>
</edge>
<edge source="THE SCHEDULER" target="EARLIEST ARRIVED REQUESTS">
  <data key="d0">TAKES UP TO NUMBER OF</data>
</edge>
<edge source="THE SCHEDULER" target="THE BATCH TO FASTERTRANS-FORMER FOR PROCESSING">
  <data key="d0">SENDS</data>
</edge>
<edge source="SYNCHRONIZATION" target="COORDINATION OF THE SCHEDULER">
  <data key="d0">OCCURS WITHOUT</data>
</edge>
<edge source="THE FRONTEND" target="THE OPENAI API 34 INTERFACE">
  <data key="d0">EXTENDS</data>
</edge>
<edge source="THE FRONTEND" target="USERS TO CUSTOMIZE SAMPLING PARAMETERS FOR EACH REQUEST">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="SAMPLING PARAMETERS" target="THE MAXIMUM SEQUENCE LENGTH">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="SAMPLING PARAMETERS" target="THE BEAM WIDTH">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE VLLM ENGINE" target="8.5K LINES OF PYTHON">
  <data key="d0">IS WRITTEN IN</data>
</edge>
<edge source="THE VLLM ENGINE" target="2K LINES OF CCUDA CODE">
  <data key="d0">IS WRITTEN IN</data>
</edge>
<edge source="CONTROL-RELATED COMPONENTS" target="THE SCHEDULER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CONTROL-RELATED COMPONENTS" target="THE BLOCK MANAGER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CUSTOM CUDA KERNELS" target="KEY OPERATIONS">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="KEY OPERATIONS" target="PAGEDATTENTION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="INPUT AND OUTPUT LENGTH DISTRIBUTIONS" target="THE (A) SHAREGPT DATASET">
  <data key="d0">ARE OF</data>
</edge>
<edge source="INPUT AND OUTPUT LENGTH DISTRIBUTIONS" target="THE (B) ALPACA DATASET">
  <data key="d0">ARE OF</data>
</edge>
<edge source="THE SHAREGPT DATASET" target="THE ALPACA DATASET">
  <data key="d0">HAS OUTPUTS 5.8 TIMES LONGER THAN</data>
</edge>
<edge source="THE SHAREGPT DATASET" target="A COLLECTION OF USER-SHARED CONVERSATIONS WITH CHATGPT 35">
  <data key="d0">IS</data>
</edge>
<edge source="THE ALPACA DATASET" target="A SIMILAR TREND TO THE SHAREGPT DATASET">
  <data key="d0">FOLLOWS</data>
</edge>
<edge source="PYTORCH" target="39">
  <data key="d0">VERSION</data>
</edge>
<edge source="PYTORCH" target="AN IMPERATIVE STYLE HIGH-PERFORMANCE DEEP LEARNING LIBRARY">
  <data key="d0">IS</data>
</edge>
<edge source="TRANSFORMERS" target="58">
  <data key="d0">VERSION</data>
</edge>
<edge source="TRANSFORMERS" target="STATE-OF-THE-ART NATURAL LANGUAGE PROCESSING">
  <data key="d0">ARE</data>
</edge>
<edge source="MEMORY ACCESS PATTERNS" target="EXISTING SYSTEMS">
  <data key="d0">ARE NOT EFFICIENTLY SUPPORTED BY</data>
</edge>
<edge source="SEVERAL GPU KERNELS" target="OPTIMIZING PAGEDATTENTION">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="THE DYNAMIC BLOCK MAPPING IN PAGEDATTENTION" target="THE PERFORMANCE OF THE GPU OPERATIONS INVOLVING THE STORED KV CACHE">
  <data key="d0">AFFECTS</data>
</edge>
<edge source="THE GPU OPERATIONS" target="THE STORED KV CACHE">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="THE GPU OPERATIONS" target="BLOCK READWRITES AND ATTENTION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FUSED RE-SHAPE" target="BLOCK WRITE">
  <data key="d0">AND</data>
</edge>
<edge source="FUSED BLOCK COPY" target="3">
  <data key="d0">IS</data>
</edge>
<edge source="NEW KV CACHE" target="BLOCKS">
  <data key="d0">ARE SPLIT INTO</data>
</edge>
<edge source="NEW KV CACHE" target="A MEMORY LAYOUT OPTIMIZED FOR BLOCK READ">
  <data key="d0">ARE RESHAPED TO</data>
</edge>
<edge source="NEW KV CACHE" target="POSITIONS SPECIFIED BY THE BLOCK TABLE">
  <data key="d0">ARE SAVED AT</data>
</edge>
<edge source="FUSING THEM INTO A SINGLE KERNEL" target="KERNEL LAUNCH OVERHEADS">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="A KERNEL" target="THE COPY OPERATIONS FOR DIFFERENT BLOCKS">
  <data key="d0">BATCHES</data>
</edge>
<edge source="THE COPY OPERATIONS FOR DIFFERENT BLOCKS" target="A SINGLE KERNEL LAUNCH">
  <data key="d0">ARE BATCHED INTO</data>
</edge>
<edge source="BLOCK COPY OPERATIONS" target="THE COPY-ON-WRITE MECHANISM">
  <data key="d0">ARE ISSUED BY</data>
</edge>
<edge source="BLOCK COPY OPERATIONS" target="DISCONTINUOUS BLOCKS">
  <data key="d0">MAY OPERATE ON</data>
</edge>
<edge source="THE FORK METHOD" target="A NEW SEQUENCE FROM AN EXISTING ONE">
  <data key="d0">CREATES</data>
</edge>
<edge source="THE APPEND METHOD" target="A NEW TOKEN TO THE SEQUENCE">
  <data key="d0">APPENDS</data>
</edge>
<edge source="THE FREE METHOD" target="THE SEQUENCE">
  <data key="d0">DELETES</data>
</edge>
<edge source="FUTURE DECODING ALGORITHMS" target="COMBINING THESE METHODS">
  <data key="d0">CAN BE SUPPORTED BY</data>
</edge>
<edge source="PARALLEL GENERATION" target="2">
  <data key="d0">HAS PARALLEL SIZE</data>
</edge>
<edge source="NORMALIZED LATENCY" target="STOKEN">
  <data key="d0">MEASURED IN</data>
</edge>
<edge source="NORMALIZED LATENCY" target="THE MEAN OF EVERY REQUESTS END-TO-END LATENCY DIVIDED BY ITS OUTPUT LENGTH">
  <data key="d0">IS</data>
</edge>
<edge source="REQUEST RATE" target="REQS">
  <data key="d0">MEASURED IN</data>
</edge>
<edge source="REQUEST RATE" target="CAPACITY OF THE SERVING SYSTEM">
  <data key="d0">SURPASSES</data>
</edge>
<edge source="FIGURE" target="14">
  <data key="d0">NUMBER</data>
</edge>
<edge source="FIGURE" target="19">
  <data key="d0">NUMBER</data>
</edge>
<edge source="14" target="RESULTS FOR BEAM SEARCH WITH DIFFERENT BEAM WIDTHS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="FIGURE 17" target="REQUEST RATE (REQS)">
  <data key="d0">DISPLAYS</data>
</edge>
<edge source="FIGURE 17" target="NORMALIZED LATENCY (STOKEN)">
  <data key="d0">DISPLAYS</data>
</edge>
<edge source="FIGURE 17" target="ORCA (MAX)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="FIGURE 17" target="ORCA (POW2)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="FIGURE 17" target="ORCA (ORACLE)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="FIGURE 17" target="VLLM">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="NORMALIZED LATENCY (STOKEN)" target="0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="SINGLE SEQUENCE GENERATION" target="OPT MODELS">
  <data key="d0">IS DONE WITH</data>
</edge>
<edge source="OPT MODELS" target="SHAREGPT DATASET">
  <data key="d0">ARE APPLIED ON</data>
</edge>
<edge source="OPT MODELS" target="ALPACA DATASET">
  <data key="d0">ARE APPLIED ON</data>
</edge>
<edge source="SHAREGPT DATASET" target="16.2 - 30.5">
  <data key="d0">HAS MEMORY SAVING ON PARALLEL SAMPLING</data>
</edge>
<edge source="SHAREGPT DATASET" target="44.3 - 66.3">
  <data key="d0">HAS MEMORY SAVING ON BEAM SEARCH</data>
</edge>
<edge source="SHAREGPT DATASET" target="MANY LONG CONVERSATIONS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="ALPACA DATASET" target="SHORT SEQUENCES">
  <data key="d0">HAS</data>
</edge>
<edge source="BATCHED REQUESTS" target="0 5 10 15 20 25 30 35 FOR SHAREGPT">
  <data key="d0">HAVE VALUES</data>
</edge>
<edge source="BATCHED REQUESTS" target="0 25 50 75 100 125 150 FOR ALPACA">
  <data key="d0">HAVE VALUES</data>
</edge>
<edge source="AVERAGE NUMBER OF BATCHED REQUESTS" target="WHEN SERVING OPT-13B FOR THE SHAREGPT (2 REQSS) AND ALPACA (30 REQSS) TRACES">
  <data key="d0">IS</data>
</edge>
<edge source="6.1" target="EXPERIMENTAL SETUP MODEL AND SERVER CONFIGURATIONS">
  <data key="d0">IS</data>
</edge>
<edge source="OPT 62 MODELS" target="13B PARAMETERS">
  <data key="d0">HAVE</data>
</edge>
<edge source="OPT 62 MODELS" target="66B PARAMETERS">
  <data key="d0">HAVE</data>
</edge>
<edge source="OPT 62 MODELS" target="175B PARAMETERS">
  <data key="d0">HAVE</data>
</edge>
<edge source="13B" target="POPULAR SIZES FOR LLMS">
  <data key="d0">ARE</data>
</edge>
<edge source="13B" target="THE RESULTS ON THE ALPACA DATASET">
  <data key="d0">SHOWS</data>
</edge>
<edge source="66B" target="POPULAR SIZES FOR LLMS">
  <data key="d0">ARE</data>
</edge>
<edge source="13B AND 66B" target="LLM LEADERBOARD 38">
  <data key="d0">ARE SHOWN IN</data>
</edge>
<edge source="175B" target="THE SIZE OF THE FAMOUS GPT-3 5 MODEL">
  <data key="d0">IS</data>
</edge>
<edge source="SHAREGPT 51 AND ALPACA 50 DATASETS" target="INPUT AND OUTPUT TEXTS OF REAL LLM SERVICES">
  <data key="d0">CONTAIN</data>
</edge>
<edge source="THEIR INPUT AND OUTPUT LENGTHS" target="CLIENT REQUESTS">
  <data key="d0">TO SYNTHESIZE</data>
</edge>
<edge source="REQUEST ARRIVAL TIMES" target="POISSON DISTRIBUTION">
  <data key="d0">ARE GENERATED USING</data>
</edge>
<edge source="POISSON DISTRIBUTION" target="DIFFERENT REQUEST RATES">
  <data key="d0">HAS</data>
</edge>
<edge source="THESE DATASETS" target="TIMESTAMPS">
  <data key="d0">DO NOT INCLUDE</data>
</edge>
<edge source="BASELINE 1" target="FASTERTRANSFORMER">
  <data key="d0">IS</data>
</edge>
<edge source="CUSTOM SCHEDULER" target="A DYNAMIC BATCHING MECHANISM">
  <data key="d0">HAS</data>
</edge>
<edge source="DYNAMIC BATCHING MECHANISM" target="EXISTING SERVING SYSTEMS SUCH AS TRITON 30">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="MAXIMUM BATCH SIZE" target="THE GPU MEMORY CAPACITY">
  <data key="d0">IS SET ACCORDING TO</data>
</edge>
<edge source="BASELINE 2" target="ORCA">
  <data key="d0">IS</data>
</edge>
<edge source="THE THREE ORCA BASELINES" target="SIMILARLY">
  <data key="d0">BEHAVE</data>
</edge>
<edge source="BUDDY ALLOCATION ALGORITHM" target="MEMORY ADDRESS TO STORE KV CACHE">
  <data key="d0">DETERMINES</data>
</edge>
<edge source="BUDDY ALLOCATION ALGORITHM" target="ORCA BASELINES TO RESERVE THE SPACE FOR 1024 TOKENS FOR THE REQUEST OUTPUTS">
  <data key="d0">CAUSES</data>
</edge>
<edge source="THE UPPER-BOUND PERFORMANCE OF ORCA" target="INFEASIBLE TO ACHIEVE IN PRACTICE">
  <data key="d0">IS</data>
</edge>
<edge source="TRUE OUTPUT LENGTH" target="25">
  <data key="d0">IS</data>
</edge>
<edge source="USING THE WORKLOADS WITH DIFFERENT REQUEST RATES" target="SPECIFICALLY">
  <data key="d0">IS</data>
</edge>
<edge source="A HIGH-THROUGHPUT SERVING SYSTEM" target="LOW NORMALIZED LATENCY">
  <data key="d0">SHOULD RETAIN</data>
</edge>
<edge source="LOW NORMALIZED LATENCY" target="HIGH REQUEST RATES">
  <data key="d0">IS RETAINED AGAINST</data>
</edge>
<edge source="15-MINUTE TRACES" target="AS AN EXCEPTION">
  <data key="d0">ARE USED</data>
</edge>
<edge source="15-MINUTE TRACES" target="DUE TO THE COST LIMIT">
  <data key="d0">ARE USED</data>
</edge>
<edge source="PARALLEL GENERATION AND BEAM SEARCH" target="OPT-13B">
  <data key="d0">IS USED WITH</data>
</edge>
<edge source="OPT-13B" target="THE ALPACA DATASET">
  <data key="d0">IS USED ON</data>
</edge>
<edge source="BASIC SAMPLING" target="ONE SAMPLE PER REQUEST">
  <data key="d0">IS</data>
</edge>
<edge source="12" target="THE RESULTS ON THE SHAREGPT DATASET">
  <data key="d0">SHOWS</data>
</edge>
<edge source="THE CURVES" target="AS THE REQUEST RATE INCREASES, THE LATENCY INITIALLY INCREASES AT A GRADUAL PACE BUT THEN SUDDENLY EXPLODES">
  <data key="d0">ILLUSTRATE</data>
</edge>
<edge source="QUEUE LENGTH" target="INFINITELY">
  <data key="d0">CONTINUES TO GROW</data>
</edge>
<edge source="LATENCY OF THE REQUESTS" target="SO">
  <data key="d0">DOES</data>
</edge>
<edge source="13A, FOR OPT-13B VLLM" target="2.2 MORE REQUESTS AT THE SAME TIME THAN ORCA (ORACLE)">
  <data key="d0">PROCESSES</data>
</edge>
<edge source="13A, FOR OPT-13B VLLM" target="4.3 MORE REQUESTS THAN ORCA (MAX)">
  <data key="d0">PROCESSES</data>
</edge>
<edge source="VLLMS PAGEDATTENTION" target="MEMORY USAGE EFFICIENTLY">
  <data key="d0">CAN MANAGE</data>
</edge>
<edge source="VLLMS PAGEDATTENTION" target="BATCHING MORE REQUESTS THAN ORCA">
  <data key="d0">ENABLES</data>
</edge>
<edge source="ONE EXCEPTION" target="FIG.">
  <data key="d0">IS</data>
</edge>
<edge source="VLLMS" target="ORCA (ORACLE)">
  <data key="d0">HAS ADVANTAGE OVER</data>
</edge>
<edge source="VLLMS" target="ORCA (POW2)">
  <data key="d0">HAS ADVANTAGE OVER</data>
</edge>
<edge source="ADVANTAGE OF VLLMS OVER ORCA (ORACLE) AND ORCA (POW2)" target="LESS PRONOUNCED">
  <data key="d0">IS</data>
</edge>
<edge source="MODEL AND SERVER CONFIGURATION FOR OPT-175B" target="LARGE GPU MEMORY SPACE AVAILABLE TO STORE KV CACHE">
  <data key="d0">ALLOWS FOR</data>
</edge>
<edge source="ORCA (ORACLE) AND ORCA (POW2)" target="A LARGE NUMBER OF REQUESTS">
  <data key="d0">CAN BATCH</data>
</edge>
<edge source="ORCA (ORACLE) AND ORCA (POW2)" target="REQUESTS DESPITE THE INEFFICIENCIES IN THEIR MEMORY MANAGEMENT">
  <data key="d0">CAN BATCH</data>
</edge>
<edge source="OUTPUT SEQUENCES" target="2 4 6">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="OUTPUT SEQUENCES" target="0 4 8 12">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MEMORY SAVING" target="6.09">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="8.53">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="9.79">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="37.56">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="53.13">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="55.16">
  <data key="d0">VALUE</data>
</edge>
<edge source="BEAM WIDTH" target="2 4 6">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="BEAM WIDTH" target="0 20 40 60">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="AVERAGE AMOUNT OF MEMORY SAVING" target="SHARING KV BLOCKS">
  <data key="d0">IS FROM</data>
</edge>
<edge source="SHARING KV BLOCKS" target="SERVING OPT-13B FOR THE ALPACA TRACE">
  <data key="d0">OCCURS WHEN</data>
</edge>
<edge source="PARALLEL SAMPLING AND BEAM SEARCH" target="TWO POPULAR SAMPLING METHODS">
  <data key="d0">ARE</data>
</edge>
<edge source="2 HIGHER REQUEST RATES" target="THE THREE ORCA BASELINES">
  <data key="d0">ARE COMPARED TO</data>
</edge>
<edge source="15" target="THE AMOUNT OF MEMORY SAVING">
  <data key="d0">PLOTS</data>
</edge>
<edge source="THE AMOUNT OF MEMORY SAVING" target="THE NUMBER OF BLOCKS WE SAVED BY SHARING DIVIDED BY THE NUMBER OF TOTAL BLOCKS WITHOUT SHARING">
  <data key="d0">IS COMPUTED BY</data>
</edge>
<edge source="INPUT PROMPTS" target="A COMMON PREFIX">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE PREFIX" target="(A) 1 EXAMPLE WITH 80 TOKENS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE PREFIX" target="(B) 5 EXAMPLES WITH 341 TOKENS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PERFORMANCE" target="CHATBOT WORKLOAD">
  <data key="d0">ON</data>
</edge>
<edge source="PERFORMANCE" target="OPT-13B WITH THE SHAREGPT TRACES AT THE SAME REQUEST RATE">
  <data key="d0">IS WHEN SERVING</data>
</edge>
<edge source="LLAMA-13B 52" target="THE MODEL">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="LLAMA-13B 52" target="MULTILINGUAL">
  <data key="d0">IS</data>
</edge>
<edge source="TWO PREFIXES" target="AN INSTRUCTION AND A FEW TRANSLATION EXAMPLES">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE FIRST PREFIX" target="A SINGLE EXAMPLE (I.E., ONE-SHOT)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE OTHER PREFIX" target="5 EXAMPLES (I.E., FEW-SHOT)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="CHATBOT" target="ONE OF THE MOST IMPORTANT APPLICATIONS OF LLMS">
  <data key="d0">IS</data>
</edge>
<edge source="A RESPONSE" target="CONCATENATING THE CHATTING HISTORY AND THE LAST USER QUERY INTO A PROMPT">
  <data key="d0">IS GENERATED BY</data>
</edge>
<edge source="THE CONTEXT LENGTH OF THE OPT-13B MODEL" target="LIMITED">
  <data key="d0">IS</data>
</edge>
<edge source="DOING THIS" target="THE SPACE FOR OTHER REQUESTS BETWEEN THE CONVERSATION ROUNDS">
  <data key="d0">WOULD OCCUPY</data>
</edge>
<edge source="INPUT PROMPTS FOR MOST REQUESTS" target="1024 TOKENS">
  <data key="d0">HAVE</data>
</edge>
<edge source="ORCA BASELINES" target="SPACE FOR 1024 TOKENS FOR THE REQUEST OUTPUTS">
  <data key="d0">RESERVE</data>
</edge>
<edge source="ORCA BASELINES" target="REGARDLESS OF HOW THEY PREDICT THE OUTPUT LENGTHS">
  <data key="d0">RESERVE SPACE</data>
</edge>
<edge source="SHAREGPT ALPACA (B)" target="END-TO-END LATENCY">
  <data key="d0">MEASURES</data>
</edge>
<edge source="END-TO-END LATENCY" target="DIFFERENT BLOCK SIZES">
  <data key="d0">VARIES WITH</data>
</edge>
<edge source="THE DESIGN CHOICES" target="ABLATION EXPERIMENTS">
  <data key="d0">ARE MADE WITH</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="EXTRA OVERHEADS OF ACCESSING THE BLOCK TABLE">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="EXECUTING EXTRA BRANCHES">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="HANDLING VARIABLE SEQUENCE LENGTHS">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="THE EXISTING SYSTEMS">
  <data key="d0">ARE COMPARED TO</data>
</edge>
<edge source="18A" target="2026 HIGHER ATTENTION KERNEL LATENCY">
  <data key="d0">LEADS TO</data>
</edge>
<edge source="2026 HIGHER ATTENTION KERNEL LATENCY" target="HIGHLY-OPTIMIZED FASTERTRANSFORMER IMPLEMENTATION">
  <data key="d0">IS COMPARED TO</data>
</edge>
<edge source="THE OVERHEAD" target="SMALL">
  <data key="d0">IS</data>
</edge>
<edge source="THE OVERHEAD" target="THE ATTENTION OPERATOR">
  <data key="d0">AFFECTS</data>
</edge>
<edge source="THE OVERHEAD" target="THE OTHER OPERATORS IN THE MODEL">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="THE OTHER OPERATORS IN THE MODEL" target="LINEAR">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CHOICE OF BLOCK SIZE" target="SUBSTANTIAL IMPACT ON THE PERFORMANCE OF VLLM">
  <data key="d0">CAN HAVE</data>
</edge>
<edge source="THE PERFORMANCE OF VLLM" target="FIXED REQUEST RATES">
  <data key="d0">IS EVALUATED UNDER</data>
</edge>
<edge source="BLOCK SIZES FROM 16 TO 128" target="THE BEST PERFORMANCE">
  <data key="d0">LEAD TO</data>
</edge>
<edge source="BLOCK SIZE 16 AND 32" target="WELL IN THE ALPACA TRACE">
  <data key="d0">WORK</data>
</edge>
<edge source="LARGER BLOCK SIZES" target="THE PERFORMANCE">
  <data key="d0">DEGRADE</data>
</edge>
<edge source="SEQUENCES" target="SHORTER THAN THE BLOCK SIZES">
  <data key="d0">BECOME</data>
</edge>
<edge source="BLOCK SIZE 16" target="EFFICIENTLY UTILIZE THE GPU">
  <data key="d0">IS LARGE ENOUGH TO</data>
</edge>
<edge source="BLOCK SIZE 16" target="AVOID SIGNIFICANT INTERNAL FRAGMENTATION IN MOST WORKLOADS">
  <data key="d0">IS SMALL ENOUGH TO</data>
</edge>
<edge source="TIME" target="MS">
  <data key="d0">MEASURED IN</data>
</edge>
<edge source="MICROBENCHMARK" target="RECOMPUTE SWAP IN SWAP OUT SWAP IN OUT">
  <data key="d0">HAS LABEL</data>
</edge>
<edge source="MICROBENCHMARK" target="(A)">
  <data key="d0">LABELLED AS</data>
</edge>
<edge source="END-TO-END PERFORMANCE" target="(B)">
  <data key="d0">LABELLED AS</data>
</edge>
<edge source="OVERHEAD" target="RECOMPUTATION AND SWAPPING">
  <data key="d0">IS FOR</data>
</edge>
<edge source="OVERHEAD" target="DIFFERENT BLOCK SIZES">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE OVERHEAD OF RECOMPUTATION" target="CONSTANT ACROSS DIFFERENT BLOCK SIZES">
  <data key="d0">REMAINS</data>
</edge>
<edge source="RECOMPUTATION OVERHEAD" target="20 OF SWAPPINGS LATENCY">
  <data key="d0">IS NEVER HIGHER THAN</data>
</edge>
<edge source="SMALL BLOCK SIZES" target="NUMEROUS SMALL DATA TRANSFERS BETWEEN CPU AND GPU">
  <data key="d0">RESULT IN</data>
</edge>
<edge source="NUMEROUS SMALL DATA TRANSFERS BETWEEN CPU AND GPU" target="EFFECTIVE PCIE BANDWIDTH">
  <data key="d0">LIMIT</data>
</edge>
<edge source="THE TWO METHODS" target="COMPARABLE END-TO-END PERFORMANCE FOR MEDIUM BLOCK SIZES FROM 16 TO 64">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="VIRTUAL MEMORY AND PAGING TECHNIQUE" target="OTHER GPU WORKLOADS">
  <data key="d0">APPLIED TO</data>
</edge>
<edge source="TENSOR SHAPES" target="TYPICALLY STATIC">
  <data key="d0">ARE</data>
</edge>
<edge source="MEMORY ALLOCATION" target="OPTIMIZED AHEAD OF TIME">
  <data key="d0">CAN BE</data>
</edge>
<edge source="AN INCREASE IN MEMORY EFFICIENCY" target="ANY PERFORMANCE IMPROVEMENT">
  <data key="d0">MAY NOT RESULT IN</data>
</edge>
<edge source="LLM-SPECIFIC OPTIMIZATIONS" target="VIRTUAL MEMORY AND PAGING">
  <data key="d0">ARE APPLIED IN</data>
</edge>
<edge source="VLLMS ALL-OR-NOTHING SWAP-OUT POLICY" target="ONE EXAMPLE">
  <data key="d0">IS</data>
</edge>
<edge source="VLLMS ALL-OR-NOTHING SWAP-OUT POLICY" target="THE FACT THAT PROCESSING A REQUEST REQUIRES ALL OF ITS CORRESPONDING TOKEN STATES TO BE STORED IN GPU MEMORY">
  <data key="d0">EXPLOITS</data>
</edge>
<edge source="RECOMPUTATION METHOD" target="ANOTHER EXAMPLE">
  <data key="d0">IS</data>
</edge>
<edge source="RECOMPUTATION METHOD" target="EVICTED BLOCKS">
  <data key="d0">RECOVERS</data>
</edge>
<edge source="RECOMPUTATION METHOD" target="FEASIBLE IN OS">
  <data key="d0">IS NOT</data>
</edge>
<edge source="RELATED WORK" target="GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">IS ABOUT</data>
</edge>
<edge source="MODEL SERVING" target="AN ACTIVE AREA OF RESEARCH">
  <data key="d0">HAS BEEN</data>
</edge>
<edge source="NUMEROUS SYSTEMS" target="DIVERSE ASPECTS OF DEEP LEARNING MODEL DEPLOYMENT">
  <data key="d0">ARE PROPOSED TO TACKLE</data>
</edge>
<edge source="CLIPPER 11" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="TENSORFLOW SERVING 33" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="NEXUS 45" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="INFERLINE 10" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="CLOCKWORK 20" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="BATCH-ING" target="SERVING SINGLE OR MULTIPLE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="CACHING" target="SERVING SINGLE OR MULTIPLE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="PLACEMENT" target="SERVING SINGLE OR MULTIPLE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="DVABATCH 12" target="MULTI-ENTRY MULTI-EXIT BATCHING">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="REEF 21" target="PREEMPTION FOR SERVING">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="SHEP-HERD 61" target="PREEMPTION FOR SERVING">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="ALPASERVE 28" target="MODEL PARALLELISM">
  <data key="d0">UTILIZES</data>
</edge>
<edge source="MODEL PARALLELISM" target="STATISTICAL MULTIPLEXING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="MODEL PARALLELISM" target="EFFICIENT SERVING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="ALPASERVE" target="STATISTICAL MULTIPLEXING WITH MODEL PARALLELISM FOR DEEP LEARNING SERVING">
  <data key="d0">IS</data>
</edge>
<edge source="GENERAL SYSTEMS" target="AUTO-REGRESSIVE PROPERTY AND TOKEN STATE OF LLM INFERENCE">
  <data key="d0">FAIL TO TAKE INTO ACCOUNT</data>
</edge>
<edge source="SPECIALIZED SERVING SYSTEMS" target="TRANSFORMERS">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="NUMEROUS SPECIALIZED SERVING SYSTEMS" target="THE TRANSFORMER ARCHITECTURE">
  <data key="d0">HAVE BEEN DEVELOPED FOR</data>
</edge>
<edge source="THESE SYSTEMS" target="GPU KERNEL OPTIMIZATIONS">
  <data key="d0">UTILIZE</data>
</edge>
<edge source="THESE SYSTEMS" target="ADVANCED BATCHING MECHANISMS">
  <data key="d0">UTILIZE</data>
</edge>
<edge source="THESE SYSTEMS" target="MODEL PARALLELISM">
  <data key="d0">UTILIZE</data>
</edge>
<edge source="THESE SYSTEMS" target="PARAMETER SHARING">
  <data key="d0">UTILIZE</data>
</edge>
<edge source="GPU KERNEL OPTIMIZATIONS" target="EFFICIENT SERVING">
  <data key="d0">ARE USED FOR</data>
</edge>
<edge source="ADVANCED BATCHING MECHANISMS" target="EFFICIENT SERVING">
  <data key="d0">ARE USED FOR</data>
</edge>
<edge source="PARAMETER SHARING" target="EFFICIENT SERVING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="FINE-GRAINED SCHEDULING AND INTERLEAVING OF THE REQUESTS LIKE IN ORCA" target="MEMORY MANAGEMENT MORE CHALLENGING">
  <data key="d0">MAKES</data>
</edge>
<edge source="TECHNIQUES PROPOSED IN VLLM" target="MORE CRUCIAL">
  <data key="d0">ARE</data>
</edge>
<edge source="THE WIDENING GAP BETWEEN THE COMPUTE CAPABILITY AND MEMORY CAPACITY OF ACCELERATORS" target="MEMORY TO BECOME A BOTTLENECK FOR BOTH TRAINING AND INFERENCE">
  <data key="d0">HAS CAUSED</data>
</edge>
<edge source="SWAPPING 23, 42, 55, RECOMPUTATION 7, 24 AND THEIR COMBINATION 40" target="REDUCE THE PEAK MEMORY OF TRAINING">
  <data key="d0">HAVE BEEN UTILIZED TO</data>
</edge>
<edge source="FLEXGEN 46 STUDIES" target="HOW TO SWAP WEIGHTS AND TOKEN STATES FOR LLM INFERENCE WITH 623 LIMITED GPU MEMORY">
  <data key="d0">STUDIES</data>
</edge>
<edge source="OLLA 48" target="THE LIFETIME AND LOCATION OF TENSORS">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="OLLA 48" target="FRAGMENTATION">
  <data key="d0">REDUCES</data>
</edge>
<edge source="OLLA 48" target="FINE-GRAINED BLOCK-LEVEL MANAGEMENT">
  <data key="d0">DOES NOT DO</data>
</edge>
<edge source="OLLA 48" target="ONLINE SERVING">
  <data key="d0">DOES NOT DO</data>
</edge>
<edge source="FLASHAT-TENTION 13" target="TILING AND KERNEL OPTIMIZATIONS">
  <data key="d0">APPLIES</data>
</edge>
<edge source="FLASHAT-TENTION 13" target="THE PEAK MEMORY OF ATTENTION COMPUTATION">
  <data key="d0">REDUCES</data>
</edge>
<edge source="FLASHAT-TENTION 13" target="IO COSTS">
  <data key="d0">REDUCES</data>
</edge>
<edge source="ZHIFENG CHEN" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="OUR SHEPHERD" target="LIDONG ZHOU">
  <data key="d0">IS</data>
</edge>
<edge source="XIAOXUAN LIU, ZHIFENG CHEN, YAN-PING HUANG, ANONYMOUS SOSP REVIEWERS, AND LIDONG ZHOU" target="INSIGHTFUL FEEDBACK">
  <data key="d0">PROVIDE</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM ANDREESSEN HOROWITZ">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM ANYSCALE">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM ASTRONOMER">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM GOOGLE">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM IBM">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM INTEL">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM LACEWORK">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM MICROSOFT">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM MOHAMED BIN ZAYED UNIVERSITY OF ARTIFICIAL INTELLIGENCE">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM SAMSUNG SDS">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM UBER">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM VMWARE">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="REFERENCES" target="REZA YAZDANI AMINABADI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="SAMYAM RAJBHANDARI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="MINJIA ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="AMMAR AHMAD AWAN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="CHENG LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="DU LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="ELTON ZHENG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="JEFF RASLEY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="SHADEN SMITH">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="OLATUNJI RUWASE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="DEEPSPEED INFERENCE" target="EFFICIENT INFERENCE OF TRANSFORMER MODELS AT UNPRECEDENTED SCALE">
  <data key="d0">ENABLES</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2207.00032">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2022">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1607.06450">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2016">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2107.03374">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2021">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1604.06174">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2204.02311">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2104.08691">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2101.00190">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2302.11665">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2023">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1712.06139">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2017">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2211.05102">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2303.06865">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1909.08053">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2019">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2302.13971">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2212.10560">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1609.08144">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2205.01068">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="JIMMY LEI BA" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JAMIE RYAN KIROS" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="GEOFFREY E HINTON" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="LAYER NORMALIZATION" target="A TECHNIQUE">
  <data key="d0">IS</data>
</edge>
<edge source="YOSHUA BENGIO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="RJEAN DUCHARME" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="PASCAL VINCENT" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="NEURAL PROBABILISTIC LANGUAGE MODEL" target="A MODEL">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="13">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="33">
  <data key="d0">IS VOLUME</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="35">
  <data key="d0">IS VOLUME</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="27TH EDITION">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="30TH EDITION">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="32ND EDITION">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13" target="2000">
  <data key="d0">YEAR</data>
</edge>
<edge source="4 OND REJ BOJAR" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="RAJEN CHATTERJEE" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="CHRISTIAN FEDERMANN" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="YVETTE GRAHAM" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="BARRY HADDOW" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MATTHIAS HUCK" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="ANTONIO JIMENO YEPES" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="PHILIPP KOEHN" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="VARVARA LOGACHEVA" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="CHRISTOF MONZ" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MATTEO NEGRI" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="AURELIE NEVEOL" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MARIANA NEVES" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MARTIN POPEL" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MATT POST" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="RAPHAEL RUBINO" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="CAROLINA SCARTON" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="LUCIA SPECIA" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MARCO TURCHI" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="KARIN VERSPOOR" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MARCOS ZAMPIERI" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="2016 CONFERENCE" target="MACHINE TRANSLATION">
  <data key="d0">IS ON</data>
</edge>
<edge source="PROCEEDINGS" target="THE FIRST CONFERENCE ON MACHINE TRANSLATION">
  <data key="d0">ARE OF</data>
</edge>
<edge source="PROCEEDINGS" target="THE IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION">
  <data key="d0">ARE OF</data>
</edge>
<edge source="PROCEEDINGS" target="2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
  <data key="d0">ARE OF</data>
</edge>
<edge source="PROCEEDINGS" target="HUMAN LANGUAGE TECHNOLOGIES: INDUSTRY PAPERS">
  <data key="d0">ARE TITLED</data>
</edge>
<edge source="PROCEEDINGS" target="THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS">
  <data key="d0">ARE OF</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS" target="BERLIN, GERMANY">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS" target="131198">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="TOM BROWN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="BENJAMIN MANN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="NICK RYDER">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="MELANIE SUBBIAH">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="JARED D KAPLAN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="PRAFULLA DHARIWAL">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="ARVIND NEELAKANTAN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="PRANAV SHYAM">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="GIRISH SASTRY">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="AMANDA ASKELL">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="LANGUAGE MODELS" target="FEW-SHOT LEARNERS">
  <data key="d0">ARE</data>
</edge>
<edge source="33" target="CHRISTOPHER OLSTON">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="NOAH FIEDEL">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="KIRIL GOROVOY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="JEREMIAH HARMSEN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="LI LAO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="FANGWEI LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="VINU RAJASHEKHAR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="SUKRITI RAMESH">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="JORDAN SOYKE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 33" target="2020">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 33" target="1877-1901">
  <data key="d0">HAS PAGE RANGE</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35" target="2022">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35" target="16344-16359">
  <data key="d0">HAS PAGE RANGE</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27" target="2014">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30" target="2017">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="6 MARK CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JERRY TWOREK" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HEEWOO JUN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="QIMING YUAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HENRIQUE PONDE DE OLIVEIRA PINTO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JARED KAPLAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HARRI EDWARDS" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="YURI BURDA" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="NICHOLAS JOSEPH" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="GREG BROCKMAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="LARGE LANGUAGE MODELS" target="CODE">
  <data key="d0">ARE TRAINED ON</data>
</edge>
<edge source="7 TIANQI CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="BING XU" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="CHIYUAN ZHANG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="CARLOS GUESTRIN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="14 JIARUI FANG" target="YANG YU">
  <data key="d0">AND</data>
</edge>
<edge source="14 JIARUI FANG" target="CHENGDUO ZHAO">
  <data key="d0">AND</data>
</edge>
<edge source="14 JIARUI FANG" target="JIE ZHOU">
  <data key="d0">AND</data>
</edge>
<edge source="21 MINGCONG HAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HANZE ZHANG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="RONG CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HAIBO CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="22" target="KAIMING HE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="22" target="XIANGYU ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="22" target="SHAOQING REN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="22" target="JIAN SUN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="23" target="CHIEN-CHIN HUANG, GU JIN, AND JINYANG LI">
  <data key="d0">IS</data>
</edge>
<edge source="54 JING WANG" target="YOUYOU LU">
  <data key="d0">AND</data>
</edge>
<edge source="54 JING WANG" target="QING WANG">
  <data key="d0">AND</data>
</edge>
<edge source="54 JING WANG" target="MINHUI XIE">
  <data key="d0">AND</data>
</edge>
<edge source="54 JING WANG" target="KEJI HUANG">
  <data key="d0">AND</data>
</edge>
<edge source="54 JING WANG" target="JIWU SHU">
  <data key="d0">AND</data>
</edge>
<edge source="56" target="XIAOHUI WANG">
  <data key="d0">IS</data>
</edge>
<edge source="56" target="YING XIONG">
  <data key="d0">IS</data>
</edge>
<edge source="56" target="YANG WEI">
  <data key="d0">IS</data>
</edge>
<edge source="56" target="MINGXUAN WANG">
  <data key="d0">IS</data>
</edge>
<edge source="56" target="LEI LI">
  <data key="d0">IS</data>
</edge>
<edge source="64" target="ZHE ZHOU">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="64" target="XUECHAO WEI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="64" target="JIEJING ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="64" target="GUANGYU SUN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="TRAINING DEEP NETS" target="SUBLINEAR MEMORY COST">
  <data key="d0">HAS</data>
</edge>
<edge source="8 WEI-LIN CHIANG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ZHUOHAN LI" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ZI LIN" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="YING SHENG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ZHANGHAO WU" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="HAO ZHANG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="LIANMIN ZHENG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="SIYUAN ZHUANG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="YONGHAO ZHUANG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="JOSEPH E. GONZALEZ" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ION STOICA" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ION STOICA" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="ION STOICA" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="ERIC P. XING" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="12 WEIHAO CUI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HAN ZHAO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="QUAN CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HAO WEI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="ZIRUI LI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="DEZE ZENG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="CHAO LI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="MINYI GUO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="28" target="ZHUOHAN LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="LIANMIN ZHENG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="YINMIN ZHONG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="VINCENT LIU">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="YING SHENG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="XIN JIN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="YANPING HUANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="ZHIFENG CHEN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="HAO ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="JOSEPH E GONZALEZ">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="ET AL">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="JOSEPH E GONZALEZ" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="59" target="NUMBER">
  <data key="d0">IS A</data>
</edge>
<edge source="YONGHUI WU" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="MIKE SCHUSTER" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="QUOC V LE" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="MOHAMMAD NOROUZI" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="WOLFGANG MACHEREY" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="MAXIM KRIKUN" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="YUAN CAO" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="QIN GAO" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="KLAUS MACHEREY" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="63 LIANMIN ZHENG, ZHUOHAN LI, HAO ZHANG, YONGHAO ZHUANG, ZHIFENG CHEN, YANPING HUANG, YIDA WANG, YUANZHONG XU, DANYANG ZHUO, ERIC P XING" target="ET AL.">
  <data key="d0">IS</data>
</edge>
<edge source="VICUNA" target="AN OPEN-SOURCE CHATBOT">
  <data key="d0">IS</data>
</edge>
<edge source="VICUNA" target="GPT-4">
  <data key="d0">IMPRESSES</data>
</edge>
<edge source="VICUNA" target="90 CHATGPT QUALITY">
  <data key="d0">HAS</data>
</edge>
<edge source="GPT-4" target="TECHNICAL REPORT">
  <data key="d0">IS</data>
</edge>
<edge source="ORGBLOG2023-03-30-VICUNA 9" target="AAKANKSHA CHOWDHERY, SHARAN NARANG, JACOB DEVLIN, MAARTEN BOSMA, GAURAV MISHRA, ADAM ROBERTS, PAUL BARHAM, HYUNG WON CHUNG, CHARLES SUTTON, SEBASTIAN GEHRMANN, ET AL.">
  <data key="d0">HAS AUTHORS</data>
</edge>
<edge source="PALM" target="SCALING LANGUAGE MODELING WITH PATHWAYS">
  <data key="d0">IS</data>
</edge>
<edge source="DANIEL CRANKSHAW" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JOSEPH GONZALEZ" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="XIN WANG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="GUILIO ZHOU" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="MICHAEL J FRANKLIN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="INFERLINE" target="LATENCY-AWARE PROVISIONING AND SCALING FOR PREDICTION SERVING PIPELINES">
  <data key="d0">IS</data>
</edge>
<edge source="11TH ACM SYMPOSIUM" target="CLOUD COMPUTING">
  <data key="d0">IS ON</data>
</edge>
<edge source="CLIPPER" target="A LOW-LATENCY ONLINE PREDICTION SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION" target="NSDI 17">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="20TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION" target="NSDI 23">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="DVABATCH" target="DIVERSITY-AWARE MULTI-ENTRY MULTI-EXIT BATCHING">
  <data key="d0">IS</data>
</edge>
<edge source="DVABATCH" target="EFFICIENT PROCESSING OF DNN SERVICES ON GPUS">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="USENIX ANNUAL TECHNICAL CONFERENCE" target="2022">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="USENIX ANNUAL TECHNICAL CONFERENCE" target="USENIX ATC 22">
  <data key="d0">ABBREVIATED AS</data>
</edge>
<edge source="13 TRI DAO, DAN FU, STEFANO ERMON, ATRI RUDRA, AND CHRISTOPHER R." target="IN 2022">
  <data key="d0">PUBLISHED</data>
</edge>
<edge source="FLASHATTENTION" target="FAST AND MEMORY-EFFICIENT EXACT ATTENTION WITH IO-AWARENESS">
  <data key="d0">IS</data>
</edge>
<edge source="TURBOTRANSFORMERS" target="AN EFFICIENT GPU SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="TURBOTRANSFORMERS" target="TRANSFORMER MODELS">
  <data key="d0">FOR</data>
</edge>
<edge source="26TH ACM SIGPLAN SYMPOSIUM" target="PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING">
  <data key="d0">IS ON</data>
</edge>
<edge source="THE 23RD ACM SIGPLAN SYMPOSIUM" target="PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING">
  <data key="d0">IS ON</data>
</edge>
<edge source="FASTAPI" target="15">
  <data key="d0">IS</data>
</edge>
<edge source="FASTAPI" target="A WEB FRAMEWORK">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS:GITHUB.COM" target="TIANGOLOFASTAPI">
  <data key="d0">HOSTS</data>
</edge>
<edge source="HTTPS:GITHUB.COM" target="NVIDIA FASTERTRANSFORMER">
  <data key="d0">HOSTS</data>
</edge>
<edge source="16" target="PIN GAO">
  <data key="d0">IS</data>
</edge>
<edge source="16" target="LINGFAN YU">
  <data key="d0">IS</data>
</edge>
<edge source="16" target="YONGWEI WU">
  <data key="d0">IS</data>
</edge>
<edge source="16" target="JINYANG LI">
  <data key="d0">IS</data>
</edge>
<edge source="LOW LATENCY RNN INFERENCE" target="CELLULAR BATCHING">
  <data key="d0">IS WITH</data>
</edge>
<edge source="THIRTEENTH EUROSYS CONFERENCE" target="PROCEEDINGS">
  <data key="d0">IS</data>
</edge>
<edge source="17 AMIR GHOLAMI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="ZHEWEI YAO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="SEHOON KIM" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="MICHAEL W MAHONEY" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="KURT KEUTZER" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="AI" target="MEMORY WALL">
  <data key="d0">AND</data>
</edge>
<edge source="RISELAB MEDIUM POST 1" target="2021">
  <data key="d0">HAS YEAR</data>
</edge>
<edge source="RISELAB MEDIUM POST 1" target="6">
  <data key="d0">HAS PAGE</data>
</edge>
<edge source="GITHUB" target="18">
  <data key="d0">HAS NUMBER</data>
</edge>
<edge source="GITHUB" target="COPILOT">
  <data key="d0">HAS FEATURE</data>
</edge>
<edge source="COPILOT" target="GITHUB">
  <data key="d0">IS FEATURE OF</data>
</edge>
<edge source="GOOGLE" target="TEXT">
  <data key="d0">IS MENTIONED</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="ARPAN GUJARATI">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="REZA KARIMI">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="SAFYA ALZAYAT">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="WEI HAO">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="ANTOINE KAUFMANN">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="YMIR VIGFUSSON">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="JONATHAN MACE">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="SERVING DNNS" target="LIKE CLOCKWORK">
  <data key="d0">IS</data>
</edge>
<edge source="PERFORMANCE PREDICTABILITY" target="FROM THE BOTTOM UP">
  <data key="d0">IS</data>
</edge>
<edge source="OSDI 20" target="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS HELD IN</data>
</edge>
<edge source="14TH USENIX CONFERENCE" target="OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS ON</data>
</edge>
<edge source="OSDI 22" target="16TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS</data>
</edge>
<edge source="MICROSECOND-SCALE PREEMPTION" target="CONCURRENT GPU-ACCELERATED DNN INFERENCES">
  <data key="d0">IS FOR</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING" target="IMAGE RECOGNITION">
  <data key="d0">IS FOR</data>
</edge>
<edge source="SWAPADVISOR" target="DEEP LEARNING BEYOND THE GPU MEMORY LIMIT VIA SMART SWAPPING">
  <data key="d0">PUSHES</data>
</edge>
<edge source="THE TWENTY-FIFTH INTERNATIONAL CONFERENCE" target="ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS">
  <data key="d0">IS ON</data>
</edge>
<edge source="24 PARAS JAIN" target="AJAY JAIN">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="ANIRUDDHA NRUSIMHA">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="AMIR GHOLAMI">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="PIETER ABBEEL">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="JOSEPH GONZALEZ">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="KURT KEUTZER">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="ION STOICA">
  <data key="d0">AND</data>
</edge>
<edge source="40 SHISHIR G PATIL" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="PARAS JAIN" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="PRABAL DUTTA" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="CHECK-MATE" target="BREAKING THE MEMORY WALL WITH OPTIMAL TENSOR REMATERIALIZATION">
  <data key="d0">IS</data>
</edge>
<edge source="PROCEEDINGS OF MACHINE LEARNING AND SYSTEMS" target="2">
  <data key="d0">VOLUME</data>
</edge>
<edge source="PROCEEDINGS OF MACHINE LEARNING AND SYSTEMS" target="2020">
  <data key="d0">YEAR</data>
</edge>
<edge source="PROCEEDINGS OF MACHINE LEARNING AND SYSTEMS" target="497-511">
  <data key="d0">PAGE NUMBERS</data>
</edge>
<edge source="TOM KILBURN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="DAVID BG EDWARDS" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="MICHAEL J LANIGAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="FRANK H SUMNER" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="1962" target="YEAR">
  <data key="d0">IS A</data>
</edge>
<edge source="ONE-LEVEL STORAGE SYSTEM" target="SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="IRE TRANSACTIONS ON ELECTRONIC COMPUTERS" target="2">
  <data key="d0">VOLUME</data>
</edge>
<edge source="IRE TRANSACTIONS ON ELECTRONIC COMPUTERS" target="1962">
  <data key="d0">YEAR</data>
</edge>
<edge source="IRE TRANSACTIONS ON ELECTRONIC COMPUTERS" target="223-235">
  <data key="d0">PAGE NUMBERS</data>
</edge>
<edge source="26" target="BRIAN LESTER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="26" target="RAMI AL-RFOU">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="26" target="NOAH CONSTANT">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="POWER OF SCALE" target="PARAMETER-EFFICIENT PROMPT TUNING">
  <data key="d0">IS FOR</data>
</edge>
<edge source="XIANG LISA LI" target="PERCY LIANG">
  <data key="d0">AND</data>
</edge>
<edge source="PREFIX-TUNING" target="OPTIMIZING CONTINUOUS PROMPTS FOR GENERATION">
  <data key="d0">IS</data>
</edge>
<edge source="29 LINGXIAO MA" target="ZHIQIANG XIE">
  <data key="d0">AND</data>
</edge>
<edge source="ZHIQIANG XIE" target="ZHI YANG">
  <data key="d0">AND</data>
</edge>
<edge source="ZHI YANG" target="JILONG XUE">
  <data key="d0">AND</data>
</edge>
<edge source="JILONG XUE" target="YOUSHAN MIAO">
  <data key="d0">AND</data>
</edge>
<edge source="YOUSHAN MIAO" target="WEI CUI">
  <data key="d0">AND</data>
</edge>
<edge source="WEI CUI" target="WENXIANG HU">
  <data key="d0">AND</data>
</edge>
<edge source="WENXIANG HU" target="FAN YANG">
  <data key="d0">AND</data>
</edge>
<edge source="FAN YANG" target="LINTAO ZHANG">
  <data key="d0">AND</data>
</edge>
<edge source="LINTAO ZHANG" target="LIDONG ZHOU">
  <data key="d0">AND</data>
</edge>
<edge source="RAMMER" target="HOLISTIC DEEP LEARNING COMPILER OPTIMIZATIONS WITH RTASKS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="NVIDIA" target="30">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="NVIDIA" target="31">
  <data key="d0">IS</data>
</edge>
<edge source="NVIDIA" target="32">
  <data key="d0">IS</data>
</edge>
<edge source="N. D." target="TRITON INFERENCE SERVER">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS://DEVELOPER.NVIDIA.COM" target="NVIDIA-TRITON-INFERENCE-SERVER">
  <data key="d0">HOSTS</data>
</edge>
<edge source="HTTPS" target="DEVELOPER.NVIDIA.COMNCCL">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="OPENAI.COM/BLOG/CUSTOM-INSTRUCTIONS-FOR-CHATGPT">
  <data key="d0">IS</data>
</edge>
<edge source="NCCL" target="THE NVIDIA COLLECTIVE COMMUNICATION LIBRARY">
  <data key="d0">IS</data>
</edge>
<edge source="TENSORFLOW-SERVING" target="FLEXIBLE ML SERVING">
  <data key="d0">IS</data>
</edge>
<edge source="TENSORFLOW-SERVING" target="HIGH-PERFORMANCE ML SERVING">
  <data key="d0">IS</data>
</edge>
<edge source="OPENAI" target="34">
  <data key="d0">IS</data>
</edge>
<edge source="OPENAI" target="HTTPS://OPENAI.COM/BLOG/OPENAI-API">
  <data key="d0">HAS WEBSITE</data>
</edge>
<edge source="OPENAI" target="HTTPS://OPENAI.COM/BLOG/CHATGPT">
  <data key="d0">HAS WEBSITE</data>
</edge>
<edge source="OPENAI" target="BLOG CUSTOM INSTRUCTIONS FOR CHATGPT">
  <data key="d0">HAS</data>
</edge>
<edge source="CHATBOT ARENA LEADERBOARD" target="WEEK 8">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="WEEK 8" target="MT-BENCH">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="WEEK 8" target="VICUNA-33B">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="HTTPS:LMSYS.ORG" target="BLOG POST DATE 2023-06-22">
  <data key="d0">HAS</data>
</edge>
<edge source="HTTPS:LMSYS.ORG" target="LEADERBOARD">
  <data key="d0">HAS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32" target="2019">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="POET" target="NEURAL NETWORKS">
  <data key="d0">TRAINS</data>
</edge>
<edge source="POET" target="INTEGRATED REMATERIALIZATION AND PAGING">
  <data key="d0">USES</data>
</edge>
<edge source="NEURAL NETWORKS" target="TINY DEVICES">
  <data key="d0">ARE TRAINED ON</data>
</edge>
<edge source="INTERNATIONAL CONFERENCE ON MACHINE LEARNING" target="AN EVENT">
  <data key="d0">IS</data>
</edge>
<edge source="PMLR" target="1757317583">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="41" target="REINER POPE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="SHOLTO DOUGLAS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="AAKANKSHA CHOWDHERY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="JACOB DEVLIN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="JAMES BRADBURY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="ANSELM LEVSKAYA">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="JONATHAN HEEK">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="KEFAN XIAO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="SHIVANI AGRAWAL">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="JEFF DEAN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ZERO-OFFLOAD" target="DEMOCRATIZING BILLION-SCALE MODEL TRAINING">
  <data key="d0">IS</data>
</edge>
<edge source="AMAZON WEB SERVICES" target="HTTPS://WWW.REUTERS.COM/TECHNOLOGY/TECH-GIANTS-AI-LIKE-BING-BARD-POSES-BILLION-DOLLAR-SEARCH-PROBLEM-2023-02-22">
  <data key="d0">IS MENTIONED IN</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="HAICHEN SHEN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="LEQUN CHEN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="YUCHEN JIN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="LIANGYU ZHAO">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="BINGYU KONG">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="MATTHAI PHILIPOSE">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="ARVIND KRISHNAMURTHY">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="RAVI SUNDARAM">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="NEXUS" target="A GPU CLUSTER ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="NEXUS" target="ACCELERATING DNN-BASED VIDEO ANALYSIS">
  <data key="d0">PURPOSE</data>
</edge>
<edge source="27TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES" target="PROCEEDINGS">
  <data key="d0">IS</data>
</edge>
<edge source="HIGH-THROUGHPUT GENERATIVE INFERENCE" target="LARGE LANGUAGE MODELS">
  <data key="d0">IS OF</data>
</edge>
<edge source="HIGH-THROUGHPUT GENERATIVE INFERENCE" target="A SINGLE GPU">
  <data key="d0">USES</data>
</edge>
<edge source="47" target="MOHAMMAD SHOEYBI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="MOSTOFA PATWARY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="RAUL PURI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="PATRICK LEGRESLEY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="JARED CASPER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="BRYAN CATANZARO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MEGATRON-LM" target="TRAINING MULTI-BILLION PARAMETER LANGUAGE MODELS">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="MEGATRON-LM" target="MODEL PARALLELISM">
  <data key="d0">USES</data>
</edge>
<edge source="48" target="BENOIT STEINER">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="48" target="MOSTAFA ELHOUSHI">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="48" target="JACOB KAHN">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="48" target="JAMES HEGARTY">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="OLLA" target="THE LIFETIME OF ARRAYS">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="OLLA" target="THE LOCATION OF ARRAYS">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="OLLA" target="THE MEMORY USAGE OF NEURAL NETWORKS">
  <data key="d0">REDUCES</data>
</edge>
<edge source="DOI.ORG/10.48550/ARXIV.2210.12924" target="ILYA SUTSKEVER">
  <data key="d0">HAS AUTHOR</data>
</edge>
<edge source="DOI.ORG/10.48550/ARXIV.2210.12924" target="ORIOL VINYALS">
  <data key="d0">HAS AUTHOR</data>
</edge>
<edge source="DOI.ORG/10.48550/ARXIV.2210.12924" target="QUOC V LE">
  <data key="d0">HAS AUTHOR</data>
</edge>
<edge source="SEQUENCE TO SEQUENCE LEARNING" target="NEURAL NETWORKS">
  <data key="d0">IS DONE WITH</data>
</edge>
<edge source="50" target="ROHAN TAORI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="ISHAAN GULRAJANI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="TIANYI ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="YANN DUBOIS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="XUECHEN LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="CARLOS GUESTRIN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="PERCY LIANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="TATSUNORI B. HASHIMOTO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="55" target="LINNAN WANG">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="JINMIAN YE">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="YIYANG ZHAO">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="WEI WU">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="ANG LI">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="SHUAI-WEN LEON SONG">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="ZENGLIN XU">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="TIM KRASKA">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="57 YIZHONG WANG" target="YEGANEH KORDI">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="SWAROOP MISHRA">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="ALISA LIU">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="NOAH A SMITH">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="DANIEL KHASHABI">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="HANNANEH HAJISHIRZI">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="STANFORD ALPACA" target="AN INSTRUCTION-FOLLOWING LLAMA MODEL">
  <data key="d0">IS</data>
</edge>
<edge source="GITHUB.COM/TATSU-LABS" target="STANFORD ALPACA">
  <data key="d0">HOSTS</data>
</edge>
<edge source="SHAREGPT TEAM" target="51">
  <data key="d0">HAS NUMBER</data>
</edge>
<edge source="HTTPS:SHAREGPT.COM" target="HUGO TOUVRON, THIBAUT LAVRIL, GAUTIER IZACARD, XAVIER MARTINET, MARIE-ANNE LACHAUX, TIMOTHE LACROIX, BAPTISTE ROZIRE, NAMAN GOYAL, ERIC HAMBRO, FAISAL AZHAR, ET AL.">
  <data key="d0">HAS AUTHORS</data>
</edge>
<edge source="LLAMA" target="OPEN AND EFFICIENT FOUNDATION LANGUAGE MODELS">
  <data key="d0">IS</data>
</edge>
<edge source="53" target="ASHISH VASWANI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="NOAM SHAZEER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="NIKI PARMAR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="JAKOB USZKOREIT">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="LLION JONES">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="AIDAN N GOMEZ">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="UKASZ KAISER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="ILLIA POLOSUKHIN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ATTENTION" target="ALL YOU NEED">
  <data key="d0">IS</data>
</edge>
<edge source="PACMAN" target="AN EFFICIENT COMPACTION APPROACH">
  <data key="d0">IS</data>
</edge>
<edge source="PACMAN" target="LOG-STRUCTURED KEY-VALUE STORE">
  <data key="d0">APPLIES TO</data>
</edge>
<edge source="LOG-STRUCTURED KEY-VALUE STORE" target="PERSISTENT MEMORY">
  <data key="d0">OPERATES ON</data>
</edge>
<edge source="SUPERNEURONS" target="DYNAMIC GPU MEMORY MANAGEMENT FOR TRAINING DEEP NEURAL NETWORKS">
  <data key="d0">IS</data>
</edge>
<edge source="LIGHTSEQ" target="A HIGH PERFORMANCE INFERENCE LIBRARY FOR TRANSFORMERS">
  <data key="d0">IS</data>
</edge>
<edge source="2021 CONFERENCE" target="NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
  <data key="d0">IS OF</data>
</edge>
<edge source="NORTH AMERICAN CHAPTER" target="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
  <data key="d0">IS OF</data>
</edge>
<edge source="SELF-INSTRUCT" target="ALIGNING LANGUAGE MODEL WITH SELF GENERATED INSTRUCTIONS">
  <data key="d0">IS ABOUT</data>
</edge>
<edge source="GOOGLE'S NEURAL MACHINE TRANSLATION SYSTEM" target="THE GAP BETWEEN HUMAN AND MACHINE TRANSLATION">
  <data key="d0">BRIDGES</data>
</edge>
<edge source="60 GYEONG-IN YU" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JOO SEONG JEONG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="GEON-WOO KIM" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="SOOJEONG KIM" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="BYUNG-GON CHUN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="61" target="HONG ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="61" target="YUPENG TANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="61" target="ANURAG KHANDELWAL">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="61" target="ION STOICA">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="SHEPHERD" target="DNNS IN THE WILD">
  <data key="d0">SERVES</data>
</edge>
<edge source="USENIX ASSOCIATION" target="BOSTON, MA">
  <data key="d0">LOCATION</data>
</edge>
<edge source="USENIX ASSOCIATION" target="787808">
  <data key="d0">ZIP CODE</data>
</edge>
<edge source="SUSAN ZHANG" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="STEPHEN ROLLER" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="NAMAN GOYAL" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="MIKEL ARTETXE" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="MOYA CHEN" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="SHUOHUI CHEN" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="CHRISTOPHER DEWAN" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="MONA DIAB" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="XIAN LI" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="XI VICTORIA LIN" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="NSDI23" target="CONFERENCE">
  <data key="d0">IS A</data>
</edge>
<edge source="HTTPS://WWW.USENIX.ORG/CONFERENCE/NSDI23/PRESENTATION/ZHANG-HONG" target="URL">
  <data key="d0">IS A</data>
</edge>
<edge source="ALPA" target="AUTOMATING INTER-AND INTRA-OPERATOR PARALLELISM FOR DISTRIBUTED DEEP LEARNING">
  <data key="d0">IS</data>
</edge>
<edge source="PETS" target="A UNIFIED FRAMEWORK FOR PARAMETER-EFFICIENT TRANSFORMERS SERVING">
  <data key="d0">IS</data>
</edge>
</graph></graphml>