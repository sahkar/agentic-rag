<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd">
  <key id="d0" for="edge" attr.name="relationship" attr.type="string" />
  <graph edgedefault="directed">
    <node id="This paper" />
    <node id="Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation" />
    <node id="November 4-6, 2020" />
    <node id="978-1-939133-19-9" />
    <node id="Open access to the Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation" />
    <node id="USENIX" />
    <node id="PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications" />
    <node id="Zhihao Bai, Zhen Zhang, Yibo Zhu, Xin Jin" />
    <node id="Zhihao Bai" />
    <node id="Johns Hopkins University" />
    <node id="Zhen Zhang" />
    <node id="Yibo Zhu" />
    <node id="ByteDance Inc." />
    <node id="Xin Jin" />
    <node id="Deep learning workloads" />
    <node id="throughput-intensive training tasks and latency-sensitive inference tasks" />
    <node id="The dominant practice today" />
    <node id="to provision dedicated GPU clusters for training and inference separately" />
    <node id="GPU Clusters" />
    <node id="Shared GPU clusters" />
    <node id="training and inference" />
    <node id="GPUs" />
    <node id="the current practice" />
    <node id="dedicated clusters for training and inference separately" />
    <node id="We" />
    <node id="GPU clusters" />
    <node id="different applications including training and inference" />
    <node id="based on the peak load" />
    <node id="limited sharing between applications and task types" />
    <node id="the need to meet strict Service-Level Objectives (SLOs)" />
    <node id="PipeSwitch" />
    <node id="a system" />
    <node id="unused cycles of an inference application to be filled by training or other inference applications" />
    <node id="multiple DL applications" />
    <node id="the same GPU" />
    <node id="the entire GPU memory" />
    <node id="GPU utilization" />
    <node id="significantly" />
    <node id="without sacrificing SLOs" />
    <node id="a task startup overhead of 3.66.6 ms" />
    <node id="a total overhead of 5.434.6 ms" />
    <node id="1050 better than NVIDIA MPS" />
    <node id="near 100 GPU utilization" />
    <node id="single-GPU tasks for training and inference" />
    <node id="Multi-GPU inference tasks" />
    <node id="performing PipeSwitch on each GPU with transactions" />
    <node id="single-GPU training for training tasks" />
    <node id="asynchronous multi-GPU training for data parallel strategies" />
    <node id="preempting one GPU" />
    <node id="other GPUs" />
    <node id="a significant fraction of tasks in real-world workloads" />
    <node id="a single GPU" />
    <node id="synchronous multi-GPU training" />
    <node id="elastic synchronous training" />
    <node id="dynamic changing of the number of GPUs used for training" />
    <node id="high throughput close to the upper bound" />
    <node id="the performance of PipeSwitch" />
    <node id="experiments" />
    <node id="a variety of DNN models and GPU cards" />
    <node id="the agility of DL applications" />
    <node id="introducing pipelined context switching" />
    <node id="The key idea" />
    <node id="the layered structure of neural network models and their layer-by-layer computation pattern" />
    <node id="model transmission over the PCIe and task execution in the GPU with model-aware grouping" />
    <node id="we" />
    <node id="a pipelined model transmission mechanism" />
    <node id="model transmission over the PCIe and model computation in the GPU" />
    <node id="Transmitting a task from CPU to GPU" />
    <node id="the PCIe bandwidth" />
    <node id="unified memory management mechanisms" />
    <node id="active-standby worker switching mechanisms" />
    <node id="unified memory management and active-standby worker switching mechanisms" />
    <node id="the pipelining" />
    <node id="process-level isolation" />
    <node id="an active-standby mechanism for fast worker switching and process-level isolation" />
    <node id="a PipeSwitch prototype" />
    <node id="it with PyTorch" />
    <node id="the model structure" />
    <node id="hooks for PyTorch to wait for transmission or synchronize the execution" />
    <node id="Deep learning (DL)" />
    <node id="an emerging family of intelligent applications" />
    <node id="intelligent applications" />
    <node id="many domains" />
    <node id="retail" />
    <node id="transportation" />
    <node id="finance" />
    <node id="healthcare" />
    <node id="one of the most widely-used classes of accelerators for DL" />
    <node id="DL workloads" />
    <node id="throughput-intensive training tasks" />
    <node id="latency-sensitive inference tasks" />
    <node id="Inference tasks" />
    <node id="training clusters under ash crowds" />
    <node id="Training tasks" />
    <node id="inference clusters when the inference load is low" />
    <node id="training cluster" />
    <node id="training tasks for inference tasks" />
    <node id="inference clusters" />
    <node id="the peak load" />
    <node id="Inference clusters" />
    <node id="user requests" />
    <node id="strict SLOs" />
    <node id="production systems" />
    <node id="each application on per-GPU granularity" />
    <node id="provisioning on per-GPU granularity" />
    <node id="the interference between applications" />
    <node id="GPUs to applications on per-GPU granularity" />
    <node id="GPUs to the VMs, containers or processes of an application" />
    <node id="the interference between different applications" />
    <node id="the SLO requirements" />
    <node id="the same GPU server" />
    <node id="packing multiple DL applications to the same GPU server" />
    <node id="GPU utilization via time-sharing" />
    <node id="operating systems" />
    <node id="high CPU utilization" />
    <node id="task scheduling and context switching" />
    <node id="The idea of ne-grained CPU time-sharing" />
    <node id="cluster scheduling" />
    <node id="ne-grained time-sharing" />
    <node id="CPU workloads" />
    <node id="better utilization than provisioning dedicated resources" />
    <node id="necessary process-level isolation" />
    <node id="scheduling cycles" />
    <node id="ne-grained enabled" />
    <node id="Google Borg 1" />
    <node id="online services and batch jobs" />
    <node id="20-30 machines" />
    <node id="not packing them" />
    <node id="GPU" />
    <node id="high overhead when switching between tasks" />
    <node id="The gap" />
    <node id="the precious GPU memory and slow switching" />
    <node id="naively using GPUs in the same way as CPUs" />
    <node id="the requirements of DL inference that have strict SLOs in the range of tens to hundreds of milliseconds" />
    <node id="a GPU" />
    <node id="a DNN model (e.g., ResNet)" />
    <node id="the GPU" />
    <node id="state-of-the-art tricks like CUDA unified memory" />
    <node id="multiple seconds delay before serving the first inference request" />
    <node id="CPU applications" />
    <node id="milliseconds or even microseconds" />
    <node id="the existing solution" />
    <node id="spatially share the GPU memory" />
    <node id="this approach" />
    <node id="strong GPU memory isolation between applications" />
    <node id="NVIDIA Multiple Process Sharing (MPS) 6" />
    <node id="multiple processes to use the same GPU" />
    <node id="Salus 7" />
    <node id="NVIDIA Multiple Process Sharing (MPS) 6 and Salus 7" />
    <node id="all processes data to be preloaded into the GPU memory" />
    <node id="multi-process support from NVIDIA" />
    <node id="the inference process to share the GPU with the training process" />
    <node id="NVIDIA MPS 6" />
    <node id="official support for sharing a GPU between multiple processes" />
    <node id="GPU memory" />
    <node id="much more limited than host memory" />
    <node id="many applications" />
    <node id="one single memory-intensive training task" />
    <node id="all the GPU memory" />
    <node id="The training task" />
    <node id="its GPU environment" />
    <node id="freeing the GPU memory" />
    <node id="when inference tasks come" />
    <node id="memory footprints of inference tasks" />
    <node id="memory footprints" />
    <node id="models" />
    <node id="larger" />
    <node id="request batching" />
    <node id="to increase throughput" />
    <node id="increasing throughput" />
    <node id="GPU memory requirement of inference applications" />
    <node id="a context switching design" />
    <node id="the switching overhead" />
    <node id="the contents on GPU memory" />
    <node id="a better approach for efficiently time-sharing GPUs" />
    <node id="no existing solution" />
    <node id="such context switching abstraction for GPU" />
    <node id="a new technology called pipelined context switching" />
    <node id="pipelined context switching" />
    <node id="the characteristics of DL applications" />
    <node id="millisecond-scale overhead for switching tasks on GPUs" />
    <node id="a major challenge fast GPU context switching between different processes" />
    <node id="pipeline" />
    <node id="computation and GPU memory swapping" />
    <node id="fast context switching" />
    <node id="application" />
    <node id="context switching" />
    <node id="if the application is already loaded in the GPU" />
    <node id="task switching overhead on GPUs for DL applications" />
    <node id="DNN models" />
    <node id="host memory" />
    <node id="much larger and cheaper than GPU memory" />
    <node id="the models" />
    <node id="context-switching" />
    <node id="training or inference" />
    <node id="enterprises" />
    <node id="multiple users" />
    <node id="privately shared" />
    <node id="publicly shared" />
    <node id="11 M. Jeon, S. Venkataraman, A. Phanishayee, u. Qian, W. Xiao, and F. Yang" />
    <node id="Analysis of large-scale multi-tenant GPU clusters for DNN training workloads" />
    <node id="USENIX ATC" />
    <node id="2019" />
    <node id="USENIX ATC, 2019" />
    <node id="512 14th USENIX Symposium on Operating Systems Design and Implementation" />
    <node id="USENIX Association" />
    <node id="the number of applications that can be multiplexed" />
    <node id="the GPU memory size" />
    <node id="each application" />
    <node id="the entire GPU compute and memory resources during its time slice" />
    <node id="GPU-efficient multiplexing of many DL applications on GPU servers via fine-grained time-sharing" />
    <node id="millisecond-scale latencies and high throughput as dedicated servers" />
    <node id="GPU-efficient fine-grained time-sharing for multiple DL applications" />
    <node id="millisecond-scale context switching latencies" />
    <node id="high throughput" />
    <node id="all the ideas into our system" />
    <node id="the gap of GPU memory sharing and switching" />
    <node id="the design of an efficient time-sharing GPU cluster for DL workloads" />
    <node id="GPU-efficient multiplexing of multiple DL applications on GPU servers" />
    <node id="millisecond-scale task switching time" />
    <node id="DL applications on time-sharing GPUs to meet strict SLOs" />
    <node id="small switching overhead" />
    <node id="DL applications" />
    <node id="strict SLO requirements" />
    <node id="a measurement study" />
    <node id="the task switching overhead" />
    <node id="the overhead of each component" />
    <node id="measurement study" />
    <node id="profile the task switching overhead" />
    <node id="switching overhead" />
    <node id="four components" />
    <node id="old task cleaning" />
    <node id="new task initialization" />
    <node id="GPU memory allocation" />
    <node id="model transmission via PCIe from CPU to GPU" />
    <node id="Instance Type" />
    <node id="g4dn.2xlarge" />
    <node id="p3.2xlarge" />
    <node id="GPU Type of g4dn.2xlarge" />
    <node id="NVIDIA T4" />
    <node id="GPU Type of p3.2xlarge" />
    <node id="NVIDIA V100" />
    <node id="Task Cleaning time on g4dn.2xlarge" />
    <node id="155 ms" />
    <node id="Task Cleaning time on p3.2xlarge" />
    <node id="165 ms" />
    <node id="Task Initialization time on g4dn.2xlarge" />
    <node id="5530 ms" />
    <node id="Task Initialization time on p3.2xlarge" />
    <node id="7290 ms" />
    <node id="Memory Allocation time on g4dn.2xlarge" />
    <node id="10 ms" />
    <node id="Memory Allocation time on p3.2xlarge" />
    <node id="13 ms" />
    <node id="Model Transmission time on g4dn.2xlarge" />
    <node id="91 ms" />
    <node id="Model Transmission time on p3.2xlarge" />
    <node id="81 ms" />
    <node id="Total Overhead on g4dn.2xlarge" />
    <node id="5787 ms" />
    <node id="Total Overhead on p3.2xlarge" />
    <node id="7551 ms" />
    <node id="Inference Time on g4dn.2xlarge" />
    <node id="105 ms" />
    <node id="Inference Time on p3.2xlarge" />
    <node id="32 ms" />
    <node id="Table 1" />
    <node id="Measurement results of task switching overhead and the breakdown of individual components" />
    <node id="Every component" />
    <node id="a considerable amount of time" />
    <node id="amount of time" />
    <node id="tens of milliseconds to seconds" />
    <node id="inference task" />
    <node id="tens of milliseconds on a GPU" />
    <node id="latency SLOs" />
    <node id="typically a small multiple of the inference time" />
    <node id="One source of the overhead" />
    <node id="the contentions both on the computation and memory of the GPU" />
    <node id="the training task" />
    <node id="when an inference task comes" />
    <node id="a holistic approach" />
    <node id="the overhead of all the components" />
    <node id="Our design" />
    <node id="a key observation" />
    <node id="a layered structure" />
    <node id="a layer-by-layer computation pattern" />
    <node id="usually deep" />
    <node id="multiple layers stacking one on another" />
    <node id="computation of DNN models" />
    <node id="layer by layer" />
    <node id="there" />
    <node id="the entire model to be transmitted to the GPU before starting computation" />
    <node id="a task" />
    <node id="the entire model to be transmitted to the GPU before beginning the computation" />
    <node id="Naive pipelining on per-layer granularity" />
    <node id="high overhead on tensor transmission and synchronization" />
    <node id="Pipelining on per-layer granularity" />
    <node id="synchronization for every layer" />
    <node id="layers into groups" />
    <node id="an optimal model-aware grouping algorithm" />
    <node id="the best grouping strategy for a given model" />
    <node id="an algorithm to find the optimal grouping strategy for a given model" />
    <node id="The computation of a DL task" />
    <node id="a simple, regular pattern for memory allocation" />
    <node id="A DL task" />
    <node id="two important types of data in the GPU memory" />
    <node id="two important types of data" />
    <node id="the DNN model (including the model parameters) and the intermediate results" />
    <node id="The default general-purpose GPU memory management" />
    <node id="an overkill" />
    <node id="unnecessary overhead" />
    <node id="NVIDIA" />
    <node id="CUDA unified memory 4" />
    <node id="memory movement between the host memory and the GPU memory" />
    <node id="applications" />
    <node id="unified memory management with a dedicated memory daemon" />
    <node id="the overhead" />
    <node id="unified memory management with the memory daemon" />
    <node id="minimal memory footprint" />
    <node id="extra memory copies" />
    <node id="The daemon" />
    <node id="the GPU memory" />
    <node id="it to each task" />
    <node id="the expensive GPU memory manager" />
    <node id="the memory daemon" />
    <node id="cudaMalloc to obtain the GPU memory when the system starts" />
    <node id="the memory to the workers at runtime" />
    <node id="the GPU memory manager" />
    <node id="the existing system" />
    <node id="minimal changes" />
    <node id="The DNN models" />
    <node id="only once in the memory daemon" />
    <node id="in every worker" />
    <node id="Storing the DNN models only once in the memory daemon" />
    <node id="memory footprint" />
    <node id="that the memory allocation for a DNN model is deterministic" />
    <node id="memory allocation for a DNN model" />
    <node id="deterministic" />
    <node id="extra memory copies between the daemon and the workers" />
    <node id="the IPC overhead" />
    <node id="no unified memory management" />
    <node id="each worker to keep a copy for each DNN model" />
    <node id="keeping a copy for each DNN model" />
    <node id="the memory footprint" />
    <node id="Each server" />
    <node id="an active worker" />
    <node id="multiple standby workers" />
    <node id="A server" />
    <node id="one or more standby workers" />
    <node id="The active worker" />
    <node id="the current task on the GPU" />
    <node id="The standby workers" />
    <node id="on the CPU" />
    <node id="the next task" />
    <node id="the worker that currently executes a task in the GPU" />
    <node id="worker" />
    <node id="a process that executes tasks on one GPU" />
    <node id="active worker" />
    <node id="current task" />
    <node id="controller" />
    <node id="memory daemon" />
    <node id="standby worker" />
    <node id="task to GPU" />
    <node id="task with pipelined model transmission" />
    <node id="Our mechanism" />
    <node id="old task cleaning in the active worker and new task initialization in the standby worker" />
    <node id="worker switching overhead" />
    <node id="Table 2" />
    <node id="worker switching mechanisms" />
    <node id="an active and standby worker switching mechanism" />
    <node id="active and standby worker switching mechanism" />
    <node id="the overhead of both task cleaning and task initialization" />
    <node id="us to address new technical challenges on memory management and worker switching across different processes" />
    <node id="pipelining" />
    <node id="the same task" />
    <node id="fast task switching" />
    <node id="all other components of PipeSwitch the same" />
    <node id="the following mechanisms discussed in 4.4" />
    <node id="the active-standby worker switching mechanism used by PipeSwitch" />
    <node id="an active-standby worker switching mechanism" />
    <node id="parallelize old task cleaning and new task initialization" />
    <node id="minimal overhead" />
    <node id="Pipelining" />
    <node id="a canonical technique" />
    <node id="computer systems" />
    <node id="system performance" />
    <node id="resource utilization" />
    <node id="two sources of system overheads" />
    <node id="Prior work in DL systems such as PipeDream 8 and ByteScheduler 9" />
    <node id="pipelining to distributed training" />
    <node id="These solutions" />
    <node id="inter-batch pipelining" />
    <node id="computation and gradient transmission of different batches" />
    <node id="computation and gradient transmission" />
    <node id="training workloads of the same DNN model" />
    <node id="intra-batch pipelining" />
    <node id="model transmission and computation" />
    <node id="the overhead of switching between different DNN models" />
    <node id="different DNN models" />
    <node id="either inference or training" />
    <node id="new techniques" />
    <node id="training" />
    <node id="inference that has strict SLOs" />
    <node id="pipelined model transmission" />
    <node id="unified memory management" />
    <node id="active-standby worker switching" />
    <node id="Pipelined context switching" />
    <node id="three key techniques" />
    <node id="pipelined model transmission, unified memory management and active-standby worker switching" />
    <node id="a system prototype" />
    <node id="it with Py-Torch" />
    <node id="this section" />
    <node id="inefficiencies in today's shared GPU clusters" />
    <node id="running DL workloads on GPUs in the fine-grained time-sharing model" />
    <node id="to pack multiple DL applications onto the same GPU via fine-grained time-sharing abstraction" />
    <node id="fine-grained time-sharing abstraction" />
    <node id="more flexible fine-grained scheduling" />
    <node id="GPU utilization for dynamic workloads" />
    <node id="dedicated physical forms and power supplies" />
    <node id="high speed networks" />
    <node id="specialized task schedulers" />
    <node id="500 14th USENIX Symposium on Operating Systems Design and Implementation" />
    <node id="14th USENIX Symposium on Operating Systems Design and Implementation" />
    <node id="Kubernetes" />
    <node id="514" />
    <node id="The main reason" />
    <node id="to bring down the cost" />
    <node id="The demand of training" />
    <node id="well predictable" />
    <node id="the progress of different developers" />
    <node id="The demand of inference" />
    <node id="more predictable" />
    <node id="an inference task for a particular application" />
    <node id="a daily periodical pattern based on the application usage" />
    <node id="the patterns" />
    <node id="across different tasks" />
    <node id="a shared cluster by different tasks" />
    <node id="the resource utilization via time-sharing" />
    <node id="traditional CPU workloads" />
    <node id="inference" />
    <node id="shared clusters" />
    <node id="running at high utilization" />
    <node id="Training clusters" />
    <node id="powerful GPUs" />
    <node id="training tasks" />
    <node id="often elastic" />
    <node id="strict deadlines" />
    <node id="GPUs designed for inference tasks" />
    <node id="too wimpy for training tasks" />
    <node id="the arrival of new GPU hardware" />
    <node id="this" />
    <node id="new GPU hardware" />
    <node id="up to 32GB GPU memory" />
    <node id="15.7 TFLOPS (single-precision)" />
    <node id="16GB GPU memory" />
    <node id="8.1 TFLOPS (single-precision)" />
    <node id="comparable performance with NVIDIA V100" />
    <node id="new algorithms and systems for distributed training" />
    <node id="multiple GPUs to accelerate training" />
    <node id="Our industry collaborator" />
    <node id="a leading online service provider" />
    <node id="this observation" />
    <node id="This service provider" />
    <node id="more than 10K V100 GPUs for training" />
    <node id="at least 5 as many T4 GPUs for inference" />
    <node id="The computation power on both sides" />
    <node id="the same order of magnitude" />
    <node id="The inference workload" />
    <node id="the number of active users" />
    <node id="clear peaks and valleys within each day" />
    <node id="The peak demand during daytime" />
    <node id="2 of the valley at midnight" />
    <node id="inference GPUs" />
    <node id="during less busy times" />
    <node id="training models" />
    <node id="daily updates with latest data" />
    <node id="A good example" />
    <node id="ne-tune BERT using daily news" />
    <node id="Borg-like 1 systems for GPUs" />
    <node id="great opportunity in improving GPU utilization" />
    <node id="inference and training workloads" />
    <node id="complementary usage patterns" />
    <node id="Online inference services" />
    <node id="more idle during midnight" />
    <node id="many training developers" />
    <node id="a time-consuming job at night" />
    <node id="inference loads on different models" />
    <node id="different patterns" />
    <node id="time sharing" />
    <node id="any server" />
    <node id="any task" />
    <node id="switch between different applications" />
    <node id="low overhead" />
    <node id="A modern server" />
    <node id="several TB of host memory" />
    <node id="it to load many applications" />
    <node id="task execution on GPUs" />
    <node id="very limited even on high-end GPUs" />
    <node id="T4" />
    <node id="16 GB" />
    <node id="V100" />
    <node id="32 GB" />
    <node id="task execution" />
    <node id="storing the state of idle applications" />
    <node id="DL tasks, especially training" />
    <node id="a large amount, or even all of the memory on a GPU" />
    <node id="large models" />
    <node id="large amounts of intermediate results" />
    <node id="large models and large amounts of intermediate results" />
    <node id="a lot of GPU memory" />
    <node id="training tasks which are memory-intensive" />
    <node id="multiple inference tasks which have large models" />
    <node id="state-of-the-art models" />
    <node id="deeper and larger" />
    <node id="even idle applications" />
    <node id="large memory space" />
    <node id="the active application" />
    <node id="the number of applications that can be served by a GPU server" />
    <node id="its host memory size" />
    <node id="switching a task" />
    <node id="heavy memory swapping" />
    <node id="many online inference workloads" />
    <node id="naive memory swapping between the host memory and the GPU memory" />
    <node id="requests to be handled in small batches for low latency" />
    <node id="the strawman scenario" />
    <node id="stop a training task and then start an inference task" />
    <node id="The rst inference batch" />
    <node id="several seconds to nish (4.1)" />
    <node id="Existing support such as NVIDIA MPS" />
    <node id="hundreds of milliseconds overhead" />
    <node id="NVIDIA MPS" />
    <node id="lower overhead compared to stop-and-start" />
    <node id="several hundred milliseconds overhead" />
    <node id="MPS from meeting strict SLOs" />
    <node id="Figure 1" />
    <node id="PipeSwitch architecture" />
    <node id="Throughput" />
    <node id="batches per second" />
    <node id="eight p3.2xlarge instances" />
    <node id="Upper bound" />
    <node id="MPS" />
    <node id="Stop-and-start" />
    <node id="both training and inference tasks" />
    <node id="fast switching across tasks" />
    <node id="well-defined structures" />
    <node id="the structure and computation pattern of DNN models" />
    <node id="us to highly optimize task switching" />
    <node id="us to achieve millisecond-scale overhead" />
    <node id="feasible and effective" />
    <node id="other challenges like memory management and worker switching" />
    <node id="an overview of the architecture and task execution" />
    <node id="the architecture of a PipeSwitch server" />
    <node id="PipeSwitch pipelines" />
    <node id="transmission and task execution" />
    <node id="This server" />
    <node id="four types of components" />
    <node id="a controller" />
    <node id="a memory daemon" />
    <node id="The controller" />
    <node id="the central component" />
    <node id="the memory daemon and the workers" />
    <node id="the tasks" />
    <node id="Memory" />
    <node id="daemon" />
    <node id="Controller" />
    <node id="The memory daemon" />
    <node id="the DNN models" />
    <node id="The server" />
    <node id="the DNN models in the host memory" />
    <node id="active" />
    <node id="All components" />
    <node id="the SLOs" />
    <node id="standby" />
    <node id="A standby worker" />
    <node id="idle" />
    <node id="initializing a new task" />
    <node id="cleaning its environment for the previous task" />
    <node id="The standby worker" />
    <node id="the new active worker to execute the new task" />
    <node id="a standby worker" />
    <node id="the environment for the previous task" />
    <node id="a new task" />
    <node id="a standby worker finishes cleaning a previous task" />
    <node id="the new task" />
    <node id="wait" />
    <node id="waiting" />
    <node id="its startup time" />
    <node id="a set of tasks received from the clients" />
    <node id="a scheduling policy" />
    <node id="which task to execute next" />
    <node id="The scheduling" />
    <node id="preemptive" />
    <node id="the controller" />
    <node id="the current task for the next one based on the scheduling policy" />
    <node id="the specific scheduling algorithm" />
    <node id="orthogonal to this paper" />
    <node id="the current task" />
    <node id="a training task" />
    <node id="an inference task" />
    <node id="a strict latency SLO" />
    <node id="an idle standby worker" />
    <node id="its environment for the new task" />
    <node id="the memory to the standby worker (4.3)" />
    <node id="the model used by the new task from the host memory to the GPU memory" />
    <node id="the model from the host memory to the GPU memory" />
    <node id="transmitting the model from the host memory to the GPU memory" />
    <node id="the extra memory copy from the memory daemon to the worker" />
    <node id="the worker" />
    <node id="the relevant GPU memory handlers to the worker" />
    <node id="the model" />
    <node id="its task" />
    <node id="model transmission" />
    <node id="GPU memory handlers to workers" />
    <node id="The primary goal of this paper" />
    <node id="design a set of techniques based on the characteristics of DL applications" />
    <node id="The set of techniques" />
    <node id="minimize the task switching overhead in this process" />
    <node id="task switching overhead" />
    <node id="individual components" />
    <node id="end-to-end experiments to demonstrate the benefits of PipeSwitch" />
    <node id="the effectiveness of the design choices on each component" />
    <node id="our design" />
    <node id="a server" />
    <node id="a training task running on the GPU" />
    <node id="The DNN model used in the measurement" />
    <node id="ResNet152 17" />
    <node id="The measurement" />
    <node id="two types of instances on Amazon AWS" />
    <node id="g4dn.2xlarge with NVIDA T4" />
    <node id="p3.2xlarge with NVIDIA V100" />
    <node id="the server" />
    <node id="measuring the time to start and execute it on the GPU" />
    <node id="the network time" />
    <node id="the task queueing time" />
    <node id="total times to start the inference task on the GPUs" />
    <node id="5787 ms and 7551 ms, respectively" />
    <node id="the overhead into the four components" />
    <node id="task cleaning" />
    <node id="time" />
    <node id="The inference task" />
    <node id="its environment" />
    <node id="process launching" />
    <node id="PyTorch CUDA runtime loading" />
    <node id="CUDA context initialization" />
    <node id="its neural network model" />
    <node id="Model" />
    <node id="transmission" />
    <node id="Grouped transmission" />
    <node id="a concept" />
    <node id="inference time on V100" />
    <node id="inference time on T4" />
    <node id="inference time on V100 and inference time on T4" />
    <node id="total overheads" />
    <node id="lower overhead on T4" />
    <node id="task switching largely depends on CPU" />
    <node id="better CPU than p3.2xlarge" />
    <node id="better CPU" />
    <node id="Intel Platinum 8259CL" />
    <node id="Intel Xeon E5-2686 v4" />
    <node id="A strawman solution" />
    <node id="the old task" />
    <node id="A strawman solution that simply stops the old task and starts the new task" />
    <node id="SLOs" />
    <node id="all the components" />
    <node id="considerable time compared to the inference time" />
    <node id="minimal switching overhead" />
    <node id="The PCIe bandwidth" />
    <node id="the physical limit on how fast an arbitrary task can be loaded to the GPU" />
    <node id="to circumvent this physical limit" />
    <node id="The computation" />
    <node id="An inference task" />
    <node id="a forward pass from the first layer to the final layer" />
    <node id="a forward pass to make a prediction" />
    <node id="each iteration in a training task" />
    <node id="a forward pass" />
    <node id="a backward pass" />
    <node id="the task" />
    <node id="the computation of a layer as soon as the layer is loaded in the GPU and the input of the layer is ready" />
    <node id="the input of the layer" />
    <node id="the previous layers have finished their computation" />
    <node id="regardless of its following layers" />
    <node id="Figure 2" />
    <node id="the advantage of pipelining over the strawman solution" />
    <node id="the knowledge of models" />
    <node id="pipelining mechanism with optimal model-aware grouping" />
    <node id="model-aware grouping" />
    <node id="the best trade-off between pipeline overhead and efficiency" />
    <node id="model" />
    <node id="PCIe" />
    <node id="task" />
    <node id="PCIe GPU E0 E1 En-1 E2" />
    <node id="pipeline model transmission and task execution" />
    <node id="The example" />
    <node id="an inference task that only has a forward pass in task execution" />
    <node id="Adding hooks" />
    <node id="automated" />
    <node id="the DNN framework" />
    <node id="PyTorch" />
    <node id="the model structure information" />
    <node id="transparent to users and cluster managers" />
    <node id="The basic way for pipelining" />
    <node id="to pipeline on per-layer granularity" />
    <node id="the system" />
    <node id="the layers to the GPU memory one by one" />
    <node id="the computation for a layer" />
    <node id="the layer is transmitted" />
    <node id="One" />
    <node id="the overhead to invoke multiple calls to PCIe to transmit the data" />
    <node id="transmission overhead" />
    <node id="data size" />
    <node id="dividing the model into many layers" />
    <node id="significant extra overhead when invoking a PCIe call for each layer" />
    <node id="some layers" />
    <node id="very small" />
    <node id="synchronization overhead" />
    <node id="between transmission and computation" />
    <node id="the computation to know when a layer is ready to compute" />
    <node id="grouping" />
    <node id="these two sources of overhead" />
    <node id="multiple layers into a group" />
    <node id="per-group granularity" />
    <node id="pipelining overhead" />
    <node id="once for each group" />
    <node id="instead of each layer" />
    <node id="Grouping" />
    <node id="a trade-off between pipelining efficiency and pipelining overhead" />
    <node id="using small groups" />
    <node id="more overlap between transmission and computation" />
    <node id="pipelining efficiency" />
    <node id="more pipelining overhead" />
    <node id="using big groups" />
    <node id="minimal pipelining overhead" />
    <node id="the chance for overlapping" />
    <node id="model-aware" />
    <node id="different structures" />
    <node id="the number of layers" />
    <node id="the size of each layer" />
    <node id="all possible combinations" />
    <node id="the optimal grouping strategy" />
    <node id="two pruning techniques" />
    <node id="two insights" />
    <node id="hundreds of layers" />
    <node id="the time complexity for enumeration" />
    <node id="exponential" />
    <node id="PCIe GPU" />
    <node id="lower bound of F(Group(0, i), i1)" />
    <node id="Group(0, i)" />
    <node id="Group(i1, j)" />
    <node id="j" />
    <node id="j1 to n-1" />
    <node id="case" />
    <node id="lower bound current optimal time" />
    <node id="cases" />
    <node id="i to j" />
    <node id="batch" />
    <node id="i1 to j" />
    <node id="Figure 3" />
    <node id="examples for two pruning techniques" />
    <node id="the cases that group from layer (i 1) to j j" />
    <node id="j j" />
    <node id="number of layers" />
    <node id="n" />
    <node id="F(B,i)" />
    <node id="a function" />
    <node id="the total time of the optimal grouping strategy from layer i to n-1" />
    <node id="layer 0 to i-1" />
    <node id="groups represented by B" />
    <node id="F(,0)" />
    <node id="min i F(group(0,i),i1)" />
    <node id="optimal grouping strategy" />
    <node id="the entire model (i.e., F(, 0))" />
    <node id="n cases" />
    <node id="how the first group is formed" />
    <node id="case i" />
    <node id="the first group contains layer 0 to i" />
    <node id="This formula" />
    <node id="compute F(group(0,i),i1)" />
    <node id="recursively" />
    <node id="the rst group" />
    <node id="too many layers" />
    <node id="the computation of the rst group" />
    <node id="too much" />
    <node id="the delay" />
    <node id="the pipeline efciency" />
    <node id="multiple layers in a group based on the progress of computation" />
    <node id="packing multiple layers in a group based on the progress of computation" />
    <node id="pipeline efficiency" />
    <node id="other than the rst group" />
    <node id="packing multiple layers in a group" />
    <node id="T(i, j)" />
    <node id="the transmission time for a group from layer i to j" />
    <node id="E(i, j)" />
    <node id="the execution time for a group from layer i to j" />
    <node id="the size of layer i to j and PCIe bandwidth" />
    <node id="the overhead of invoking multiple calls" />
    <node id="a lower bound for the total time for each case in Equation 1" />
    <node id="The lower bound" />
    <node id="the best case that all the remaining layers are combined in one group for transmission and computation" />
    <node id="the computation and communication can be perfectly overlapped" />
    <node id="Its computation" />
    <node id="right after the computation of the first group finishes" />
    <node id="Figure 3(b)" />
    <node id="an example for this insight" />
    <node id="the first group" />
    <node id="layer 0 to i" />
    <node id="Equation 1" />
    <node id="enumerate the cases for the PCIe GPU B.delay" />
    <node id="Group" />
    <node id="Group(a, i), Group(i1, n-1), Group(x, i), Group(i1, n-1)" />
    <node id="Figure 4" />
    <node id="general case for the two pruning techniques" />
    <node id="the transmission of the second group into the computation of the first group" />
    <node id="the transmission" />
    <node id="no later than the computation of the first group" />
    <node id="The least number of layers to group" />
    <node id="the following equation" />
    <node id="j argmax j T(i1, j) E(0,i)" />
    <node id="Group from layer (i1) to j" />
    <node id="grouping from (i-1) to j" />
    <node id="higher pipeline overhead" />
    <node id="this algorithm" />
    <node id="offline to find the strategy" />
    <node id="the resulting strategy" />
    <node id="PipeSwitch for context switching" />
    <node id="Algorithm 1" />
    <node id="the pseudo code" />
    <node id="The function FindOptGrouping" />
    <node id="Equation 1 (line 1-27)" />
    <node id="B" />
    <node id="the groups that have already formed" />
    <node id="x" />
    <node id="the first layer that have not formed a group" />
    <node id="all layers from x to n1" />
    <node id="optgroups" />
    <node id="the best grouping strategy from layer x given B" />
    <node id="none (line 2)" />
    <node id="The algorithm" />
    <node id="the second pruning insight" />
    <node id="the rst group from layer x" />
    <node id="the problem into k - 1 cases" />
    <node id="case i (0 ≤ i ≤ k)" />
    <node id="the first group from layer x to x_i" />
    <node id="Equation 3 and Figure 3(b)" />
    <node id="this insight with a special example" />
    <node id="one group from layer 0 to i" />
    <node id="multiple groups formed by previous layers" />
    <node id="B.delay to denote the time to which the group can be formed" />
    <node id="based on B.delay (line 4-9)" />
    <node id="The enumeration for i" />
    <node id="the layers from x to j-1 (line 11)" />
    <node id="the algorithm" />
    <node id="the rst insight" />
    <node id="the lower bound" />
    <node id="line 12-17" />
    <node id="the computation from x" />
    <node id="both its transmission (T(x,i)) and the computation of the previous groups (B.delay)" />
    <node id="the current optimal time" />
    <node id="line 18-19" />
    <node id="a heuristic that bootstraps optgroups with a relative good strategy" />
    <node id="good strategy" />
    <node id="group every ten layers" />
    <node id="The two pruning techniques" />
    <node id="most of the strategies" />
    <node id="the optimal one quickly" />
    <node id="the total time for the pipeline" />
    <node id="m n x" />
    <node id="the number of layers the function considers" />
    <node id="induction on m" />
    <node id="FindOptGrouping(B,x)" />
    <node id="the optimal grouping strategy from layer x to n 1" />
    <node id="previous layers" />
    <node id="the optimal strategy" />
    <node id="the function" />
    <node id="one layer" />
    <node id="m" />
    <node id="1" />
    <node id="layer x itself" />
    <node id="one group" />
    <node id="this strategy" />
    <node id="k1 layers" />
    <node id="The optimal strategy for this case" />
    <node id="these cases" />
    <node id="exclusive" />
    <node id="the entire search space" />
    <node id="the optimal grouping strategy for m k 1" />
    <node id="the optimal grouping strategy from these cases" />
    <node id="The rst technique" />
    <node id="the cases" />
    <node id="their lower bounds" />
    <node id="the current found optimal" />
    <node id="this technique" />
    <node id="the optimality" />
    <node id="The second technique" />
    <node id="the case" />
    <node id="their rst groups" />
    <node id="layer x to j j" />
    <node id="the computation to an earlier point than grouping from x to at least j" />
    <node id="pruning these cases" />
    <node id="optimality for a given list of layers" />
    <node id="layers or operators in a DNN model" />
    <node id="an arbitrary computation graph" />
    <node id="Models like ResNet and Inception" />
    <node id="technically non-linear directed acyclic graphs (DAGs)" />
    <node id="layersoperators in the DAG" />
    <node id="the GPU one by one" />
    <node id="execution order" />
    <node id="any special assumptions on the execution order" />
    <node id="grouping the layers" />
    <node id="high pipelining efficiency and low pipelining overhead" />
    <node id="the order" />
    <node id="the first time an operator is executed" />
    <node id="The order" />
    <node id="correctness" />
    <node id="an operator" />
    <node id="it is transmitted to the GPU and the input is ready" />
    <node id="our pipelined model transmission" />
    <node id="the general case" />
    <node id="Pipelined Model Transmission" />
    <node id="keeping all other components of PipeSwitch the same and comparing mechanisms discussed in 4.2" />
    <node id="Figure 7" />
    <node id="Effectiveness of pipelined model transmission" />
    <node id="Task execution in a GPU" />
    <node id="A GPU" />
    <node id="its own memory management system" />
    <node id="a malloc function" />
    <node id="malloc function" />
    <node id="cudaMalloc for NVIDIA GPUs" />
    <node id="CPUs for memory allocation" />
    <node id="each task" />
    <node id="the native cudaMallocManaged function for GPU memory allocation" />
    <node id="model transmission to CUDA unified memory" />
    <node id="allocating GPU memory" />
    <node id="sharing the GPU memory to workers through CUDA IPC API" />
    <node id="getting the shared GPU memory" />
    <node id="Each worker" />
    <node id="cudaMalloc" />
    <node id="the model to GPU" />
    <node id="its own worker" />
    <node id="GPU memory with cudaMallocManaged" />
    <node id="CUDA" />
    <node id="the model to GPU when needed" />
    <node id="This solution" />
    <node id="high overhead for DL applications" />
    <node id="native cudaMalloc function" />
    <node id="general-purpose applications" />
    <node id="CUDA unified memory" />
    <node id="native cudaMalloc function and CUDA unified memory" />
    <node id="unnecessary overhead for DL applications" />
    <node id="more than one hundred milliseconds overhead than PipeSwitch" />
    <node id="two characteristics of DL applications" />
    <node id="minimize GPU memory management overhead" />
    <node id="The general-purpose GPU memory management" />
    <node id="these characteristics" />
    <node id="too heavy-weight for DL applications that require fast task switching" />
    <node id="the amount of memory allocated to the DNN model" />
    <node id="fixed" />
    <node id="during task execution" />
    <node id="the model parameters" />
    <node id="the DNN structure" />
    <node id="the amount of memory needed to store them" />
    <node id="the same" />
    <node id="the model for inference" />
    <node id="the model itself" />
    <node id="the intermediate results" />
    <node id="a simple, regular pattern" />
    <node id="memory fragmentation" />
    <node id="intermediate results" />
    <node id="the outputs of each layer" />
    <node id="the next layer" />
    <node id="A training task" />
    <node id="the intermediate results generated in the forward pass cannot be immediately freed" />
    <node id="the intermediate results generated in the forward pass" />
    <node id="the backward pass to update the weights" />
    <node id="the backward pass" />
    <node id="reverse order as that the forward pass generates them" />
    <node id="first-in-last-out" />
    <node id="The memory allocation and release" />
    <node id="a simple stack-like mechanism" />
    <node id="memory allocation overhead" />
    <node id="" />
    <node id="minimized" />
    <node id="avoided" />
    <node id="a memory management mechanism tailored for DL applications" />
    <node id="a dedicated memory daemon" />
    <node id="a 64-bit integer offset for the shared GPU memory to workers" />
    <node id="223 ms" />
    <node id="eliminating the memory allocation overhead with the memory daemon" />
    <node id="memory pointers to the workers" />
    <node id="passing memory pointers to the workers" />
    <node id="light-weight" />
    <node id="that each time only one worker owns the GPU memory" />
    <node id="one worker" />
    <node id="owning the GPU memory" />
    <node id="memory isolation between workers" />
    <node id="a memory pool to allocate the memory to store its model and intermediate results" />
    <node id="the memory to the pool after the intermediate results are no longer needed" />
    <node id="The memory management of PipeSwitch" />
    <node id="that of Py-Torch" />
    <node id="GPU memory blocks to PyTorch GPU memory pool" />
    <node id="tensors on them" />
    <node id="memory management in PyTorch" />
    <node id="memory allocation for a task itself" />
    <node id="Replicating the models in each worker" />
    <node id="high memory footprint" />
    <node id="the number of models a server can store" />
    <node id="reducing the number of models a server can store" />
    <node id="the types of tasks the server can execute" />
    <node id="storing the models in a dedicate process" />
    <node id="each model" />
    <node id="only once" />
    <node id="an extra memory copy from this process to a worker to start a task" />
    <node id="the task switching time" />
    <node id="the models in the memory daemon" />
    <node id="one copy of each model in the host memory" />
    <node id="IPC overhead" />
    <node id="a property of DL applications" />
    <node id="IPC APIs" />
    <node id="cudaIpcOpenMemHandle for NVIDIA GPUs" />
    <node id="the performance of these IPC APIs" />
    <node id="these IPC APIs" />
    <node id="high overhead" />
    <node id="The overhead" />
    <node id="the pipeline" />
    <node id="the IPCs frequently" />
    <node id="the IPCs" />
    <node id="synchronize model transmission and task execution for every pipeline group" />
    <node id="only once for the entire model transmission (not)" />
    <node id="memory allocation process for a neural network model" />
    <node id="memory daemon and the worker" />
    <node id="the same order to allocate memory for the model parameters" />
    <node id="memory pointers for the parameters" />
    <node id="the neural network model" />
    <node id="known and given" />
    <node id="the same order to transmit the model as the worker would" />
    <node id="the usage of expensive GPU IPCs" />
    <node id="latency" />
    <node id="no unified memory management without IPC optimization" />
    <node id="Pin memory" />
    <node id="pin memory" />
    <node id="no" />
    <node id="The OS" />
    <node id="a memory page to disk" />
    <node id="the page is inactive for a certain amount of time" />
    <node id="a page in the host memory to be pinned (or page-locked)" />
    <node id="a page in the host memory" />
    <node id="in order to transmit the data in the page to the GPU memory" />
    <node id="a temporary pinned page" />
    <node id="the pages of the memory daemon to the host memory" />
    <node id="Process-level isolation" />
    <node id="desirable" />
    <node id="one task cannot read the memory of another task" />
    <node id="the crashing of one task does not affect other tasks or the entire system" />
    <node id="separate processes" />
    <node id="A naive solution" />
    <node id="the new task after the current task is stopped" />
    <node id="sequential execution" />
    <node id="long delay" />
    <node id="old task cleaning and new task initialization" />
    <node id="Another possible solution" />
    <node id="to let the current and new tasks share the same process with a warm CUDA context" />
    <node id="the GPU environment of the current task" />
    <node id="The process of the old task" />
    <node id="the GPU environment" />
    <node id="another process" />
    <node id="The process" />
    <node id="the environment for the new task" />
    <node id="a major job" />
    <node id="asynchronous CUDA functions queued on the GPU" />
    <node id="synchronization points into training tasks" />
    <node id="the number of queued functions" />
    <node id="limited" />
    <node id="quickly cleared" />
    <node id="Synchronization points" />
    <node id="inference tasks" />
    <node id="short" />
    <node id="not preempted" />
    <node id="Another job" />
    <node id="free its GPU memory" />
    <node id="cleaning procedure" />
    <node id="does not modify the content of the memory" />
    <node id="metadata" />
    <node id="GPU memory pointers" />
    <node id="pointers pointing to the tensor data" />
    <node id="the actual data" />
    <node id="its model to the GPU memory" />
    <node id="the task cleaning of the current task and the pipelined model transmission of the new task" />
    <node id="parallelizing the task cleaning and pipelined model transmission" />
    <node id="hide the task cleaning overhead" />
    <node id="This choice" />
    <node id="performance" />
    <node id="a trusted environment" />
    <node id="a latter process" />
    <node id="the memory data of a previous process" />
    <node id="an additional zero-out operation" />
    <node id="if this is a concern" />
    <node id="high memory bandwidth" />
    <node id="memory bandwidth of GPU" />
    <node id="900GBs for V100" />
    <node id="zeroing-out most models like ResNet-152 (around 240MB)" />
    <node id="sub-millisecond overhead" />
    <node id="the current active worker to stop" />
    <node id="the GPU memory allocated to the current active worker" />
    <node id="the GPU memory to the new active worker" />
    <node id="current active worker to stop" />
    <node id="parameters of the new model to the GPU" />
    <node id="receiving the current active worker's reply" />
    <node id="only one active worker" />
    <node id="exclusive occupation of the GPU" />
    <node id="number of standby workers" />
    <node id="their GPU memory consumption" />
    <node id="every standby worker" />
    <node id="its own CUDA context" />
    <node id="a few hundred MB GPU memory" />
    <node id="always at least one idle standby worker" />
    <node id="two standby workers" />
    <node id="at least one idle worker" />
    <node id="one idle worker" />
    <node id="the waiting time" />
    <node id="moderate GPU memory consumption" />
    <node id="A transaction" />
    <node id="a model is switched in or out on all of its GPUs to enable or disable inference on this model" />
    <node id="a production GPU training trace from Microsoft 19,20" />
    <node id="111,883 tasks in this trace" />
    <node id="96,662 tasks" />
    <node id="single-GPU training tasks" />
    <node id="86% of all the tasks" />
    <node id="these jobs" />
    <node id="18 of total GPU hours" />
    <node id="the share of multi-GPU jobs to increase in the future" />
    <node id="current training frameworks" />
    <node id="mature support of elastic training" />
    <node id="the best" />
    <node id="C and Python functions to the GPU memory management module of PyTorch" />
    <node id="functions" />
    <node id="the received GPU memory into PyTorch GPU memory pool for a specific CUDA stream" />
    <node id="the GPU memory from the pool" />
    <node id="shared GPU memory" />
    <node id="PyTorch GPU memory pool" />
    <node id="different CUDA streams" />
    <node id="only one of these CUDA streams is active" />
    <node id="The controller process" />
    <node id="a TCP thread and a scheduler thread" />
    <node id="the scheduler and the memory daemon" />
    <node id="for better performance" />
    <node id="The TCP thread" />
    <node id="task through TCP from clients" />
    <node id="the task to the scheduler thread" />
    <node id="The scheduler thread" />
    <node id="the GPU memory with workers" />
    <node id="workers" />
    <node id="the task to a worker" />
    <node id="parameters for the corresponding model to the GPU memory" />
    <node id="the user" />
    <node id="the model in the scheduler" />
    <node id="the scheduler" />
    <node id="the model from the disk to the CPU memory" />
    <node id="Parameters" />
    <node id="groups" />
    <node id="a pipeline" />
    <node id="computing the corresponding layers" />
    <node id="The worker process" />
    <node id="two threads" />
    <node id="The termination thread" />
    <node id="the termination signal from the controller" />
    <node id="the main thread" />
    <node id="The main thread" />
    <node id="the computation for inference or training" />
    <node id="user to register the model before starting a task" />
    <node id="the hooks to wait for parameter transmission or terminate on notification" />
    <node id="the model structures" />
    <node id="small" />
    <node id="The parameters" />
    <node id="once in the memory daemon" />
    <node id="their parameters" />
    <node id="locations in the shared GPU memory" />
    <node id="Different models" />
    <node id="the same GPU memory location" />
    <node id="the value" />
    <node id="the controller transfers the corresponding parameters to these locations" />
    <node id="All experiments" />
    <node id="AWS" />
    <node id="two EC2 instance types" />
    <node id="8 vCPUs (Intel Xeon E5-2686 v4)" />
    <node id="1 GPU (NVIDIA V100 with 16 GB GPU memory)" />
    <node id="PCIe 3.0 16" />
    <node id="61 GB memory" />
    <node id="8 vCPUs (Intel Platinum 8259CL)" />
    <node id="1 GPU (NVIDIA T4 with 16 GB GPU memory)" />
    <node id="PCIe 3.0 8" />
    <node id="32 GB memory" />
    <node id="The software environment" />
    <node id="PyTorch-1.3.0" />
    <node id="torchvision-0.4.2" />
    <node id="scipy-1.3.2" />
    <node id="CUDA-10.1" />
    <node id="PyTorch with our plugins for all mechanisms in comparison for consistency" />
    <node id="PyTorch with our plugins" />
    <node id="better results for stop-and-start than native PyTorch from Python-PyPI used in Table 1" />
    <node id="The models" />
    <node id="Inceptionv3 22" />
    <node id="Bertbase 23" />
    <node id="ResNet152 17, Inceptionv3 22 and Bertbase 23" />
    <node id="standard benchmarks for evaluating DL systems" />
    <node id="representative configurations for each model" />
    <node id="The experiments" />
    <node id="both training and inference" />
    <node id="single-GPU inference and training tasks" />
    <node id="4.5" />
    <node id="their models to the host memory periodically" />
    <node id="the latest checkpoint after preemption" />
    <node id="The checkpointing frequency of training tasks" />
    <node id="the scheduling cycle" />
    <node id="checkpointing overhead" />
    <node id="default batch size for training" />
    <node id="32" />
    <node id="default batch size for inference" />
    <node id="8" />
    <node id="throughput and latency as evaluation metrics" />
    <node id="the end-to-end latency experienced by the client" />
    <node id="Figure 5" />
    <node id="total latency experienced by the client for different mechanisms" />
    <node id="the latency experienced by the client" />
    <node id="Table 3" />
    <node id="the total overhead" />
    <node id="Each number" />
    <node id="the average of 100 runs" />
    <node id="Figure 6(b)" />
    <node id="minimum and maximum latencies using the error bar" />
    <node id="latency of the first batch and those of later batches in one scheduling cycle" />
    <node id="significantly due to switching overhead" />
    <node id="6.1 End-to-End Experiments" />
    <node id="end-to-end overhead" />
    <node id="a client" />
    <node id="an inference task to a GPU server" />
    <node id="the GPU server" />
    <node id="the inference task" />
    <node id="a reply back to the client" />
    <node id="There" />
    <node id="no training task" />
    <node id="the required model loaded in the GPU" />
    <node id="the lowest latency we can achieve for an inference task" />
    <node id="existing systems like Gandiva 24" />
    <node id="Gandiva 24" />
    <node id="task switching" />
    <node id="similar second-scale overhead" />
    <node id="separate processes in advance" />
    <node id="memory swapping" />
    <node id="unified memory" />
    <node id="The properties" />
    <node id="4" />
    <node id="Latency" />
    <node id="milliseconds (ms)" />
    <node id="7500 to 10000 ms" />
    <node id="Models" />
    <node id="Ready model, PipeSwitch, MPS, Stop-and-start, ResNet152, Inceptionv3, Bertbase" />
    <node id="NVIDIA V100 GPU" />
    <node id="PCIe 3.0 x16 interface" />
    <node id="PCIe 3.0 x8" />
    <node id="Latency (ms)" />
    <node id="5000 to 10000" />
    <node id="ResNet152, Inceptionv3, Bertbase" />
    <node id="Ready model, PipeSwitch, MPS, Stop-and-start" />
    <node id="ResNet152" />
    <node id="the text" />
    <node id="Inceptionv3" />
    <node id="Bertbase" />
    <node id="ms" />
    <node id="pipeline method" />
    <node id="Per-layer pipeline" />
    <node id="No optimization" />
    <node id="3.0" />
    <node id="16" />
    <node id="510 14th USENIX Symposium on Operating Systems Design and Implementation" />
    <node id="3.0 16" />
    <node id="No memory management" />
    <node id="No IPC optimization" />
    <node id="No pin memory" />
    <node id="6000 to 8000 ms" />
    <node id="One process" />
    <node id="Two processes" />
    <node id="5000 to 7500 ms" />
    <node id="3.62 ms" />
    <node id="4.82 ms" />
    <node id="2.53 ms" />
    <node id="5.49 ms" />
    <node id="6.57 ms" />
    <node id="the first layer" />
    <node id="Table 4" />
    <node id="task startup overhead for PipeSwitch" />
    <node id="the difference between the time for ResNet152, Inceptionv3, Bertbase" />
    <node id="464" />
    <node id="189" />
    <node id="139" />
    <node id="1.33 s" />
    <node id="0.18 s" />
    <node id="0.34 s" />
    <node id="Only Pruning 1" />
    <node id="2.09 s" />
    <node id="0.30 s" />
    <node id="0.88 s" />
    <node id="Only Pruning 2" />
    <node id="3.44 h" />
    <node id="5.07 s" />
    <node id="24 h" />
    <node id="No Pruning" />
    <node id="Table 5" />
    <node id="effectiveness of two pruning techniques" />
    <node id="the models to be preloaded to the GPU" />
    <node id="several limitations described in 2.2" />
    <node id="Its performance" />
    <node id="the ready model when the model is preloaded" />
    <node id="NVIDIA MPS when the model is in the host memory" />
    <node id="The total overhead" />
    <node id="the difference between the latency of a mechanism and that of the ready model" />
    <node id="stop-and-start" />
    <node id="the worst" />
    <node id="several seconds" />
    <node id="The main source of the overhead" />
    <node id="CUDA context initialization and rst-time library loading operations in PyTorch" />
    <node id="Another source" />
    <node id="GPU memory swapping" />
    <node id="The overhead of PipeSwitch for most configurations" />
    <node id="10ms" />
    <node id="The overhead of PipeSwitch for BERT on T4" />
    <node id="the large model size" />
    <node id="the smaller PCIe bandwidth on T4 than that on V100" />
    <node id="computing BERT on T4" />
    <node id="120ms" />
    <node id="relative overhead" />
    <node id="acceptable" />
    <node id="computing the rst layer" />
    <node id="the ready model" />
    <node id="computing" />
    <node id="The startup overhead of PipeSwitch" />
    <node id="only a few milliseconds" />
    <node id="only a few milliseconds overhead for task switching" />
    <node id="low latency close to the lower bound" />
    <node id="throughput and end-to-end latency of different mechanisms under different scheduling cycles" />
    <node id="each scheduling cycle" />
    <node id="Figure 6(a)" />
    <node id="the inference throughput" />
    <node id="The dashed line" />
    <node id="the upper bound" />
    <node id="the throughput of the ready model assuming no task switching" />
    <node id="the average latency of the ready model assuming no task switching" />
    <node id="throughput of stop-and-start" />
    <node id="nearly zero for scheduling cycles smaller than 10 s" />
    <node id="poor throughput around 100 batches per second" />
    <node id="the ratio to the upper bound" />
    <node id="the average latency of the inference tasks" />
    <node id="The error bar" />
    <node id="the minimum and maximum latency" />
    <node id="Stop- and-start" />
    <node id="poor latency" />
    <node id="the rst batch" />
    <node id="several seconds overhead" />
    <node id="about 80 ms average latency" />
    <node id="several hundred milliseconds latency for the rst batch" />
    <node id="Throughput and latency" />
    <node id="different scheduling cycles for ResNet on p3.2xlarge" />
    <node id="Computation" />
    <node id="once parameters are transmitted" />
    <node id="the total time measured by the client for an inference task to preempt a training task and finish its inference" />
    <node id="Figure 8" />
    <node id="the total time measured by the client" />
    <node id="the worst in most cases" />
    <node id="no optimization" />
    <node id="the layers of the model into one big tensor" />
    <node id="it in one group" />
    <node id="transmission and computation" />
    <node id="overlaps" />
    <node id="layer" />
    <node id="models with many layers but relatively light computation such as ResNet152 and Inception" />
    <node id="grouped transmission" />
    <node id="sometimes even no pipeline" />
    <node id="this reduction" />
    <node id="significant" />
    <node id="the optimizations on memory management and worker switching have already been applied" />
    <node id="that to meet strict SLOs, it is important to reduce all overheads for task switching, not only the most significant one" />
    <node id="the running time of Algorithm 1" />
    <node id="the effects of the two pruning techniques mentioned in 4.2" />
    <node id="both weighted and unweighted layers" />
    <node id="the computation time" />
    <node id="the parameter size and running time for each layer" />
    <node id="only several seconds to compute an optimal grouping strategy" />
    <node id="no pruning" />
    <node id="for all three models after running for 24 hours" />
    <node id="unied memory management" />
    <node id="effectiveness" />
    <node id="memory management" />
    <node id="not unified" />
    <node id="the following five mechanisms discussed in 4.3" />
    <node id="IPC" />
    <node id="the pages of the memory daemon" />
    <node id="the main memory" />
    <node id="this experiment" />
    <node id="all the optimizations on memory management are effective" />
    <node id="unied memory management mechanism" />
    <node id="IPC optimization" />
    <node id="important" />
    <node id="latency by 1648 ms" />
    <node id="pinning the pages to the host memory" />
    <node id="the latency with a few milliseconds" />
    <node id="a process" />
    <node id="Figure 9" />
    <node id="Effectiveness of active-standby switching" />
    <node id="the results" />
    <node id="The new process" />
    <node id="a new CUDA environment" />
    <node id="the total time" />
    <node id="the CUDA environment" />
    <node id="the overhead to clean the environment" />
    <node id="Many frameworks" />
    <node id="deep learning" />
    <node id="TensorFlow" />
    <node id="MXNet" />
    <node id="Several algorithms and systems" />
    <node id="executing and scheduling deep learning tasks on clusters" />
    <node id="These scheduling solutions" />
    <node id="orthogonal and complementary to PipeSwitch" />
    <node id="how to realize a scheduling decision" />
    <node id="the scheduler to change the resource allocation more often with millisecond-scale task switching" />
    <node id="Many techniques and systems" />
    <node id="optimize communication" />
    <node id="improve distributed training" />
    <node id="The most relevant ones" />
    <node id="PipeDream 8, ByteScheduler 9 and Poseidon 40" />
    <node id="vDNN 43 and SwapAdvisor 44" />
    <node id="GPU memory management module" />
    <node id="memory management for a single training task of large models" />
    <node id="Cluster managers 4548" />
    <node id="GPUs to VMs or containers at device granularity" />
    <node id="Several solutions" />
    <node id="a GPU at application granularity using techniques like library interception" />
    <node id="efforts on GPU optimization" />
    <node id="the performance of running a single task" />
    <node id="tensor fusion" />
    <node id="kernel-level concurrency and scheduling" />
    <node id="Madan Musuvathi" />
    <node id="the anonymous reviewers" />
    <node id="Madan Musuvathi and the anonymous reviewers" />
    <node id="valuable feedback" />
    <node id="an AWS Machine Learning Research Award" />
    <node id="A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes" />
    <node id="Large-scale cluster management at Google with Borg" />
    <node id="EuroSys" />
    <node id="2015" />
    <node id="Nexus" />
    <node id="a GPU cluster engine for accelerating DNN-based video analysis" />
    <node id="ACM SOSP, 2019" />
    <node id="3 H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Philipose, A. Krishnamurthy, and R. Sundaram" />
    <node id="Fried, J. Behrens, A. Belay, and H. Balakrishnan" />
    <node id="Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads" />
    <node id="USENIX NSDI" />
    <node id="CUDA Multi-Process Service" />
    <node id="6" />
    <node id="CUDAMultiProcessServiceOverview.pdf" />
    <node id="https://docs.nvidia.com/deploy/pdf" />
    <node id="Salus" />
    <node id="Fine-grained GPU sharing primitives for deep learning applications" />
    <node id="7 P. Yu and M. Chowdhury" />
    <node id="Conference on Machine Learning and Systems" />
    <node id="2020" />
    <node id="PipeDream" />
    <node id="generalized pipeline parallelism for DNN training" />
    <node id="D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia" />
    <node id="Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo" />
    <node id="A generic communication scheduler for distributed DNN training acceleration" />
    <node id="ACM SOSP" />
    <node id="Tiresias" />
    <node id="a GPU cluster manager for distributed deep learning" />
    <node id="Authors" />
    <node id="J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo" />
    <node id="Poseidon" />
    <node id="an efficient communication architecture" />
    <node id="distributed deep learning on GPU clusters" />
    <node id="USENIX ATC, 2017" />
    <node id="H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie, and E. P. Xing" />
    <node id="Amazon Web Services" />
    <node id="12" />
    <node id="Microsoft Azure" />
    <node id="13" />
    <node id="Google Cloud Platform" />
    <node id="14" />
    <node id="Horovod" />
    <node id="fast and easy distributed deep learning in TensorFlow" />
    <node id="15 A. Sergeev and M. Del Balso" />
    <node id="Horovod: fast and easy distributed deep learning in TensorFlow" />
    <node id="Horovod paper" />
    <node id="arXiv preprint arXiv:1802.05799" />
    <node id="2018" />
    <node id="Su" />
    <node id="Scaling distributed machine learning with the parameter server" />
    <node id="USENIX OSDI" />
    <node id="2014" />
    <node id="Deep residual learning for image recognition" />
    <node id="Sun" />
    <node id="IEEE Conference on Computer Vision and Pattern Recognition" />
    <node id="2016" />
    <node id="Nvidia data center deep learning product" />
    <node id="Philly" />
    <node id="20 traces" />
    <node id="philly-traces" />
    <node id="https://github.com/msr-fiddle/philly-traces" />
    <node id="21" />
    <node id="22 C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna" />
    <node id="Rethinking the inception architecture for computer vision" />
    <node id="J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova" />
    <node id="BERT: Pre-training of deep bidirectional transformers for language understanding" />
    <node id="Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)" />
    <node id="Gandiva" />
    <node id="Introspective cluster scheduling for deep learning" />
    <node id="24 W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P. Patel, X. Peng, H. Zhao, Q. Zhang, et al." />
    <node id="25" />
    <node id="TensorFlow XLA" />
    <node id="54" />
    <node id="26" />
    <node id="https://mxnet.apache.org" />
    <node id="a website URL" />
    <node id="M. J. Freedman" />
    <node id="Slaq: quality-driven scheduling for distributed machine learning" />
    <node id="ACM Symposium on Cloud Computing" />
    <node id="2017" />
    <node id="Optimus" />
    <node id="an efficient dynamic resource scheduler for deep learning clusters" />
    <node id="Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo" />
    <node id="Themis" />
    <node id="Fair and efficient GPU cluster scheduling" />
    <node id="K. Mahajan, A. Balasubramanian, A. Singhvi, S. Venkataraman, A. Akella, A. Phanishayee, and S. Chawla" />
    <node id="HyperSched" />
    <node id="Dynamic resource reallocation for model development on a deadline" />
    <node id="R. Liaw, R. Bhardwaj, L. Dunlap, Y. Zou, J. E. Gonzalez, I. Stoica, and A. Tumanov" />
    <node id="CHET" />
    <node id="an optimizing compiler for fully-homomorphic neural-network inferencing" />
    <node id="ACM Conference on Programming Language Design and Implementation" />
    <node id="R. Dathathri, O. Saarikivi, H. Chen, K. Laine, K. Lauter, S. Maleki, M. Musuvathi, and T. Mytkowicz" />
    <node id="TVM" />
    <node id="an automated end-to-end optimizing compiler for deep learning" />
    <node id="T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, et al." />
    <node id="Gpipe" />
    <node id="efficient training of giant neural networks using pipeline parallelism" />
    <node id="Advances in Neural Information Processing Systems, 2019" />
    <node id="33 Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al." />
    <node id="Blink" />
    <node id="Fast and generic collectives for distributed ML" />
    <node id="G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica" />
    <node id="NVIDIA Collective Communications Library" />
    <node id="NCCL" />
    <node id="36 J. Liu, J. Wu, and D. K. Panda" />
    <node id="High performance RDMA-based MPI implementation over inniband" />
    <node id="37 Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing" />
    <node id="More effective distributed ML via a stale synchronous parallel parameter server" />
    <node id="Advances in Neural Information Processing Systems" />
    <node id="2013" />
    <node id="A. Awan, C.-H. Chu, H. Subramoni, and D. K. Panda" />
    <node id="Optimized broadcast for deep learning workloads on dense-GPU inniband clusters: MPI or NCCL?" />
    <node id="Proceedings of the 25th European MPI Users Group Meeting" />
    <node id="A. Vishnu, C. Siegel, T. Warfel, and V. Amatya" />
    <node id="GossipGraD: Scalable deep learning using gossip communication based asynchronous gradient descent" />
    <node id="GossipGraD" />
    <node id="CoRR" />
    <node id="41 Z. Zhang, C. Chang, H. Lin, Y. Wang, R. Arora, and X. Jin" />
    <node id="Is network the bottleneck of distributed training?" />
    <node id="ACM SIGCOMM Workshop on Network Meets AI ML (NetAI)" />
    <node id="August 2020" />
    <node id="Y. Chen, Z. Liu, B. Ren, and X. Jin" />
    <node id="On efficient constructions of checkpoints" />
    <node id="International Conference on Machine Learning (ICML)" />
    <node id="July 2020" />
    <node id="vDNN" />
    <node id="Virtualized deep neural networks for scalable, memory-efficient neural network design" />
    <node id="2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)" />
    <node id="M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler" />
    <node id="SwapAdvisor" />
    <node id="C.-C. Huang, G. Jin, and J. Li" />
    <node id="the GPU memory limit" />
    <node id="smart swapping" />
    <node id="ACM ASPLOS" />
    <node id="NVIDIA Container Runtime" />
    <node id="Docker" />
    <node id="nvidia-docker" />
    <node id="https://github.com/NVIDIA/nvidia-docker" />
    <node id="Mesos" />
    <node id="a platform for fine-grained resource sharing in the data center" />
    <node id="USENIX NSDI, 2011" />
    <node id="47 B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H. Katz, S. Shenker, and I. Stoica" />
    <node id="Mesos: A platform for fine-grained resource sharing in the data center" />
    <node id="Apache Hadoop YARN" />
    <node id="Yet another resource negotiator" />
    <node id="V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al." />
    <node id="49 G. Giunta, R. Montella, G. Agrillo, and G. Coviello" />
    <node id="A GPGPU transparent virtualization component for high performance computing clouds" />
    <node id="European Conference on Parallel Processing" />
    <node id="2010" />
    <node id="V. T. Ravi, M. Becchi, G. Agrawal, and S. Chakradhar" />
    <node id="Supporting GPU sharing in cloud environments with a transparent runtime consolidation framework" />
    <node id="Proceedings of the 20th international symposium on High performance distributed computing" />
    <node id="2011" />
    <node id="GViM" />
    <node id="GPU-accelerated virtual machines" />
    <node id="Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing" />
    <node id="2009" />
    <node id="V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan" />
    <node id="GViM: GPU-accelerated virtual machines" />
    <node id="rCUDA" />
    <node id="the number of GPU-based accelerators in high performance clusters" />
    <node id="51 J. Duato, A. J. Pena, F. Silla, R. Mayo, and E. S. Quintana-Ort" />
    <node id="rCUDA: Reducing the number of GPU-based accelerators in high performance clusters" />
    <node id="2010 International Conference on High Performance Computing Simulation" />
    <node id="vCUDA" />
    <node id="GPU-accelerated high-performance computing in virtual machines" />
    <node id="Sun and K. Li" />
    <node id="vCUDA: GPU-accelerated high-performance computing in virtual machines" />
    <node id="IEEE Transactions on Computers" />
    <node id="a flexible and efficient machine learning library" />
    <node id="heterogeneous distributed systems" />
    <node id="55 T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang" />
    <node id="MXNet paper" />
    <node id="arXiv preprint arXiv:1512.01274" />
    <node id="56 C. Gregg, J. Dorn, K. Hazelwood, and K. Skadron" />
    <node id="Fine-grained resource sharing for concurrent GPGPU kernels" />
    <node id="4th USENIX Workshop on Hot Topics in Parallelism" />
    <node id="2012" />
    <node id="57 S. Pai, M. J. Thazhuthaveetil, and R. Govindarajan" />
    <node id="Improving GPGPU concurrency with elastic kernels" />
    <node id="ACM SIGARCH Computer Architecture News" />
    <node id="Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia, and A. Aiken" />
    <node id="TASO: optimizing deep learning computation with automatic generation of graph substitutions" />
    <edge source="This paper" target="Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation" target="November 4-6, 2020">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation" target="978-1-939133-19-9">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Open access to the Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation" target="USENIX">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications" target="Zhihao Bai, Zhen Zhang, Yibo Zhu, Xin Jin">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Zhihao Bai" target="Johns Hopkins University">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Zhihao Bai" target="an AWS Machine Learning Research Award">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Zhen Zhang" target="Johns Hopkins University">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Zhen Zhang" target="an AWS Machine Learning Research Award">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Yibo Zhu" target="ByteDance Inc.">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Xin Jin" target="Johns Hopkins University">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Xin Jin" target="an AWS Machine Learning Research Award">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Deep learning workloads" target="throughput-intensive training tasks and latency-sensitive inference tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The dominant practice today" target="to provision dedicated GPU clusters for training and inference separately">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU Clusters" target="Shared GPU clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="training and inference" target="GPUs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="training and inference" target="eight p3.2xlarge instances">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPUs" target="one of the most widely-used classes of accelerators for DL">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPUs" target="a page in the host memory to be pinned (or page-locked)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the current practice" target="dedicated clusters for training and inference separately">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="GPU clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the performance of PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="introducing pipelined context switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="unified memory management mechanisms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="active-standby worker switching mechanisms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="an active-standby mechanism for fast worker switching and process-level isolation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="a PipeSwitch prototype">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="it with PyTorch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="a new technology called pipelined context switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="pipelined context switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="a holistic approach">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the characteristics of DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="layers into groups">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="an optimal model-aware grouping algorithm">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="unified memory management with a dedicated memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="unified memory management with the memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="that the memory allocation for a DNN model is deterministic">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="extra memory copies between the daemon and the workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the IPC overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="an active and standby worker switching mechanism">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="new techniques">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="a system prototype">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="it with Py-Torch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="to pack multiple DL applications onto the same GPU via fine-grained time-sharing abstraction">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="fast context switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="measuring the time to start and execute it on the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the network time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the task queueing time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the overhead into the four components">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="to circumvent this physical limit">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="grouping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="multiple layers into a group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the transmission of the second group into the computation of the first group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="induction on m">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="two characteristics of DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="a property of DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the performance of these IPC APIs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the pages of the memory daemon to the host memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="synchronization points into training tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="a production GPU training trace from Microsoft 19,20">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="C and Python functions to the GPU memory management module of PyTorch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="two EC2 instance types">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="PyTorch with our plugins for all mechanisms in comparison for consistency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="representative configurations for each model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="single-GPU inference and training tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="throughput and latency as evaluation metrics">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the end-to-end latency experienced by the client">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="separate processes in advance">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="ResNet152">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="both training and inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="training and inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="each scheduling cycle">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="that to meet strict SLOs, it is important to reduce all overheads for task switching, not only the most significant one">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the parameter size and running time for each layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="Madan Musuvathi">
      <data key="d0">predicate</data>
    </edge>
    <edge source="We" target="the anonymous reviewers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="different applications including training and inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="based on the peak load">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="limited sharing between applications and task types">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="the need to meet strict Service-Level Objectives (SLOs)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="multiple users">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="privately shared">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="publicly shared">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="dedicated physical forms and power supplies">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="high speed networks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU clusters" target="specialized task schedulers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="a system">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="unused cycles of an inference application to be filled by training or other inference applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="GPU utilization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="significantly">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="without sacrificing SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="a task startup overhead of 3.66.6 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="a total overhead of 5.434.6 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="1050 better than NVIDIA MPS">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="near 100 GPU utilization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="single-GPU tasks for training and inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="single-GPU training for training tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="asynchronous multi-GPU training for data parallel strategies">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="a significant fraction of tasks in real-world workloads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="synchronous multi-GPU training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="high throughput close to the upper bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the agility of DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the model structure">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="hooks for PyTorch to wait for transmission or synchronize the execution">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="GPU-efficient multiplexing of many DL applications on GPU servers via fine-grained time-sharing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="millisecond-scale latencies and high throughput as dedicated servers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="GPU-efficient fine-grained time-sharing for multiple DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="millisecond-scale context switching latencies">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="high throughput">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="all the ideas into our system">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the gap of GPU memory sharing and switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the design of an efficient time-sharing GPU cluster for DL workloads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="GPU-efficient multiplexing of multiple DL applications on GPU servers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="millisecond-scale task switching time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="DL applications on time-sharing GPUs to meet strict SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="process-level isolation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="us to address new technical challenges on memory management and worker switching across different processes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="fast task switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="an active worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="multiple standby workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the active-standby worker switching mechanism used by PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="an active-standby worker switching mechanism">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="minimal overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="intra-batch pipelining">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="fast switching across tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the knowledge of models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="pipelining mechanism with optimal model-aware grouping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="model-aware grouping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the best trade-off between pipeline overhead and efficiency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the DNN framework">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the model structure information">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="transparent to users and cluster managers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="a dedicated memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="a 64-bit integer offset for the shared GPU memory to workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="223 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="eliminating the memory allocation overhead with the memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="no unified memory management">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="GPU memory blocks to PyTorch GPU memory pool">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the models in the memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the best">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the lower bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="pipeline method">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="No memory management">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="No IPC optimization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="No pin memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="CUDA unified memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="6000 to 8000 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="One process">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="Two processes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the first layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="computing the rst layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="only a few milliseconds overhead for task switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="low latency close to the lower bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="how to realize a scheduling decision">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch" target="the scheduler to change the resource allocation more often with millisecond-scale task switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="multiple DL applications" target="the same GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="multiple DL applications" target="the same GPU server">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the same GPU" target="the entire GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU utilization" target="the ratio to the upper bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Multi-GPU inference tasks" target="performing PipeSwitch on each GPU with transactions">
      <data key="d0">predicate</data>
    </edge>
    <edge source="preempting one GPU" target="other GPUs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a significant fraction of tasks in real-world workloads" target="a single GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="elastic synchronous training" target="dynamic changing of the number of GPUs used for training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="experiments" target="a variety of DNN models and GPU cards">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The key idea" target="the layered structure of neural network models and their layer-by-layer computation pattern">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The key idea" target="model transmission over the PCIe and task execution in the GPU with model-aware grouping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="a pipelined model transmission mechanism">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="a major challenge fast GPU context switching between different processes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="a measurement study">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="an algorithm to find the optimal grouping strategy for a given model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="all other components of PipeSwitch the same">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="the following mechanisms discussed in 4.4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="the strawman scenario">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="other challenges like memory management and worker switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="an overview of the architecture and task execution">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="end-to-end experiments to demonstrate the benefits of PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="the effectiveness of the design choices on each component">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="our design">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="all possible combinations">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="the optimal grouping strategy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="two pruning techniques">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="the cases that group from layer (i 1) to j j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="j j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="multiple layers in a group based on the progress of computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="a lower bound for the total time for each case in Equation 1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="B.delay to denote the time to which the group can be formed">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="a heuristic that bootstraps optgroups with a relative good strategy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="allocating GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="sharing the GPU memory to workers through CUDA IPC API">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="getting the shared GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="a memory management mechanism tailored for DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="separate processes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="the task cleaning of the current task and the pipelined model transmission of the new task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="the share of multi-GPU jobs to increase in the future">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="throughput and end-to-end latency of different mechanisms under different scheduling cycles">
      <data key="d0">predicate</data>
    </edge>
    <edge source="we" target="the following five mechanisms discussed in 4.3">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a pipelined model transmission mechanism" target="model transmission over the PCIe and model computation in the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Transmitting a task from CPU to GPU" target="the PCIe bandwidth">
      <data key="d0">predicate</data>
    </edge>
    <edge source="unified memory management and active-standby worker switching mechanisms" target="the pipelining">
      <data key="d0">predicate</data>
    </edge>
    <edge source="unified memory management and active-standby worker switching mechanisms" target="process-level isolation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the pipelining" target="per-group granularity">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Deep learning (DL)" target="an emerging family of intelligent applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="intelligent applications" target="many domains">
      <data key="d0">predicate</data>
    </edge>
    <edge source="many domains" target="retail">
      <data key="d0">predicate</data>
    </edge>
    <edge source="many domains" target="transportation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="many domains" target="finance">
      <data key="d0">predicate</data>
    </edge>
    <edge source="many domains" target="healthcare">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DL workloads" target="throughput-intensive training tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DL workloads" target="latency-sensitive inference tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DL workloads" target="well-defined structures">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inference tasks" target="training clusters under ash crowds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Training tasks" target="inference clusters when the inference load is low">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Training tasks" target="their models to the host memory periodically">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Training tasks" target="the latest checkpoint after preemption">
      <data key="d0">predicate</data>
    </edge>
    <edge source="training cluster" target="training tasks for inference tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference clusters" target="the peak load">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference clusters" target="running at high utilization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference clusters" target="training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inference clusters" target="the peak load">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inference clusters" target="user requests">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inference clusters" target="strict SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="strict SLOs" target="requests to be handled in small batches for low latency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="production systems" target="each application on per-GPU granularity">
      <data key="d0">predicate</data>
    </edge>
    <edge source="production systems" target="GPUs to applications on per-GPU granularity">
      <data key="d0">predicate</data>
    </edge>
    <edge source="production systems" target="GPUs to the VMs, containers or processes of an application">
      <data key="d0">predicate</data>
    </edge>
    <edge source="production systems" target="the interference between different applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="production systems" target="the SLO requirements">
      <data key="d0">predicate</data>
    </edge>
    <edge source="provisioning on per-GPU granularity" target="the interference between applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="packing multiple DL applications to the same GPU server" target="GPU utilization via time-sharing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="operating systems" target="high CPU utilization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="high CPU utilization" target="task scheduling and context switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The idea of ne-grained CPU time-sharing" target="cluster scheduling">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ne-grained time-sharing" target="CPU workloads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ne-grained time-sharing" target="better utilization than provisioning dedicated resources">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ne-grained time-sharing" target="necessary process-level isolation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="scheduling cycles" target="ne-grained enabled">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Google Borg 1" target="online services and batch jobs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Google Borg 1" target="20-30 machines">
      <data key="d0">predicate</data>
    </edge>
    <edge source="20-30 machines" target="not packing them">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU" target="high overhead when switching between tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU" target="the models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU" target="high memory bandwidth">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The gap" target="the precious GPU memory and slow switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="naively using GPUs in the same way as CPUs" target="the requirements of DL inference that have strict SLOs in the range of tens to hundreds of milliseconds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a GPU" target="a DNN model (e.g., ResNet)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a DNN model (e.g., ResNet)" target="the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="state-of-the-art tricks like CUDA unified memory" target="multiple seconds delay before serving the first inference request">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CPU applications" target="milliseconds or even microseconds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the existing solution" target="spatially share the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this approach" target="strong GPU memory isolation between applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA Multiple Process Sharing (MPS) 6" target="multiple processes to use the same GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus 7" target="multiple processes to use the same GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus 7" target="training tasks which are memory-intensive">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus 7" target="multiple inference tasks which have large models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus 7" target="">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus 7" target="the models to be preloaded to the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus 7" target="several limitations described in 2.2">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA Multiple Process Sharing (MPS) 6 and Salus 7" target="all processes data to be preloaded into the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="multi-process support from NVIDIA" target="the inference process to share the GPU with the training process">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA MPS 6" target="official support for sharing a GPU between multiple processes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU memory" target="much more limited than host memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU memory" target="many applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU memory" target="very limited even on high-end GPUs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU memory" target="task execution">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU memory" target="storing the state of idle applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU memory" target="its neural network model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU memory" target="PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="one single memory-intensive training task" target="all the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The training task" target="its GPU environment">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The training task" target="the entire GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The training task" target="when inference tasks come">
      <data key="d0">predicate</data>
    </edge>
    <edge source="its GPU environment" target="freeing the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory footprints of inference tasks" target="memory footprints">
      <data key="d0">predicate</data>
    </edge>
    <edge source="models" target="larger">
      <data key="d0">predicate</data>
    </edge>
    <edge source="models" target="the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="models" target="different structures">
      <data key="d0">predicate</data>
    </edge>
    <edge source="models" target="different CUDA streams">
      <data key="d0">predicate</data>
    </edge>
    <edge source="request batching" target="to increase throughput">
      <data key="d0">predicate</data>
    </edge>
    <edge source="increasing throughput" target="GPU memory requirement of inference applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a context switching design" target="the switching overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a context switching design" target="the contents on GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a context switching design" target="a better approach for efficiently time-sharing GPUs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="no existing solution" target="such context switching abstraction for GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelined context switching" target="the characteristics of DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelined context switching" target="millisecond-scale overhead for switching tasks on GPUs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelined context switching" target="task switching overhead on GPUs for DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelined context switching" target="pipelined model transmission">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelined context switching" target="unified memory management">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelined context switching" target="active-standby worker switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelined context switching" target="switching overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelined context switching" target="process-level isolation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the characteristics of DL applications" target="the overhead of all the components">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipeline" target="computation and GPU memory swapping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipeline" target="fast context switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipeline" target="feasible and effective">
      <data key="d0">predicate</data>
    </edge>
    <edge source="application" target="GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="context switching" target="if the application is already loaded in the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DNN models" target="host memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DNN models" target="a layered structure">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DNN models" target="a layer-by-layer computation pattern">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DNN models" target="usually deep">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DNN models" target="multiple layers stacking one on another">
      <data key="d0">predicate</data>
    </edge>
    <edge source="host memory" target="much larger and cheaper than GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="context-switching" target="training or inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="enterprises" target="GPU clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="11 M. Jeon, S. Venkataraman, A. Phanishayee, u. Qian, W. Xiao, and F. Yang" target="Analysis of large-scale multi-tenant GPU clusters for DNN training workloads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Analysis of large-scale multi-tenant GPU clusters for DNN training workloads" target="USENIX ATC">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Analysis of large-scale multi-tenant GPU clusters for DNN training workloads" target="2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Analysis of large-scale multi-tenant GPU clusters for DNN training workloads" target="USENIX ATC, 2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="512 14th USENIX Symposium on Operating Systems Design and Implementation" target="USENIX Association">
      <data key="d0">predicate</data>
    </edge>
    <edge source="USENIX Association" target="14th USENIX Symposium on Operating Systems Design and Implementation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the number of applications that can be multiplexed" target="the GPU memory size">
      <data key="d0">predicate</data>
    </edge>
    <edge source="each application" target="the entire GPU compute and memory resources during its time slice">
      <data key="d0">predicate</data>
    </edge>
    <edge source="small switching overhead" target="DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DL applications" target="strict SLO requirements">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DL applications" target="large models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DL applications" target="large amounts of intermediate results">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a measurement study" target="the task switching overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a measurement study" target="the overhead of each component">
      <data key="d0">predicate</data>
    </edge>
    <edge source="measurement study" target="profile the task switching overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="switching overhead" target="four components">
      <data key="d0">predicate</data>
    </edge>
    <edge source="four components" target="old task cleaning">
      <data key="d0">predicate</data>
    </edge>
    <edge source="four components" target="new task initialization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="four components" target="GPU memory allocation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="four components" target="model transmission via PCIe from CPU to GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Instance Type" target="g4dn.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Instance Type" target="p3.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="g4dn.2xlarge" target="better CPU than p3.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="g4dn.2xlarge" target="8 vCPUs (Intel Platinum 8259CL)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="g4dn.2xlarge" target="1 GPU (NVIDIA T4 with 16 GB GPU memory)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="g4dn.2xlarge" target="PCIe 3.0 8">
      <data key="d0">predicate</data>
    </edge>
    <edge source="g4dn.2xlarge" target="32 GB memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="g4dn.2xlarge" target="NVIDIA T4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="Intel Xeon E5-2686 v4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="8 vCPUs (Intel Xeon E5-2686 v4)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="1 GPU (NVIDIA V100 with 16 GB GPU memory)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="PCIe 3.0 16">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="61 GB memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="NVIDIA V100 GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="NVIDIA V100">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="3.0">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="16">
      <data key="d0">predicate</data>
    </edge>
    <edge source="p3.2xlarge" target="3.0 16">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU Type of g4dn.2xlarge" target="NVIDIA T4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA T4" target="16GB GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA T4" target="8.1 TFLOPS (single-precision)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA T4" target="comparable performance with NVIDIA V100">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA T4" target="PCIe 3.0 x8">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA T4" target="PCIe 3.0 8">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPU Type of p3.2xlarge" target="NVIDIA V100">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA V100" target="up to 32GB GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA V100" target="15.7 TFLOPS (single-precision)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA V100" target="PCIe 3.0 x16 interface">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA V100" target="PCIe 3.0 16">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Task Cleaning time on g4dn.2xlarge" target="155 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Task Cleaning time on p3.2xlarge" target="165 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Task Initialization time on g4dn.2xlarge" target="5530 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Task Initialization time on p3.2xlarge" target="7290 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Memory Allocation time on g4dn.2xlarge" target="10 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Memory Allocation time on p3.2xlarge" target="13 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Model Transmission time on g4dn.2xlarge" target="91 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Model Transmission time on p3.2xlarge" target="81 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Total Overhead on g4dn.2xlarge" target="5787 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Total Overhead on p3.2xlarge" target="7551 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inference Time on g4dn.2xlarge" target="105 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inference Time on p3.2xlarge" target="32 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Table 1" target="Measurement results of task switching overhead and the breakdown of individual components">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Every component" target="a considerable amount of time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="amount of time" target="tens of milliseconds to seconds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference task" target="tens of milliseconds on a GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference task" target="the server">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference task" target="the model for inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference task" target="the model itself">
      <data key="d0">predicate</data>
    </edge>
    <edge source="latency SLOs" target="typically a small multiple of the inference time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="One source of the overhead" target="the contentions both on the computation and memory of the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the training task" target="when an inference task comes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Our design" target="a key observation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="computation of DNN models" target="layer by layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="there" target="the entire model to be transmitted to the GPU before starting computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="there" target="always at least one idle standby worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a task" target="the entire model to be transmitted to the GPU before beginning the computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Naive pipelining on per-layer granularity" target="high overhead on tensor transmission and synchronization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pipelining on per-layer granularity" target="synchronization for every layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="an optimal model-aware grouping algorithm" target="the best grouping strategy for a given model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The computation of a DL task" target="layer by layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The computation of a DL task" target="a simple, regular pattern for memory allocation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A DL task" target="two important types of data in the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="two important types of data" target="the DNN model (including the model parameters) and the intermediate results">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The default general-purpose GPU memory management" target="an overkill">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The default general-purpose GPU memory management" target="unnecessary overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA" target="CUDA unified memory 4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA unified memory 4" target="memory movement between the host memory and the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA unified memory 4" target="applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="unified memory management with a dedicated memory daemon" target="the overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="unified memory management with the memory daemon" target="minimal memory footprint">
      <data key="d0">predicate</data>
    </edge>
    <edge source="unified memory management with the memory daemon" target="extra memory copies">
      <data key="d0">predicate</data>
    </edge>
    <edge source="extra memory copies" target="avoided">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The daemon" target="the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The daemon" target="it to each task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The daemon" target="the expensive GPU memory manager">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The daemon" target="that each time only one worker owns the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="cudaMalloc to obtain the GPU memory when the system starts">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="the memory to the workers at runtime">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="the GPU memory manager">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="the existing system">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="minimal changes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="the model from the host memory to the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="the worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="the relevant GPU memory handlers to the worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="the same order to transmit the model as the worker would">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon" target="the usage of expensive GPU IPCs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The DNN models" target="only once in the memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The DNN models" target="in every worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Storing the DNN models only once in the memory daemon" target="memory footprint">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory footprint" target="minimized">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory allocation for a DNN model" target="deterministic">
      <data key="d0">predicate</data>
    </edge>
    <edge source="no unified memory management" target="each worker to keep a copy for each DNN model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="keeping a copy for each DNN model" target="the memory footprint">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Each server" target="an active worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Each server" target="multiple standby workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A server" target="one or more standby workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The active worker" target="the current task on the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The active worker" target="the worker that currently executes a task in the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The active worker" target="a standby worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The active worker" target="the environment for the previous task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The standby workers" target="on the CPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The standby workers" target="the next task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="worker" target="a process that executes tasks on one GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="worker" target="active">
      <data key="d0">predicate</data>
    </edge>
    <edge source="worker" target="standby">
      <data key="d0">predicate</data>
    </edge>
    <edge source="worker" target="computing the corresponding layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="worker" target="user to register the model before starting a task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="worker" target="the models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="worker" target="the hooks to wait for parameter transmission or terminate on notification">
      <data key="d0">predicate</data>
    </edge>
    <edge source="active worker" target="current task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="controller" target="memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="controller" target="standby worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="controller" target="current active worker to stop">
      <data key="d0">predicate</data>
    </edge>
    <edge source="controller" target="parameters of the new model to the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="controller" target="receiving the current active worker's reply">
      <data key="d0">predicate</data>
    </edge>
    <edge source="controller" target="only one of these CUDA streams is active">
      <data key="d0">predicate</data>
    </edge>
    <edge source="controller" target="worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="standby worker" target="task to GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="standby worker" target="task with pipelined model transmission">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Our mechanism" target="old task cleaning in the active worker and new task initialization in the standby worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Our mechanism" target="worker switching overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Table 2" target="worker switching mechanisms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="active and standby worker switching mechanism" target="the overhead of both task cleaning and task initialization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="active and standby worker switching mechanism" target="process-level isolation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelining" target="the same task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="fast task switching" target="more flexible fine-grained scheduling">
      <data key="d0">predicate</data>
    </edge>
    <edge source="an active-standby worker switching mechanism" target="parallelize old task cleaning and new task initialization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pipelining" target="a canonical technique">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pipelining" target="computer systems">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pipelining" target="system performance">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pipelining" target="resource utilization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pipelining" target="two sources of system overheads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Prior work in DL systems such as PipeDream 8 and ByteScheduler 9" target="pipelining to distributed training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="These solutions" target="inter-batch pipelining">
      <data key="d0">predicate</data>
    </edge>
    <edge source="These solutions" target="PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inter-batch pipelining" target="computation and gradient transmission of different batches">
      <data key="d0">predicate</data>
    </edge>
    <edge source="computation and gradient transmission" target="training workloads of the same DNN model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="intra-batch pipelining" target="model transmission and computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="intra-batch pipelining" target="the overhead of switching between different DNN models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="intra-batch pipelining" target="both training and inference tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="different DNN models" target="either inference or training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="new techniques" target="training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="new techniques" target="inference that has strict SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="training" target="inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pipelined context switching" target="three key techniques">
      <data key="d0">predicate</data>
    </edge>
    <edge source="three key techniques" target="pipelined model transmission, unified memory management and active-standby worker switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this section" target="inefficiencies in today's shared GPU clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this section" target="running DL workloads on GPUs in the fine-grained time-sharing model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="fine-grained time-sharing abstraction" target="GPU utilization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="more flexible fine-grained scheduling" target="GPU utilization for dynamic workloads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="500 14th USENIX Symposium on Operating Systems Design and Implementation" target="USENIX Association">
      <data key="d0">predicate</data>
    </edge>
    <edge source="14th USENIX Symposium on Operating Systems Design and Implementation" target="Kubernetes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="514" target="14th USENIX Symposium on Operating Systems Design and Implementation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The main reason" target="to bring down the cost">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The demand of training" target="well predictable">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The demand of training" target="the progress of different developers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The demand of inference" target="more predictable">
      <data key="d0">predicate</data>
    </edge>
    <edge source="an inference task for a particular application" target="a daily periodical pattern based on the application usage">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the patterns" target="across different tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a shared cluster by different tasks" target="the resource utilization via time-sharing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a shared cluster by different tasks" target="traditional CPU workloads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="shared clusters" target="training and inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Training clusters" target="powerful GPUs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Training clusters" target="training tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="training tasks" target="often elastic">
      <data key="d0">predicate</data>
    </edge>
    <edge source="training tasks" target="strict deadlines">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GPUs designed for inference tasks" target="too wimpy for training tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the arrival of new GPU hardware" target="this">
      <data key="d0">predicate</data>
    </edge>
    <edge source="new GPU hardware" target="NVIDIA T4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="new algorithms and systems for distributed training" target="multiple GPUs to accelerate training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Our industry collaborator" target="a leading online service provider">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Our industry collaborator" target="this observation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This service provider" target="more than 10K V100 GPUs for training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This service provider" target="at least 5 as many T4 GPUs for inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The computation power on both sides" target="the same order of magnitude">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The inference workload" target="the number of active users">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The inference workload" target="clear peaks and valleys within each day">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The peak demand during daytime" target="2 of the valley at midnight">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference GPUs" target="during less busy times">
      <data key="d0">predicate</data>
    </edge>
    <edge source="training models" target="daily updates with latest data">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A good example" target="ne-tune BERT using daily news">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Borg-like 1 systems for GPUs" target="great opportunity in improving GPU utilization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference and training workloads" target="complementary usage patterns">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Online inference services" target="more idle during midnight">
      <data key="d0">predicate</data>
    </edge>
    <edge source="many training developers" target="a time-consuming job at night">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference loads on different models" target="different patterns">
      <data key="d0">predicate</data>
    </edge>
    <edge source="different patterns" target="time sharing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="any server" target="any task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="switch between different applications" target="low overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A modern server" target="several TB of host memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="several TB of host memory" target="it to load many applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="task execution on GPUs" target="GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="T4" target="16 GB">
      <data key="d0">predicate</data>
    </edge>
    <edge source="V100" target="32 GB">
      <data key="d0">predicate</data>
    </edge>
    <edge source="task execution" target="GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="DL tasks, especially training" target="a large amount, or even all of the memory on a GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="large models" target="hundreds of layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="large models and large amounts of intermediate results" target="a lot of GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="state-of-the-art models" target="deeper and larger">
      <data key="d0">predicate</data>
    </edge>
    <edge source="even idle applications" target="large memory space">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the active application" target="the entire GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the number of applications that can be served by a GPU server" target="its host memory size">
      <data key="d0">predicate</data>
    </edge>
    <edge source="switching a task" target="heavy memory swapping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="many online inference workloads" target="strict SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="naive memory swapping between the host memory and the GPU memory" target="strict SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the strawman scenario" target="stop a training task and then start an inference task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The rst inference batch" target="several seconds to nish (4.1)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Existing support such as NVIDIA MPS" target="DL workloads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Existing support such as NVIDIA MPS" target="hundreds of milliseconds overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA MPS" target="lower overhead compared to stop-and-start">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA MPS" target="several hundred milliseconds overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA MPS" target="NVIDIA MPS">
      <data key="d0">predicate</data>
    </edge>
    <edge source="several hundred milliseconds overhead" target="MPS from meeting strict SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 1" target="PipeSwitch architecture">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 1" target="the architecture of a PipeSwitch server">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Throughput" target="batches per second">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Throughput" target="eight p3.2xlarge instances">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Throughput" target="Upper bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Throughput" target="PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Throughput" target="MPS">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Throughput" target="Stop-and-start">
      <data key="d0">predicate</data>
    </edge>
    <edge source="MPS" target="poor throughput around 100 batches per second">
      <data key="d0">predicate</data>
    </edge>
    <edge source="MPS" target="about 80 ms average latency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="MPS" target="several hundred milliseconds latency for the rst batch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the structure and computation pattern of DNN models" target="us to highly optimize task switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the structure and computation pattern of DNN models" target="us to achieve millisecond-scale overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeSwitch pipelines" target="transmission and task execution">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This server" target="four types of components">
      <data key="d0">predicate</data>
    </edge>
    <edge source="four types of components" target="a controller">
      <data key="d0">predicate</data>
    </edge>
    <edge source="four types of components" target="a memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="four types of components" target="an active worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="four types of components" target="multiple standby workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The controller" target="the central component">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The controller" target="a set of tasks received from the clients">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The controller" target="only one active worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the memory daemon and the workers" target="the tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Memory" target="daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Controller" target="memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory daemon" target="the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory daemon" target="the DNN models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory daemon" target="the memory to the standby worker (4.3)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory daemon" target="the model used by the new task from the host memory to the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory daemon" target="GPU memory allocation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory daemon" target="model transmission">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory daemon" target="GPU memory handlers to workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory daemon" target="memory pointers to the workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The server" target="the DNN models in the host memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="All components" target="the SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A standby worker" target="idle">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A standby worker" target="initializing a new task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A standby worker" target="cleaning its environment for the previous task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The standby worker" target="the new active worker to execute the new task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a new task" target="a standby worker finishes cleaning a previous task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the new task" target="wait">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the new task" target="the GPU environment of the current task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the new task" target="its model to the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="waiting" target="its startup time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a scheduling policy" target="which task to execute next">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The scheduling" target="preemptive">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the controller" target="the current task for the next one based on the scheduling policy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the controller" target="the current task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the controller" target="an idle standby worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the controller" target="a task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the controller" target="the current active worker to stop">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the controller" target="the GPU memory allocated to the current active worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the controller" target="the GPU memory to the new active worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the controller" target="the model from the disk to the CPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the specific scheduling algorithm" target="orthogonal to this paper">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the current task" target="a training task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a training task" target="the model parameters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a training task" target="the DNN structure">
      <data key="d0">predicate</data>
    </edge>
    <edge source="an inference task" target="a strict latency SLO">
      <data key="d0">predicate</data>
    </edge>
    <edge source="an idle standby worker" target="its environment for the new task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="transmitting the model from the host memory to the GPU memory" target="the extra memory copy from the memory daemon to the worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the worker" target="the model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the worker" target="its task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the worker" target="the model structures">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the worker" target="the model parameters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the model" target="its own worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The primary goal of this paper" target="design a set of techniques based on the characteristics of DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The set of techniques" target="minimize the task switching overhead in this process">
      <data key="d0">predicate</data>
    </edge>
    <edge source="task switching overhead" target="individual components">
      <data key="d0">predicate</data>
    </edge>
    <edge source="our design" target="the overhead of each component">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a server" target="a training task running on the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a server" target="an inference task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The DNN model used in the measurement" target="ResNet152 17">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The measurement" target="two types of instances on Amazon AWS">
      <data key="d0">predicate</data>
    </edge>
    <edge source="two types of instances on Amazon AWS" target="g4dn.2xlarge with NVIDA T4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="two types of instances on Amazon AWS" target="p3.2xlarge with NVIDIA V100">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the server" target="one copy of each model in the host memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="total times to start the inference task on the GPUs" target="5787 ms and 7551 ms, respectively">
      <data key="d0">predicate</data>
    </edge>
    <edge source="task cleaning" target="time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The inference task" target="its environment">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The inference task" target="GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The inference task" target="the model from the host memory to the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="its environment" target="process launching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="its environment" target="PyTorch CUDA runtime loading">
      <data key="d0">predicate</data>
    </edge>
    <edge source="its environment" target="CUDA context initialization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Model" target="transmission">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Grouped transmission" target="a concept">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Grouped transmission" target="pipeline method">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Grouped transmission" target="no optimization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Grouped transmission" target="the layers of the model into one big tensor">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Grouped transmission" target="it in one group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference time on V100" target="inference time on T4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference time on V100 and inference time on T4" target="total overheads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="lower overhead on T4" target="task switching largely depends on CPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="better CPU" target="Intel Platinum 8259CL">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A strawman solution" target="the old task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A strawman solution" target="the new task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A strawman solution that simply stops the old task and starts the new task" target="SLOs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="all the components" target="considerable time compared to the inference time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="all the components" target="minimal switching overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The PCIe bandwidth" target="the physical limit on how fast an arbitrary task can be loaded to the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The computation" target="layer by layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="An inference task" target="a forward pass from the first layer to the final layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="An inference task" target="a forward pass to make a prediction">
      <data key="d0">predicate</data>
    </edge>
    <edge source="each iteration in a training task" target="a forward pass">
      <data key="d0">predicate</data>
    </edge>
    <edge source="each iteration in a training task" target="a backward pass">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the task" target="the computation of a layer as soon as the layer is loaded in the GPU and the input of the layer is ready">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the task" target="regardless of its following layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the input of the layer" target="the previous layers have finished their computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 2" target="the advantage of pipelining over the strawman solution">
      <data key="d0">predicate</data>
    </edge>
    <edge source="model" target="PCIe">
      <data key="d0">predicate</data>
    </edge>
    <edge source="model" target="GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="task" target="GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PCIe GPU E0 E1 En-1 E2" target="pipeline model transmission and task execution">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The example" target="an inference task that only has a forward pass in task execution">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Adding hooks" target="automated">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the DNN framework" target="PyTorch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PyTorch" target="tensors on them">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The basic way for pipelining" target="to pipeline on per-layer granularity">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the system" target="the layers to the GPU memory one by one">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the computation for a layer" target="the layer is transmitted">
      <data key="d0">predicate</data>
    </edge>
    <edge source="One" target="the overhead to invoke multiple calls to PCIe to transmit the data">
      <data key="d0">predicate</data>
    </edge>
    <edge source="One" target="p3.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="transmission overhead" target="data size">
      <data key="d0">predicate</data>
    </edge>
    <edge source="dividing the model into many layers" target="significant extra overhead when invoking a PCIe call for each layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="some layers" target="very small">
      <data key="d0">predicate</data>
    </edge>
    <edge source="synchronization overhead" target="between transmission and computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="synchronization overhead" target="the computation to know when a layer is ready to compute">
      <data key="d0">predicate</data>
    </edge>
    <edge source="grouping" target="these two sources of overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelining overhead" target="once for each group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pipelining overhead" target="instead of each layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Grouping" target="a trade-off between pipelining efficiency and pipelining overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Grouping" target="model-aware">
      <data key="d0">predicate</data>
    </edge>
    <edge source="using small groups" target="more overlap between transmission and computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="using small groups" target="more pipelining overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="more overlap between transmission and computation" target="pipelining efficiency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="using big groups" target="minimal pipelining overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="using big groups" target="the chance for overlapping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="different structures" target="the number of layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="different structures" target="the size of each layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the number of layers" target="both weighted and unweighted layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="all possible combinations" target="n cases">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the optimal grouping strategy" target="Equation 1 (line 1-27)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the optimal grouping strategy" target="the total time for the pipeline">
      <data key="d0">predicate</data>
    </edge>
    <edge source="two pruning techniques" target="two insights">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the time complexity for enumeration" target="exponential">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PCIe GPU" target="lower bound of F(Group(0, i), i1)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Group(0, i)" target="Group(i1, j)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="j" target="j1 to n-1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="case" target="lower bound current optimal time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="cases" target="i to j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="cases" target="how the first group is formed">
      <data key="d0">predicate</data>
    </edge>
    <edge source="batch" target="i1 to j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 3" target="examples for two pruning techniques">
      <data key="d0">predicate</data>
    </edge>
    <edge source="number of layers" target="n">
      <data key="d0">predicate</data>
    </edge>
    <edge source="F(B,i)" target="a function">
      <data key="d0">predicate</data>
    </edge>
    <edge source="F(B,i)" target="the total time of the optimal grouping strategy from layer i to n-1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="layer 0 to i-1" target="groups represented by B">
      <data key="d0">predicate</data>
    </edge>
    <edge source="F(,0)" target="min i F(group(0,i),i1)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="optimal grouping strategy" target="the entire model (i.e., F(, 0))">
      <data key="d0">predicate</data>
    </edge>
    <edge source="case i" target="the first group contains layer 0 to i">
      <data key="d0">predicate</data>
    </edge>
    <edge source="case i" target="line 18-19">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This formula" target="compute F(group(0,i),i1)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This formula" target="recursively">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the rst group" target="too many layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the rst group" target="all layers from x to n1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the computation of the rst group" target="too much">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the delay" target="the pipeline efciency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="packing multiple layers in a group based on the progress of computation" target="pipeline efficiency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="other than the rst group" target="packing multiple layers in a group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="T(i, j)" target="the transmission time for a group from layer i to j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="T(i, j)" target="the size of layer i to j and PCIe bandwidth">
      <data key="d0">predicate</data>
    </edge>
    <edge source="E(i, j)" target="the execution time for a group from layer i to j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="E(i, j)" target="the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the overhead of invoking multiple calls" target="T(i, j)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The lower bound" target="the best case that all the remaining layers are combined in one group for transmission and computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The lower bound" target="the computation and communication can be perfectly overlapped">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Its computation" target="right after the computation of the first group finishes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 3(b)" target="an example for this insight">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the first group" target="layer 0 to i">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Equation 1" target="enumerate the cases for the PCIe GPU B.delay">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Group" target="Group(a, i), Group(i1, n-1), Group(x, i), Group(i1, n-1)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 4" target="general case for the two pruning techniques">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the transmission" target="no later than the computation of the first group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The least number of layers to group" target="the following equation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="j argmax j T(i1, j) E(0,i)" target="Group from layer (i1) to j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Group from layer (i1) to j" target="grouping from (i-1) to j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="grouping from (i-1) to j" target="pipeline efficiency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="grouping from (i-1) to j" target="higher pipeline overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this algorithm" target="offline to find the strategy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the resulting strategy" target="PipeSwitch for context switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Algorithm 1" target="the pseudo code">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Algorithm 1" target="the optimal grouping strategy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Algorithm 1" target="optimality for a given list of layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Algorithm 1" target="any special assumptions on the execution order">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Algorithm 1" target="1.33 s">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Algorithm 1" target="0.18 s">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Algorithm 1" target="0.34 s">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Algorithm 1" target="only several seconds to compute an optimal grouping strategy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The function FindOptGrouping" target="the optimal grouping strategy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="B" target="the groups that have already formed">
      <data key="d0">predicate</data>
    </edge>
    <edge source="B" target="one group from layer 0 to i">
      <data key="d0">predicate</data>
    </edge>
    <edge source="B" target="multiple groups formed by previous layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="x" target="the first layer that have not formed a group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="optgroups" target="the best grouping strategy from layer x given B">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the best grouping strategy from layer x given B" target="none (line 2)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The algorithm" target="the second pruning insight">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The algorithm" target="the rst group from layer x">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The algorithm" target="the problem into k - 1 cases">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The algorithm" target="based on B.delay (line 4-9)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The algorithm" target="two pruning techniques">
      <data key="d0">predicate</data>
    </edge>
    <edge source="case i (0 ≤ i ≤ k)" target="the first group from layer x to x_i">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Equation 3 and Figure 3(b)" target="this insight with a special example">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The enumeration for i" target="the layers from x to j-1 (line 11)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the algorithm" target="the rst insight">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the algorithm" target="the lower bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the algorithm" target="k1 layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the algorithm" target="the optimal grouping strategy for m k 1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the algorithm" target="the optimal grouping strategy from these cases">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the lower bound" target="line 12-17">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the lower bound" target="the current optimal time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the lower bound" target="the lowest latency we can achieve for an inference task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the lower bound" target="the average latency of the ready model assuming no task switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the computation from x" target="both its transmission (T(x,i)) and the computation of the previous groups (B.delay)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="good strategy" target="group every ten layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The two pruning techniques" target="most of the strategies">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The two pruning techniques" target="the optimal one quickly">
      <data key="d0">predicate</data>
    </edge>
    <edge source="m n x" target="the number of layers the function considers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="FindOptGrouping(B,x)" target="the optimal grouping strategy from layer x to n 1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="FindOptGrouping(B,x)" target="the optimal strategy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="previous layers" target="groups represented by B">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the function" target="one layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="m" target="1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="layer x itself" target="one group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this strategy" target="the optimal strategy">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The optimal strategy for this case" target="one group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="these cases" target="exclusive">
      <data key="d0">predicate</data>
    </edge>
    <edge source="these cases" target="the entire search space">
      <data key="d0">predicate</data>
    </edge>
    <edge source="these cases" target="the computation to an earlier point than grouping from x to at least j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The rst technique" target="the cases">
      <data key="d0">predicate</data>
    </edge>
    <edge source="their lower bounds" target="the current found optimal">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this technique" target="the optimality">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The second technique" target="the case">
      <data key="d0">predicate</data>
    </edge>
    <edge source="their rst groups" target="layer x to j j">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pruning these cases" target="the optimality">
      <data key="d0">predicate</data>
    </edge>
    <edge source="layers or operators in a DNN model" target="an arbitrary computation graph">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Models like ResNet and Inception" target="technically non-linear directed acyclic graphs (DAGs)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="layersoperators in the DAG" target="the GPU one by one">
      <data key="d0">predicate</data>
    </edge>
    <edge source="execution order" target="layersoperators in the DAG">
      <data key="d0">predicate</data>
    </edge>
    <edge source="grouping the layers" target="high pipelining efficiency and low pipelining overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the order" target="the first time an operator is executed">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The order" target="correctness">
      <data key="d0">predicate</data>
    </edge>
    <edge source="an operator" target="it is transmitted to the GPU and the input is ready">
      <data key="d0">predicate</data>
    </edge>
    <edge source="our pipelined model transmission" target="the general case">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pipelined Model Transmission" target="keeping all other components of PipeSwitch the same and comparing mechanisms discussed in 4.2">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 7" target="Effectiveness of pipelined model transmission">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 7" target="the total time measured by the client for an inference task to preempt a training task and finish its inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Task execution in a GPU" target="GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A GPU" target="its own memory management system">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A GPU" target="a malloc function">
      <data key="d0">predicate</data>
    </edge>
    <edge source="malloc function" target="cudaMalloc for NVIDIA GPUs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="malloc function" target="CPUs for memory allocation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="each task" target="the native cudaMallocManaged function for GPU memory allocation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="each task" target="model transmission to CUDA unified memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Each worker" target="cudaMalloc">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Each worker" target="the model to GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Each worker" target="GPU memory with cudaMallocManaged">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Each worker" target="a memory pool to allocate the memory to store its model and intermediate results">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Each worker" target="the memory to the pool after the intermediate results are no longer needed">
      <data key="d0">predicate</data>
    </edge>
    <edge source="cudaMalloc" target="GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA" target="the model to GPU when needed">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA" target="unified memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This solution" target="high overhead for DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This solution" target="the lower bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This solution" target="existing systems like Gandiva 24">
      <data key="d0">predicate</data>
    </edge>
    <edge source="native cudaMalloc function" target="general-purpose applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA unified memory" target="general-purpose applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA unified memory" target="DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA unified memory" target="more than one hundred milliseconds overhead than PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA unified memory" target="memory swapping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="native cudaMalloc function and CUDA unified memory" target="unnecessary overhead for DL applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="two characteristics of DL applications" target="minimize GPU memory management overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The general-purpose GPU memory management" target="these characteristics">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The general-purpose GPU memory management" target="too heavy-weight for DL applications that require fast task switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the amount of memory allocated to the DNN model" target="fixed">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the amount of memory allocated to the DNN model" target="during task execution">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the amount of memory needed to store them" target="the same">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the intermediate results" target="a simple, regular pattern">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the intermediate results" target="memory fragmentation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the intermediate results" target="first-in-last-out">
      <data key="d0">predicate</data>
    </edge>
    <edge source="intermediate results" target="the outputs of each layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="intermediate results" target="the next layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A training task" target="the intermediate results generated in the forward pass cannot be immediately freed">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the intermediate results generated in the forward pass" target="the backward pass to update the weights">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the backward pass" target="the intermediate results">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the backward pass" target="reverse order as that the forward pass generates them">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory allocation and release" target="a simple stack-like mechanism">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a simple stack-like mechanism" target="memory fragmentation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory allocation overhead" target="">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a dedicated memory daemon" target="the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="passing memory pointers to the workers" target="light-weight">
      <data key="d0">predicate</data>
    </edge>
    <edge source="one worker" target="the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="owning the GPU memory" target="memory isolation between workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The memory management of PipeSwitch" target="that of Py-Torch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory management in PyTorch" target="memory allocation for a task itself">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Replicating the models in each worker" target="high memory footprint">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Replicating the models in each worker" target="the number of models a server can store">
      <data key="d0">predicate</data>
    </edge>
    <edge source="reducing the number of models a server can store" target="the types of tasks the server can execute">
      <data key="d0">predicate</data>
    </edge>
    <edge source="storing the models in a dedicate process" target="minimal memory footprint">
      <data key="d0">predicate</data>
    </edge>
    <edge source="storing the models in a dedicate process" target="an extra memory copy from this process to a worker to start a task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="each model" target="only once">
      <data key="d0">predicate</data>
    </edge>
    <edge source="an extra memory copy from this process to a worker to start a task" target="the task switching time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="IPC overhead" target="minimized">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a property of DL applications" target="the IPC overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="IPC APIs" target="cudaIpcOpenMemHandle for NVIDIA GPUs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="these IPC APIs" target="high overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The overhead" target="the pipeline">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the pipeline" target="the IPCs frequently">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the pipeline" target="only once for the entire model transmission (not)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the IPCs" target="synchronize model transmission and task execution for every pipeline group">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory allocation process for a neural network model" target="deterministic">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory daemon and the worker" target="the same order to allocate memory for the model parameters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory pointers for the parameters" target="the same">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the neural network model" target="known and given">
      <data key="d0">predicate</data>
    </edge>
    <edge source="latency" target="no unified memory management without IPC optimization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Pin memory" target="a concept">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pin memory" target="no">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The OS" target="a memory page to disk">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The OS" target="the page is inactive for a certain amount of time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a page in the host memory" target="in order to transmit the data in the page to the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a temporary pinned page" target="the transmission">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Process-level isolation" target="desirable">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Process-level isolation" target="one task cannot read the memory of another task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Process-level isolation" target="the crashing of one task does not affect other tasks or the entire system">
      <data key="d0">predicate</data>
    </edge>
    <edge source="separate processes" target="process-level isolation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A naive solution" target="separate processes">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A naive solution" target="the new task after the current task is stopped">
      <data key="d0">predicate</data>
    </edge>
    <edge source="sequential execution" target="long delay">
      <data key="d0">predicate</data>
    </edge>
    <edge source="long delay" target="old task cleaning and new task initialization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Another possible solution" target="to let the current and new tasks share the same process with a warm CUDA context">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The process of the old task" target="the GPU environment">
      <data key="d0">predicate</data>
    </edge>
    <edge source="another process" target="the new task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The process" target="the environment for the new task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The process" target="the required model loaded in the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a major job" target="asynchronous CUDA functions queued on the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the number of queued functions" target="limited">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the number of queued functions" target="quickly cleared">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Synchronization points" target="inference tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference tasks" target="short">
      <data key="d0">predicate</data>
    </edge>
    <edge source="inference tasks" target="not preempted">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Another job" target="free its GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="cleaning procedure" target="does not modify the content of the memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="cleaning procedure" target="metadata">
      <data key="d0">predicate</data>
    </edge>
    <edge source="cleaning procedure" target="pointers pointing to the tensor data">
      <data key="d0">predicate</data>
    </edge>
    <edge source="cleaning procedure" target="the actual data">
      <data key="d0">predicate</data>
    </edge>
    <edge source="metadata" target="GPU memory pointers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="parallelizing the task cleaning and pipelined model transmission" target="hide the task cleaning overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This choice" target="performance">
      <data key="d0">predicate</data>
    </edge>
    <edge source="This choice" target="a trusted environment">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a latter process" target="the memory data of a previous process">
      <data key="d0">predicate</data>
    </edge>
    <edge source="an additional zero-out operation" target="if this is a concern">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory bandwidth of GPU" target="900GBs for V100">
      <data key="d0">predicate</data>
    </edge>
    <edge source="zeroing-out most models like ResNet-152 (around 240MB)" target="sub-millisecond overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="only one active worker" target="exclusive occupation of the GPU">
      <data key="d0">predicate</data>
    </edge>
    <edge source="number of standby workers" target="their GPU memory consumption">
      <data key="d0">predicate</data>
    </edge>
    <edge source="every standby worker" target="its own CUDA context">
      <data key="d0">predicate</data>
    </edge>
    <edge source="its own CUDA context" target="a few hundred MB GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="two standby workers" target="at least one idle worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="one idle worker" target="the waiting time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="one idle worker" target="moderate GPU memory consumption">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A transaction" target="a model is switched in or out on all of its GPUs to enable or disable inference on this model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="111,883 tasks in this trace" target="96,662 tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="96,662 tasks" target="single-GPU training tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="96,662 tasks" target="86% of all the tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="these jobs" target="18 of total GPU hours">
      <data key="d0">predicate</data>
    </edge>
    <edge source="current training frameworks" target="mature support of elastic training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="functions" target="the received GPU memory into PyTorch GPU memory pool for a specific CUDA stream">
      <data key="d0">predicate</data>
    </edge>
    <edge source="functions" target="the GPU memory from the pool">
      <data key="d0">predicate</data>
    </edge>
    <edge source="shared GPU memory" target="PyTorch GPU memory pool">
      <data key="d0">predicate</data>
    </edge>
    <edge source="shared GPU memory" target="different CUDA streams">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The controller process" target="a TCP thread and a scheduler thread">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the scheduler and the memory daemon" target="for better performance">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The TCP thread" target="task through TCP from clients">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The TCP thread" target="the task to the scheduler thread">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The scheduler thread" target="the GPU memory with workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The scheduler thread" target="workers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The scheduler thread" target="the task to a worker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The scheduler thread" target="parameters for the corresponding model to the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the user" target="the model in the scheduler">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the scheduler" target="the controller">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Parameters" target="the GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Parameters" target="groups">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Parameters" target="a pipeline">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The worker process" target="two threads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The termination thread" target="the termination signal from the controller">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The termination thread" target="the main thread">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The main thread" target="the DNN models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The main thread" target="the computation for inference or training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the model structures" target="small">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The parameters" target="once in the memory daemon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The parameters" target="minimal memory footprint">
      <data key="d0">predicate</data>
    </edge>
    <edge source="their parameters" target="locations in the shared GPU memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Different models" target="the same GPU memory location">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the value" target="the controller transfers the corresponding parameters to these locations">
      <data key="d0">predicate</data>
    </edge>
    <edge source="All experiments" target="AWS">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The software environment" target="PyTorch-1.3.0">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The software environment" target="torchvision-0.4.2">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The software environment" target="scipy-1.3.2">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The software environment" target="CUDA-10.1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PyTorch with our plugins" target="better results for stop-and-start than native PyTorch from Python-PyPI used in Table 1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The models" target="ResNet152 17">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The models" target="Inceptionv3 22">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The models" target="Bertbase 23">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152 17, Inceptionv3 22 and Bertbase 23" target="standard benchmarks for evaluating DL systems">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The experiments" target="both training and inference">
      <data key="d0">predicate</data>
    </edge>
    <edge source="single-GPU inference and training tasks" target="4.5">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The checkpointing frequency of training tasks" target="the scheduling cycle">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The checkpointing frequency of training tasks" target="checkpointing overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="default batch size for training" target="32">
      <data key="d0">predicate</data>
    </edge>
    <edge source="default batch size for inference" target="8">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 5" target="total latency experienced by the client for different mechanisms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 5" target="the latency experienced by the client">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Table 3" target="the total overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Each number" target="the average of 100 runs">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 6(b)" target="minimum and maximum latencies using the error bar">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 6(b)" target="the average latency of the inference tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="latency of the first batch and those of later batches in one scheduling cycle" target="significantly due to switching overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="6.1 End-to-End Experiments" target="end-to-end overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a client" target="an inference task to a GPU server">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the GPU server" target="the training task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the GPU server" target="the inference task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the GPU server" target="a reply back to the client">
      <data key="d0">predicate</data>
    </edge>
    <edge source="There" target="no training task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Gandiva 24" target="task switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="task switching" target="similar second-scale overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="task switching" target="several seconds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The properties" target="4">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency" target="milliseconds (ms)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency" target="7500 to 10000 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency" target="ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency" target="5000 to 7500 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Models" target="Ready model, PipeSwitch, MPS, Stop-and-start, ResNet152, Inceptionv3, Bertbase">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Models" target="ResNet152, Inceptionv3, Bertbase">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Models" target="ResNet152">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Models" target="Inceptionv3">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Models" target="Bertbase">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency (ms)" target="5000 to 10000">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency (ms)" target="Ready model, PipeSwitch, MPS, Stop-and-start">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency (ms)" target="ResNet152">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency (ms)" target="Inceptionv3">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Latency (ms)" target="Bertbase">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152" target="the text">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152" target="PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152" target="p3.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152" target="3.62 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152" target="g4dn.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152" target="2.53 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152" target="464">
      <data key="d0">predicate</data>
    </edge>
    <edge source="ResNet152" target="hundreds of layers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inceptionv3" target="the text">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inceptionv3" target="PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inceptionv3" target="p3.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inceptionv3" target="4.82 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inceptionv3" target="g4dn.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inceptionv3" target="5.49 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Inceptionv3" target="189">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Bertbase" target="the text">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Bertbase" target="PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Bertbase" target="p3.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Bertbase" target="3.62 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Bertbase" target="g4dn.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Bertbase" target="6.57 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Bertbase" target="139">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Per-layer pipeline" target="pipeline method">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Per-layer pipeline" target="a concept">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Per-layer pipeline" target="transmission and computation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="No optimization" target="pipeline method">
      <data key="d0">predicate</data>
    </edge>
    <edge source="No optimization" target="the worst in most cases">
      <data key="d0">predicate</data>
    </edge>
    <edge source="510 14th USENIX Symposium on Operating Systems Design and Implementation" target="USENIX Association">
      <data key="d0">predicate</data>
    </edge>
    <edge source="One process" target="a process">
      <data key="d0">predicate</data>
    </edge>
    <edge source="One process" target="the CUDA environment">
      <data key="d0">predicate</data>
    </edge>
    <edge source="One process" target="the overhead to clean the environment">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Two processes" target="the worst">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Table 4" target="task startup overhead for PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="task startup overhead for PipeSwitch" target="the difference between the time for ResNet152, Inceptionv3, Bertbase">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Only Pruning 1" target="2.09 s">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Only Pruning 1" target="0.30 s">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Only Pruning 1" target="0.88 s">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Only Pruning 2" target="3.44 h">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Only Pruning 2" target="5.07 s">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Only Pruning 2" target="24 h">
      <data key="d0">predicate</data>
    </edge>
    <edge source="No Pruning" target="24 h">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Table 5" target="effectiveness of two pruning techniques">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Table 5" target="the running time of Algorithm 1">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Table 5" target="the effects of the two pruning techniques mentioned in 4.2">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Its performance" target="the ready model when the model is preloaded">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Its performance" target="NVIDIA MPS when the model is in the host memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The total overhead" target="the difference between the latency of a mechanism and that of the ready model">
      <data key="d0">predicate</data>
    </edge>
    <edge source="stop-and-start" target="the worst">
      <data key="d0">predicate</data>
    </edge>
    <edge source="stop-and-start" target="several seconds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The main source of the overhead" target="CUDA context initialization and rst-time library loading operations in PyTorch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Another source" target="GPU memory swapping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The overhead of PipeSwitch for most configurations" target="10ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The overhead of PipeSwitch for BERT on T4" target="the large model size">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The overhead of PipeSwitch for BERT on T4" target="the smaller PCIe bandwidth on T4 than that on V100">
      <data key="d0">predicate</data>
    </edge>
    <edge source="computing BERT on T4" target="120ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="relative overhead" target="acceptable">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the ready model" target="computing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The startup overhead of PipeSwitch" target="only a few milliseconds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 6(a)" target="the inference throughput">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The dashed line" target="the upper bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The dashed line" target="the lower bound">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the upper bound" target="the throughput of the ready model assuming no task switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="throughput of stop-and-start" target="nearly zero for scheduling cycles smaller than 10 s">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The error bar" target="the minimum and maximum latency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Stop- and-start" target="poor latency">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the rst batch" target="several seconds overhead">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Throughput and latency" target="different scheduling cycles for ResNet on p3.2xlarge">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Computation" target="once parameters are transmitted">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 8" target="the total time measured by the client">
      <data key="d0">predicate</data>
    </edge>
    <edge source="overlaps" target="layer">
      <data key="d0">predicate</data>
    </edge>
    <edge source="models with many layers but relatively light computation such as ResNet152 and Inception" target="grouped transmission">
      <data key="d0">predicate</data>
    </edge>
    <edge source="models with many layers but relatively light computation such as ResNet152 and Inception" target="sometimes even no pipeline">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this reduction" target="significant">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this reduction" target="the optimizations on memory management and worker switching have already been applied">
      <data key="d0">predicate</data>
    </edge>
    <edge source="both weighted and unweighted layers" target="the computation time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="no pruning" target="for all three models after running for 24 hours">
      <data key="d0">predicate</data>
    </edge>
    <edge source="unied memory management" target="effectiveness">
      <data key="d0">predicate</data>
    </edge>
    <edge source="memory management" target="not unified">
      <data key="d0">predicate</data>
    </edge>
    <edge source="IPC" target="no optimization">
      <data key="d0">predicate</data>
    </edge>
    <edge source="the pages of the memory daemon" target="the main memory">
      <data key="d0">predicate</data>
    </edge>
    <edge source="this experiment" target="all the optimizations on memory management are effective">
      <data key="d0">predicate</data>
    </edge>
    <edge source="unied memory management mechanism" target="PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="IPC optimization" target="important">
      <data key="d0">predicate</data>
    </edge>
    <edge source="IPC optimization" target="latency by 1648 ms">
      <data key="d0">predicate</data>
    </edge>
    <edge source="pinning the pages to the host memory" target="the latency with a few milliseconds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 9" target="Effectiveness of active-standby switching">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Figure 9" target="the results">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The new process" target="a new CUDA environment">
      <data key="d0">predicate</data>
    </edge>
    <edge source="a new CUDA environment" target="the total time">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Many frameworks" target="deep learning">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Many frameworks" target="TensorFlow">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Many frameworks" target="PyTorch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Many frameworks" target="MXNet">
      <data key="d0">predicate</data>
    </edge>
    <edge source="MXNet" target="a flexible and efficient machine learning library">
      <data key="d0">predicate</data>
    </edge>
    <edge source="MXNet" target="heterogeneous distributed systems">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Several algorithms and systems" target="executing and scheduling deep learning tasks on clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Several algorithms and systems" target="both training and inference tasks">
      <data key="d0">predicate</data>
    </edge>
    <edge source="These scheduling solutions" target="orthogonal and complementary to PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Many techniques and systems" target="optimize communication">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Many techniques and systems" target="improve distributed training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="The most relevant ones" target="PipeDream 8, ByteScheduler 9 and Poseidon 40">
      <data key="d0">predicate</data>
    </edge>
    <edge source="vDNN 43 and SwapAdvisor 44" target="GPU memory management module">
      <data key="d0">predicate</data>
    </edge>
    <edge source="vDNN 43 and SwapAdvisor 44" target="memory management for a single training task of large models">
      <data key="d0">predicate</data>
    </edge>
    <edge source="vDNN 43 and SwapAdvisor 44" target="PipeSwitch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Cluster managers 4548" target="GPUs to VMs or containers at device granularity">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Several solutions" target="a GPU at application granularity using techniques like library interception">
      <data key="d0">predicate</data>
    </edge>
    <edge source="efforts on GPU optimization" target="the performance of running a single task">
      <data key="d0">predicate</data>
    </edge>
    <edge source="efforts on GPU optimization" target="tensor fusion">
      <data key="d0">predicate</data>
    </edge>
    <edge source="efforts on GPU optimization" target="kernel-level concurrency and scheduling">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Madan Musuvathi and the anonymous reviewers" target="valuable feedback">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes" target="Large-scale cluster management at Google with Borg">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Large-scale cluster management at Google with Borg" target="EuroSys">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Large-scale cluster management at Google with Borg" target="2015">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Nexus" target="a GPU cluster engine for accelerating DNN-based video analysis">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Nexus" target="ACM SOSP, 2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="3 H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Philipose, A. Krishnamurthy, and R. Sundaram" target="Nexus">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Fried, J. Behrens, A. Belay, and H. Balakrishnan" target="Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads" target="USENIX NSDI">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads" target="2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDA Multi-Process Service" target="6">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CUDAMultiProcessServiceOverview.pdf" target="https://docs.nvidia.com/deploy/pdf">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus" target="Fine-grained GPU sharing primitives for deep learning applications">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus" target="Conference on Machine Learning and Systems">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Salus" target="2020">
      <data key="d0">predicate</data>
    </edge>
    <edge source="7 P. Yu and M. Chowdhury" target="Salus">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeDream" target="generalized pipeline parallelism for DNN training">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeDream" target="ACM SOSP, 2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="PipeDream" target="D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo" target="A generic communication scheduler for distributed DNN training acceleration">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A generic communication scheduler for distributed DNN training acceleration" target="ACM SOSP">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A generic communication scheduler for distributed DNN training acceleration" target="2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Tiresias" target="a GPU cluster manager for distributed deep learning">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Tiresias" target="USENIX NSDI">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Tiresias" target="2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Authors" target="J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Authors" target="Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Authors" target="G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Authors" target="M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Poseidon" target="an efficient communication architecture">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Poseidon" target="distributed deep learning on GPU clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Poseidon" target="USENIX ATC, 2017">
      <data key="d0">predicate</data>
    </edge>
    <edge source="H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie, and E. P. Xing" target="Poseidon">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Amazon Web Services" target="12">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Microsoft Azure" target="13">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Google Cloud Platform" target="14">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Horovod" target="fast and easy distributed deep learning in TensorFlow">
      <data key="d0">predicate</data>
    </edge>
    <edge source="15 A. Sergeev and M. Del Balso" target="Horovod: fast and easy distributed deep learning in TensorFlow">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Horovod paper" target="arXiv preprint arXiv:1802.05799">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Horovod paper" target="2018">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Su" target="Scaling distributed machine learning with the parameter server">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Scaling distributed machine learning with the parameter server" target="USENIX OSDI">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Scaling distributed machine learning with the parameter server" target="2014">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Deep residual learning for image recognition" target="Sun">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Deep residual learning for image recognition" target="IEEE Conference on Computer Vision and Pattern Recognition">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Deep residual learning for image recognition" target="2016">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Nvidia data center deep learning product" target="performance">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Philly" target="20 traces">
      <data key="d0">predicate</data>
    </edge>
    <edge source="philly-traces" target="https://github.com/msr-fiddle/philly-traces">
      <data key="d0">predicate</data>
    </edge>
    <edge source="21" target="PyTorch">
      <data key="d0">predicate</data>
    </edge>
    <edge source="22 C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna" target="Rethinking the inception architecture for computer vision">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Rethinking the inception architecture for computer vision" target="IEEE Conference on Computer Vision and Pattern Recognition">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Rethinking the inception architecture for computer vision" target="2016">
      <data key="d0">predicate</data>
    </edge>
    <edge source="J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova" target="BERT: Pre-training of deep bidirectional transformers for language understanding">
      <data key="d0">predicate</data>
    </edge>
    <edge source="BERT: Pre-training of deep bidirectional transformers for language understanding" target="Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)" target="2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Gandiva" target="Introspective cluster scheduling for deep learning">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Gandiva" target="USENIX OSDI">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Gandiva" target="2018">
      <data key="d0">predicate</data>
    </edge>
    <edge source="24 W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P. Patel, X. Peng, H. Zhao, Q. Zhang, et al." target="Gandiva">
      <data key="d0">predicate</data>
    </edge>
    <edge source="25" target="TensorFlow">
      <data key="d0">predicate</data>
    </edge>
    <edge source="TensorFlow XLA" target="54">
      <data key="d0">predicate</data>
    </edge>
    <edge source="26" target="MXNet">
      <data key="d0">predicate</data>
    </edge>
    <edge source="https://mxnet.apache.org" target="a website URL">
      <data key="d0">predicate</data>
    </edge>
    <edge source="M. J. Freedman" target="Slaq: quality-driven scheduling for distributed machine learning">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Slaq: quality-driven scheduling for distributed machine learning" target="ACM Symposium on Cloud Computing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Slaq: quality-driven scheduling for distributed machine learning" target="2017">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Optimus" target="an efficient dynamic resource scheduler for deep learning clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Optimus" target="EuroSys">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Optimus" target="2018">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Themis" target="Fair and efficient GPU cluster scheduling">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Themis" target="USENIX NSDI">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Themis" target="2020">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Themis" target="K. Mahajan, A. Balasubramanian, A. Singhvi, S. Venkataraman, A. Akella, A. Phanishayee, and S. Chawla">
      <data key="d0">predicate</data>
    </edge>
    <edge source="HyperSched" target="Dynamic resource reallocation for model development on a deadline">
      <data key="d0">predicate</data>
    </edge>
    <edge source="HyperSched" target="ACM Symposium on Cloud Computing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="HyperSched" target="2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="HyperSched" target="R. Liaw, R. Bhardwaj, L. Dunlap, Y. Zou, J. E. Gonzalez, I. Stoica, and A. Tumanov">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CHET" target="an optimizing compiler for fully-homomorphic neural-network inferencing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CHET" target="ACM Conference on Programming Language Design and Implementation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="CHET" target="2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="R. Dathathri, O. Saarikivi, H. Chen, K. Laine, K. Lauter, S. Maleki, M. Musuvathi, and T. Mytkowicz" target="CHET">
      <data key="d0">predicate</data>
    </edge>
    <edge source="TVM" target="an automated end-to-end optimizing compiler for deep learning">
      <data key="d0">predicate</data>
    </edge>
    <edge source="TVM" target="USENIX OSDI">
      <data key="d0">predicate</data>
    </edge>
    <edge source="TVM" target="2018">
      <data key="d0">predicate</data>
    </edge>
    <edge source="T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, et al." target="TVM">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Gpipe" target="efficient training of giant neural networks using pipeline parallelism">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Gpipe" target="Advances in Neural Information Processing Systems, 2019">
      <data key="d0">predicate</data>
    </edge>
    <edge source="33 Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al." target="Gpipe">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Blink" target="Fast and generic collectives for distributed ML">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Blink" target="Conference on Machine Learning and Systems">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Blink" target="2020">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA Collective Communications Library" target="NCCL">
      <data key="d0">predicate</data>
    </edge>
    <edge source="36 J. Liu, J. Wu, and D. K. Panda" target="High performance RDMA-based MPI implementation over inniband">
      <data key="d0">predicate</data>
    </edge>
    <edge source="37 Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing" target="More effective distributed ML via a stale synchronous parallel parameter server">
      <data key="d0">predicate</data>
    </edge>
    <edge source="More effective distributed ML via a stale synchronous parallel parameter server" target="Advances in Neural Information Processing Systems">
      <data key="d0">predicate</data>
    </edge>
    <edge source="More effective distributed ML via a stale synchronous parallel parameter server" target="2013">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A. Awan, C.-H. Chu, H. Subramoni, and D. K. Panda" target="Optimized broadcast for deep learning workloads on dense-GPU inniband clusters: MPI or NCCL?">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Optimized broadcast for deep learning workloads on dense-GPU inniband clusters: MPI or NCCL?" target="Proceedings of the 25th European MPI Users Group Meeting">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Proceedings of the 25th European MPI Users Group Meeting" target="2018">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A. Vishnu, C. Siegel, T. Warfel, and V. Amatya" target="GossipGraD: Scalable deep learning using gossip communication based asynchronous gradient descent">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GossipGraD" target="CoRR">
      <data key="d0">predicate</data>
    </edge>
    <edge source="41 Z. Zhang, C. Chang, H. Lin, Y. Wang, R. Arora, and X. Jin" target="Is network the bottleneck of distributed training?">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Is network the bottleneck of distributed training?" target="ACM SIGCOMM Workshop on Network Meets AI ML (NetAI)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Is network the bottleneck of distributed training?" target="August 2020">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Y. Chen, Z. Liu, B. Ren, and X. Jin" target="On efficient constructions of checkpoints">
      <data key="d0">predicate</data>
    </edge>
    <edge source="On efficient constructions of checkpoints" target="International Conference on Machine Learning (ICML)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="International Conference on Machine Learning (ICML)" target="July 2020">
      <data key="d0">predicate</data>
    </edge>
    <edge source="vDNN" target="Virtualized deep neural networks for scalable, memory-efficient neural network design">
      <data key="d0">predicate</data>
    </edge>
    <edge source="vDNN" target="2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)">
      <data key="d0">predicate</data>
    </edge>
    <edge source="vDNN" target="2016">
      <data key="d0">predicate</data>
    </edge>
    <edge source="SwapAdvisor" target="C.-C. Huang, G. Jin, and J. Li">
      <data key="d0">predicate</data>
    </edge>
    <edge source="SwapAdvisor" target="the GPU memory limit">
      <data key="d0">predicate</data>
    </edge>
    <edge source="SwapAdvisor" target="smart swapping">
      <data key="d0">predicate</data>
    </edge>
    <edge source="SwapAdvisor" target="ACM ASPLOS">
      <data key="d0">predicate</data>
    </edge>
    <edge source="SwapAdvisor" target="2020">
      <data key="d0">predicate</data>
    </edge>
    <edge source="NVIDIA Container Runtime" target="Docker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="nvidia-docker" target="https://github.com/NVIDIA/nvidia-docker">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Mesos" target="a platform for fine-grained resource sharing in the data center">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Mesos" target="USENIX NSDI, 2011">
      <data key="d0">predicate</data>
    </edge>
    <edge source="47 B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H. Katz, S. Shenker, and I. Stoica" target="Mesos: A platform for fine-grained resource sharing in the data center">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Apache Hadoop YARN" target="Yet another resource negotiator">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Apache Hadoop YARN" target="ACM Symposium on Cloud Computing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Apache Hadoop YARN" target="2013">
      <data key="d0">predicate</data>
    </edge>
    <edge source="V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al." target="Apache Hadoop YARN">
      <data key="d0">predicate</data>
    </edge>
    <edge source="49 G. Giunta, R. Montella, G. Agrillo, and G. Coviello" target="A GPGPU transparent virtualization component for high performance computing clouds">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A GPGPU transparent virtualization component for high performance computing clouds" target="European Conference on Parallel Processing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="A GPGPU transparent virtualization component for high performance computing clouds" target="2010">
      <data key="d0">predicate</data>
    </edge>
    <edge source="V. T. Ravi, M. Becchi, G. Agrawal, and S. Chakradhar" target="Supporting GPU sharing in cloud environments with a transparent runtime consolidation framework">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Supporting GPU sharing in cloud environments with a transparent runtime consolidation framework" target="Proceedings of the 20th international symposium on High performance distributed computing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Proceedings of the 20th international symposium on High performance distributed computing" target="2011">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GViM" target="GPU-accelerated virtual machines">
      <data key="d0">predicate</data>
    </edge>
    <edge source="GViM" target="Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing" target="2009">
      <data key="d0">predicate</data>
    </edge>
    <edge source="V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan" target="GViM: GPU-accelerated virtual machines">
      <data key="d0">predicate</data>
    </edge>
    <edge source="rCUDA" target="the number of GPU-based accelerators in high performance clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="rCUDA" target="2010 International Conference on High Performance Computing Simulation">
      <data key="d0">predicate</data>
    </edge>
    <edge source="51 J. Duato, A. J. Pena, F. Silla, R. Mayo, and E. S. Quintana-Ort" target="rCUDA: Reducing the number of GPU-based accelerators in high performance clusters">
      <data key="d0">predicate</data>
    </edge>
    <edge source="2010 International Conference on High Performance Computing Simulation" target="2010">
      <data key="d0">predicate</data>
    </edge>
    <edge source="vCUDA" target="GPU-accelerated high-performance computing in virtual machines">
      <data key="d0">predicate</data>
    </edge>
    <edge source="vCUDA" target="IEEE Transactions on Computers">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Sun and K. Li" target="vCUDA: GPU-accelerated high-performance computing in virtual machines">
      <data key="d0">predicate</data>
    </edge>
    <edge source="55 T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang" target="MXNet">
      <data key="d0">predicate</data>
    </edge>
    <edge source="MXNet paper" target="2015">
      <data key="d0">predicate</data>
    </edge>
    <edge source="MXNet paper" target="arXiv preprint arXiv:1512.01274">
      <data key="d0">predicate</data>
    </edge>
    <edge source="56 C. Gregg, J. Dorn, K. Hazelwood, and K. Skadron" target="Fine-grained resource sharing for concurrent GPGPU kernels">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Fine-grained resource sharing for concurrent GPGPU kernels" target="4th USENIX Workshop on Hot Topics in Parallelism">
      <data key="d0">predicate</data>
    </edge>
    <edge source="4th USENIX Workshop on Hot Topics in Parallelism" target="2012">
      <data key="d0">predicate</data>
    </edge>
    <edge source="57 S. Pai, M. J. Thazhuthaveetil, and R. Govindarajan" target="Improving GPGPU concurrency with elastic kernels">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Improving GPGPU concurrency with elastic kernels" target="ACM SIGARCH Computer Architecture News">
      <data key="d0">predicate</data>
    </edge>
    <edge source="Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia, and A. Aiken" target="TASO: optimizing deep learning computation with automatic generation of graph substitutions">
      <data key="d0">predicate</data>
    </edge>
    <edge source="TASO: optimizing deep learning computation with automatic generation of graph substitutions" target="ACM SOSP, 2019">
      <data key="d0">predicate</data>
    </edge>
  </graph>
</graphml>
