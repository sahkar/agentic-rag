<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d0" for="edge" attr.name="relationship" attr.type="string"/>
<graph edgedefault="directed"><node id="This paper"/>
<node id="the proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation"/>
<node id="The proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation"/>
<node id="USENIX"/>
<node id="PipeSwitch"/>
<node id="fast pipelined context switching for deep learning applications"/>
<node id="Zhihao Bai"/>
<node id="Johns Hopkins University"/>
<node id="Zhen Zhang"/>
<node id="Yibo Zhu"/>
<node id="ByteDance Inc."/>
<node id="Xin Jin"/>
<node id="Deep learning workloads"/>
<node id="throughput-intensive training tasks"/>
<node id="latency-sensitive inference tasks"/>
<node id="The dominant practice today"/>
<node id="to provision dedicated GPU clusters for training and inference separately"/>
<node id="training and inference"/>
<node id="GPUs"/>
<node id="the current practice"/>
<node id="dedicated clusters for training and inference separately"/>
<node id="We"/>
<node id="fine-grained time-sharing GPU clusters"/>
<node id="GPU clusters"/>
<node id="different applications including training and inference"/>
<node id="based on the peak load"/>
<node id="between applications and task types"/>
<node id="the need to meet strict service-level objectives (SLOs)"/>
<node id="PIPESWITCH"/>
<node id="a system"/>
<node id="unused cycles of an inference application to be filled by training or other inference applications"/>
<node id="GPU utilization"/>
<node id="SLOs"/>
<node id="Experiments on a variety of DL models and GPU cards"/>
<node id="PipeSwitch only incurs a task startup overhead of 3.66.6 ms"/>
<node id="PipeSwitch incurs a total overhead of 5.434.6 ms"/>
<node id="1050 better than NVIDIA MPS"/>
<node id="near 100 GPU utilization"/>
<node id="PipeSwitch has a total overhead of 5.434.6 ms"/>
<node id="PipeSwitch total overhead"/>
<node id="single-GPU tasks for training and inference"/>
<node id="MULTI-GPU INFERENCE TASKS"/>
<node id="performing PIPESWITCH on each GPU with transactions"/>
<node id="single-GPU training for training tasks"/>
<node id="asynchronous multi-GPU training for data parallel strategies"/>
<node id="preempting one GPU"/>
<node id="other GPUs"/>
<node id="Elastic synchronous training"/>
<node id="the dynamic changing of the number of GPUs used for training"/>
<node id="high throughput close to the upper bound"/>
<node id="the performance of PipeSwitch with experiments on a variety of DNN models and GPU cards"/>
<node id="the agility of DL applications"/>
<node id="introducing pipelined context switching"/>
<node id="the key idea"/>
<node id="the layered structure of neural network models and their layer-by-layer computation pattern"/>
<node id="model transmission over the PCIe and task execution in the GPU with model-aware grouping"/>
<node id="WE"/>
<node id="a pipelined model transmission mechanism"/>
<node id="model transmission over the PCIe"/>
<node id="model computation in the GPU"/>
<node id="transmitting a task from CPU to GPU"/>
<node id="the PCIe bandwidth"/>
<node id="unied memory management and active-standby worker switching mechanisms"/>
<node id="the pipelining"/>
<node id="process-level isolation"/>
<node id="we"/>
<node id="an active-standby mechanism"/>
<node id="active-standby mechanism"/>
<node id="fast worker switching"/>
<node id="a PipeSwitch prototype"/>
<node id="it with PyTorch"/>
<node id="a system prototype for PipeSwitch"/>
<node id="The system prototype for PipeSwitch"/>
<node id="3600 lines of code"/>
<node id="The 3600 lines of code"/>
<node id="C and Python"/>
<node id="PyTorch 21"/>
<node id="Deep Learning (DL)"/>
<node id="an emerging family of intelligent applications"/>
<node id="intelligent applications"/>
<node id="many domains"/>
<node id="retail"/>
<node id="transportation"/>
<node id="finance"/>
<node id="healthcare"/>
<node id="GPUS"/>
<node id="one of the most widely-used classes of accelerators for DL"/>
<node id="DL workloads"/>
<node id="Inference tasks"/>
<node id="training clusters under ash crowds"/>
<node id="Training tasks"/>
<node id="inference clusters when the inference load is low"/>
<node id="training cluster"/>
<node id="training tasks for inference tasks"/>
<node id="Inference clusters"/>
<node id="the peak load"/>
<node id="strict service level objectives (SLOs)"/>
<node id="production systems"/>
<node id="each application on per-GPU granularity"/>
<node id="provisioning on per-GPU granularity"/>
<node id="the interference between applications"/>
<node id="GPUs to applications on per-GPU granularity"/>
<node id="binding GPUs"/>
<node id="the VMs, containers or processes of an application"/>
<node id="in order to limit the interference between different applications"/>
<node id="in order to satisfy the SLO requirements"/>
<node id="multiple DL applications"/>
<node id="the same GPU server"/>
<node id="packing multiple DL applications to the same GPU server"/>
<node id="GPU utilization via time-sharing"/>
<node id="Operating systems"/>
<node id="high CPU utilization via task scheduling and context switching"/>
<node id="THE IDEA OF NE-GRAINED CPU TIME-SHARING"/>
<node id="CLUSTER SCHEDULING"/>
<node id="NE-GRAINED TIME-SHARING"/>
<node id="better utilization than provisioning dedicated resources"/>
<node id="necessary process-level isolation"/>
<node id="CPU workloads"/>
<node id="NE-GRAINED SCHEDULING CYCLES"/>
<node id="enabled"/>
<node id="Google Borg"/>
<node id="online services and batch jobs"/>
<node id="20-30 machines"/>
<node id="not packing them"/>
<node id="GPU"/>
<node id="high overhead when switching between tasks"/>
<node id="THE GAP"/>
<node id="THE PRECIOUS GPU MEMORY AND SLOW SWITCHING"/>
<node id="Naively using GPUs in the same way as CPUs"/>
<node id="the requirements of DL inference that have strict SLOs in the range of tens to hundreds of milliseconds"/>
<node id="A GPU"/>
<node id="a DNN model (e.g., ResNet)"/>
<node id="The DNN model (e.g., ResNet)"/>
<node id="the GPU"/>
<node id="State-of-the-art tricks like CUDA unified memory"/>
<node id="multiple seconds delay"/>
<node id="CPU applications"/>
<node id="milliseconds or even microseconds"/>
<node id="the existing solution"/>
<node id="spatially share the GPU memory"/>
<node id="this approach"/>
<node id="strong GPU memory isolation between applications"/>
<node id="NVIDIA Multiple Process Sharing (MPS) 6"/>
<node id="multiple processes to use the same GPU"/>
<node id="Salus 7"/>
<node id="all processes data (e.g., DNN models) to be preloaded into the GPU memory"/>
<node id="multi-process support from NVIDIA"/>
<node id="the inference process to share the GPU with the training process"/>
<node id="NVIDIA MPS 6"/>
<node id="official support for sharing a GPU between multiple processes"/>
<node id="GPU memory"/>
<node id="much more limited than host memory"/>
<node id="preload many applications"/>
<node id="one single memory-intensive training task"/>
<node id="all the GPU memory"/>
<node id="THE TRAINING TASK"/>
<node id="ITS GPU ENVIRONMENT"/>
<node id="the GPU MEMORY"/>
<node id="THE ENTIRE GPU MEMORY"/>
<node id="WHEN INFERENCE TASKS COME"/>
<node id="models"/>
<node id="larger"/>
<node id="request batching"/>
<node id="to increase throughput"/>
<node id="Request batching"/>
<node id="the GPU memory requirement of inference applications"/>
<node id="A context switching design"/>
<node id="the switching overhead"/>
<node id="the contents on GPU memory"/>
<node id="a better approach for efficiently time-sharing GPUs"/>
<node id="no existing solution"/>
<node id="such context switching abstraction for GPU"/>
<node id="a new technology called pipelined context switching"/>
<node id="pipelined context switching"/>
<node id="the characteristics of DL applications"/>
<node id="millisecond-scale overhead for switching tasks on GPUs"/>
<node id="a major challenge fast GPU context switching between different processes"/>
<node id="pipeline"/>
<node id="computation and GPU memory swapping"/>
<node id="fast context switching"/>
<node id="context switching"/>
<node id="if the application is already loaded in the GPU"/>
<node id="task switching overhead on GPUs for DL applications"/>
<node id="DNN models"/>
<node id="host memory"/>
<node id="much larger and cheaper than GPU memory"/>
<node id="the models either for training or inference"/>
<node id="Enterprises"/>
<node id="privately shared by multiple users"/>
<node id="publicly shared by multiple users"/>
<node id="M. Jeon, S. Venkataraman, A. Phanishayee, J. Qian, W. Xiao, and F. Yang"/>
<node id="Analysis of large-scale multi-tenant GPU clusters for DNN training workloads"/>
<node id="USENIX ATC, 2019"/>
<node id="512 14th USENIX Symposium on Operating Systems Design and Implementation"/>
<node id="USENIX Association"/>
<node id="the number of applications that can be multiplexed"/>
<node id="the GPU memory size"/>
<node id="each application"/>
<node id="the entire GPU compute and memory resources during its time slice"/>
<node id="GPU-efficient multiplexing of many DL applications on GPU servers via fine-grained time-sharing"/>
<node id="millisecond-scale latencies and high throughput as dedicated servers"/>
<node id="GPU-efficient fine-grained time-sharing for multiple DL applications"/>
<node id="millisecond-scale context switching latencies"/>
<node id="high throughput"/>
<node id="all the ideas into our system"/>
<node id="the gap of GPU memory sharing and switching"/>
<node id="the design of an efficient time-sharing GPU cluster for DL workloads"/>
<node id="GPU-efficient multiplexing of multiple DL applications on GPU servers"/>
<node id="millisecond-scale task switching time"/>
<node id="DL applications on time-sharing GPUs to meet strict SLOs"/>
<node id="small switching overhead"/>
<node id="DL applications to satisfy strict SLO requirements"/>
<node id="millisecond-scale task switching overhead"/>
<node id="SLO requirements"/>
<node id="a measurement study"/>
<node id="measurement study"/>
<node id="the task switching overhead"/>
<node id="the overhead of each component"/>
<node id="the measurement study"/>
<node id="probe the task switching overhead"/>
<node id="switching overhead"/>
<node id="four components"/>
<node id="old task cleaning"/>
<node id="new task initialization"/>
<node id="GPU memory allocation"/>
<node id="model transmission via PCIe from CPU to GPU"/>
<node id="INSTANCE TYPE"/>
<node id="G4DN.2XLARGE"/>
<node id="P3.2XLARGE"/>
<node id="GPU TYPE of G4DN.2XLARGE"/>
<node id="NVIDIA T4"/>
<node id="GPU TYPE of P3.2XLARGE"/>
<node id="NVIDIA V100"/>
<node id="TASK CLEANING time on G4DN.2XLARGE"/>
<node id="155 MS"/>
<node id="TASK CLEANING time on P3.2XLARGE"/>
<node id="165 MS"/>
<node id="TASK INITIALIZATION time on G4DN.2XLARGE"/>
<node id="5530 MS"/>
<node id="TASK INITIALIZATION time on P3.2XLARGE"/>
<node id="7290 MS"/>
<node id="MEMORY ALLOCATION time on G4DN.2XLARGE"/>
<node id="10 MS"/>
<node id="MEMORY ALLOCATION time on P3.2XLARGE"/>
<node id="13 MS"/>
<node id="MODEL TRANSMISSION time on G4DN.2XLARGE"/>
<node id="91 MS"/>
<node id="MODEL TRANSMISSION time on P3.2XLARGE"/>
<node id="81 MS"/>
<node id="TOTAL OVERHEAD on G4DN.2XLARGE"/>
<node id="5787 MS"/>
<node id="TOTAL OVERHEAD on P3.2XLARGE"/>
<node id="7551 MS"/>
<node id="INFERENCE TIME on G4DN.2XLARGE"/>
<node id="105 MS"/>
<node id="INFERENCE TIME on P3.2XLARGE"/>
<node id="32 MS"/>
<node id="every component"/>
<node id="a considerable amount of time"/>
<node id="time"/>
<node id="tens of milliseconds to seconds"/>
<node id="inference task"/>
<node id="tens of milliseconds on a GPU"/>
<node id="latency SLOs"/>
<node id="a small multiple of the inference time"/>
<node id="one source of the overhead"/>
<node id="the contentions both on the computation and memory of the GPU"/>
<node id="the training task"/>
<node id="when an inference task comes"/>
<node id="a holistic approach"/>
<node id="the overhead of all the components"/>
<node id="a layered structure"/>
<node id="a layer-by-layer computation pattern"/>
<node id="Our design"/>
<node id="a key observation"/>
<node id="usually deep"/>
<node id="multiple layers stacking one on another"/>
<node id="computation of DNN models"/>
<node id="layer by layer"/>
<node id="there"/>
<node id="the entire model to be transmitted to the GPU before starting computation"/>
<node id="A task"/>
<node id="the entire model to be transmitted to the GPU before beginning the computation"/>
<node id="Naive pipelining on per-layer granularity"/>
<node id="high overhead on tensor transmission and synchronization"/>
<node id="Pipelining on per-layer granularity"/>
<node id="synchronization for every layer"/>
<node id="layers into groups"/>
<node id="an optimal model-aware grouping algorithm"/>
<node id="algorithm"/>
<node id="the best grouping strategy for a given model"/>
<node id="an algorithm to find the optimal grouping strategy for a given model"/>
<node id="The computation of a DL task"/>
<node id="a simple, regular pattern for memory allocation"/>
<node id="A DL task"/>
<node id="two important types of data in the GPU memory"/>
<node id="two important types of data"/>
<node id="the DNN model (including the model parameters) and the intermediate results"/>
<node id="The default general-purpose GPU memory management (e.g., CUDA Unified Memory 4)"/>
<node id="an overkill"/>
<node id="unnecessary overhead"/>
<node id="NVIDIA"/>
<node id="CUDA Unified Memory 4"/>
<node id="memory movement between the host memory and the GPU memory"/>
<node id="memory movement"/>
<node id="applications"/>
<node id="unified memory management with a dedicated memory daemon"/>
<node id="dedicated memory daemon"/>
<node id="the overhead"/>
<node id="unified memory management with the memory daemon"/>
<node id="minimal memory footprint"/>
<node id="extra memory copies"/>
<node id="THE DAEMON"/>
<node id="THE GPU MEMORY"/>
<node id="IT TO EACH TASK"/>
<node id="THE EXPENSIVE GPU MEMORY MANAGER"/>
<node id="the memory daemon"/>
<node id="cudamalloc to obtain the GPU memory when the system starts"/>
<node id="the memory to the workers at runtime"/>
<node id="the GPU memory manager"/>
<node id="the existing system"/>
<node id="minimal changes"/>
<node id="only once in the memory daemon"/>
<node id="in every worker"/>
<node id="storing DNN models only once in the memory daemon"/>
<node id="memory footprint"/>
<node id="memory allocation for a DNN model"/>
<node id="deterministic"/>
<node id="that the memory allocation for a DNN model is deterministic"/>
<node id="deterministic memory allocation"/>
<node id="extra memory copies between the daemon and the workers"/>
<node id="the IPC overhead"/>
<node id="No unified memory management"/>
<node id="each worker to keep a copy for each DNN model"/>
<node id="Keeping a copy for each DNN model"/>
<node id="the memory footprint"/>
<node id="each server"/>
<node id="an active worker"/>
<node id="multiple standby workers"/>
<node id="A SERVER"/>
<node id="ONE OR MORE STANDBY WORKERS"/>
<node id="THE ACTIVE WORKER"/>
<node id="THE CURRENT TASK ON THE GPU"/>
<node id="THE STANDBY WORKERS"/>
<node id="ON THE CPU"/>
<node id="THE NEXT TASK"/>
<node id="THE WORKER THAT CURRENTLY EXECUTES A TASK IN THE GPU"/>
<node id="worker"/>
<node id="a process that executes tasks on one GPU"/>
<node id="the active worker"/>
<node id="the current task"/>
<node id="the controller"/>
<node id="the standby worker"/>
<node id="the memory daemon and the standby worker"/>
<node id="the task to GPU"/>
<node id="the task"/>
<node id="pipelined model transmission (4.2)"/>
<node id="OUR MECHANISM"/>
<node id="OLD TASK CLEANING IN THE ACTIVE WORKER AND NEW TASK INITIALIZATION IN THE STANDBY WORKER"/>
<node id="WORKER SWITCHING OVERHEAD"/>
<node id="Table 2"/>
<node id="worker switching mechanisms"/>
<node id="an active and standby worker switching mechanism"/>
<node id="active and standby worker switching mechanism"/>
<node id="the overhead of both task cleaning and task initialization"/>
<node id="us to address new technical challenges on memory management and worker switching across different processes"/>
<node id="fast task switching"/>
<node id="all other components of PipeSwitch the same"/>
<node id="the following mechanisms discussed in 4.4"/>
<node id="the active-standby worker switching mechanism used by PIPESWITCH"/>
<node id="Pipelining"/>
<node id="a canonical technique"/>
<node id="computer systems"/>
<node id="system performance"/>
<node id="resource utilization"/>
<node id="PIPELINING"/>
<node id="TWO SOURCES OF SYSTEM OVERHEADS"/>
<node id="Prior work in DL systems such as Pipedream 8 and Bytescheduler 9"/>
<node id="pipelining to distributed training"/>
<node id="These solutions"/>
<node id="inter-batch pipelining"/>
<node id="Inter-batch pipelining"/>
<node id="computation and gradient transmission of different batches"/>
<node id="Computation and gradient transmission"/>
<node id="training workloads of the same DNN model"/>
<node id="intra-batch pipelining"/>
<node id="model transmission and computation"/>
<node id="overlapping model transmission and computation"/>
<node id="the overhead of switching between different DNN models"/>
<node id="different DNN models"/>
<node id="either inference or training"/>
<node id="new techniques"/>
<node id="training"/>
<node id="inference that has strict SLOs"/>
<node id="pipelined model transmission"/>
<node id="unified memory management"/>
<node id="active-standby worker switching"/>
<node id="Pipelined context switching"/>
<node id="three key techniques"/>
<node id="a system prototype"/>
<node id="the inefficiencies in today's shared GPU clusters"/>
<node id="running DL workloads on GPUs in the fine-grained time-sharing model"/>
<node id="to pack multiple DL applications onto the same GPU via NE-grained time-sharing abstraction to maximize GPU utilization"/>
<node id="more flexible fine-grained scheduling"/>
<node id="GPU utilization for dynamic workloads"/>
<node id="dedicated physical forms and power supplies"/>
<node id="high speed networks"/>
<node id="specialized task schedulers"/>
<node id="500 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="USENIX ASSOCIATION"/>
<node id="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="shared cluster"/>
<node id="dedicated cluster for each user"/>
<node id="14th USENIX Symposium on Operating Systems Design and Implementation"/>
<node id="Kubernetes"/>
<node id="514"/>
<node id="the main reason"/>
<node id="to bring down the cost"/>
<node id="The demand of training"/>
<node id="well predictable"/>
<node id="the progress of different developers"/>
<node id="an inference task for a particular application"/>
<node id="a daily periodical pattern based on the application usage"/>
<node id="patterns"/>
<node id="across different tasks"/>
<node id="A shared cluster"/>
<node id="the resource utilization via time-sharing"/>
<node id="inference"/>
<node id="shared clusters"/>
<node id="high utilization"/>
<node id="GPUs designed for inference tasks"/>
<node id="too wimpy for training tasks"/>
<node id="The arrival of new GPU hardware"/>
<node id="this"/>
<node id="up to 32GB GPU memory"/>
<node id="15.7 TFLOPS (single-precision)"/>
<node id="comparable performance with NVIDIA V100"/>
<node id="16GB GPU memory"/>
<node id="8.1 TFLOPS (single-precision)"/>
<node id="new algorithms and systems for distributed training"/>
<node id="multiple GPUs to accelerate training"/>
<node id="OUR INDUSTRY COLLABORATOR"/>
<node id="A LEADING ONLINE SERVICE PROVIDER"/>
<node id="THIS OBSERVATION"/>
<node id="THIS SERVICE PROVIDER"/>
<node id="more than 10K V100 GPUs for training"/>
<node id="at least 5 as many T4 GPUs for inference"/>
<node id="computation power on both sides"/>
<node id="the same order of magnitude"/>
<node id="inference workload"/>
<node id="number of active users"/>
<node id="clear peaks and valleys within each day"/>
<node id="peak demand during daytime"/>
<node id="2 times the valley at midnight"/>
<node id="inference GPUs"/>
<node id="during less busy times"/>
<node id="training models"/>
<node id="daily updates with latest data"/>
<node id="A good example"/>
<node id="ne-tune BERT using daily news"/>
<node id="BORG-LIKE 1 SYSTEMS FOR GPUS"/>
<node id="great opportunity in improving GPU utilization"/>
<node id="Inference and training workloads"/>
<node id="complementary usage patterns"/>
<node id="Inference loads on different models"/>
<node id="different patterns"/>
<node id="Different patterns"/>
<node id="time sharing"/>
<node id="any server"/>
<node id="any task"/>
<node id="low overhead to switch between different applications"/>
<node id="A modern server"/>
<node id="several TB of host memory"/>
<node id="Several TB of host memory"/>
<node id="it to load many applications"/>
<node id="Task execution on GPUs"/>
<node id="very limited even on high-end GPUs"/>
<node id="T4 GPU"/>
<node id="16 GB GPU memory"/>
<node id="V100 GPU"/>
<node id="32 GB GPU memory"/>
<node id="task execution"/>
<node id="storing the state of idle applications"/>
<node id="DL tasks, especially training"/>
<node id="a large amount, or even all of the memory on a GPU"/>
<node id="DL applications"/>
<node id="large models"/>
<node id="large amounts of intermediate results"/>
<node id="a lot of GPU memory"/>
<node id="SALUS 7"/>
<node id="training tasks which are memory-intensive"/>
<node id="multiple inference tasks which have large models"/>
<node id="state-of-the-art models"/>
<node id="deeper and larger"/>
<node id="idle applications"/>
<node id="large memory space"/>
<node id="the active application"/>
<node id="the entire GPU memory for its purpose"/>
<node id="the number of applications that can be served by a GPU server"/>
<node id="its host memory size"/>
<node id="switching a task"/>
<node id="heavy memory swapping"/>
<node id="many online inference workloads"/>
<node id="strict SLOs"/>
<node id="naive memory swapping between the host memory and the GPU memory"/>
<node id="the strawman scenario"/>
<node id="a training task"/>
<node id="an inference task"/>
<node id="THE RST INFERENCE BATCH"/>
<node id="several seconds to finish"/>
<node id="Existing support such as NVIDIA MPS"/>
<node id="hundreds of milliseconds overhead"/>
<node id="NVIDIA MPS"/>
<node id="stop-and-start"/>
<node id="several hundred milliseconds overhead"/>
<node id="MPS from meeting strict SLOs"/>
<node id="Figure 1"/>
<node id="PipeSwitch architecture"/>
<node id="throughput measurements"/>
<node id="batches per second"/>
<node id="throughput"/>
<node id="PIPESWITCH MPS STOP-AND-START"/>
<node id="eight P3.2xlarge instances"/>
<node id="training and inference tasks"/>
<node id="fast switching across tasks"/>
<node id="well-defined structures"/>
<node id="the structure and computation pattern of DNN models"/>
<node id="us to highly optimize task switching"/>
<node id="us to achieve millisecond-scale overhead"/>
<node id="other challenges like memory management and worker switching"/>
<node id="FIGURE 1"/>
<node id="the architecture of a PipeSwitch server"/>
<node id="PIPESWITCH PIPELINES"/>
<node id="transmission and task execution"/>
<node id="THIS SERVER"/>
<node id="four types of components"/>
<node id="a controller"/>
<node id="a memory daemon"/>
<node id="THE CONTROLLER"/>
<node id="THE CENTRAL COMPONENT"/>
<node id="the memory daemon and the workers"/>
<node id="the tasks"/>
<node id="MEMORY DAEMON"/>
<node id="a daemon"/>
<node id="THE MEMORY DAEMON"/>
<node id="THE DNN MODELS"/>
<node id="THE SERVER"/>
<node id="THE DNN MODELS IN THE HOST MEMORY"/>
<node id="all components"/>
<node id="the SLOs"/>
<node id="A STANDBY WORKER"/>
<node id="idle"/>
<node id="initializing a new task"/>
<node id="cleaning its environment for the previous task"/>
<node id="The standby worker"/>
<node id="the new active worker"/>
<node id="The new active worker"/>
<node id="the new task"/>
<node id="The active worker"/>
<node id="a standby worker"/>
<node id="the environment for the previous task"/>
<node id="a new task"/>
<node id="a standby worker finishes cleaning a previous task"/>
<node id="wait"/>
<node id="waiting"/>
<node id="its startup time"/>
<node id="A SET OF TASKS RECEIVED FROM THE CLIENTS"/>
<node id="a scheduling policy"/>
<node id="which task to execute next"/>
<node id="The scheduling"/>
<node id="preemptive"/>
<node id="The controller"/>
<node id="the current task for the next one"/>
<node id="the scheduling policy"/>
<node id="canonical scheduling policies"/>
<node id="RST COME RST SERVE (FCFS)"/>
<node id="earliest deadline RST (EDF)"/>
<node id="The specific scheduling algorithm"/>
<node id="orthogonal to this paper"/>
<node id="a strict latency SLO"/>
<node id="the current task to finish if it is inference"/>
<node id="the current task by notifying the active worker to stop if it is training"/>
<node id="AN IDLE STANDBY WORKER"/>
<node id="ITS ENVIRONMENT FOR THE NEW TASK"/>
<node id="a task"/>
<node id="THE MEMORY TO THE STANDBY WORKER (4.3)"/>
<node id="THE MODEL USED BY THE NEW TASK FROM THE HOST MEMORY TO THE GPU MEMORY"/>
<node id="The memory daemon"/>
<node id="the model from the host memory to the GPU memory"/>
<node id="Transmitting the model from the host memory to the GPU memory"/>
<node id="the extra memory copy from the memory daemon to the worker"/>
<node id="the worker"/>
<node id="the relevant GPU memory handlers to the worker"/>
<node id="the model"/>
<node id="its task"/>
<node id="GPU MEMORY ALLOCATION"/>
<node id="MODEL TRANSMISSION"/>
<node id="GPU MEMORY HANDLERS"/>
<node id="GPU MEMORY HANDLERS TO WORKERS"/>
<node id="THE PRIMARY GOAL OF THIS PAPER"/>
<node id="a set of techniques based on the characteristics of DL applications to minimize the task switching overhead in this process"/>
<node id="task switching overhead"/>
<node id="individual components"/>
<node id="end-to-end experiments"/>
<node id="the benefits of PipeSwitch"/>
<node id="the effectiveness of the design choices on each component"/>
<node id="our design"/>
<node id="A server"/>
<node id="a training task running on the GPU"/>
<node id="The DNN model used in the measurement"/>
<node id="ResNet152 17"/>
<node id="THE MEASUREMENT"/>
<node id="two types of instances on Amazon AWS"/>
<node id="G4dn.2xlarge with NVIDIA T4"/>
<node id="P3.2xlarge with NVIDIA V100"/>
<node id="the inference task"/>
<node id="the server"/>
<node id="the time to start and execute it on the GPU"/>
<node id="the network time"/>
<node id="the task queueing time"/>
<node id="total times to start the inference task on the GPUs"/>
<node id="5787 ms and 7551 ms, respectively"/>
<node id="the overhead into the four components"/>
<node id="TASK CLEANING"/>
<node id="THE INFERENCE TASK"/>
<node id="its environment"/>
<node id="process launching"/>
<node id="PyTorch CUDA runtime loading"/>
<node id="CUDA context initialization"/>
<node id="GPU MEMORY FOR ITS NEURAL NETWORK MODEL"/>
<node id="THE MODEL FROM THE HOST MEMORY TO THE GPU MEMORY"/>
<node id="inference time on V100"/>
<node id="inference time on T4"/>
<node id="inference time on V100 and inference time on T4"/>
<node id="total overheads"/>
<node id="Lower overhead on T4"/>
<node id="task switching largely depends on CPU"/>
<node id="G4dn.2xlarge"/>
<node id="better CPU than P3.2xlarge"/>
<node id="Better CPU on G4dn.2xlarge"/>
<node id="Intel Platinum 8259CL"/>
<node id="CPU on P3.2xlarge"/>
<node id="Intel Xeon E5-2686 v4"/>
<node id="A strawman solution"/>
<node id="the old task"/>
<node id="A strawman solution that simply stops the old task and starts the new task"/>
<node id="all the components"/>
<node id="considerable time compared to the inference time"/>
<node id="all the components should be optimized to achieve minimal switching overhead and meet the SLOs"/>
<node id="PCIE bandwidth"/>
<node id="the physical limit on how fast an arbitrary task can be loaded to the GPU"/>
<node id="to circumvent this physical limit"/>
<node id="The computation"/>
<node id="a forward pass from the RST layer to the NAL layer"/>
<node id="a forward pass to make a prediction"/>
<node id="each iteration in a training task"/>
<node id="a forward pass"/>
<node id="a backward pass"/>
<node id="the computation of a layer as soon as the layer is loaded in the GPU and the input of the layer is ready"/>
<node id="the input of the layer"/>
<node id="the previous layers have finished their computation"/>
<node id="regardless of its following layers"/>
<node id="FIGURE 2"/>
<node id="the advantage of pipelining over the strawman solution"/>
<node id="the knowledge of models"/>
<node id="pipelining mechanism"/>
<node id="optimal model-aware grouping in PipeSwitch"/>
<node id="model-aware grouping"/>
<node id="the best trade-off between pipeline overhead and efficiency"/>
<node id="MODEL"/>
<node id="PCIE"/>
<node id="TASK"/>
<node id="PCIE GPU E0 E1 EN-1 E2 (B)"/>
<node id="model transmission and task execution"/>
<node id="THE EXAMPLE"/>
<node id="only a forward pass in task execution"/>
<node id="ADDING HOOKS"/>
<node id="automated"/>
<node id="a part of the DNN framework"/>
<node id="DNN framework"/>
<node id="PYTORCH"/>
<node id="the model structure information"/>
<node id="transparent to users and cluster managers"/>
<node id="the basic way for pipelining"/>
<node id="to pipeline on per-layer granularity"/>
<node id="the system"/>
<node id="the layers to the GPU memory one by one"/>
<node id="the computation for a layer"/>
<node id="before the layer is transmitted"/>
<node id="ONE"/>
<node id="the overhead to invoke multiple calls to PCIe to transmit the data"/>
<node id="transmission overhead"/>
<node id="data size"/>
<node id="dividing the model into many layers and invoking a PCIe call for each layer"/>
<node id="significant extra overhead"/>
<node id="the other"/>
<node id="the synchronization overhead between transmission and computation"/>
<node id="the synchronization overhead"/>
<node id="the computation to know when a layer is ready to compute"/>
<node id="grouping"/>
<node id="these two sources of overhead"/>
<node id="multiple layers into a group"/>
<node id="per-group granularity"/>
<node id="pipelining overhead"/>
<node id="once for each group"/>
<node id="instead of each layer"/>
<node id="GROUPING"/>
<node id="a trade-off between pipelining efficiency and pipelining overhead"/>
<node id="using small groups"/>
<node id="more overlap between transmission and computation"/>
<node id="pipelining efficiency"/>
<node id="more pipelining overhead"/>
<node id="using big groups"/>
<node id="minimal pipelining overhead"/>
<node id="the chance for overlapping"/>
<node id="model-aware"/>
<node id="different structures"/>
<node id="the number of layers"/>
<node id="the size of each layer"/>
<node id="all possible combinations"/>
<node id="the optimal grouping strategy"/>
<node id="two pruning techniques"/>
<node id="two insights"/>
<node id="hundreds of layers"/>
<node id="time complexity for enumeration"/>
<node id="exponential"/>
<node id="PCIE GPU"/>
<node id="LOWER BOUND OF F(GROUP(0, I), I1)"/>
<node id="GROUP(0, I)"/>
<node id="GROUP(I1, J)"/>
<node id="J, J1"/>
<node id="N-1"/>
<node id="Case (A)"/>
<node id="LOWER BOUND CURRENT OPTIMAL TIME"/>
<node id="cases"/>
<node id="I to J"/>
<node id="batch"/>
<node id="I1 to J"/>
<node id="Figure 3"/>
<node id="GROUP(I1, N-1)"/>
<node id="the cases that group from layer (I 1) to J J"/>
<node id="J J algorithm"/>
<node id="N"/>
<node id="F(B,I)"/>
<node id="a function"/>
<node id="the total time of the optimal grouping strategy from layer I to N-1"/>
<node id="layers 0 to I-1"/>
<node id="groups represented by B"/>
<node id="the function"/>
<node id="the optimal groups from layer I1 to N-1"/>
<node id="optgroups"/>
<node id="the current strategy is better"/>
<node id="the group"/>
<node id="from layer X to I"/>
<node id="all possible combinations into N cases"/>
<node id="The first group"/>
<node id="layer 0 to I"/>
<node id="Case I"/>
<node id="the first group contains layer 0 to I"/>
<node id="The optimal grouping strategy"/>
<node id="the entire model"/>
<node id="THIS FORMULA"/>
<node id="F(GROUP(0,I),I1)"/>
<node id="The RST group"/>
<node id="too many layers"/>
<node id="The computation of the RST group"/>
<node id="too much"/>
<node id="The delay"/>
<node id="compensate the pipeline efficiency"/>
<node id="multiple layers in a group based on the progress of computation"/>
<node id="packing multiple layers in a group based on the progress of computation"/>
<node id="pipeline efficiency"/>
<node id="other than the RST group"/>
<node id="T(I, J)"/>
<node id="the transmission time for a group from layer I to J"/>
<node id="E(I, J)"/>
<node id="the execution time for a group from layer I to J"/>
<node id="the size of layer I to J and PCIe bandwidth"/>
<node id="the overhead of invoking multiple calls"/>
<node id="a lower bound for the total time for each case in Equation 1"/>
<node id="the lower bound"/>
<node id="the best case that all the remaining layers are combined in one group for transmission and computation"/>
<node id="the computation and communication"/>
<node id="perfectly overlapped"/>
<node id="its computation"/>
<node id="right after the computation of the rst group finishes"/>
<node id="RST group"/>
<node id="0 to I"/>
<node id="Equation 1"/>
<node id="enumerate the cases for the PCIE GPU B.delay"/>
<node id="Group"/>
<node id="X to J"/>
<node id="0, 1, ..., X-1"/>
<node id="Lower bound"/>
<node id="F(B Group(A,I), I1)"/>
<node id="Figure 4"/>
<node id="general case for the two pruning techniques"/>
<node id="the transmission of the second group into the computation of the RST group"/>
<node id="the transmission"/>
<node id="the computation of the RST group"/>
<node id="the least number of layers to group"/>
<node id="the following equation"/>
<node id="JIS"/>
<node id="grouping from (I1) to J"/>
<node id="higher pipeline overhead"/>
<node id="this algorithm"/>
<node id="offline to find the strategy"/>
<node id="the resulting strategy"/>
<node id="PipeSwitch for context switching"/>
<node id="ALGORITHM 1"/>
<node id="the pseudo code"/>
<node id="the recursive function FINDOPTGROUPING(B,X)"/>
<node id="B"/>
<node id="the groups that have already formed"/>
<node id="X"/>
<node id="the rst layer that have not formed a group"/>
<node id="all layers from X to N1"/>
<node id="the best grouping strategy from layer X given B"/>
<node id="none (line 2)"/>
<node id="THE ALGORITHM"/>
<node id="THE SECOND PRUNING INSIGHT"/>
<node id="THE RST GROUP FROM LAYER X (LINE 3-9)"/>
<node id="THE PROBLEM INTO K 1 CASES"/>
<node id="CASE I"/>
<node id="THE RST GROUP FROM LAYER X TO XI"/>
<node id="I"/>
<node id="0 TO K"/>
<node id="EQUATION 3 AND FIGURE 3(B)"/>
<node id="this insight with a special example"/>
<node id="one group from layer 0 to I"/>
<node id="multiple groups formed by previous layers"/>
<node id="B.DELAY to denote the time to which the group can be formed"/>
<node id="B.DELAY (LINE 4-9)"/>
<node id="THE ENUMERATION FOR I"/>
<node id="the layers from X to J-1 (LINE 11)"/>
<node id="THE RST INSIGHT"/>
<node id="THE LOWER BOUND"/>
<node id="The example in Equation 2 and Figure 3(A)"/>
<node id="a special case when X is 0"/>
<node id="the computation from X"/>
<node id="both its transmission (i.e., T(X,I)) and the computation of the previous groups (i.e., B.DELAY)"/>
<node id="the current optimal time"/>
<node id="case I"/>
<node id="line 18-19"/>
<node id="a heuristic that bootstraps optgroups"/>
<node id="Algorithm 1"/>
<node id="Optimal Model-Aware Grouping"/>
<node id="Function FINDOPTGROUPING"/>
<node id="B, X"/>
<node id="OPTGROUPS"/>
<node id="0"/>
<node id="OPTGROUPS.TIME"/>
<node id="Layer I"/>
<node id="X to N1"/>
<node id="If T(X,I)"/>
<node id="B.DELAY"/>
<node id="J"/>
<node id="Else"/>
<node id="Break"/>
<node id="min(TRANSTIME, EXECTIME)"/>
<node id="TRANSTIME"/>
<node id="T(X,I) + T(I+1,N1)"/>
<node id="EXECTIME"/>
<node id="max(T(X,I), B.DELAY)"/>
<node id="If LOWERBOUND"/>
<node id="FirstGroup"/>
<node id="GROUP(X,I)"/>
<node id="RestGroups"/>
<node id="FINDOPTGROUPING(B, FirstGroup, I+1)"/>
<node id="CurGroups"/>
<node id="FirstGroup + RestGroups"/>
<node id="If CURGROUPS.TIME"/>
<node id="CURGROUPS"/>
<node id="Good strategy"/>
<node id="group every ten layers"/>
<node id="The two pruning techniques"/>
<node id="most of the strategies"/>
<node id="TWO PRUNING TECHNIQUES"/>
<node id="ALGORITHM 1 NDS"/>
<node id="the total time for the pipeline"/>
<node id="M N X"/>
<node id="the number of layers the function considers"/>
<node id="induction on M"/>
<node id="FINDOPTGROUPING(B,X)"/>
<node id="the optimal grouping strategy from layer X to N 1"/>
<node id="previous layers"/>
<node id="the optimal strategy"/>
<node id="FINDOPTGROUPING(B GROUP(X,X I),X I 1)"/>
<node id="K I K layers"/>
<node id="the optimal grouping strategy for case I"/>
<node id="the assumption"/>
<node id="THE FUNCTION"/>
<node id="ONE LAYER"/>
<node id="M"/>
<node id="1"/>
<node id="Layer X"/>
<node id="one group"/>
<node id="This strategy"/>
<node id="the algorithm"/>
<node id="K1 layers"/>
<node id="the optimal strategy for this case"/>
<node id="these cases"/>
<node id="exclusive"/>
<node id="the entire search space"/>
<node id="the optimal grouping strategy for M K 1"/>
<node id="the optimal grouping strategy from these cases"/>
<node id="THE RST TECHNIQUE"/>
<node id="the cases"/>
<node id="their lower bounds"/>
<node id="the current found optimal"/>
<node id="this technique"/>
<node id="the optimality"/>
<node id="THE SECOND TECHNIQUE"/>
<node id="THE CASE"/>
<node id="THEIR RST GROUPS"/>
<node id="LAYER X TO J J"/>
<node id="the computation to an earlier point than grouping from X to at least J"/>
<node id="pruning these cases"/>
<node id="optimality for a given list of layers"/>
<node id="layers or operators in a DNN model"/>
<node id="an arbitrary computation graph"/>
<node id="Models like ResNet and Inception"/>
<node id="technically non-linear directed acyclic graphs (DAGs)"/>
<node id="execution order"/>
<node id="that the layers/operators in the DAG are issued to the GPU one by one"/>
<node id="grouping the layers"/>
<node id="high pipelining efficiency and low pipelining overhead"/>
<node id="the order"/>
<node id="the rst time an operator is executed"/>
<node id="an operator"/>
<node id="it is transmitted to the GPU and the input is ready"/>
<node id="OUR PIPELINED MODEL TRANSMISSION"/>
<node id="THE GENERAL CASE"/>
<node id="keeping all other components of PipeSwitch the same and comparing mechanisms discussed in 4.2"/>
<node id="UNIFIED MEMORY MANAGEMENT TASK EXECUTION IN A GPU"/>
<node id="GPU MEMORY"/>
<node id="its own memory management system"/>
<node id="a malloc function"/>
<node id="malloc function"/>
<node id="CPUs for memory allocation"/>
<node id="cudaMalloc for NVIDIA GPUs"/>
<node id="each task"/>
<node id="the native cudaMallocManaged function for GPU memory allocation"/>
<node id="model transmission to CUDA unified memory"/>
<node id="functions for allocating GPU memory"/>
<node id="functions for sharing the GPU memory to workers through CUDA IPC API"/>
<node id="functions for getting the shared GPU memory"/>
<node id="each worker"/>
<node id="cudaMalloc to allocate GPU memory"/>
<node id="the model to GPU by its own"/>
<node id="Each worker"/>
<node id="GPU memory with cudamallocmanaged"/>
<node id="CUDA"/>
<node id="the model to GPU when needed"/>
<node id="THIS SOLUTION"/>
<node id="HIGH OVERHEAD FOR DL APPLICATIONS"/>
<node id="THE NATIVE CUDAMALLOC FUNCTION AND CUDA UNIFIED MEMORY"/>
<node id="GENERAL-PURPOSE APPLICATIONS"/>
<node id="UNNECESSARY OVERHEAD FOR DL APPLICATIONS"/>
<node id="CUDA Unified Memory"/>
<node id="more than one hundred milliseconds overhead than PipeSwitch"/>
<node id="two characteristics of DL applications"/>
<node id="exploiting two characteristics of DL applications"/>
<node id="GPU memory management overhead"/>
<node id="THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT"/>
<node id="THESE CHARACTERISTICS"/>
<node id="too heavy-weight for DL applications that require fast task switching"/>
<node id="the amount of memory allocated to the DNN model"/>
<node id="fixed"/>
<node id="during task execution"/>
<node id="the model parameters"/>
<node id="the weights of the neural network"/>
<node id="the DNN structure"/>
<node id="the amount of memory needed to store the model parameters"/>
<node id="the same"/>
<node id="INFERENCE TASK"/>
<node id="the model for inference"/>
<node id="the model itself"/>
<node id="THE INTERMEDIATE RESULTS"/>
<node id="A SIMPLE, REGULAR PATTERN"/>
<node id="MEMORY FRAGMENTATION"/>
<node id="the intermediate results"/>
<node id="the outputs of each layer"/>
<node id="the next layer"/>
<node id="the backward pass"/>
<node id="reverse order"/>
<node id="the forward pass"/>
<node id="RST-IN-LAST-OUT"/>
<node id="memory allocation and release"/>
<node id="a simple stack-like mechanism"/>
<node id="memory fragmentation"/>
<node id="You"/>
<node id="a memory management mechanism tailored for DL applications"/>
<node id="a dedicated memory daemon"/>
<node id="the GPU memory"/>
<node id="a 64-bit integer offset for the shared GPU memory to workers"/>
<node id="223 ms"/>
<node id="eliminating the memory allocation overhead with the memory daemon"/>
<node id="compared to"/>
<node id="memory pointers to the workers"/>
<node id="passing memory pointers to the workers"/>
<node id="light-weight"/>
<node id="that each time only one worker owns the GPU memory"/>
<node id="one worker"/>
<node id="memory isolation"/>
<node id="between workers"/>
<node id="a memory pool to allocate the memory to store its model and intermediate results"/>
<node id="the memory to the pool after the intermediate results are no longer needed"/>
<node id="THE MEMORY MANAGEMENT OF PIPESWITCH"/>
<node id="THAT OF PYTORCH"/>
<node id="GPU MEMORY BLOCKS TO PYTORCH GPU MEMORY POOL"/>
<node id="TENSORS ON THEM"/>
<node id="The memory management in PyTorch"/>
<node id="memory allocation for a task itself"/>
<node id="replicating the models in each worker"/>
<node id="high memory footprint"/>
<node id="the number of models a server can store"/>
<node id="reducing the number of models a server can store"/>
<node id="the types of tasks the server can execute"/>
<node id="storing the models in a dedicate process"/>
<node id="each model"/>
<node id="only once"/>
<node id="an extra memory copy from this process to a worker to start a task"/>
<node id="the task switching time"/>
<node id="the models in the memory daemon"/>
<node id="one copy of each model in the host memory"/>
<node id="IPC overhead"/>
<node id="minimized"/>
<node id="a property of DL applications to minimize the IPC overhead"/>
<node id="IPC APIs"/>
<node id="CUDAIPCOPENMEMHANDLE"/>
<node id="NVIDIA GPUs"/>
<node id="these IPC APIs"/>
<node id="high overhead"/>
<node id="The overhead"/>
<node id="the pipeline"/>
<node id="The pipeline"/>
<node id="the IPCs frequently"/>
<node id="The frequent invocation of the IPCs"/>
<node id="synchronize model transmission and task execution for every pipeline group"/>
<node id="the IPC only once for the entire model transmission"/>
<node id="memory allocation process for a neural network model"/>
<node id="memory daemon and the worker"/>
<node id="the same order to allocate memory for the model parameters"/>
<node id="memory pointers for the parameters"/>
<node id="the same order to transmit the model"/>
<node id="the neural network model"/>
<node id="known and given"/>
<node id="the memory daemon and the worker"/>
<node id="the same order"/>
<node id="THE USAGE OF EXPENSIVE GPU IPCS"/>
<node id="latency"/>
<node id="no unified memory management without IPC optimization"/>
<node id="NO PIN"/>
<node id="MEMORY"/>
<node id="THE OS"/>
<node id="a memory page to disk"/>
<node id="the page"/>
<node id="inactive for a certain amount of time"/>
<node id="a page in the host memory to be pinned (or page-locked)"/>
<node id="a page in the host memory"/>
<node id="in order to transmit the data in the page to the GPU memory"/>
<node id="A temporary pinned page"/>
<node id="the pages of the memory daemon to the host memory"/>
<node id="Process-level isolation"/>
<node id="because it ensures that one task cannot read the memory of another task"/>
<node id="one task cannot read the memory of another task"/>
<node id="the crashing of one task does not affect other tasks or the entire system"/>
<node id="separate processes"/>
<node id="A naive solution"/>
<node id="the new task after the current task is stopped"/>
<node id="SEQUENTIAL EXECUTION"/>
<node id="LONG DELAY"/>
<node id="OLD TASK CLEANING AND NEW TASK INITIALIZATION"/>
<node id="another possible solution"/>
<node id="the current and new tasks share the same process with a warm CUDA context"/>
<node id="the GPU environment of the current task"/>
<node id="THE PROCESS OF THE OLD TASK"/>
<node id="THE GPU ENVIRONMENT"/>
<node id="ANOTHER PROCESS"/>
<node id="for THE NEW TASK"/>
<node id="THE PROCESS"/>
<node id="THE ENVIRONMENT FOR THE NEW TASK"/>
<node id="a separate process"/>
<node id="its own GPU environment (i.e., CUDA context)"/>
<node id="GPU environment (i.e., CUDA context)"/>
<node id="when it is rst created"/>
<node id="a major job"/>
<node id="asynchronous CUDA functions queued on the GPU"/>
<node id="synchronization points into training tasks"/>
<node id="the number of queued functions"/>
<node id="limited"/>
<node id="quickly cleared"/>
<node id="Synchronization points"/>
<node id="inference tasks"/>
<node id="short"/>
<node id="preempted"/>
<node id="another job"/>
<node id="free its GPU memory"/>
<node id="the cleaning procedure"/>
<node id="the content of the memory"/>
<node id="the metadata"/>
<node id="GPU memory pointers"/>
<node id="cleaning procedure"/>
<node id="pointers pointing to the tensor data"/>
<node id="the actual data"/>
<node id="its model to the GPU memory at the same time"/>
<node id="the task cleaning of the current task and the pipelined model transmission of the new task"/>
<node id="parallelizing the task cleaning and the pipelined model transmission"/>
<node id="hide the task cleaning overhead"/>
<node id="THIS CHOICE"/>
<node id="performance"/>
<node id="a trusted environment"/>
<node id="a latter process"/>
<node id="the memory data of a previous process"/>
<node id="an additional zero-out operation"/>
<node id="if this is a concern"/>
<node id="high memory bandwidth"/>
<node id="900GBS for V100"/>
<node id="zeroing-out most models like ResNet-152 (around 240MB)"/>
<node id="sub-millisecond overhead"/>
<node id="the current active worker to stop"/>
<node id="the GPU memory allocated to the current active worker"/>
<node id="the GPU memory to the new active worker"/>
<node id="the parameters of the new model to the GPU"/>
<node id="receiving the current active worker's reply"/>
<node id="only one active worker"/>
<node id="exclusive occupation of the GPU"/>
<node id="number of standby workers"/>
<node id="their GPU memory consumption"/>
<node id="every standby worker"/>
<node id="its own CUDA context"/>
<node id="CUDA context"/>
<node id="a few hundred MB GPU memory"/>
<node id="many standby workers"/>
<node id="at least one idle standby worker"/>
<node id="two standby workers"/>
<node id="at least one idle worker"/>
<node id="one idle worker"/>
<node id="the waiting time"/>
<node id="moderate GPU memory consumption"/>
<node id="A transaction here"/>
<node id="a model is switched in or out on all of its GPUs to enable or disable inference on this model"/>
<node id="a production GPU training trace from Microsoft"/>
<node id="these jobs"/>
<node id="18 of total GPU hours"/>
<node id="the share of multi-GPU jobs to increase in the future"/>
<node id="current training frameworks"/>
<node id="mature support of elastic training"/>
<node id="These scheduling solutions"/>
<node id="orthogonal and complementary to PipeSwitch"/>
<node id="C and Python functions to the GPU memory management module of PyTorch"/>
<node id="functions"/>
<node id="the received GPU memory into PyTorch GPU memory pool for a specific CUDA stream"/>
<node id="the GPU memory from the pool"/>
<node id="shared GPU memory"/>
<node id="PyTorch GPU memory pool"/>
<node id="different CUDA streams"/>
<node id="only one of these CUDA streams is active"/>
<node id="THE CONTROLLER PROCESS"/>
<node id="A TCP THREAD"/>
<node id="A SCHEDULER THREAD"/>
<node id="the scheduler and the memory daemon"/>
<node id="together"/>
<node id="THE TCP THREAD"/>
<node id="TASK THROUGH TCP FROM CLIENTS"/>
<node id="THE TASK TO THE SCHEDULER THREAD"/>
<node id="THE SCHEDULER THREAD"/>
<node id="THE GPU MEMORY WITH WORKERS"/>
<node id="WORKERS"/>
<node id="THE TASK TO A WORKER"/>
<node id="PARAMETERS FOR THE CORRESPONDING MODEL TO THE GPU MEMORY"/>
<node id="the user"/>
<node id="the model in the scheduler"/>
<node id="the scheduler"/>
<node id="the model from the disk to the CPU memory"/>
<node id="parameters"/>
<node id="groups"/>
<node id="a pipeline"/>
<node id="the worker to start computing the corresponding layers"/>
<node id="THE WORKER PROCESS"/>
<node id="TWO THREADS"/>
<node id="THE TERMINATION THREAD"/>
<node id="THE TERMINATION SIGNAL FROM THE CONTROLLER"/>
<node id="THE MAIN THREAD"/>
<node id="the user to register the model before starting a task"/>
<node id="the models"/>
<node id="the hooks to wait for parameter transmission or terminate on notification"/>
<node id="the model structures"/>
<node id="small"/>
<node id="the parameters"/>
<node id="storing the parameters in the memory daemon"/>
<node id="different models"/>
<node id="the same GPU memory location"/>
<node id="the value"/>
<node id="the controller transfers the corresponding parameters to these locations"/>
<node id="the scheduler to transfer required parameters for DNN models"/>
<node id="inference or training"/>
<node id="ALL EXPERIMENTS"/>
<node id="AWS"/>
<node id="two EC2 instance types"/>
<node id="8 vCPUs (Intel Xeon E5-2686 v4)"/>
<node id="1 GPU (NVIDIA V100 with 16 GB GPU memory)"/>
<node id="PCIE 3.0 16"/>
<node id="61 GB memory"/>
<node id="8 vCPUs (Intel Platinum 8259CL)"/>
<node id="1 GPU (NVIDIA T4 with 16 GB GPU memory)"/>
<node id="PCIe 3.0 8"/>
<node id="32 GB memory"/>
<node id="The software environment"/>
<node id="PyTorch-1.3.0"/>
<node id="TorchVision-0.4.2"/>
<node id="SciPy-1.3.2"/>
<node id="CUDA-10.1"/>
<node id="PyTorch with our plugins for all mechanisms in comparison for consistency"/>
<node id="PyTorch with our plugins"/>
<node id="better results for stop-and-start than native PyTorch from Python-PyPI used in Table 1"/>
<node id="RESNET152 17"/>
<node id="standard benchmark for evaluating DL systems"/>
<node id="INCEPTIONV3 22"/>
<node id="BERTBASE 23"/>
<node id="representative configurations for each model"/>
<node id="THE EXPERIMENTS"/>
<node id="BOTH TRAINING AND INFERENCE"/>
<node id="single-GPU inference and training tasks"/>
<node id="their models to the host memory"/>
<node id="the latest checkpoint after preemption"/>
<node id="checkpointing frequency of training tasks"/>
<node id="scheduling cycle"/>
<node id="checkpointing overhead"/>
<node id="default batch size for training"/>
<node id="32"/>
<node id="default batch size for inference"/>
<node id="8"/>
<node id="throughput and latency as evaluation metrics"/>
<node id="the end-to-end latency experienced by the client"/>
<node id="FIGURE 5"/>
<node id="the latency experienced by the client"/>
<node id="TABLE 3"/>
<node id="the total overhead"/>
<node id="each number"/>
<node id="the average of 100 runs"/>
<node id="Figure 6(B)"/>
<node id="minimum and maximum latencies using the error bar"/>
<node id="latency of the RST batch and those of later batches in one scheduling cycle"/>
<node id="significantly"/>
<node id="difference in latency"/>
<node id="a client"/>
<node id="an inference task to a GPU server"/>
<node id="the GPU server"/>
<node id="a reply back to the client"/>
<node id="the process with the required model"/>
<node id="THE LOWEST LATENCY WE CAN ACHIEVE FOR AN INFERENCE TASK"/>
<node id="separate processes in advance"/>
<node id="CUDA UNIED MEMORY"/>
<node id="MEMORY SWAPPING"/>
<node id="Unified Memory"/>
<node id="URL"/>
<node id="https://devblogs.nvidia.com/unified-memory-cuda-beginners"/>
<node id="THE PROPERTIES"/>
<node id="4"/>
<node id="LATENCY (MS)"/>
<node id="READY MODEL PIPESWITCH MPS STOP-AND-START RESNET152 INCEPTIONV3 BERTBASE"/>
<node id="NVIDIA V100, PCIE 3.0 16"/>
<node id="5000 to 10000"/>
<node id="PIPESWITCH, MPS, STOP-AND-START, RESNET152, INCEPTIONV3, BERTBASE"/>
<node id="PCIe VERSION"/>
<node id="3.0"/>
<node id="GPU COUNT"/>
<node id="3.0 16"/>
<node id="3.0 8"/>
<node id="STOP-AND-START on P3.2XLARGE with RESNET152"/>
<node id="6475.40 MS"/>
<node id="STOP-AND-START on P3.2XLARGE with INCEPTIONV3"/>
<node id="7536.07 MS"/>
<node id="STOP-AND-START on P3.2XLARGE with BERTBASE"/>
<node id="6371.32 MS"/>
<node id="STOP-AND-START on G4DN.2XLARGE with RESNET152"/>
<node id="5486.74 MS"/>
<node id="STOP-AND-START on G4DN.2XLARGE with INCEPTIONV3"/>
<node id="6558.76 MS"/>
<node id="STOP-AND-START on G4DN.2XLARGE with BERTBASE"/>
<node id="5355.95 MS"/>
<node id="NVIDIA MPS on P3.2XLARGE with RESNET152"/>
<node id="307.02 MS"/>
<node id="NVIDIA MPS on P3.2XLARGE with INCEPTIONV3"/>
<node id="232.25 MS"/>
<node id="NVIDIA MPS on P3.2XLARGE with BERTBASE"/>
<node id="204.52 MS"/>
<node id="NVIDIA MPS on G4DN.2XLARGE with RESNET152"/>
<node id="259.20 MS"/>
<node id="NVIDIA MPS on G4DN.2XLARGE with INCEPTIONV3"/>
<node id="193.05 MS"/>
<node id="NVIDIA MPS on G4DN.2XLARGE with BERTBASE"/>
<node id="338.25 MS"/>
<node id="PIPESWITCH on P3.2XLARGE with RESNET152"/>
<node id="6.01 MS"/>
<node id="PIPESWITCH on P3.2XLARGE with INCEPTIONV3"/>
<node id="5.40 MS"/>
<node id="PIPESWITCH on P3.2XLARGE with BERTBASE"/>
<node id="10.27 MS"/>
<node id="PIPESWITCH on G4DN.2XLARGE with RESNET152"/>
<node id="5.57 MS"/>
<node id="PIPESWITCH on G4DN.2XLARGE with INCEPTIONV3"/>
<node id="7.66 MS"/>
<node id="PIPESWITCH on G4DN.2XLARGE with BERTBASE"/>
<node id="34.56 MS"/>
<node id="RESNET152"/>
<node id="the text"/>
<node id="INCEPTIONV3"/>
<node id="BERTBASE"/>
<node id="Latency"/>
<node id="milliseconds (MS)"/>
<node id="P3.2xlarge"/>
<node id="Optimization"/>
<node id="Pipeswitch per-layer pipeline grouped transmission no optimization"/>
<node id="PER-LAYER PIPELINE"/>
<node id="GROUPED TRANSMISSION"/>
<node id="NO OPTIMIZATION"/>
<node id="PCIE 3.0 x8"/>
<node id="510 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="NO MEMORY MANAGEMENT"/>
<node id="NO IPC OPTIMIZATION"/>
<node id="NO PIN MEMORY"/>
<node id="CUDA UNIFIED MEMORY"/>
<node id="milliseconds"/>
<node id="unified memory"/>
<node id="5000 to 7500"/>
<node id="ONE PROCESS"/>
<node id="TWO PROCESSES"/>
<node id="Models"/>
<node id="RESNET152, INCEPTIONV3, BERTBASE"/>
<node id="NVIDIA T4, PCIE 3.0 x8"/>
<node id="RESNET152 on P3.2XLARGE"/>
<node id="3.62 MS"/>
<node id="INCEPTIONV3 on P3.2XLARGE"/>
<node id="4.82 MS"/>
<node id="BERTBASE on P3.2XLARGE"/>
<node id="RESNET152 on G4DN.2XLARGE"/>
<node id="2.53 MS"/>
<node id="INCEPTIONV3 on G4DN.2XLARGE"/>
<node id="5.49 MS"/>
<node id="BERTBASE on G4DN.2XLARGE"/>
<node id="6.57 MS"/>
<node id="Table 4"/>
<node id="the startup overhead for PipeSwitch to start computing the RST layer"/>
<node id="task startup overhead for PipeSwitch"/>
<node id="the difference between the time for ResNet152, InceptionV3, BERTBase"/>
<node id="ResNet152"/>
<node id="464 layers"/>
<node id="InceptionV3"/>
<node id="189 layers"/>
<node id="BERTBase"/>
<node id="139 layers"/>
<node id="1.33 s, 0.18 s, 0.34 s"/>
<node id="Only Pruning 1"/>
<node id="2.09 s, 0.30 s, 0.88 s"/>
<node id="Only Pruning 2"/>
<node id="3.44 h, 5.07 s, 24 h"/>
<node id="No Pruning"/>
<node id="24 h, 24 h, 24 h"/>
<node id="Table 5"/>
<node id="effectiveness of two pruning techniques"/>
<node id="because it requires the models to be preloaded to the GPU"/>
<node id="several limitations described in 2.2"/>
<node id="ITS PERFORMANCE"/>
<node id="THE READY MODEL"/>
<node id="preloaded"/>
<node id="THE MODEL"/>
<node id="THE HOST MEMORY"/>
<node id="THE TOTAL OVERHEAD"/>
<node id="the difference between the latency of a mechanism and that of the ready model"/>
<node id="STOP-AND-START"/>
<node id="the worst"/>
<node id="several seconds"/>
<node id="The main source of the overhead"/>
<node id="CUDA context initialization and rst-time library loading operations in PyTorch"/>
<node id="ANOTHER SOURCE"/>
<node id="GPU MEMORY SWAPPING"/>
<node id="the best"/>
<node id="computing BERT on T4"/>
<node id="120ms"/>
<node id="relative overhead"/>
<node id="acceptable"/>
<node id="computing the RST layer"/>
<node id="computing for the ready model"/>
<node id="The startup overhead of PipeSwitch"/>
<node id="only a few milliseconds"/>
<node id="only a few milliseconds overhead for task switching"/>
<node id="low latency close to the lower bound"/>
<node id="throughput and end-to-end latency of different mechanisms under different scheduling cycles"/>
<node id="ResNet152 for both training and inference"/>
<node id="eight p3.2xlarge instances"/>
<node id="training and inference after each scheduling cycle"/>
<node id="FIGURE 6(A)"/>
<node id="the inference throughput"/>
<node id="THE DASHED LINE"/>
<node id="THE UPPER BOUND"/>
<node id="THE THROUGHPUT OF THE READY MODEL ASSUMING NO TASK SWITCHING"/>
<node id="THE AVERAGE LATENCY OF THE READY MODEL"/>
<node id="NO TASK SWITCHING"/>
<node id="throughput of stop-and-start"/>
<node id="zero for scheduling cycles smaller than 10 s"/>
<node id="several seconds for task switching"/>
<node id="MPS"/>
<node id="around 100 batches per second"/>
<node id="the ratio to the upper bound"/>
<node id="FIGURE 6(B)"/>
<node id="the average latency of the inference tasks"/>
<node id="THE ERROR BAR"/>
<node id="THE MINIMUM AND MAXIMUM LATENCY"/>
<node id="STOP- AND-START"/>
<node id="poor latency"/>
<node id="RST BATCH"/>
<node id="several seconds overhead"/>
<node id="several seconds overhead of RST BATCH"/>
<node id="about 80 ms average latency"/>
<node id="several hundred milliseconds latency for the RST batch"/>
<node id="7500 to 10000"/>
<node id="PIPESWITCH MPS"/>
<node id="1S 2S 5S 10S 30S"/>
<node id="LOWER BOUND"/>
<node id="LATENCY"/>
<node id="0 200 400"/>
<node id="Throughput and latency"/>
<node id="different scheduling cycles"/>
<node id="ResNet"/>
<node id="p3.2xlarge"/>
<node id="Computation"/>
<node id="once parameters are transmitted"/>
<node id="FIGURE 8"/>
<node id="the total time measured by the client"/>
<node id="the worst in most cases"/>
<node id="THE LAYERS OF THE MODEL INTO ONE BIG TENSOR"/>
<node id="ONE BIG TENSOR IN ONE GROUP"/>
<node id="transmission and computation"/>
<node id="overlaps"/>
<node id="layer"/>
<node id="PCIE overhead and synchronization overhead"/>
<node id="for every layer"/>
<node id="models with many layers but relatively light computation such as ResNet152 and Inception"/>
<node id="worse than grouped transmission"/>
<node id="sometimes even worse than no pipeline"/>
<node id="this reduction"/>
<node id="significant"/>
<node id="the optimizations on memory management and worker switching have already been applied"/>
<node id="to meet strict SLOs it is important to reduce all overheads for task switching, not only the most significant one"/>
<node id="TABLE 5"/>
<node id="the running time of Algorithm 1"/>
<node id="the effects of the two pruning techniques mentioned in 4.2"/>
<node id="both weighted and unweighted layers"/>
<node id="the computation time"/>
<node id="the parameter size and running time for each layer in advance"/>
<node id="only several seconds to compute an optimal grouping strategy"/>
<node id="no pruning"/>
<node id="for all three models after running for 24 hours"/>
<node id="UNIED MEMORY MANAGEMENT"/>
<node id="the effectiveness of UNIED MEMORY MANAGEMENT"/>
<node id="Memory management"/>
<node id="not unified"/>
<node id="the following VE mechanisms discussed in 4.3"/>
<node id="IPC"/>
<node id="no optimization"/>
<node id="the pages of the memory daemon"/>
<node id="the main memory"/>
<node id="this experiment"/>
<node id="all the optimizations on memory management are effective"/>
<node id="Unified Memory Management Mechanism"/>
<node id="IPC optimization"/>
<node id="important"/>
<node id="latency by 1648 ms"/>
<node id="Pinning the pages to the host memory"/>
<node id="the latency with a few milliseconds"/>
<node id="two processes"/>
<node id="a new process for the new task"/>
<node id="THE NEW PROCESS"/>
<node id="A NEW CUDA ENVIRONMENT"/>
<node id="THE TOTAL TIME"/>
<node id="THE CUDA ENVIRONMENT"/>
<node id="THE OVERHEAD TO CLEAN THE ENVIRONMENT"/>
<node id="Several algorithms and systems"/>
<node id="executing and scheduling deep learning tasks on clusters"/>
<node id="Deep learning tasks on clusters"/>
<node id="both training and inference tasks"/>
<node id="how to realize a scheduling decision"/>
<node id="the scheduler to change the resource allocation more often with millisecond-scale task switching"/>
<node id="many techniques and systems"/>
<node id="optimize communication and improve distributed training"/>
<node id="THE MOST RELEVANT ONES"/>
<node id="PIPEDREAM 8"/>
<node id="BYTESCHEDULER 9"/>
<node id="POSEIDON 40"/>
<node id="VDNN 43"/>
<node id="GPU memory management module"/>
<node id="SWAPADVISOR 44"/>
<node id="VDNN 43 and SWAPADVISOR 44"/>
<node id="memory management for a single training task of large models"/>
<node id="CLUSTER MANAGERS 4548"/>
<node id="GPUS to VMS or CONTAINERS at device granularity"/>
<node id="Several solutions"/>
<node id="to share a GPU at application granularity using techniques like library interception"/>
<node id="Efforts on GPU optimization"/>
<node id="the performance of running a single task"/>
<node id="tensor fusion"/>
<node id="kernel-level concurrency and scheduling"/>
<node id="our shepherd Madan Musu-Vathi"/>
<node id="the anonymous reviewers"/>
<node id="Madan Musu-Vathi and the anonymous reviewers"/>
<node id="valuable feedback"/>
<node id="ZHIHAO BAI, ZHEN ZHANG AND XIN JIN"/>
<node id="an AWS Machine Learning Research Award"/>
<node id="A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes"/>
<node id="Large-scale cluster management at Google with Borg"/>
<node id="EuroSys"/>
<node id="2015"/>
<node id="NEXUS"/>
<node id="a GPU cluster engine"/>
<node id="accelerating DNN-based video analysis"/>
<node id="3 H. SHEN, L. CHEN, Y. JIN, L. ZHAO, B. KONG, M. PHILIPOSE, A. KRISHNAMURTHY, and R. SUNDARAM"/>
<node id="NEXUS: A GPU cluster engine for accelerating DNN-based video analysis"/>
<node id="NEXUS paper"/>
<node id="ACM SOSP"/>
<node id="2019"/>
<node id="FRIED, J. BEHRENS, A. BELAY, AND H. BALAKRISHNAN"/>
<node id="SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS"/>
<node id="USENIX NSDI"/>
<node id="7 P. YU AND M. CHOWDHURY"/>
<node id="SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS"/>
<node id="CONFERENCE ON MACHINE LEARNING AND SYSTEMS"/>
<node id="2020"/>
<node id="D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia"/>
<node id="Pipedream: Generalized Pipeline Parallelism for DNN Training"/>
<node id="Y. PENG, Y. ZHU, Y. CHEN, Y. BAO, B. YI, C. LAN, C. WU, AND C. GUO"/>
<node id="A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION"/>
<node id="TIRESIAS"/>
<node id="a GPU cluster manager for distributed deep learning"/>
<node id="J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo"/>
<node id="POSEIDON"/>
<node id="an efficient communication architecture for distributed deep learning on GPU clusters"/>
<node id="USENIX ATC, 2017"/>
<node id="H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie, and E. P. Xing"/>
<node id="AMAZON WEB SERVICES"/>
<node id="12"/>
<node id="HTTPS"/>
<node id="AWS.AMAZON.COM"/>
<node id="MICROSOFT AZURE"/>
<node id="a cloud computing service"/>
<node id="GOOGLE CLOUD PLATFORM"/>
<node id="14"/>
<node id="CLOUD.GOOGLE.COM"/>
<node id="HOROVOD"/>
<node id="fast and easy distributed deep learning in TensorFlow"/>
<node id="15 A. Sergeev and M. Del Balso"/>
<node id="HOROVOD: Fast and Easy Distributed Deep Learning in TensorFlow"/>
<node id="HOROVOD paper"/>
<node id="arXiv preprint arXiv:1802.05799"/>
<node id="2018"/>
<node id="SU"/>
<node id="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER"/>
<node id="USENIX OSDI"/>
<node id="2014"/>
<node id="DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION"/>
<node id="SUN"/>
<node id="IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"/>
<node id="2016"/>
<node id="PHILLY"/>
<node id="20 TRACES"/>
<node id="21"/>
<node id="C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna"/>
<node id="Rethinking the Inception Architecture for Computer Vision"/>
<node id="IEEE Conference on Computer Vision and Pattern Recognition"/>
<node id="J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova"/>
<node id="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"/>
<node id="Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"/>
<node id="GANDIVA"/>
<node id="introspective cluster scheduling for deep learning"/>
<node id="USENIX OSDI, 2018"/>
<node id="24 W. XIAO, R. BHARDWAJ, R. RAMJEE, M. SIVATHANU, N. KWATRA, Z. HAN, P. PATEL, X. PENG, H. ZHAO, Q. ZHANG, ET AL."/>
<node id="25"/>
<node id="TENSORFLOW"/>
<node id="54"/>
<node id="TENSORFLOW XLA"/>
<node id="XLA"/>
<node id="https://www.tensorflow.org"/>
<node id="26"/>
<node id="MXNET"/>
<node id="M. J. Freedman"/>
<node id="SLAQ: Quality-Driven Scheduling for Distributed Machine Learning"/>
<node id="ACM Symposium on Cloud Computing"/>
<node id="2017"/>
<node id="OPTIMUS"/>
<node id="an efficient dynamic resource scheduler for deep learning clusters"/>
<node id="Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo"/>
<node id="THEMIS"/>
<node id="fair and efficient GPU cluster scheduling"/>
<node id="K. Mahajan, A. Balasubramanian, A. Singhvi, S. Venkataraman, A. Akella, A. Phanishayee, and S. Chawla"/>
<node id="R. LIAW, R. BHARDWAJ, L. DUNLAP, Y. ZOU, J. E. GONZALEZ, I. STOICA, AND A. TUMANOV"/>
<node id="HYPERSCHED: DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE"/>
<node id="ACM SYMPOSIUM ON CLOUD COMPUTING"/>
<node id="CHET"/>
<node id="an optimizing compiler for fully-homomorphic neural-network inferencing"/>
<node id="ACM Conference on Programming Language Design and Implementation"/>
<node id="R. Dathathri, O. Saarikivi, H. Chen, K. Laine, K. Lauter, S. Maleki, M. Musuvathi, and T. Mytkowicz"/>
<node id="TVM"/>
<node id="an automated end-to-end optimizing compiler for deep learning"/>
<node id="T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, et al."/>
<node id="TVM: An automated end-to-end optimizing compiler for deep learning"/>
<node id="GPIPE"/>
<node id="33 Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al."/>
<node id="efficient training of giant neural networks using pipeline parallelism"/>
<node id="Advances in Neural Information Processing Systems"/>
<node id="BLINK"/>
<node id="fast and generic collectives for distributed ML"/>
<node id="Conference on Machine Learning and Systems"/>
<node id="Authors"/>
<node id="G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica"/>
<node id="NCCL"/>
<node id="NVIDIA Collective Communications Library"/>
<node id="developer.nvidia.com/nccl"/>
<node id="36 J. LIU, J. WU, AND D. K. PANDA"/>
<node id="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION OVER INNIBAND"/>
<node id="Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing"/>
<node id="More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server"/>
<node id="2013"/>
<node id="A. AWAN, C.-H. CHU, H. SUBRAMONI, AND D. K. PANDA"/>
<node id="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?"/>
<node id="PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING"/>
<node id="A. Vishnu, C. Siegel, T. Warfel, and V. Amatya"/>
<node id="GossipGrad: Scalable Deep Learning Using Gossip Communication Based Asynchronous Gradient Descent"/>
<node id="41 Z. ZHANG, C. CHANG, H. LIN, Y. WANG, R. ARORA, AND X. JIN"/>
<node id="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?"/>
<node id="ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)"/>
<node id="AUGUST 2020"/>
<node id="42 Y. CHEN, Z. LIU, B. REN, AND X. JIN"/>
<node id="ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS"/>
<node id="INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)"/>
<node id="JULY 2020"/>
<node id="VDNN"/>
<node id="Virtualized Deep Neural Networks for scalable, memory-efficient neural network design"/>
<node id="2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)"/>
<node id="M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler"/>
<node id="SWAPADVISOR"/>
<node id="C.-C. Huang, G. Jin, and J. Li"/>
<node id="ACM ASPLOS"/>
<node id="GPU memory limit via smart swapping"/>
<node id="KUBERNETES.IO"/>
<node id="GITHUB.COMNVIDIANVIDIA-DOCKER"/>
<node id="MESOS"/>
<node id="a platform for fine-grained resource sharing in the data center"/>
<node id="47 B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H. Katz, S. Shenker, and I. Stoica"/>
<node id="MESOS: A platform for fine-grained resource sharing in the data center"/>
<node id="MESOS paper"/>
<node id="USENIX NSDI, 2011"/>
<node id="V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al."/>
<node id="Apache Hadoop YARN: Yet Another Resource Negotiator"/>
<node id="G. GIUNTA, R. MONTELLA, G. AGRILLO, AND G. COVIELLO"/>
<node id="A GPGPU Transparent Virtualization Component for High Performance Computing Clouds"/>
<node id="European Conference on Parallel Processing"/>
<node id="2010"/>
<node id="V. T. RAVI, M. BECCHI, G. AGRAWAL, AND S. CHAKRADHAR"/>
<node id="SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK"/>
<node id="PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING"/>
<node id="2011"/>
<node id="GVIM"/>
<node id="GPU-accelerated virtual machines"/>
<node id="Proceedings of the 3rd ACM Workshop on System-Level Virtualization for High Performance Computing"/>
<node id="2009"/>
<node id="V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan"/>
<node id="J. DUATO, A. J. PENA, F. SILLA, R. MAYO, AND E. S. QUINTANA-ORT"/>
<node id="RCUDA: REDUCING THE NUMBER OF GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS"/>
<node id="2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION"/>
<node id="VCUDA"/>
<node id="GPU-accelerated high-performance computing in virtual machines"/>
<node id="SUN and K. LI"/>
<node id="VCUDA: GPU-accelerated high-performance computing in virtual machines"/>
<node id="IEEE Transactions on Computers"/>
<node id="MXNet"/>
<node id="a flexible and efficient machine learning library"/>
<node id="heterogeneous distributed systems"/>
<node id="55 T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang"/>
<node id="MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems"/>
<node id="MXNet paper"/>
<node id="arXiv preprint arXiv:1512.01274"/>
<node id="56 C. Gregg, J. Dorn, K. Hazelwood, and K. Skadron"/>
<node id="Fine-Grained Resource Sharing for Concurrent GPGPU Kernels"/>
<node id="4th USENIX Workshop on Hot Topics in Parallelism"/>
<node id="2012"/>
<node id="57 S. PAI, M. J. THAZHUTHAVEETIL, AND R. GOVINDARAJAN"/>
<node id="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS"/>
<node id="ACM SIGARCH COMPUTER ARCHITECTURE NEWS"/>
<node id="TASO"/>
<node id="58 Z. JIA, O. PADON, J. THOMAS, T. WARSZAWSKI, M. ZAHARIA, AND A. AIKEN, TASO: OPTIMIZING DEEP LEARNING COMPUTATION WITH AUTOMATIC GENERATION OF GRAPH SUBSTITUTIONS, IN ACM SOSP, 2019"/>
<edge source="This paper" target="the proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation">
  <data key="d0">is included in</data>
</edge>
<edge source="The proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation" target="USENIX">
  <data key="d0">are sponsored by</data>
</edge>
<edge source="PipeSwitch" target="fast pipelined context switching for deep learning applications">
  <data key="d0">is</data>
</edge>
<edge source="PipeSwitch" target="1050 better than NVIDIA MPS">
  <data key="d0">has overhead</data>
</edge>
<edge source="PipeSwitch" target="near 100 GPU utilization">
  <data key="d0">achieves</data>
</edge>
<edge source="PipeSwitch" target="training and inference tasks">
  <data key="d0">fast starts</data>
</edge>
<edge source="PipeSwitch" target="fast switching across tasks">
  <data key="d0">enables</data>
</edge>
<edge source="Zhihao Bai" target="Johns Hopkins University">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Zhen Zhang" target="Johns Hopkins University">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Yibo Zhu" target="ByteDance Inc.">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Xin Jin" target="Johns Hopkins University">
  <data key="d0">is affiliated with</data>
</edge>
<edge source="Deep learning workloads" target="throughput-intensive training tasks">
  <data key="d0">include</data>
</edge>
<edge source="Deep learning workloads" target="latency-sensitive inference tasks">
  <data key="d0">include</data>
</edge>
<edge source="The dominant practice today" target="to provision dedicated GPU clusters for training and inference separately">
  <data key="d0">is</data>
</edge>
<edge source="training and inference" target="GPUs">
  <data key="d0">use</data>
</edge>
<edge source="the current practice" target="dedicated clusters for training and inference separately">
  <data key="d0">is to build</data>
</edge>
<edge source="We" target="fine-grained time-sharing GPU clusters">
  <data key="d0">envision to build</data>
</edge>
<edge source="We" target="the performance of PipeSwitch with experiments on a variety of DNN models and GPU cards">
  <data key="d0">demonstrate</data>
</edge>
<edge source="We" target="introducing pipelined context switching">
  <data key="d0">achieve</data>
</edge>
<edge source="We" target="unied memory management and active-standby worker switching mechanisms">
  <data key="d0">design</data>
</edge>
<edge source="We" target="a PipeSwitch prototype">
  <data key="d0">have built</data>
</edge>
<edge source="We" target="it with PyTorch">
  <data key="d0">have integrated</data>
</edge>
<edge source="We" target="a system prototype for PipeSwitch">
  <data key="d0">have implemented</data>
</edge>
<edge source="We" target="PyTorch 21">
  <data key="d0">have integrated it with</data>
</edge>
<edge source="We" target="pipelined context switching">
  <data key="d0">introduce</data>
</edge>
<edge source="We" target="layers into groups">
  <data key="d0">divide</data>
</edge>
<edge source="We" target="an optimal model-aware grouping algorithm">
  <data key="d0">design</data>
</edge>
<edge source="We" target="unified memory management with a dedicated memory daemon">
  <data key="d0">design</data>
</edge>
<edge source="We" target="unified memory management with the memory daemon">
  <data key="d0">use</data>
</edge>
<edge source="We" target="an active and standby worker switching mechanism">
  <data key="d0">design</data>
</edge>
<edge source="We" target="new techniques">
  <data key="d0">design</data>
</edge>
<edge source="We" target="the inefficiencies in today's shared GPU clusters">
  <data key="d0">identify</data>
</edge>
<edge source="We" target="running DL workloads on GPUs in the fine-grained time-sharing model">
  <data key="d0">motivate</data>
</edge>
<edge source="We" target="to pack multiple DL applications onto the same GPU via NE-grained time-sharing abstraction to maximize GPU utilization">
  <data key="d0">propose</data>
</edge>
<edge source="We" target="fast context switching">
  <data key="d0">focus on</data>
</edge>
<edge source="We" target="the network time">
  <data key="d0">exclude</data>
</edge>
<edge source="We" target="the task queueing time">
  <data key="d0">exclude</data>
</edge>
<edge source="We" target="to circumvent this physical limit">
  <data key="d0">exploit the characteristics of DL applications</data>
</edge>
<edge source="We" target="all possible combinations into N cases">
  <data key="d0">divide</data>
</edge>
<edge source="We" target="a heuristic that bootstraps optgroups">
  <data key="d0">use</data>
</edge>
<edge source="We" target="induction on M">
  <data key="d0">use</data>
</edge>
<edge source="We" target="two characteristics of DL applications">
  <data key="d0">exploit</data>
</edge>
<edge source="We" target="a production GPU training trace from Microsoft">
  <data key="d0">have analyzed</data>
</edge>
<edge source="We" target="two EC2 instance types">
  <data key="d0">use</data>
</edge>
<edge source="We" target="PyTorch with our plugins for all mechanisms in comparison for consistency">
  <data key="d0">use</data>
</edge>
<edge source="We" target="single-GPU inference and training tasks">
  <data key="d0">use</data>
</edge>
<edge source="We" target="throughput and latency as evaluation metrics">
  <data key="d0">use</data>
</edge>
<edge source="We" target="separate processes in advance">
  <data key="d0">initialize</data>
</edge>
<edge source="We" target="ResNet152 for both training and inference">
  <data key="d0">use</data>
</edge>
<edge source="We" target="training and inference after each scheduling cycle">
  <data key="d0">switch between</data>
</edge>
<edge source="We" target="the following VE mechanisms discussed in 4.3">
  <data key="d0">compare</data>
</edge>
<edge source="We" target="our shepherd Madan Musu-Vathi">
  <data key="d0">thank</data>
</edge>
<edge source="We" target="the anonymous reviewers">
  <data key="d0">thank</data>
</edge>
<edge source="GPU clusters" target="different applications including training and inference">
  <data key="d0">can be shared across</data>
</edge>
<edge source="GPU clusters" target="based on the peak load">
  <data key="d0">are often over-provisioned</data>
</edge>
<edge source="GPU clusters" target="between applications and task types">
  <data key="d0">have limited sharing</data>
</edge>
<edge source="GPU clusters" target="the need to meet strict service-level objectives (SLOs)">
  <data key="d0">are over-provisioned due to</data>
</edge>
<edge source="GPU clusters" target="privately shared by multiple users">
  <data key="d0">are</data>
</edge>
<edge source="GPU clusters" target="publicly shared by multiple users">
  <data key="d0">are</data>
</edge>
<edge source="GPU clusters" target="dedicated physical forms and power supplies">
  <data key="d0">are designed with</data>
</edge>
<edge source="GPU clusters" target="high speed networks">
  <data key="d0">are designed with</data>
</edge>
<edge source="GPU clusters" target="specialized task schedulers">
  <data key="d0">are designed with</data>
</edge>
<edge source="PIPESWITCH" target="a system">
  <data key="d0">is</data>
</edge>
<edge source="PIPESWITCH" target="unused cycles of an inference application to be filled by training or other inference applications">
  <data key="d0">enables</data>
</edge>
<edge source="PIPESWITCH" target="GPU utilization">
  <data key="d0">can significantly increase</data>
</edge>
<edge source="PIPESWITCH" target="SLOs">
  <data key="d0">does not sacrifice</data>
</edge>
<edge source="PIPESWITCH" target="single-GPU tasks for training and inference">
  <data key="d0">is focused on</data>
</edge>
<edge source="PIPESWITCH" target="single-GPU training for training tasks">
  <data key="d0">supports</data>
</edge>
<edge source="PIPESWITCH" target="asynchronous multi-GPU training for data parallel strategies">
  <data key="d0">supports</data>
</edge>
<edge source="PIPESWITCH" target="high throughput close to the upper bound">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH" target="the agility of DL applications">
  <data key="d0">can improve</data>
</edge>
<edge source="PIPESWITCH" target="GPU-efficient multiplexing of many DL applications on GPU servers via fine-grained time-sharing">
  <data key="d0">enables</data>
</edge>
<edge source="PIPESWITCH" target="millisecond-scale latencies and high throughput as dedicated servers">
  <data key="d0">achieves</data>
</edge>
<edge source="PIPESWITCH" target="GPU-efficient fine-grained time-sharing for multiple DL applications">
  <data key="d0">enables</data>
</edge>
<edge source="PIPESWITCH" target="millisecond-scale context switching latencies">
  <data key="d0">achieves</data>
</edge>
<edge source="PIPESWITCH" target="high throughput">
  <data key="d0">achieves</data>
</edge>
<edge source="PIPESWITCH" target="all the ideas into our system">
  <data key="d0">combines</data>
</edge>
<edge source="PIPESWITCH" target="the gap of GPU memory sharing and switching">
  <data key="d0">closes</data>
</edge>
<edge source="PIPESWITCH" target="the design of an efficient time-sharing GPU cluster for DL workloads">
  <data key="d0">enables</data>
</edge>
<edge source="PIPESWITCH" target="GPU-efficient multiplexing of multiple DL applications on GPU servers">
  <data key="d0">enables</data>
</edge>
<edge source="PIPESWITCH" target="millisecond-scale task switching time">
  <data key="d0">is able to achieve</data>
</edge>
<edge source="PIPESWITCH" target="DL applications on time-sharing GPUs to meet strict SLOs">
  <data key="d0">enables</data>
</edge>
<edge source="PIPESWITCH" target="process-level isolation">
  <data key="d0">aims to ensure</data>
</edge>
<edge source="PIPESWITCH" target="us to address new technical challenges on memory management and worker switching across different processes">
  <data key="d0">requires</data>
</edge>
<edge source="PIPESWITCH" target="fast task switching">
  <data key="d0">aims to provide</data>
</edge>
<edge source="PIPESWITCH" target="an active worker">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH" target="multiple standby workers">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH" target="the active-standby worker switching mechanism used by PIPESWITCH">
  <data key="d0">is</data>
</edge>
<edge source="PIPESWITCH" target="intra-batch pipelining">
  <data key="d0">introduces</data>
</edge>
<edge source="PIPESWITCH" target="the knowledge of models">
  <data key="d0">requires</data>
</edge>
<edge source="PIPESWITCH" target="model-aware grouping">
  <data key="d0">uses</data>
</edge>
<edge source="PIPESWITCH" target="the best trade-off between pipeline overhead and efficiency">
  <data key="d0">achieves</data>
</edge>
<edge source="PIPESWITCH" target="a part of the DNN framework">
  <data key="d0">can be implemented as</data>
</edge>
<edge source="PIPESWITCH" target="the model structure information">
  <data key="d0">can gather</data>
</edge>
<edge source="PIPESWITCH" target="transparent to users and cluster managers">
  <data key="d0">remains</data>
</edge>
<edge source="PIPESWITCH" target="a dedicated memory daemon">
  <data key="d0">uses</data>
</edge>
<edge source="PIPESWITCH" target="a 64-bit integer offset for the shared GPU memory to workers">
  <data key="d0">sends</data>
</edge>
<edge source="PIPESWITCH" target="223 ms">
  <data key="d0">saves</data>
</edge>
<edge source="PIPESWITCH" target="eliminating the memory allocation overhead with the memory daemon">
  <data key="d0">saves time by</data>
</edge>
<edge source="PIPESWITCH" target="GPU MEMORY BLOCKS TO PYTORCH GPU MEMORY POOL">
  <data key="d0">inserts</data>
</edge>
<edge source="PIPESWITCH" target="the models in the memory daemon">
  <data key="d0">stores</data>
</edge>
<edge source="PIPESWITCH" target="pipeline">
  <data key="d0">is a type of</data>
</edge>
<edge source="PIPESWITCH" target="NO MEMORY MANAGEMENT">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH" target="NO IPC OPTIMIZATION">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH" target="NO PIN MEMORY">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH" target="CUDA UNIFIED MEMORY">
  <data key="d0">uses</data>
</edge>
<edge source="PIPESWITCH" target="ONE PROCESS">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH" target="TWO PROCESSES">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH" target="the best">
  <data key="d0">performs</data>
</edge>
<edge source="PIPESWITCH" target="the lower bound">
  <data key="d0">is close to</data>
</edge>
<edge source="PIPESWITCH" target="computing the RST layer">
  <data key="d0">starts</data>
</edge>
<edge source="PIPESWITCH" target="computing for the ready model">
  <data key="d0">starts</data>
</edge>
<edge source="PIPESWITCH" target="only a few milliseconds overhead for task switching">
  <data key="d0">incurs</data>
</edge>
<edge source="PIPESWITCH" target="low latency close to the lower bound">
  <data key="d0">achieves</data>
</edge>
<edge source="PIPESWITCH" target="how to realize a scheduling decision">
  <data key="d0">focuses on</data>
</edge>
<edge source="PIPESWITCH" target="the scheduler to change the resource allocation more often with millisecond-scale task switching">
  <data key="d0">enables</data>
</edge>
<edge source="GPU utilization" target="the ratio to the upper bound">
  <data key="d0">is defined as</data>
</edge>
<edge source="Experiments on a variety of DL models and GPU cards" target="PipeSwitch only incurs a task startup overhead of 3.66.6 ms">
  <data key="d0">show</data>
</edge>
<edge source="Experiments on a variety of DL models and GPU cards" target="PipeSwitch incurs a total overhead of 5.434.6 ms">
  <data key="d0">show</data>
</edge>
<edge source="Experiments on a variety of DL models and GPU cards" target="PipeSwitch has a total overhead of 5.434.6 ms">
  <data key="d0">show</data>
</edge>
<edge source="PipeSwitch total overhead" target="1050 better than NVIDIA MPS">
  <data key="d0">is</data>
</edge>
<edge source="MULTI-GPU INFERENCE TASKS" target="performing PIPESWITCH on each GPU with transactions">
  <data key="d0">can be supported by</data>
</edge>
<edge source="preempting one GPU" target="other GPUs">
  <data key="d0">does not affect</data>
</edge>
<edge source="Elastic synchronous training" target="the dynamic changing of the number of GPUs used for training">
  <data key="d0">allows</data>
</edge>
<edge source="the key idea" target="the layered structure of neural network models and their layer-by-layer computation pattern">
  <data key="d0">is to leverage</data>
</edge>
<edge source="the key idea" target="model transmission over the PCIe and task execution in the GPU with model-aware grouping">
  <data key="d0">is to pipeline</data>
</edge>
<edge source="WE" target="a pipelined model transmission mechanism">
  <data key="d0">design</data>
</edge>
<edge source="WE" target="a property of DL applications to minimize the IPC overhead">
  <data key="d0">leverage</data>
</edge>
<edge source="WE" target="the pages of the memory daemon to the host memory">
  <data key="d0">pin</data>
</edge>
<edge source="a pipelined model transmission mechanism" target="model transmission over the PCIe">
  <data key="d0">pipelines</data>
</edge>
<edge source="a pipelined model transmission mechanism" target="model computation in the GPU">
  <data key="d0">pipelines</data>
</edge>
<edge source="transmitting a task from CPU to GPU" target="the PCIe bandwidth">
  <data key="d0">is bounded by</data>
</edge>
<edge source="unied memory management and active-standby worker switching mechanisms" target="the pipelining">
  <data key="d0">accompany</data>
</edge>
<edge source="unied memory management and active-standby worker switching mechanisms" target="process-level isolation">
  <data key="d0">ensure</data>
</edge>
<edge source="the pipelining" target="per-group granularity">
  <data key="d0">is performed on</data>
</edge>
<edge source="we" target="an active-standby mechanism">
  <data key="d0">use</data>
</edge>
<edge source="we" target="a new technology called pipelined context switching">
  <data key="d0">introduce</data>
</edge>
<edge source="we" target="a major challenge fast GPU context switching between different processes">
  <data key="d0">face</data>
</edge>
<edge source="we" target="a measurement study">
  <data key="d0">perform</data>
</edge>
<edge source="we" target="the overhead of each component">
  <data key="d0">analyze</data>
</edge>
<edge source="we" target="a holistic approach">
  <data key="d0">take</data>
</edge>
<edge source="we" target="the characteristics of DL applications">
  <data key="d0">exploit</data>
</edge>
<edge source="we" target="the overhead of all the components">
  <data key="d0">minimize</data>
</edge>
<edge source="we" target="an algorithm to find the optimal grouping strategy for a given model">
  <data key="d0">design</data>
</edge>
<edge source="we" target="that the memory allocation for a DNN model is deterministic">
  <data key="d0">exploit</data>
</edge>
<edge source="we" target="all other components of PipeSwitch the same">
  <data key="d0">keep</data>
</edge>
<edge source="we" target="the following mechanisms discussed in 4.4">
  <data key="d0">compare</data>
</edge>
<edge source="we" target="a system prototype">
  <data key="d0">implement</data>
</edge>
<edge source="we" target="it with PyTorch">
  <data key="d0">integrate</data>
</edge>
<edge source="we" target="the strawman scenario">
  <data key="d0">test</data>
</edge>
<edge source="we" target="a training task">
  <data key="d0">stop</data>
</edge>
<edge source="we" target="an inference task">
  <data key="d0">start</data>
</edge>
<edge source="we" target="other challenges like memory management and worker switching">
  <data key="d0">need to resolve</data>
</edge>
<edge source="we" target="end-to-end experiments">
  <data key="d0">use</data>
</edge>
<edge source="we" target="the effectiveness of the design choices on each component">
  <data key="d0">show</data>
</edge>
<edge source="we" target="our design">
  <data key="d0">describe</data>
</edge>
<edge source="we" target="the time to start and execute it on the GPU">
  <data key="d0">focus on measuring</data>
</edge>
<edge source="we" target="the overhead into the four components">
  <data key="d0">break down</data>
</edge>
<edge source="we" target="all the components should be optimized to achieve minimal switching overhead and meet the SLOs">
  <data key="d0">emphasize</data>
</edge>
<edge source="we" target="grouping">
  <data key="d0">use</data>
</edge>
<edge source="we" target="multiple layers into a group">
  <data key="d0">combine</data>
</edge>
<edge source="we" target="all possible combinations">
  <data key="d0">can enumerate</data>
</edge>
<edge source="we" target="the optimal grouping strategy">
  <data key="d0">can find</data>
</edge>
<edge source="we" target="two pruning techniques">
  <data key="d0">introduce</data>
</edge>
<edge source="we" target="the cases that group from layer (I 1) to J J">
  <data key="d0">can prune</data>
</edge>
<edge source="we" target="J J algorithm">
  <data key="d0">only search for</data>
</edge>
<edge source="we" target="multiple layers in a group based on the progress of computation">
  <data key="d0">can safely pack</data>
</edge>
<edge source="we" target="a lower bound for the total time for each case in Equation 1">
  <data key="d0">compute</data>
</edge>
<edge source="we" target="the transmission of the second group into the computation of the RST group">
  <data key="d0">can hide</data>
</edge>
<edge source="we" target="B.DELAY to denote the time to which the group can be formed">
  <data key="d0">use</data>
</edge>
<edge source="we" target="functions for allocating GPU memory">
  <data key="d0">add</data>
</edge>
<edge source="we" target="functions for sharing the GPU memory to workers through CUDA IPC API">
  <data key="d0">add</data>
</edge>
<edge source="we" target="functions for getting the shared GPU memory">
  <data key="d0">add</data>
</edge>
<edge source="we" target="a memory management mechanism tailored for DL applications">
  <data key="d0">design</data>
</edge>
<edge source="we" target="separate processes">
  <data key="d0">use</data>
</edge>
<edge source="we" target="synchronization points into training tasks">
  <data key="d0">insert</data>
</edge>
<edge source="we" target="the task cleaning of the current task and the pipelined model transmission of the new task">
  <data key="d0">can parallelize</data>
</edge>
<edge source="we" target="the share of multi-GPU jobs to increase in the future">
  <data key="d0">expect</data>
</edge>
<edge source="we" target="C and Python functions to the GPU memory management module of PyTorch">
  <data key="d0">add</data>
</edge>
<edge source="we" target="functions">
  <data key="d0">add</data>
</edge>
<edge source="we" target="representative configurations for each model">
  <data key="d0">use</data>
</edge>
<edge source="we" target="the end-to-end latency experienced by the client">
  <data key="d0">measure</data>
</edge>
<edge source="we" target="throughput and end-to-end latency of different mechanisms under different scheduling cycles">
  <data key="d0">compare</data>
</edge>
<edge source="we" target="to meet strict SLOs it is important to reduce all overheads for task switching, not only the most significant one">
  <data key="d0">would like to emphasize</data>
</edge>
<edge source="we" target="the parameter size and running time for each layer in advance">
  <data key="d0">measure</data>
</edge>
<edge source="active-standby mechanism" target="fast worker switching">
  <data key="d0">is used for</data>
</edge>
<edge source="active-standby mechanism" target="process-level isolation">
  <data key="d0">is used for</data>
</edge>
<edge source="The system prototype for PipeSwitch" target="3600 lines of code">
  <data key="d0">has</data>
</edge>
<edge source="The 3600 lines of code" target="C and Python">
  <data key="d0">are written in</data>
</edge>
<edge source="Deep Learning (DL)" target="an emerging family of intelligent applications">
  <data key="d0">powers</data>
</edge>
<edge source="intelligent applications" target="many domains">
  <data key="d0">are in</data>
</edge>
<edge source="many domains" target="retail">
  <data key="d0">include</data>
</edge>
<edge source="many domains" target="transportation">
  <data key="d0">include</data>
</edge>
<edge source="many domains" target="finance">
  <data key="d0">include</data>
</edge>
<edge source="many domains" target="healthcare">
  <data key="d0">include</data>
</edge>
<edge source="GPUS" target="one of the most widely-used classes of accelerators for DL">
  <data key="d0">are</data>
</edge>
<edge source="GPUS" target="a page in the host memory to be pinned (or page-locked)">
  <data key="d0">require</data>
</edge>
<edge source="DL workloads" target="throughput-intensive training tasks">
  <data key="d0">include</data>
</edge>
<edge source="DL workloads" target="latency-sensitive inference tasks">
  <data key="d0">include</data>
</edge>
<edge source="DL workloads" target="well-defined structures">
  <data key="d0">have</data>
</edge>
<edge source="Inference tasks" target="training clusters under ash crowds">
  <data key="d0">cannot be served with</data>
</edge>
<edge source="Inference tasks" target="short">
  <data key="d0">are</data>
</edge>
<edge source="Inference tasks" target="preempted">
  <data key="d0">are not</data>
</edge>
<edge source="Training tasks" target="inference clusters when the inference load is low">
  <data key="d0">cannot utilize</data>
</edge>
<edge source="Training tasks" target="their models to the host memory">
  <data key="d0">periodically checkpoint</data>
</edge>
<edge source="Training tasks" target="the latest checkpoint after preemption">
  <data key="d0">restart from</data>
</edge>
<edge source="training cluster" target="training tasks for inference tasks">
  <data key="d0">cannot preempt</data>
</edge>
<edge source="Inference clusters" target="the peak load">
  <data key="d0">are often over-provisioned for</data>
</edge>
<edge source="Inference clusters" target="strict service level objectives (SLOs)">
  <data key="d0">are over-provisioned in order to meet</data>
</edge>
<edge source="Inference clusters" target="high utilization">
  <data key="d0">are not always running at</data>
</edge>
<edge source="Inference clusters" target="training">
  <data key="d0">cannot be utilized by</data>
</edge>
<edge source="production systems" target="each application on per-GPU granularity">
  <data key="d0">are provisioned to</data>
</edge>
<edge source="production systems" target="GPUs to applications on per-GPU granularity">
  <data key="d0">allocate</data>
</edge>
<edge source="production systems" target="in order to limit the interference between different applications">
  <data key="d0">allocate GPUs</data>
</edge>
<edge source="production systems" target="in order to satisfy the SLO requirements">
  <data key="d0">allocate GPUs</data>
</edge>
<edge source="provisioning on per-GPU granularity" target="the interference between applications">
  <data key="d0">limits</data>
</edge>
<edge source="binding GPUs" target="the VMs, containers or processes of an application">
  <data key="d0">is done to</data>
</edge>
<edge source="multiple DL applications" target="the same GPU server">
  <data key="d0">should be able to be packed to</data>
</edge>
<edge source="packing multiple DL applications to the same GPU server" target="GPU utilization via time-sharing">
  <data key="d0">maximizes</data>
</edge>
<edge source="Operating systems" target="high CPU utilization via task scheduling and context switching">
  <data key="d0">achieve</data>
</edge>
<edge source="THE IDEA OF NE-GRAINED CPU TIME-SHARING" target="CLUSTER SCHEDULING">
  <data key="d0">has been further extended to</data>
</edge>
<edge source="NE-GRAINED TIME-SHARING" target="better utilization than provisioning dedicated resources">
  <data key="d0">can provide</data>
</edge>
<edge source="NE-GRAINED TIME-SHARING" target="necessary process-level isolation">
  <data key="d0">provides</data>
</edge>
<edge source="NE-GRAINED TIME-SHARING" target="CPU workloads">
  <data key="d0">is similar to</data>
</edge>
<edge source="NE-GRAINED SCHEDULING CYCLES" target="enabled">
  <data key="d0">are</data>
</edge>
<edge source="Google Borg" target="online services and batch jobs">
  <data key="d0">packs</data>
</edge>
<edge source="Google Borg" target="20-30 machines">
  <data key="d0">saves</data>
</edge>
<edge source="20-30 machines" target="not packing them">
  <data key="d0">are saved compared with</data>
</edge>
<edge source="GPU" target="high overhead when switching between tasks">
  <data key="d0">has</data>
</edge>
<edge source="GPU" target="the models either for training or inference">
  <data key="d0">can quickly context-switch between</data>
</edge>
<edge source="GPU" target="high memory bandwidth">
  <data key="d0">has</data>
</edge>
<edge source="GPU" target="NVIDIA T4">
  <data key="d0">is</data>
</edge>
<edge source="THE GAP" target="THE PRECIOUS GPU MEMORY AND SLOW SWITCHING">
  <data key="d0">is about</data>
</edge>
<edge source="Naively using GPUs in the same way as CPUs" target="the requirements of DL inference that have strict SLOs in the range of tens to hundreds of milliseconds">
  <data key="d0">will not satisfy</data>
</edge>
<edge source="A GPU" target="a DNN model (e.g., ResNet)">
  <data key="d0">switches to</data>
</edge>
<edge source="A GPU" target="its own memory management system">
  <data key="d0">has</data>
</edge>
<edge source="A GPU" target="a malloc function">
  <data key="d0">provides</data>
</edge>
<edge source="The DNN model (e.g., ResNet)" target="the GPU">
  <data key="d0">has not been preloaded onto</data>
</edge>
<edge source="State-of-the-art tricks like CUDA unified memory" target="multiple seconds delay">
  <data key="d0">do not prevent</data>
</edge>
<edge source="CPU applications" target="milliseconds or even microseconds">
  <data key="d0">can be switched in</data>
</edge>
<edge source="the existing solution" target="spatially share the GPU memory">
  <data key="d0">is to</data>
</edge>
<edge source="this approach" target="strong GPU memory isolation between applications">
  <data key="d0">does not provide</data>
</edge>
<edge source="NVIDIA Multiple Process Sharing (MPS) 6" target="multiple processes to use the same GPU">
  <data key="d0">allow</data>
</edge>
<edge source="NVIDIA Multiple Process Sharing (MPS) 6" target="all processes data (e.g., DNN models) to be preloaded into the GPU memory">
  <data key="d0">require</data>
</edge>
<edge source="Salus 7" target="multiple processes to use the same GPU">
  <data key="d0">allow</data>
</edge>
<edge source="Salus 7" target="all processes data (e.g., DNN models) to be preloaded into the GPU memory">
  <data key="d0">require</data>
</edge>
<edge source="multi-process support from NVIDIA" target="the inference process to share the GPU with the training process">
  <data key="d0">allows</data>
</edge>
<edge source="NVIDIA MPS 6" target="official support for sharing a GPU between multiple processes">
  <data key="d0">provides</data>
</edge>
<edge source="GPU memory" target="much more limited than host memory">
  <data key="d0">is</data>
</edge>
<edge source="GPU memory" target="preload many applications">
  <data key="d0">cannot</data>
</edge>
<edge source="GPU memory" target="very limited even on high-end GPUs">
  <data key="d0">is</data>
</edge>
<edge source="GPU memory" target="task execution">
  <data key="d0">is purposed for</data>
</edge>
<edge source="GPU memory" target="storing the state of idle applications">
  <data key="d0">is not purposed for</data>
</edge>
<edge source="GPU memory" target="PipeSwitch">
  <data key="d0">is managed by</data>
</edge>
<edge source="one single memory-intensive training task" target="all the GPU memory">
  <data key="d0">may consume</data>
</edge>
<edge source="THE TRAINING TASK" target="ITS GPU ENVIRONMENT">
  <data key="d0">stops and cleans</data>
</edge>
<edge source="THE TRAINING TASK" target="the GPU MEMORY">
  <data key="d0">frees</data>
</edge>
<edge source="THE TRAINING TASK" target="THE ENTIRE GPU MEMORY">
  <data key="d0">occupies</data>
</edge>
<edge source="THE TRAINING TASK" target="WHEN INFERENCE TASKS COME">
  <data key="d0">does not stop</data>
</edge>
<edge source="models" target="larger">
  <data key="d0">are getting</data>
</edge>
<edge source="models" target="different structures">
  <data key="d0">have</data>
</edge>
<edge source="request batching" target="to increase throughput">
  <data key="d0">is used</data>
</edge>
<edge source="Request batching" target="the GPU memory requirement of inference applications">
  <data key="d0">increases</data>
</edge>
<edge source="A context switching design" target="the switching overhead">
  <data key="d0">minimizes</data>
</edge>
<edge source="A context switching design" target="the contents on GPU memory">
  <data key="d0">quickly switches</data>
</edge>
<edge source="A context switching design" target="a better approach for efficiently time-sharing GPUs">
  <data key="d0">is</data>
</edge>
<edge source="no existing solution" target="such context switching abstraction for GPU">
  <data key="d0">offers</data>
</edge>
<edge source="pipelined context switching" target="the characteristics of DL applications">
  <data key="d0">exploits</data>
</edge>
<edge source="pipelined context switching" target="millisecond-scale overhead for switching tasks on GPUs">
  <data key="d0">achieves</data>
</edge>
<edge source="pipelined context switching" target="task switching overhead on GPUs for DL applications">
  <data key="d0">minimizes</data>
</edge>
<edge source="pipelined context switching" target="pipelined model transmission">
  <data key="d0">leverages</data>
</edge>
<edge source="pipelined context switching" target="unified memory management">
  <data key="d0">leverages</data>
</edge>
<edge source="pipelined context switching" target="active-standby worker switching">
  <data key="d0">leverages</data>
</edge>
<edge source="pipelined context switching" target="switching overhead">
  <data key="d0">minimizes</data>
</edge>
<edge source="pipelined context switching" target="process-level isolation">
  <data key="d0">enforces</data>
</edge>
<edge source="pipeline" target="computation and GPU memory swapping">
  <data key="d0">overlaps</data>
</edge>
<edge source="pipeline" target="fast context switching">
  <data key="d0">enables</data>
</edge>
<edge source="context switching" target="if the application is already loaded in the GPU">
  <data key="d0">is not needed</data>
</edge>
<edge source="DNN models" target="host memory">
  <data key="d0">can be held in</data>
</edge>
<edge source="DNN models" target="a layered structure">
  <data key="d0">have</data>
</edge>
<edge source="DNN models" target="a layer-by-layer computation pattern">
  <data key="d0">have</data>
</edge>
<edge source="DNN models" target="usually deep">
  <data key="d0">are</data>
</edge>
<edge source="DNN models" target="multiple layers stacking one on another">
  <data key="d0">consist of</data>
</edge>
<edge source="DNN models" target="only once in the memory daemon">
  <data key="d0">are stored</data>
</edge>
<edge source="DNN models" target="in every worker">
  <data key="d0">are not stored</data>
</edge>
<edge source="host memory" target="much larger and cheaper than GPU memory">
  <data key="d0">is</data>
</edge>
<edge source="Enterprises" target="GPU clusters">
  <data key="d0">build</data>
</edge>
<edge source="M. Jeon, S. Venkataraman, A. Phanishayee, J. Qian, W. Xiao, and F. Yang" target="Analysis of large-scale multi-tenant GPU clusters for DNN training workloads">
  <data key="d0">authored</data>
</edge>
<edge source="Analysis of large-scale multi-tenant GPU clusters for DNN training workloads" target="USENIX ATC, 2019">
  <data key="d0">was published in</data>
</edge>
<edge source="512 14th USENIX Symposium on Operating Systems Design and Implementation" target="USENIX Association">
  <data key="d0">is organized by</data>
</edge>
<edge source="the number of applications that can be multiplexed" target="the GPU memory size">
  <data key="d0">is not limited by</data>
</edge>
<edge source="each application" target="the entire GPU compute and memory resources during its time slice">
  <data key="d0">is able to use</data>
</edge>
<edge source="small switching overhead" target="DL applications to satisfy strict SLO requirements">
  <data key="d0">is critical for</data>
</edge>
<edge source="millisecond-scale task switching overhead" target="SLO requirements">
  <data key="d0">is to satisfy</data>
</edge>
<edge source="measurement study" target="the task switching overhead">
  <data key="d0">probes</data>
</edge>
<edge source="the measurement study" target="probe the task switching overhead">
  <data key="d0">aims to</data>
</edge>
<edge source="switching overhead" target="four components">
  <data key="d0">is divided into</data>
</edge>
<edge source="four components" target="old task cleaning">
  <data key="d0">are</data>
</edge>
<edge source="four components" target="new task initialization">
  <data key="d0">are</data>
</edge>
<edge source="four components" target="GPU memory allocation">
  <data key="d0">are</data>
</edge>
<edge source="four components" target="model transmission via PCIe from CPU to GPU">
  <data key="d0">are</data>
</edge>
<edge source="INSTANCE TYPE" target="G4DN.2XLARGE">
  <data key="d0">is</data>
</edge>
<edge source="INSTANCE TYPE" target="P3.2XLARGE">
  <data key="d0">includes</data>
</edge>
<edge source="G4DN.2XLARGE" target="NVIDIA T4">
  <data key="d0">has GPU</data>
</edge>
<edge source="G4DN.2XLARGE" target="3.0 8">
  <data key="d0">has PCIE version</data>
</edge>
<edge source="G4DN.2XLARGE" target="3.0">
  <data key="d0">has PCI version</data>
</edge>
<edge source="G4DN.2XLARGE" target="8">
  <data key="d0">has PCIe lanes</data>
</edge>
<edge source="G4DN.2XLARGE" target="NVIDIA T4, PCIE 3.0 x8">
  <data key="d0">contains</data>
</edge>
<edge source="P3.2XLARGE" target="8 vCPUs (Intel Xeon E5-2686 v4)">
  <data key="d0">is configured with</data>
</edge>
<edge source="P3.2XLARGE" target="1 GPU (NVIDIA V100 with 16 GB GPU memory)">
  <data key="d0">has</data>
</edge>
<edge source="P3.2XLARGE" target="PCIE 3.0 16">
  <data key="d0">uses</data>
</edge>
<edge source="P3.2XLARGE" target="61 GB memory">
  <data key="d0">has</data>
</edge>
<edge source="P3.2XLARGE" target="NVIDIA V100, PCIE 3.0 16">
  <data key="d0">has GPU</data>
</edge>
<edge source="P3.2XLARGE" target="NVIDIA V100">
  <data key="d0">equipped with</data>
</edge>
<edge source="P3.2XLARGE" target="3.0 16">
  <data key="d0">has PCIE version</data>
</edge>
<edge source="GPU TYPE of G4DN.2XLARGE" target="NVIDIA T4">
  <data key="d0">is</data>
</edge>
<edge source="NVIDIA T4" target="comparable performance with NVIDIA V100">
  <data key="d0">has</data>
</edge>
<edge source="NVIDIA T4" target="16GB GPU memory">
  <data key="d0">has</data>
</edge>
<edge source="NVIDIA T4" target="8.1 TFLOPS (single-precision)">
  <data key="d0">has</data>
</edge>
<edge source="NVIDIA T4" target="PCIE 3.0 x8">
  <data key="d0">uses interface</data>
</edge>
<edge source="GPU TYPE of P3.2XLARGE" target="NVIDIA V100">
  <data key="d0">is</data>
</edge>
<edge source="NVIDIA V100" target="up to 32GB GPU memory">
  <data key="d0">has</data>
</edge>
<edge source="NVIDIA V100" target="15.7 TFLOPS (single-precision)">
  <data key="d0">has</data>
</edge>
<edge source="NVIDIA V100" target="PCIE 3.0 16">
  <data key="d0">connects via</data>
</edge>
<edge source="TASK CLEANING time on G4DN.2XLARGE" target="155 MS">
  <data key="d0">is</data>
</edge>
<edge source="TASK CLEANING time on P3.2XLARGE" target="165 MS">
  <data key="d0">is</data>
</edge>
<edge source="TASK INITIALIZATION time on G4DN.2XLARGE" target="5530 MS">
  <data key="d0">is</data>
</edge>
<edge source="TASK INITIALIZATION time on P3.2XLARGE" target="7290 MS">
  <data key="d0">is</data>
</edge>
<edge source="MEMORY ALLOCATION time on G4DN.2XLARGE" target="10 MS">
  <data key="d0">is</data>
</edge>
<edge source="MEMORY ALLOCATION time on P3.2XLARGE" target="13 MS">
  <data key="d0">is</data>
</edge>
<edge source="MODEL TRANSMISSION time on G4DN.2XLARGE" target="91 MS">
  <data key="d0">is</data>
</edge>
<edge source="MODEL TRANSMISSION time on P3.2XLARGE" target="81 MS">
  <data key="d0">is</data>
</edge>
<edge source="TOTAL OVERHEAD on G4DN.2XLARGE" target="5787 MS">
  <data key="d0">is</data>
</edge>
<edge source="TOTAL OVERHEAD on P3.2XLARGE" target="7551 MS">
  <data key="d0">is</data>
</edge>
<edge source="INFERENCE TIME on G4DN.2XLARGE" target="105 MS">
  <data key="d0">is</data>
</edge>
<edge source="INFERENCE TIME on P3.2XLARGE" target="32 MS">
  <data key="d0">is</data>
</edge>
<edge source="every component" target="a considerable amount of time">
  <data key="d0">takes</data>
</edge>
<edge source="time" target="tens of milliseconds to seconds">
  <data key="d0">varies from</data>
</edge>
<edge source="inference task" target="tens of milliseconds on a GPU">
  <data key="d0">takes</data>
</edge>
<edge source="latency SLOs" target="a small multiple of the inference time">
  <data key="d0">are</data>
</edge>
<edge source="one source of the overhead" target="the contentions both on the computation and memory of the GPU">
  <data key="d0">is</data>
</edge>
<edge source="the training task" target="when an inference task comes">
  <data key="d0">do not stop</data>
</edge>
<edge source="Our design" target="a key observation">
  <data key="d0">is based on</data>
</edge>
<edge source="computation of DNN models" target="layer by layer">
  <data key="d0">takes place</data>
</edge>
<edge source="there" target="the entire model to be transmitted to the GPU before starting computation">
  <data key="d0">is no need to wait for</data>
</edge>
<edge source="A task" target="the entire model to be transmitted to the GPU before beginning the computation">
  <data key="d0">does not need to wait for</data>
</edge>
<edge source="Naive pipelining on per-layer granularity" target="high overhead on tensor transmission and synchronization">
  <data key="d0">introduces</data>
</edge>
<edge source="Pipelining on per-layer granularity" target="synchronization for every layer">
  <data key="d0">requires</data>
</edge>
<edge source="algorithm" target="the best grouping strategy for a given model">
  <data key="d0">finds</data>
</edge>
<edge source="The computation of a DL task" target="layer by layer">
  <data key="d0">is</data>
</edge>
<edge source="The computation of a DL task" target="a simple, regular pattern for memory allocation">
  <data key="d0">has</data>
</edge>
<edge source="A DL task" target="two important types of data in the GPU memory">
  <data key="d0">stores</data>
</edge>
<edge source="two important types of data" target="the DNN model (including the model parameters) and the intermediate results">
  <data key="d0">are</data>
</edge>
<edge source="The default general-purpose GPU memory management (e.g., CUDA Unified Memory 4)" target="an overkill">
  <data key="d0">is</data>
</edge>
<edge source="The default general-purpose GPU memory management (e.g., CUDA Unified Memory 4)" target="unnecessary overhead">
  <data key="d0">incurs</data>
</edge>
<edge source="NVIDIA" target="CUDA Unified Memory 4">
  <data key="d0">provides</data>
</edge>
<edge source="NVIDIA" target="developer.nvidia.com/nccl">
  <data key="d0">has website</data>
</edge>
<edge source="CUDA Unified Memory 4" target="memory movement between the host memory and the GPU memory">
  <data key="d0">handles</data>
</edge>
<edge source="memory movement" target="applications">
  <data key="d0">is handled automatically for</data>
</edge>
<edge source="dedicated memory daemon" target="the overhead">
  <data key="d0">minimizes</data>
</edge>
<edge source="unified memory management with the memory daemon" target="minimal memory footprint">
  <data key="d0">achieve</data>
</edge>
<edge source="unified memory management with the memory daemon" target="extra memory copies">
  <data key="d0">eliminate</data>
</edge>
<edge source="THE DAEMON" target="THE GPU MEMORY">
  <data key="d0">pre-allocates</data>
</edge>
<edge source="THE DAEMON" target="IT TO EACH TASK">
  <data key="d0">re-allocates</data>
</edge>
<edge source="THE DAEMON" target="THE EXPENSIVE GPU MEMORY MANAGER">
  <data key="d0">does not involve</data>
</edge>
<edge source="THE DAEMON" target="that each time only one worker owns the GPU memory">
  <data key="d0">ensures</data>
</edge>
<edge source="the memory daemon" target="cudamalloc to obtain the GPU memory when the system starts">
  <data key="d0">uses</data>
</edge>
<edge source="the memory daemon" target="the memory to the workers at runtime">
  <data key="d0">allocates</data>
</edge>
<edge source="the memory daemon" target="the GPU memory manager">
  <data key="d0">does not replace</data>
</edge>
<edge source="the memory daemon" target="the existing system">
  <data key="d0">is compatible with</data>
</edge>
<edge source="the memory daemon" target="minimal changes">
  <data key="d0">incurs</data>
</edge>
<edge source="the memory daemon" target="the worker">
  <data key="d0">needs to notify</data>
</edge>
<edge source="the memory daemon" target="the relevant GPU memory handlers to the worker">
  <data key="d0">needs to export</data>
</edge>
<edge source="the memory daemon" target="the same order to transmit the model">
  <data key="d0">needs to use</data>
</edge>
<edge source="storing DNN models only once in the memory daemon" target="memory footprint">
  <data key="d0">minimizes</data>
</edge>
<edge source="memory allocation for a DNN model" target="deterministic">
  <data key="d0">is</data>
</edge>
<edge source="deterministic memory allocation" target="extra memory copies between the daemon and the workers">
  <data key="d0">eliminates</data>
</edge>
<edge source="deterministic memory allocation" target="the IPC overhead">
  <data key="d0">reduces</data>
</edge>
<edge source="No unified memory management" target="each worker to keep a copy for each DNN model">
  <data key="d0">requires</data>
</edge>
<edge source="Keeping a copy for each DNN model" target="the memory footprint">
  <data key="d0">increases</data>
</edge>
<edge source="each server" target="an active worker">
  <data key="d0">contains</data>
</edge>
<edge source="each server" target="multiple standby workers">
  <data key="d0">contains</data>
</edge>
<edge source="A SERVER" target="ONE OR MORE STANDBY WORKERS">
  <data key="d0">HAS</data>
</edge>
<edge source="THE ACTIVE WORKER" target="THE CURRENT TASK ON THE GPU">
  <data key="d0">executes</data>
</edge>
<edge source="THE ACTIVE WORKER" target="THE WORKER THAT CURRENTLY EXECUTES A TASK IN THE GPU">
  <data key="d0">is</data>
</edge>
<edge source="THE STANDBY WORKERS" target="ON THE CPU">
  <data key="d0">stay</data>
</edge>
<edge source="THE STANDBY WORKERS" target="THE NEXT TASK">
  <data key="d0">wait for</data>
</edge>
<edge source="worker" target="a process that executes tasks on one GPU">
  <data key="d0">is</data>
</edge>
<edge source="the active worker" target="the current task">
  <data key="d0">completes or stops</data>
</edge>
<edge source="the current task" target="a training task">
  <data key="d0">is</data>
</edge>
<edge source="the controller" target="the memory daemon">
  <data key="d0">notifies</data>
</edge>
<edge source="the controller" target="the standby worker">
  <data key="d0">notifies</data>
</edge>
<edge source="the controller" target="the current task">
  <data key="d0">can preempt</data>
</edge>
<edge source="the controller" target="the current task to finish if it is inference">
  <data key="d0">waits for</data>
</edge>
<edge source="the controller" target="the current task by notifying the active worker to stop if it is training">
  <data key="d0">preempts</data>
</edge>
<edge source="the controller" target="a task">
  <data key="d0">schedules</data>
</edge>
<edge source="the controller" target="the current active worker to stop">
  <data key="d0">will notify</data>
</edge>
<edge source="the controller" target="the GPU memory allocated to the current active worker">
  <data key="d0">deletes</data>
</edge>
<edge source="the controller" target="the GPU memory to the new active worker">
  <data key="d0">allocates</data>
</edge>
<edge source="the controller" target="the parameters of the new model to the GPU">
  <data key="d0">transfers</data>
</edge>
<edge source="the controller" target="receiving the current active worker's reply">
  <data key="d0">transfers the parameters after</data>
</edge>
<edge source="the controller" target="only one of these CUDA streams is active">
  <data key="d0">guarantees</data>
</edge>
<edge source="the controller" target="the model from the disk to the CPU memory">
  <data key="d0">loads</data>
</edge>
<edge source="the controller" target="the worker to start computing the corresponding layers">
  <data key="d0">notifies</data>
</edge>
<edge source="the memory daemon and the standby worker" target="the task to GPU">
  <data key="d0">load</data>
</edge>
<edge source="the task" target="pipelined model transmission (4.2)">
  <data key="d0">is executed with</data>
</edge>
<edge source="the task" target="the computation of a layer as soon as the layer is loaded in the GPU and the input of the layer is ready">
  <data key="d0">can start</data>
</edge>
<edge source="the task" target="regardless of its following layers">
  <data key="d0">can start computation</data>
</edge>
<edge source="OUR MECHANISM" target="OLD TASK CLEANING IN THE ACTIVE WORKER AND NEW TASK INITIALIZATION IN THE STANDBY WORKER">
  <data key="d0">parallelizes</data>
</edge>
<edge source="OUR MECHANISM" target="WORKER SWITCHING OVERHEAD">
  <data key="d0">aims to minimize</data>
</edge>
<edge source="Table 2" target="worker switching mechanisms">
  <data key="d0">compares</data>
</edge>
<edge source="active and standby worker switching mechanism" target="the overhead of both task cleaning and task initialization">
  <data key="d0">hides</data>
</edge>
<edge source="active and standby worker switching mechanism" target="process-level isolation">
  <data key="d0">ensures</data>
</edge>
<edge source="fast task switching" target="more flexible fine-grained scheduling">
  <data key="d0">enables</data>
</edge>
<edge source="Pipelining" target="a canonical technique">
  <data key="d0">is</data>
</edge>
<edge source="Pipelining" target="computer systems">
  <data key="d0">is widely used in</data>
</edge>
<edge source="Pipelining" target="system performance">
  <data key="d0">improves</data>
</edge>
<edge source="Pipelining" target="resource utilization">
  <data key="d0">maximizes</data>
</edge>
<edge source="PIPELINING" target="TWO SOURCES OF SYSTEM OVERHEADS">
  <data key="d0">BRINGS</data>
</edge>
<edge source="Prior work in DL systems such as Pipedream 8 and Bytescheduler 9" target="pipelining to distributed training">
  <data key="d0">has applied</data>
</edge>
<edge source="These solutions" target="inter-batch pipelining">
  <data key="d0">focus on</data>
</edge>
<edge source="These solutions" target="PipeSwitch">
  <data key="d0">are complementary to</data>
</edge>
<edge source="Inter-batch pipelining" target="computation and gradient transmission of different batches">
  <data key="d0">overlaps</data>
</edge>
<edge source="Computation and gradient transmission" target="training workloads of the same DNN model">
  <data key="d0">are for</data>
</edge>
<edge source="intra-batch pipelining" target="model transmission and computation">
  <data key="d0">overlaps</data>
</edge>
<edge source="overlapping model transmission and computation" target="the overhead of switching between different DNN models">
  <data key="d0">reduces</data>
</edge>
<edge source="different DNN models" target="either inference or training">
  <data key="d0">can be</data>
</edge>
<edge source="new techniques" target="training">
  <data key="d0">support</data>
</edge>
<edge source="new techniques" target="inference that has strict SLOs">
  <data key="d0">support</data>
</edge>
<edge source="training" target="inference">
  <data key="d0">has no sharing with</data>
</edge>
<edge source="pipelined model transmission" target="keeping all other components of PipeSwitch the same and comparing mechanisms discussed in 4.2">
  <data key="d0">is evaluated by</data>
</edge>
<edge source="Pipelined context switching" target="three key techniques">
  <data key="d0">includes</data>
</edge>
<edge source="three key techniques" target="pipelined model transmission">
  <data key="d0">are</data>
</edge>
<edge source="three key techniques" target="unified memory management">
  <data key="d0">are</data>
</edge>
<edge source="three key techniques" target="active-standby worker switching">
  <data key="d0">are</data>
</edge>
<edge source="more flexible fine-grained scheduling" target="GPU utilization for dynamic workloads">
  <data key="d0">improves</data>
</edge>
<edge source="500 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">is organized by</data>
</edge>
<edge source="USENIX ASSOCIATION" target="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">hosts</data>
</edge>
<edge source="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">is organized by</data>
</edge>
<edge source="shared cluster" target="dedicated cluster for each user">
  <data key="d0">is compared to</data>
</edge>
<edge source="14th USENIX Symposium on Operating Systems Design and Implementation" target="Kubernetes">
  <data key="d0">includes topic</data>
</edge>
<edge source="14th USENIX Symposium on Operating Systems Design and Implementation" target="throughput measurements">
  <data key="d0">includes</data>
</edge>
<edge source="514" target="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">is page number of</data>
</edge>
<edge source="the main reason" target="to bring down the cost">
  <data key="d0">is</data>
</edge>
<edge source="The demand of training" target="well predictable">
  <data key="d0">is not</data>
</edge>
<edge source="The demand of training" target="the progress of different developers">
  <data key="d0">would depend on</data>
</edge>
<edge source="an inference task for a particular application" target="a daily periodical pattern based on the application usage">
  <data key="d0">has</data>
</edge>
<edge source="patterns" target="across different tasks">
  <data key="d0">can vary</data>
</edge>
<edge source="A shared cluster" target="the resource utilization via time-sharing">
  <data key="d0">would increase</data>
</edge>
<edge source="shared clusters" target="training and inference">
  <data key="d0">are not shared between</data>
</edge>
<edge source="GPUs designed for inference tasks" target="too wimpy for training tasks">
  <data key="d0">might be</data>
</edge>
<edge source="The arrival of new GPU hardware" target="this">
  <data key="d0">has started to change</data>
</edge>
<edge source="new algorithms and systems for distributed training" target="multiple GPUs to accelerate training">
  <data key="d0">enable</data>
</edge>
<edge source="OUR INDUSTRY COLLABORATOR" target="A LEADING ONLINE SERVICE PROVIDER">
  <data key="d0">is</data>
</edge>
<edge source="OUR INDUSTRY COLLABORATOR" target="THIS OBSERVATION">
  <data key="d0">confirms</data>
</edge>
<edge source="THIS SERVICE PROVIDER" target="more than 10K V100 GPUs for training">
  <data key="d0">runs</data>
</edge>
<edge source="THIS SERVICE PROVIDER" target="at least 5 as many T4 GPUs for inference">
  <data key="d0">runs</data>
</edge>
<edge source="computation power on both sides" target="the same order of magnitude">
  <data key="d0">is within</data>
</edge>
<edge source="inference workload" target="number of active users">
  <data key="d0">fluctuates in correlation with</data>
</edge>
<edge source="inference workload" target="clear peaks and valleys within each day">
  <data key="d0">shows</data>
</edge>
<edge source="peak demand during daytime" target="2 times the valley at midnight">
  <data key="d0">is</data>
</edge>
<edge source="inference GPUs" target="during less busy times">
  <data key="d0">would be utilized</data>
</edge>
<edge source="inference GPUs" target="training models">
  <data key="d0">would be utilized for</data>
</edge>
<edge source="training models" target="daily updates with latest data">
  <data key="d0">require</data>
</edge>
<edge source="A good example" target="ne-tune BERT using daily news">
  <data key="d0">is to</data>
</edge>
<edge source="BORG-LIKE 1 SYSTEMS FOR GPUS" target="great opportunity in improving GPU utilization">
  <data key="d0">means</data>
</edge>
<edge source="Inference and training workloads" target="complementary usage patterns">
  <data key="d0">have</data>
</edge>
<edge source="Inference loads on different models" target="different patterns">
  <data key="d0">have</data>
</edge>
<edge source="Different patterns" target="time sharing">
  <data key="d0">benefit from</data>
</edge>
<edge source="any server" target="any task">
  <data key="d0">would be able to run</data>
</edge>
<edge source="any server" target="low overhead to switch between different applications">
  <data key="d0">would have</data>
</edge>
<edge source="A modern server" target="several TB of host memory">
  <data key="d0">can be equipped with</data>
</edge>
<edge source="Several TB of host memory" target="it to load many applications">
  <data key="d0">enables</data>
</edge>
<edge source="Task execution on GPUs" target="GPU memory">
  <data key="d0">requires</data>
</edge>
<edge source="T4 GPU" target="16 GB GPU memory">
  <data key="d0">has</data>
</edge>
<edge source="V100 GPU" target="32 GB GPU memory">
  <data key="d0">has</data>
</edge>
<edge source="DL tasks, especially training" target="a large amount, or even all of the memory on a GPU">
  <data key="d0">require</data>
</edge>
<edge source="DL applications" target="large models">
  <data key="d0">have</data>
</edge>
<edge source="DL applications" target="large amounts of intermediate results">
  <data key="d0">generate</data>
</edge>
<edge source="large models" target="hundreds of layers">
  <data key="d0">can have</data>
</edge>
<edge source="large amounts of intermediate results" target="a lot of GPU memory">
  <data key="d0">require</data>
</edge>
<edge source="SALUS 7" target="training tasks which are memory-intensive">
  <data key="d0">cannot support</data>
</edge>
<edge source="SALUS 7" target="multiple inference tasks which have large models">
  <data key="d0">cannot support</data>
</edge>
<edge source="SALUS 7" target="because it requires the models to be preloaded to the GPU">
  <data key="d0">is not directly comparable</data>
</edge>
<edge source="SALUS 7" target="several limitations described in 2.2">
  <data key="d0">has</data>
</edge>
<edge source="state-of-the-art models" target="deeper and larger">
  <data key="d0">are getting</data>
</edge>
<edge source="idle applications" target="large memory space">
  <data key="d0">can occupy</data>
</edge>
<edge source="the active application" target="the entire GPU memory for its purpose">
  <data key="d0">should be able to utilize</data>
</edge>
<edge source="the number of applications that can be served by a GPU server" target="its host memory size">
  <data key="d0">should be limited by</data>
</edge>
<edge source="switching a task" target="heavy memory swapping">
  <data key="d0">would require</data>
</edge>
<edge source="many online inference workloads" target="strict SLOs">
  <data key="d0">require</data>
</edge>
<edge source="naive memory swapping between the host memory and the GPU memory" target="strict SLOs">
  <data key="d0">cannot meet</data>
</edge>
<edge source="a training task" target="the model parameters">
  <data key="d0">updates</data>
</edge>
<edge source="a training task" target="the DNN structure">
  <data key="d0">does not update</data>
</edge>
<edge source="an inference task" target="a strict latency SLO">
  <data key="d0">has</data>
</edge>
<edge source="an inference task" target="a forward pass from the RST layer to the NAL layer">
  <data key="d0">performs</data>
</edge>
<edge source="an inference task" target="a forward pass to make a prediction">
  <data key="d0">performs</data>
</edge>
<edge source="an inference task" target="only a forward pass in task execution">
  <data key="d0">has</data>
</edge>
<edge source="THE RST INFERENCE BATCH" target="several seconds to finish">
  <data key="d0">would require</data>
</edge>
<edge source="Existing support such as NVIDIA MPS" target="DL workloads">
  <data key="d0">is not optimized for</data>
</edge>
<edge source="Existing support such as NVIDIA MPS" target="hundreds of milliseconds overhead">
  <data key="d0">incurs</data>
</edge>
<edge source="NVIDIA MPS" target="stop-and-start">
  <data key="d0">has lower overhead compared to</data>
</edge>
<edge source="NVIDIA MPS" target="several hundred milliseconds overhead">
  <data key="d0">incurs</data>
</edge>
<edge source="NVIDIA MPS" target="NVIDIA MPS">
  <data key="d0">is</data>
</edge>
<edge source="stop-and-start" target="several seconds for task switching">
  <data key="d0">takes</data>
</edge>
<edge source="several hundred milliseconds overhead" target="MPS from meeting strict SLOs">
  <data key="d0">prevents</data>
</edge>
<edge source="Figure 1" target="PipeSwitch architecture">
  <data key="d0">depicts</data>
</edge>
<edge source="throughput measurements" target="batches per second">
  <data key="d0">measured in</data>
</edge>
<edge source="throughput" target="PIPESWITCH MPS STOP-AND-START">
  <data key="d0">has upper bound</data>
</edge>
<edge source="throughput" target="eight P3.2xlarge instances">
  <data key="d0">measured on</data>
</edge>
<edge source="the structure and computation pattern of DNN models" target="us to highly optimize task switching">
  <data key="d0">allow</data>
</edge>
<edge source="the structure and computation pattern of DNN models" target="us to achieve millisecond-scale overhead">
  <data key="d0">allow</data>
</edge>
<edge source="FIGURE 1" target="the architecture of a PipeSwitch server">
  <data key="d0">shows</data>
</edge>
<edge source="PIPESWITCH PIPELINES" target="transmission and task execution">
  <data key="d0">model</data>
</edge>
<edge source="THIS SERVER" target="four types of components">
  <data key="d0">contains</data>
</edge>
<edge source="four types of components" target="a controller">
  <data key="d0">are</data>
</edge>
<edge source="four types of components" target="a memory daemon">
  <data key="d0">are</data>
</edge>
<edge source="four types of components" target="an active worker">
  <data key="d0">are</data>
</edge>
<edge source="four types of components" target="multiple standby workers">
  <data key="d0">are</data>
</edge>
<edge source="THE CONTROLLER" target="THE CENTRAL COMPONENT">
  <data key="d0">is</data>
</edge>
<edge source="THE CONTROLLER" target="A SET OF TASKS RECEIVED FROM THE CLIENTS">
  <data key="d0">QUEUES</data>
</edge>
<edge source="THE CONTROLLER" target="AN IDLE STANDBY WORKER">
  <data key="d0">NOTICES</data>
</edge>
<edge source="THE CONTROLLER" target="only one active worker">
  <data key="d0">ensures</data>
</edge>
<edge source="the memory daemon and the workers" target="the tasks">
  <data key="d0">execute</data>
</edge>
<edge source="MEMORY DAEMON" target="a daemon">
  <data key="d0">is</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE GPU MEMORY">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE DNN MODELS">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE MEMORY TO THE STANDBY WORKER (4.3)">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE MODEL USED BY THE NEW TASK FROM THE HOST MEMORY TO THE GPU MEMORY">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="THE MEMORY DAEMON" target="GPU MEMORY ALLOCATION">
  <data key="d0">handles</data>
</edge>
<edge source="THE MEMORY DAEMON" target="MODEL TRANSMISSION">
  <data key="d0">handles</data>
</edge>
<edge source="THE MEMORY DAEMON" target="GPU MEMORY HANDLERS">
  <data key="d0">creates</data>
</edge>
<edge source="THE MEMORY DAEMON" target="GPU MEMORY HANDLERS TO WORKERS">
  <data key="d0">sends</data>
</edge>
<edge source="THE MEMORY DAEMON" target="memory pointers to the workers">
  <data key="d0">only needs to pass</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE USAGE OF EXPENSIVE GPU IPCS">
  <data key="d0">can minimize</data>
</edge>
<edge source="THE SERVER" target="THE DNN MODELS IN THE HOST MEMORY">
  <data key="d0">STORES</data>
</edge>
<edge source="all components" target="the SLOs">
  <data key="d0">should be optimized to meet</data>
</edge>
<edge source="A STANDBY WORKER" target="idle">
  <data key="d0">is</data>
</edge>
<edge source="A STANDBY WORKER" target="initializing a new task">
  <data key="d0">is</data>
</edge>
<edge source="A STANDBY WORKER" target="cleaning its environment for the previous task">
  <data key="d0">is</data>
</edge>
<edge source="The standby worker" target="the new active worker">
  <data key="d0">becomes</data>
</edge>
<edge source="The new active worker" target="the new task">
  <data key="d0">executes</data>
</edge>
<edge source="the new task" target="wait">
  <data key="d0">needs to</data>
</edge>
<edge source="the new task" target="the GPU environment of the current task">
  <data key="d0">can reuse</data>
</edge>
<edge source="the new task" target="its model to the GPU memory at the same time">
  <data key="d0">can transmit</data>
</edge>
<edge source="The active worker" target="a standby worker">
  <data key="d0">becomes</data>
</edge>
<edge source="The active worker" target="the environment for the previous task">
  <data key="d0">cleans</data>
</edge>
<edge source="a new task" target="a standby worker finishes cleaning a previous task">
  <data key="d0">arrives before</data>
</edge>
<edge source="waiting" target="its startup time">
  <data key="d0">increases</data>
</edge>
<edge source="a scheduling policy" target="which task to execute next">
  <data key="d0">to decide</data>
</edge>
<edge source="The scheduling" target="preemptive">
  <data key="d0">is</data>
</edge>
<edge source="The controller" target="the current task for the next one">
  <data key="d0">can preempt</data>
</edge>
<edge source="The controller" target="the scheduling policy">
  <data key="d0">preempts based on</data>
</edge>
<edge source="canonical scheduling policies" target="RST COME RST SERVE (FCFS)">
  <data key="d0">include</data>
</edge>
<edge source="canonical scheduling policies" target="earliest deadline RST (EDF)">
  <data key="d0">include</data>
</edge>
<edge source="The specific scheduling algorithm" target="orthogonal to this paper">
  <data key="d0">is</data>
</edge>
<edge source="AN IDLE STANDBY WORKER" target="ITS ENVIRONMENT FOR THE NEW TASK">
  <data key="d0">INITIALIZES</data>
</edge>
<edge source="The memory daemon" target="the model from the host memory to the GPU memory">
  <data key="d0">transmits</data>
</edge>
<edge source="Transmitting the model from the host memory to the GPU memory" target="the extra memory copy from the memory daemon to the worker">
  <data key="d0">eliminates</data>
</edge>
<edge source="the worker" target="the model">
  <data key="d0">can access</data>
</edge>
<edge source="the worker" target="its task">
  <data key="d0">can execute</data>
</edge>
<edge source="the worker" target="the user to register the model before starting a task">
  <data key="d0">requires</data>
</edge>
<edge source="the worker" target="the models">
  <data key="d0">can load</data>
</edge>
<edge source="the worker" target="the hooks to wait for parameter transmission or terminate on notification">
  <data key="d0">can add</data>
</edge>
<edge source="the worker" target="the controller">
  <data key="d0">is similar to</data>
</edge>
<edge source="the worker" target="the model structures">
  <data key="d0">loads</data>
</edge>
<edge source="the worker" target="the model parameters">
  <data key="d0">does not load</data>
</edge>
<edge source="the worker" target="the scheduler to transfer required parameters for DNN models">
  <data key="d0">waits for</data>
</edge>
<edge source="the worker" target="inference or training">
  <data key="d0">performs</data>
</edge>
<edge source="the model" target="the GPU">
  <data key="d0">is transmitted to</data>
</edge>
<edge source="THE PRIMARY GOAL OF THIS PAPER" target="a set of techniques based on the characteristics of DL applications to minimize the task switching overhead in this process">
  <data key="d0">is to design</data>
</edge>
<edge source="task switching overhead" target="individual components">
  <data key="d0">is broken down to</data>
</edge>
<edge source="end-to-end experiments" target="the benefits of PipeSwitch">
  <data key="d0">demonstrate</data>
</edge>
<edge source="our design" target="the overhead of each component">
  <data key="d0">minimize</data>
</edge>
<edge source="A server" target="a training task running on the GPU">
  <data key="d0">stops</data>
</edge>
<edge source="A server" target="an inference task">
  <data key="d0">starts</data>
</edge>
<edge source="The DNN model used in the measurement" target="ResNet152 17">
  <data key="d0">is</data>
</edge>
<edge source="THE MEASUREMENT" target="two types of instances on Amazon AWS">
  <data key="d0">covers</data>
</edge>
<edge source="two types of instances on Amazon AWS" target="G4dn.2xlarge with NVIDIA T4">
  <data key="d0">are</data>
</edge>
<edge source="two types of instances on Amazon AWS" target="P3.2xlarge with NVIDIA V100">
  <data key="d0">are</data>
</edge>
<edge source="the inference task" target="the server">
  <data key="d0">has arrived at</data>
</edge>
<edge source="the server" target="one copy of each model in the host memory">
  <data key="d0">needs to keep</data>
</edge>
<edge source="total times to start the inference task on the GPUs" target="5787 ms and 7551 ms, respectively">
  <data key="d0">are</data>
</edge>
<edge source="TASK CLEANING" target="time">
  <data key="d0">takes</data>
</edge>
<edge source="THE INFERENCE TASK" target="its environment">
  <data key="d0">creates and initializes</data>
</edge>
<edge source="THE INFERENCE TASK" target="GPU MEMORY FOR ITS NEURAL NETWORK MODEL">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THE INFERENCE TASK" target="THE MODEL FROM THE HOST MEMORY TO THE GPU MEMORY">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="its environment" target="process launching">
  <data key="d0">includes</data>
</edge>
<edge source="its environment" target="PyTorch CUDA runtime loading">
  <data key="d0">includes</data>
</edge>
<edge source="its environment" target="CUDA context initialization">
  <data key="d0">includes</data>
</edge>
<edge source="inference time on V100" target="inference time on T4">
  <data key="d0">is lower than</data>
</edge>
<edge source="inference time on V100 and inference time on T4" target="total overheads">
  <data key="d0">are significantly lower than</data>
</edge>
<edge source="Lower overhead on T4" target="task switching largely depends on CPU">
  <data key="d0">is because</data>
</edge>
<edge source="G4dn.2xlarge" target="better CPU than P3.2xlarge">
  <data key="d0">is equipped with</data>
</edge>
<edge source="G4dn.2xlarge" target="8 vCPUs (Intel Platinum 8259CL)">
  <data key="d0">is configured with</data>
</edge>
<edge source="G4dn.2xlarge" target="1 GPU (NVIDIA T4 with 16 GB GPU memory)">
  <data key="d0">has</data>
</edge>
<edge source="G4dn.2xlarge" target="PCIe 3.0 8">
  <data key="d0">has</data>
</edge>
<edge source="G4dn.2xlarge" target="32 GB memory">
  <data key="d0">has</data>
</edge>
<edge source="Better CPU on G4dn.2xlarge" target="Intel Platinum 8259CL">
  <data key="d0">is</data>
</edge>
<edge source="CPU on P3.2xlarge" target="Intel Xeon E5-2686 v4">
  <data key="d0">is</data>
</edge>
<edge source="A strawman solution" target="the old task">
  <data key="d0">stops</data>
</edge>
<edge source="A strawman solution" target="the new task">
  <data key="d0">starts</data>
</edge>
<edge source="A strawman solution that simply stops the old task and starts the new task" target="SLOs">
  <data key="d0">would violate</data>
</edge>
<edge source="all the components" target="considerable time compared to the inference time">
  <data key="d0">take</data>
</edge>
<edge source="PCIE bandwidth" target="the physical limit on how fast an arbitrary task can be loaded to the GPU">
  <data key="d0">is</data>
</edge>
<edge source="The computation" target="layer by layer">
  <data key="d0">is performed</data>
</edge>
<edge source="each iteration in a training task" target="a forward pass">
  <data key="d0">performs</data>
</edge>
<edge source="each iteration in a training task" target="a backward pass">
  <data key="d0">performs</data>
</edge>
<edge source="the input of the layer" target="the previous layers have finished their computation">
  <data key="d0">is ready</data>
</edge>
<edge source="FIGURE 2" target="the advantage of pipelining over the strawman solution">
  <data key="d0">illustrates</data>
</edge>
<edge source="pipelining mechanism" target="optimal model-aware grouping in PipeSwitch">
  <data key="d0">has</data>
</edge>
<edge source="MODEL" target="PCIE">
  <data key="d0">transmit over</data>
</edge>
<edge source="MODEL" target="GPU">
  <data key="d0">transmit to</data>
</edge>
<edge source="MODEL" target="PIPESWITCH, MPS, STOP-AND-START, RESNET152, INCEPTIONV3, BERTBASE">
  <data key="d0">includes</data>
</edge>
<edge source="TASK" target="GPU">
  <data key="d0">execute on</data>
</edge>
<edge source="PCIE GPU E0 E1 EN-1 E2 (B)" target="model transmission and task execution">
  <data key="d0">pipeline</data>
</edge>
<edge source="THE EXAMPLE" target="an inference task">
  <data key="d0">shows</data>
</edge>
<edge source="ADDING HOOKS" target="automated">
  <data key="d0">can be</data>
</edge>
<edge source="DNN framework" target="PYTORCH">
  <data key="d0">example</data>
</edge>
<edge source="PYTORCH" target="TENSORS ON THEM">
  <data key="d0">creates</data>
</edge>
<edge source="the basic way for pipelining" target="to pipeline on per-layer granularity">
  <data key="d0">is</data>
</edge>
<edge source="the system" target="the layers to the GPU memory one by one">
  <data key="d0">transmits</data>
</edge>
<edge source="the computation for a layer" target="before the layer is transmitted">
  <data key="d0">is blocked</data>
</edge>
<edge source="ONE" target="the overhead to invoke multiple calls to PCIe to transmit the data">
  <data key="d0">is</data>
</edge>
<edge source="ONE" target="P3.2XLARGE">
  <data key="d0">is</data>
</edge>
<edge source="transmission overhead" target="data size">
  <data key="d0">is dominated by</data>
</edge>
<edge source="dividing the model into many layers and invoking a PCIe call for each layer" target="significant extra overhead">
  <data key="d0">would cause</data>
</edge>
<edge source="the other" target="the synchronization overhead between transmission and computation">
  <data key="d0">is</data>
</edge>
<edge source="the synchronization overhead" target="the computation to know when a layer is ready to compute">
  <data key="d0">is necessary for</data>
</edge>
<edge source="grouping" target="these two sources of overhead">
  <data key="d0">minimize</data>
</edge>
<edge source="grouping" target="model-aware">
  <data key="d0">must be</data>
</edge>
<edge source="pipelining overhead" target="once for each group">
  <data key="d0">is paid</data>
</edge>
<edge source="pipelining overhead" target="instead of each layer">
  <data key="d0">is paid</data>
</edge>
<edge source="GROUPING" target="a trade-off between pipelining efficiency and pipelining overhead">
  <data key="d0">introduces</data>
</edge>
<edge source="using small groups" target="more overlap between transmission and computation">
  <data key="d0">enables</data>
</edge>
<edge source="using small groups" target="more pipelining overhead">
  <data key="d0">pays</data>
</edge>
<edge source="more overlap between transmission and computation" target="pipelining efficiency">
  <data key="d0">improves</data>
</edge>
<edge source="using big groups" target="minimal pipelining overhead">
  <data key="d0">has</data>
</edge>
<edge source="using big groups" target="the chance for overlapping">
  <data key="d0">reduces</data>
</edge>
<edge source="different structures" target="the number of layers">
  <data key="d0">are in terms of</data>
</edge>
<edge source="different structures" target="the size of each layer">
  <data key="d0">are in terms of</data>
</edge>
<edge source="the number of layers" target="both weighted and unweighted layers">
  <data key="d0">includes</data>
</edge>
<edge source="the optimal grouping strategy" target="the total time for the pipeline">
  <data key="d0">minimizes</data>
</edge>
<edge source="two pruning techniques" target="two insights">
  <data key="d0">are based on</data>
</edge>
<edge source="time complexity for enumeration" target="exponential">
  <data key="d0">is</data>
</edge>
<edge source="PCIE GPU" target="LOWER BOUND OF F(GROUP(0, I), I1)">
  <data key="d0">has</data>
</edge>
<edge source="PCIE GPU" target="GROUP(0, I)">
  <data key="d0">contains</data>
</edge>
<edge source="PCIE GPU" target="GROUP(I1, J)">
  <data key="d0">contains</data>
</edge>
<edge source="GROUP(0, I)" target="GROUP(I1, J)">
  <data key="d0">is related to</data>
</edge>
<edge source="GROUP(0, I)" target="Figure 3">
  <data key="d0">is an example in</data>
</edge>
<edge source="J, J1" target="N-1">
  <data key="d0">range</data>
</edge>
<edge source="Case (A)" target="LOWER BOUND CURRENT OPTIMAL TIME">
  <data key="d0">should be pruned if</data>
</edge>
<edge source="cases" target="I to J">
  <data key="d0">group from</data>
</edge>
<edge source="batch" target="I1 to J">
  <data key="d0">at least from layer</data>
</edge>
<edge source="Figure 3" target="two pruning techniques">
  <data key="d0">shows</data>
</edge>
<edge source="GROUP(I1, N-1)" target="Figure 3">
  <data key="d0">is an example in</data>
</edge>
<edge source="N" target="the number of layers">
  <data key="d0">be</data>
</edge>
<edge source="F(B,I)" target="a function">
  <data key="d0">be</data>
</edge>
<edge source="F(B,I)" target="the total time of the optimal grouping strategy from layer I to N-1">
  <data key="d0">returns</data>
</edge>
<edge source="layers 0 to I-1" target="groups represented by B">
  <data key="d0">have formed</data>
</edge>
<edge source="the function" target="the optimal groups from layer I1 to N-1">
  <data key="d0">applies itself recursively</data>
</edge>
<edge source="the function" target="optgroups">
  <data key="d0">updates</data>
</edge>
<edge source="optgroups" target="the current strategy is better">
  <data key="d0">is updated if</data>
</edge>
<edge source="optgroups" target="the best grouping strategy from layer X given B">
  <data key="d0">store</data>
</edge>
<edge source="the group" target="from layer X to I">
  <data key="d0">is formed</data>
</edge>
<edge source="The first group" target="layer 0 to I">
  <data key="d0">contains</data>
</edge>
<edge source="Case I" target="the first group contains layer 0 to I">
  <data key="d0">means</data>
</edge>
<edge source="The optimal grouping strategy" target="the entire model">
  <data key="d0">is for</data>
</edge>
<edge source="THIS FORMULA" target="F(GROUP(0,I),I1)">
  <data key="d0">can be applied recursively to compute</data>
</edge>
<edge source="The RST group" target="too many layers">
  <data key="d0">contains</data>
</edge>
<edge source="The computation of the RST group" target="too much">
  <data key="d0">would be delayed</data>
</edge>
<edge source="The delay" target="compensate the pipeline efficiency">
  <data key="d0">would</data>
</edge>
<edge source="packing multiple layers in a group based on the progress of computation" target="pipeline efficiency">
  <data key="d0">does not affect</data>
</edge>
<edge source="other than the RST group" target="packing multiple layers in a group based on the progress of computation">
  <data key="d0">is exception to</data>
</edge>
<edge source="T(I, J)" target="the transmission time for a group from layer I to J">
  <data key="d0">is</data>
</edge>
<edge source="T(I, J)" target="the size of layer I to J and PCIe bandwidth">
  <data key="d0">is calculated based on</data>
</edge>
<edge source="E(I, J)" target="the execution time for a group from layer I to J">
  <data key="d0">is</data>
</edge>
<edge source="E(I, J)" target="the GPU">
  <data key="d0">is profiled on</data>
</edge>
<edge source="the overhead of invoking multiple calls" target="T(I, J)">
  <data key="d0">is included in</data>
</edge>
<edge source="the lower bound" target="the best case that all the remaining layers are combined in one group for transmission and computation">
  <data key="d0">considers</data>
</edge>
<edge source="the lower bound" target="the current optimal time">
  <data key="d0">is already bigger than</data>
</edge>
<edge source="the computation and communication" target="perfectly overlapped">
  <data key="d0">can be</data>
</edge>
<edge source="its computation" target="right after the computation of the rst group finishes">
  <data key="d0">can happen</data>
</edge>
<edge source="RST group" target="0 to I">
  <data key="d0">is from layer</data>
</edge>
<edge source="RST group" target="all layers from X to N1">
  <data key="d0">contains</data>
</edge>
<edge source="Equation 1" target="enumerate the cases for the PCIE GPU B.delay">
  <data key="d0">is applied recursively to</data>
</edge>
<edge source="Group" target="X to J">
  <data key="d0">is at least from layer</data>
</edge>
<edge source="Group" target="0, 1, ..., X-1">
  <data key="d0">includes</data>
</edge>
<edge source="Lower bound" target="F(B Group(A,I), I1)">
  <data key="d0">is of</data>
</edge>
<edge source="Lower bound" target="min(TRANSTIME, EXECTIME)">
  <data key="d0">is computed by</data>
</edge>
<edge source="Figure 4" target="general case for the two pruning techniques">
  <data key="d0">illustrates</data>
</edge>
<edge source="the transmission" target="the computation of the RST group">
  <data key="d0">finishes no later than</data>
</edge>
<edge source="the least number of layers to group" target="the following equation">
  <data key="d0">can be computed using</data>
</edge>
<edge source="JIS" target="grouping from (I1) to J">
  <data key="d0">is no better than</data>
</edge>
<edge source="JIS" target="higher pipeline overhead">
  <data key="d0">has</data>
</edge>
<edge source="grouping from (I1) to J" target="pipeline efficiency">
  <data key="d0">does not increase</data>
</edge>
<edge source="this algorithm" target="offline to find the strategy">
  <data key="d0">runs</data>
</edge>
<edge source="the resulting strategy" target="PipeSwitch for context switching">
  <data key="d0">is used by</data>
</edge>
<edge source="ALGORITHM 1" target="the pseudo code">
  <data key="d0">shows</data>
</edge>
<edge source="ALGORITHM 1" target="the recursive function FINDOPTGROUPING(B,X)">
  <data key="d0">computes</data>
</edge>
<edge source="ALGORITHM 1" target="optimality for a given list of layers">
  <data key="d0">achieves</data>
</edge>
<edge source="ALGORITHM 1" target="only several seconds to compute an optimal grouping strategy">
  <data key="d0">takes</data>
</edge>
<edge source="B" target="the groups that have already formed">
  <data key="d0">represents</data>
</edge>
<edge source="B" target="one group from layer 0 to I">
  <data key="d0">contains</data>
</edge>
<edge source="B" target="multiple groups formed by previous layers">
  <data key="d0">can contain</data>
</edge>
<edge source="X" target="the rst layer that have not formed a group">
  <data key="d0">is</data>
</edge>
<edge source="the best grouping strategy from layer X given B" target="none (line 2)">
  <data key="d0">is initialized to</data>
</edge>
<edge source="THE ALGORITHM" target="THE SECOND PRUNING INSIGHT">
  <data key="d0">applies</data>
</edge>
<edge source="THE ALGORITHM" target="THE RST GROUP FROM LAYER X (LINE 3-9)">
  <data key="d0">forms</data>
</edge>
<edge source="THE ALGORITHM" target="THE PROBLEM INTO K 1 CASES">
  <data key="d0">DIVIDES</data>
</edge>
<edge source="THE ALGORITHM" target="B.DELAY (LINE 4-9)">
  <data key="d0">is based on</data>
</edge>
<edge source="THE ALGORITHM" target="THE RST INSIGHT">
  <data key="d0">applies</data>
</edge>
<edge source="THE ALGORITHM" target="THE LOWER BOUND">
  <data key="d0">computes</data>
</edge>
<edge source="THE ALGORITHM" target="TWO PRUNING TECHNIQUES">
  <data key="d0">USES</data>
</edge>
<edge source="CASE I" target="THE RST GROUP FROM LAYER X TO XI">
  <data key="d0">FORMS</data>
</edge>
<edge source="I" target="0 TO K">
  <data key="d0">RANGES FROM</data>
</edge>
<edge source="EQUATION 3 AND FIGURE 3(B)" target="this insight with a special example">
  <data key="d0">illustrate</data>
</edge>
<edge source="THE ENUMERATION FOR I" target="the layers from X to J-1 (LINE 11)">
  <data key="d0">can skip</data>
</edge>
<edge source="THE LOWER BOUND" target="THE LOWEST LATENCY WE CAN ACHIEVE FOR AN INFERENCE TASK">
  <data key="d0">is</data>
</edge>
<edge source="THE LOWER BOUND" target="THE AVERAGE LATENCY OF THE READY MODEL">
  <data key="d0">is</data>
</edge>
<edge source="The example in Equation 2 and Figure 3(A)" target="a special case when X is 0">
  <data key="d0">is</data>
</edge>
<edge source="the computation from X" target="both its transmission (i.e., T(X,I)) and the computation of the previous groups (i.e., B.DELAY)">
  <data key="d0">has to wait for</data>
</edge>
<edge source="case I" target="line 18-19">
  <data key="d0">is pruned</data>
</edge>
<edge source="Algorithm 1" target="Optimal Model-Aware Grouping">
  <data key="d0">is</data>
</edge>
<edge source="Algorithm 1" target="1.33 s, 0.18 s, 0.34 s">
  <data key="d0">has task startup overhead</data>
</edge>
<edge source="Function FINDOPTGROUPING" target="B, X">
  <data key="d0">takes parameters</data>
</edge>
<edge source="Function FINDOPTGROUPING" target="OPTGROUPS">
  <data key="d0">returns</data>
</edge>
<edge source="OPTGROUPS" target="0">
  <data key="d0">is initialized to</data>
</edge>
<edge source="OPTGROUPS" target="CURGROUPS">
  <data key="d0">is assigned</data>
</edge>
<edge source="OPTGROUPS.TIME" target="0">
  <data key="d0">is initialized to</data>
</edge>
<edge source="Layer I" target="X to N1">
  <data key="d0">ranges from</data>
</edge>
<edge source="If T(X,I)" target="B.DELAY">
  <data key="d0">is less than or equal to</data>
</edge>
<edge source="J" target="I">
  <data key="d0">is assigned</data>
</edge>
<edge source="Else" target="Break">
  <data key="d0">action</data>
</edge>
<edge source="TRANSTIME" target="T(X,I) + T(I+1,N1)">
  <data key="d0">is calculated as</data>
</edge>
<edge source="EXECTIME" target="max(T(X,I), B.DELAY)">
  <data key="d0">is calculated as</data>
</edge>
<edge source="If LOWERBOUND" target="OPTGROUPS.TIME">
  <data key="d0">is greater than or equal to</data>
</edge>
<edge source="FirstGroup" target="GROUP(X,I)">
  <data key="d0">is assigned</data>
</edge>
<edge source="RestGroups" target="FINDOPTGROUPING(B, FirstGroup, I+1)">
  <data key="d0">is assigned</data>
</edge>
<edge source="CurGroups" target="FirstGroup + RestGroups">
  <data key="d0">is assigned</data>
</edge>
<edge source="If CURGROUPS.TIME" target="OPTGROUPS.TIME">
  <data key="d0">is less than</data>
</edge>
<edge source="Good strategy" target="group every ten layers">
  <data key="d0">is</data>
</edge>
<edge source="The two pruning techniques" target="most of the strategies">
  <data key="d0">are able to prune</data>
</edge>
<edge source="ALGORITHM 1 NDS" target="the optimal grouping strategy">
  <data key="d0">is</data>
</edge>
<edge source="M N X" target="the number of layers the function considers">
  <data key="d0">is</data>
</edge>
<edge source="FINDOPTGROUPING(B,X)" target="the optimal grouping strategy from layer X to N 1">
  <data key="d0">outputs</data>
</edge>
<edge source="FINDOPTGROUPING(B,X)" target="the optimal strategy">
  <data key="d0">outputs</data>
</edge>
<edge source="previous layers" target="groups represented by B">
  <data key="d0">have formed</data>
</edge>
<edge source="FINDOPTGROUPING(B GROUP(X,X I),X I 1)" target="K I K layers">
  <data key="d0">only considers</data>
</edge>
<edge source="FINDOPTGROUPING(B GROUP(X,X I),X I 1)" target="the optimal grouping strategy for case I">
  <data key="d0">outputs</data>
</edge>
<edge source="the optimal grouping strategy for case I" target="the assumption">
  <data key="d0">is based on</data>
</edge>
<edge source="THE FUNCTION" target="ONE LAYER">
  <data key="d0">examines</data>
</edge>
<edge source="M" target="1">
  <data key="d0">is</data>
</edge>
<edge source="Layer X" target="one group">
  <data key="d0">is</data>
</edge>
<edge source="This strategy" target="the optimal strategy">
  <data key="d0">is</data>
</edge>
<edge source="the algorithm" target="K1 layers">
  <data key="d0">considers</data>
</edge>
<edge source="the algorithm" target="the optimal grouping strategy for M K 1">
  <data key="d0">outputs</data>
</edge>
<edge source="the algorithm" target="the optimal grouping strategy from these cases">
  <data key="d0">chooses</data>
</edge>
<edge source="the optimal strategy for this case" target="one group">
  <data key="d0">is</data>
</edge>
<edge source="these cases" target="exclusive">
  <data key="d0">are</data>
</edge>
<edge source="these cases" target="the entire search space">
  <data key="d0">cover</data>
</edge>
<edge source="these cases" target="the computation to an earlier point than grouping from X to at least J">
  <data key="d0">cannot advance</data>
</edge>
<edge source="THE RST TECHNIQUE" target="the cases">
  <data key="d0">prunes</data>
</edge>
<edge source="their lower bounds" target="the current found optimal">
  <data key="d0">are no better than</data>
</edge>
<edge source="this technique" target="the optimality">
  <data key="d0">does not affect</data>
</edge>
<edge source="THE SECOND TECHNIQUE" target="THE CASE">
  <data key="d0">prunes</data>
</edge>
<edge source="THEIR RST GROUPS" target="LAYER X TO J J">
  <data key="d0">are from</data>
</edge>
<edge source="pruning these cases" target="the optimality">
  <data key="d0">do not affect</data>
</edge>
<edge source="layers or operators in a DNN model" target="an arbitrary computation graph">
  <data key="d0">can be connected as</data>
</edge>
<edge source="Models like ResNet and Inception" target="technically non-linear directed acyclic graphs (DAGs)">
  <data key="d0">are</data>
</edge>
<edge source="execution order" target="that the layers/operators in the DAG are issued to the GPU one by one">
  <data key="d0">is</data>
</edge>
<edge source="grouping the layers" target="high pipelining efficiency and low pipelining overhead">
  <data key="d0">aims to achieve</data>
</edge>
<edge source="the order" target="the rst time an operator is executed">
  <data key="d0">is based on</data>
</edge>
<edge source="an operator" target="it is transmitted to the GPU and the input is ready">
  <data key="d0">can be executed only when</data>
</edge>
<edge source="OUR PIPELINED MODEL TRANSMISSION" target="THE GENERAL CASE">
  <data key="d0">is applicable to</data>
</edge>
<edge source="UNIFIED MEMORY MANAGEMENT TASK EXECUTION IN A GPU" target="GPU MEMORY">
  <data key="d0">requires</data>
</edge>
<edge source="malloc function" target="CPUs for memory allocation">
  <data key="d0">is similar to</data>
</edge>
<edge source="malloc function" target="cudaMalloc for NVIDIA GPUs">
  <data key="d0">example</data>
</edge>
<edge source="each task" target="the native cudaMallocManaged function for GPU memory allocation">
  <data key="d0">uses</data>
</edge>
<edge source="each task" target="model transmission to CUDA unified memory">
  <data key="d0">delegates</data>
</edge>
<edge source="each worker" target="cudaMalloc to allocate GPU memory">
  <data key="d0">uses</data>
</edge>
<edge source="each worker" target="the model to GPU by its own">
  <data key="d0">transmits</data>
</edge>
<edge source="each worker" target="a memory pool to allocate the memory to store its model and intermediate results">
  <data key="d0">uses</data>
</edge>
<edge source="each worker" target="the memory to the pool after the intermediate results are no longer needed">
  <data key="d0">recycles</data>
</edge>
<edge source="Each worker" target="GPU memory with cudamallocmanaged">
  <data key="d0">allocates</data>
</edge>
<edge source="Each worker" target="a separate process">
  <data key="d0">is</data>
</edge>
<edge source="Each worker" target="its own GPU environment (i.e., CUDA context)">
  <data key="d0">initializes</data>
</edge>
<edge source="CUDA" target="the model to GPU when needed">
  <data key="d0">automatically transmits</data>
</edge>
<edge source="CUDA" target="Unified Memory">
  <data key="d0">has feature</data>
</edge>
<edge source="CUDA" target="unified memory">
  <data key="d0">uses</data>
</edge>
<edge source="THIS SOLUTION" target="HIGH OVERHEAD FOR DL APPLICATIONS">
  <data key="d0">incurs</data>
</edge>
<edge source="THIS SOLUTION" target="THE LOWER BOUND">
  <data key="d0">provides</data>
</edge>
<edge source="THE NATIVE CUDAMALLOC FUNCTION AND CUDA UNIFIED MEMORY" target="GENERAL-PURPOSE APPLICATIONS">
  <data key="d0">are designed for</data>
</edge>
<edge source="THE NATIVE CUDAMALLOC FUNCTION AND CUDA UNIFIED MEMORY" target="UNNECESSARY OVERHEAD FOR DL APPLICATIONS">
  <data key="d0">may incur</data>
</edge>
<edge source="CUDA Unified Memory" target="DL applications">
  <data key="d0">is not optimized for</data>
</edge>
<edge source="CUDA Unified Memory" target="more than one hundred milliseconds overhead than PipeSwitch">
  <data key="d0">introduces</data>
</edge>
<edge source="exploiting two characteristics of DL applications" target="GPU memory management overhead">
  <data key="d0">minimizes</data>
</edge>
<edge source="THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT" target="THESE CHARACTERISTICS">
  <data key="d0">does not consider</data>
</edge>
<edge source="THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT" target="too heavy-weight for DL applications that require fast task switching">
  <data key="d0">is</data>
</edge>
<edge source="the amount of memory allocated to the DNN model" target="fixed">
  <data key="d0">is</data>
</edge>
<edge source="the amount of memory allocated to the DNN model" target="during task execution">
  <data key="d0">does not change</data>
</edge>
<edge source="the model parameters" target="the weights of the neural network">
  <data key="d0">are</data>
</edge>
<edge source="the amount of memory needed to store the model parameters" target="the same">
  <data key="d0">stays</data>
</edge>
<edge source="INFERENCE TASK" target="the model for inference">
  <data key="d0">only uses</data>
</edge>
<edge source="INFERENCE TASK" target="the model itself">
  <data key="d0">does not change</data>
</edge>
<edge source="THE INTERMEDIATE RESULTS" target="A SIMPLE, REGULAR PATTERN">
  <data key="d0">CHANGE IN</data>
</edge>
<edge source="THE INTERMEDIATE RESULTS" target="MEMORY FRAGMENTATION">
  <data key="d0">DO NOT CAUSE</data>
</edge>
<edge source="the intermediate results" target="the outputs of each layer">
  <data key="d0">are</data>
</edge>
<edge source="the intermediate results" target="the next layer">
  <data key="d0">are used by</data>
</edge>
<edge source="the intermediate results" target="RST-IN-LAST-OUT">
  <data key="d0">are</data>
</edge>
<edge source="the backward pass" target="the intermediate results">
  <data key="d0">consumes</data>
</edge>
<edge source="the backward pass" target="reverse order">
  <data key="d0">consumes in order</data>
</edge>
<edge source="the forward pass" target="the intermediate results">
  <data key="d0">generates</data>
</edge>
<edge source="memory allocation and release" target="a simple stack-like mechanism">
  <data key="d0">can be handled by</data>
</edge>
<edge source="a simple stack-like mechanism" target="memory fragmentation">
  <data key="d0">does not cause</data>
</edge>
<edge source="You" target="memory footprint">
  <data key="d0">should minimize</data>
</edge>
<edge source="You" target="extra memory copies">
  <data key="d0">should avoid</data>
</edge>
<edge source="a dedicated memory daemon" target="the GPU memory">
  <data key="d0">manages</data>
</edge>
<edge source="compared to" target="PIPESWITCH">
  <data key="d0">no unified memory management</data>
</edge>
<edge source="passing memory pointers to the workers" target="light-weight">
  <data key="d0">is</data>
</edge>
<edge source="one worker" target="the GPU memory">
  <data key="d0">owns</data>
</edge>
<edge source="memory isolation" target="between workers">
  <data key="d0">is guaranteed</data>
</edge>
<edge source="THE MEMORY MANAGEMENT OF PIPESWITCH" target="THAT OF PYTORCH">
  <data key="d0">extends</data>
</edge>
<edge source="The memory management in PyTorch" target="memory allocation for a task itself">
  <data key="d0">handles</data>
</edge>
<edge source="replicating the models in each worker" target="high memory footprint">
  <data key="d0">incurs</data>
</edge>
<edge source="replicating the models in each worker" target="the number of models a server can store">
  <data key="d0">reduces</data>
</edge>
<edge source="reducing the number of models a server can store" target="the types of tasks the server can execute">
  <data key="d0">consequently reduces</data>
</edge>
<edge source="storing the models in a dedicate process" target="minimal memory footprint">
  <data key="d0">has</data>
</edge>
<edge source="storing the models in a dedicate process" target="an extra memory copy from this process to a worker to start a task">
  <data key="d0">incurs</data>
</edge>
<edge source="each model" target="only once">
  <data key="d0">is stored</data>
</edge>
<edge source="an extra memory copy from this process to a worker to start a task" target="the task switching time">
  <data key="d0">hurts</data>
</edge>
<edge source="IPC overhead" target="minimized">
  <data key="d0">should be</data>
</edge>
<edge source="IPC APIs" target="GPUs">
  <data key="d0">are provided by</data>
</edge>
<edge source="IPC APIs" target="these IPC APIs">
  <data key="d0">have measured performance of</data>
</edge>
<edge source="CUDAIPCOPENMEMHANDLE" target="IPC APIs">
  <data key="d0">is an example of</data>
</edge>
<edge source="CUDAIPCOPENMEMHANDLE" target="NVIDIA GPUs">
  <data key="d0">is for</data>
</edge>
<edge source="these IPC APIs" target="high overhead">
  <data key="d0">incur</data>
</edge>
<edge source="The overhead" target="the pipeline">
  <data key="d0">is exacerbated by</data>
</edge>
<edge source="The pipeline" target="the IPCs frequently">
  <data key="d0">needs to invoke</data>
</edge>
<edge source="The pipeline" target="the IPC only once for the entire model transmission">
  <data key="d0">does not invoke</data>
</edge>
<edge source="The frequent invocation of the IPCs" target="synchronize model transmission and task execution for every pipeline group">
  <data key="d0">is for</data>
</edge>
<edge source="memory allocation process for a neural network model" target="deterministic">
  <data key="d0">is</data>
</edge>
<edge source="memory daemon and the worker" target="the same order to allocate memory for the model parameters">
  <data key="d0">use</data>
</edge>
<edge source="memory pointers for the parameters" target="the same">
  <data key="d0">would be</data>
</edge>
<edge source="the neural network model" target="known and given">
  <data key="d0">is</data>
</edge>
<edge source="the memory daemon and the worker" target="the same order">
  <data key="d0">have</data>
</edge>
<edge source="latency" target="no unified memory management without IPC optimization">
  <data key="d0">is higher than</data>
</edge>
<edge source="NO PIN" target="MEMORY">
  <data key="d0">is</data>
</edge>
<edge source="THE OS" target="a memory page to disk">
  <data key="d0">would swap</data>
</edge>
<edge source="the page" target="inactive for a certain amount of time">
  <data key="d0">is</data>
</edge>
<edge source="a page in the host memory" target="in order to transmit the data in the page to the GPU memory">
  <data key="d0">is pinned</data>
</edge>
<edge source="A temporary pinned page" target="the transmission">
  <data key="d0">is created for</data>
</edge>
<edge source="Process-level isolation" target="because it ensures that one task cannot read the memory of another task">
  <data key="d0">is desirable</data>
</edge>
<edge source="Process-level isolation" target="one task cannot read the memory of another task">
  <data key="d0">ensures</data>
</edge>
<edge source="Process-level isolation" target="the crashing of one task does not affect other tasks or the entire system">
  <data key="d0">ensures</data>
</edge>
<edge source="separate processes" target="process-level isolation">
  <data key="d0">achieve</data>
</edge>
<edge source="A naive solution" target="separate processes">
  <data key="d0">is to use</data>
</edge>
<edge source="A naive solution" target="the new task after the current task is stopped">
  <data key="d0">is to start</data>
</edge>
<edge source="SEQUENTIAL EXECUTION" target="LONG DELAY">
  <data key="d0">incurs</data>
</edge>
<edge source="LONG DELAY" target="OLD TASK CLEANING AND NEW TASK INITIALIZATION">
  <data key="d0">is due to</data>
</edge>
<edge source="another possible solution" target="the current and new tasks share the same process with a warm CUDA context">
  <data key="d0">is to let</data>
</edge>
<edge source="THE PROCESS OF THE OLD TASK" target="THE GPU ENVIRONMENT">
  <data key="d0">cleans</data>
</edge>
<edge source="ANOTHER PROCESS" target="for THE NEW TASK">
  <data key="d0">is initialized</data>
</edge>
<edge source="THE PROCESS" target="THE ENVIRONMENT FOR THE NEW TASK">
  <data key="d0">REUSES</data>
</edge>
<edge source="GPU environment (i.e., CUDA context)" target="when it is rst created">
  <data key="d0">is initialized</data>
</edge>
<edge source="a major job" target="asynchronous CUDA functions queued on the GPU">
  <data key="d0">is to clear</data>
</edge>
<edge source="the number of queued functions" target="limited">
  <data key="d0">are</data>
</edge>
<edge source="the number of queued functions" target="quickly cleared">
  <data key="d0">can be</data>
</edge>
<edge source="Synchronization points" target="inference tasks">
  <data key="d0">are not needed for</data>
</edge>
<edge source="another job" target="free its GPU memory">
  <data key="d0">is to</data>
</edge>
<edge source="the cleaning procedure" target="the content of the memory">
  <data key="d0">does not modify</data>
</edge>
<edge source="the cleaning procedure" target="the metadata">
  <data key="d0">cleans</data>
</edge>
<edge source="the metadata" target="GPU memory pointers">
  <data key="d0">is</data>
</edge>
<edge source="cleaning procedure" target="pointers pointing to the tensor data">
  <data key="d0">deletes</data>
</edge>
<edge source="cleaning procedure" target="the actual data">
  <data key="d0">does not free</data>
</edge>
<edge source="parallelizing the task cleaning and the pipelined model transmission" target="hide the task cleaning overhead">
  <data key="d0">serves to</data>
</edge>
<edge source="THIS CHOICE" target="performance">
  <data key="d0">is optimized for</data>
</edge>
<edge source="THIS CHOICE" target="a trusted environment">
  <data key="d0">is not a problem for</data>
</edge>
<edge source="a latter process" target="the memory data of a previous process">
  <data key="d0">can read</data>
</edge>
<edge source="an additional zero-out operation" target="if this is a concern">
  <data key="d0">can be added</data>
</edge>
<edge source="high memory bandwidth" target="900GBS for V100">
  <data key="d0">example</data>
</edge>
<edge source="zeroing-out most models like ResNet-152 (around 240MB)" target="sub-millisecond overhead">
  <data key="d0">would incur</data>
</edge>
<edge source="only one active worker" target="exclusive occupation of the GPU">
  <data key="d0">guarantees</data>
</edge>
<edge source="number of standby workers" target="their GPU memory consumption">
  <data key="d0">has trade-off with</data>
</edge>
<edge source="every standby worker" target="its own CUDA context">
  <data key="d0">needs to maintain</data>
</edge>
<edge source="CUDA context" target="a few hundred MB GPU memory">
  <data key="d0">consumes</data>
</edge>
<edge source="many standby workers" target="at least one idle standby worker">
  <data key="d0">can have</data>
</edge>
<edge source="two standby workers" target="at least one idle worker">
  <data key="d0">are sufficient to ensure</data>
</edge>
<edge source="two standby workers" target="moderate GPU memory consumption">
  <data key="d0">have</data>
</edge>
<edge source="one idle worker" target="the waiting time">
  <data key="d0">eliminates</data>
</edge>
<edge source="A transaction here" target="a model is switched in or out on all of its GPUs to enable or disable inference on this model">
  <data key="d0">means</data>
</edge>
<edge source="these jobs" target="18 of total GPU hours">
  <data key="d0">account for</data>
</edge>
<edge source="current training frameworks" target="mature support of elastic training">
  <data key="d0">do not have</data>
</edge>
<edge source="These scheduling solutions" target="orthogonal and complementary to PipeSwitch">
  <data key="d0">are</data>
</edge>
<edge source="functions" target="the received GPU memory into PyTorch GPU memory pool for a specific CUDA stream">
  <data key="d0">insert</data>
</edge>
<edge source="functions" target="the GPU memory from the pool">
  <data key="d0">clear</data>
</edge>
<edge source="shared GPU memory" target="PyTorch GPU memory pool">
  <data key="d0">can be inserted into</data>
</edge>
<edge source="shared GPU memory" target="different CUDA streams">
  <data key="d0">can be inserted multiple times for</data>
</edge>
<edge source="THE CONTROLLER PROCESS" target="A TCP THREAD">
  <data key="d0">consists of</data>
</edge>
<edge source="THE CONTROLLER PROCESS" target="A SCHEDULER THREAD">
  <data key="d0">consists of</data>
</edge>
<edge source="the scheduler and the memory daemon" target="together">
  <data key="d0">are implemented</data>
</edge>
<edge source="THE TCP THREAD" target="TASK THROUGH TCP FROM CLIENTS">
  <data key="d0">accepts</data>
</edge>
<edge source="THE TCP THREAD" target="THE TASK TO THE SCHEDULER THREAD">
  <data key="d0">sends</data>
</edge>
<edge source="THE SCHEDULER THREAD" target="THE GPU MEMORY WITH WORKERS">
  <data key="d0">ALLOCATES AND SHARES</data>
</edge>
<edge source="THE SCHEDULER THREAD" target="WORKERS">
  <data key="d0">ACTIVATES OR DEACTIVATES</data>
</edge>
<edge source="THE SCHEDULER THREAD" target="THE TASK TO A WORKER">
  <data key="d0">SENDS</data>
</edge>
<edge source="THE SCHEDULER THREAD" target="PARAMETERS FOR THE CORRESPONDING MODEL TO THE GPU MEMORY">
  <data key="d0">TRANSFERS</data>
</edge>
<edge source="the user" target="the model in the scheduler">
  <data key="d0">should register</data>
</edge>
<edge source="the scheduler" target="the controller">
  <data key="d0">notifies</data>
</edge>
<edge source="parameters" target="the GPU memory">
  <data key="d0">are transmitted to</data>
</edge>
<edge source="parameters" target="groups">
  <data key="d0">are transmitted in</data>
</edge>
<edge source="parameters" target="a pipeline">
  <data key="d0">are transmitted in</data>
</edge>
<edge source="THE WORKER PROCESS" target="TWO THREADS">
  <data key="d0">consists of</data>
</edge>
<edge source="THE TERMINATION THREAD" target="THE TERMINATION SIGNAL FROM THE CONTROLLER">
  <data key="d0">waits for</data>
</edge>
<edge source="THE TERMINATION THREAD" target="THE MAIN THREAD">
  <data key="d0">notifies</data>
</edge>
<edge source="the model structures" target="small">
  <data key="d0">is</data>
</edge>
<edge source="the parameters" target="only once in the memory daemon">
  <data key="d0">are stored</data>
</edge>
<edge source="storing the parameters in the memory daemon" target="minimal memory footprint">
  <data key="d0">results in</data>
</edge>
<edge source="different models" target="the same GPU memory location">
  <data key="d0">might use</data>
</edge>
<edge source="the value" target="the controller transfers the corresponding parameters to these locations">
  <data key="d0">is not valid until</data>
</edge>
<edge source="ALL EXPERIMENTS" target="AWS">
  <data key="d0">ARE CONDUCTED ON</data>
</edge>
<edge source="The software environment" target="PyTorch-1.3.0">
  <data key="d0">includes</data>
</edge>
<edge source="The software environment" target="TorchVision-0.4.2">
  <data key="d0">includes</data>
</edge>
<edge source="The software environment" target="SciPy-1.3.2">
  <data key="d0">includes</data>
</edge>
<edge source="The software environment" target="CUDA-10.1">
  <data key="d0">includes</data>
</edge>
<edge source="PyTorch with our plugins" target="better results for stop-and-start than native PyTorch from Python-PyPI used in Table 1">
  <data key="d0">provides</data>
</edge>
<edge source="RESNET152 17" target="standard benchmark for evaluating DL systems">
  <data key="d0">is a</data>
</edge>
<edge source="INCEPTIONV3 22" target="standard benchmark for evaluating DL systems">
  <data key="d0">is a</data>
</edge>
<edge source="BERTBASE 23" target="standard benchmark for evaluating DL systems">
  <data key="d0">is a</data>
</edge>
<edge source="THE EXPERIMENTS" target="BOTH TRAINING AND INFERENCE">
  <data key="d0">cover</data>
</edge>
<edge source="checkpointing frequency of training tasks" target="scheduling cycle">
  <data key="d0">is set according to</data>
</edge>
<edge source="checkpointing frequency of training tasks" target="checkpointing overhead">
  <data key="d0">is set to minimize</data>
</edge>
<edge source="default batch size for training" target="32">
  <data key="d0">is</data>
</edge>
<edge source="default batch size for inference" target="8">
  <data key="d0">is</data>
</edge>
<edge source="FIGURE 5" target="the latency experienced by the client">
  <data key="d0">shows</data>
</edge>
<edge source="TABLE 3" target="the total overhead">
  <data key="d0">shows</data>
</edge>
<edge source="each number" target="the average of 100 runs">
  <data key="d0">is reported with</data>
</edge>
<edge source="Figure 6(B)" target="minimum and maximum latencies using the error bar">
  <data key="d0">reports</data>
</edge>
<edge source="latency of the RST batch and those of later batches in one scheduling cycle" target="significantly">
  <data key="d0">can differ</data>
</edge>
<edge source="difference in latency" target="switching overhead">
  <data key="d0">is due to</data>
</edge>
<edge source="a client" target="an inference task to a GPU server">
  <data key="d0">sends</data>
</edge>
<edge source="the GPU server" target="the training task">
  <data key="d0">preempts</data>
</edge>
<edge source="the GPU server" target="the inference task">
  <data key="d0">executes</data>
</edge>
<edge source="the GPU server" target="a reply back to the client">
  <data key="d0">sends</data>
</edge>
<edge source="the process with the required model" target="the GPU">
  <data key="d0">is loaded in</data>
</edge>
<edge source="CUDA UNIED MEMORY" target="MEMORY SWAPPING">
  <data key="d0">is used for</data>
</edge>
<edge source="URL" target="https://devblogs.nvidia.com/unified-memory-cuda-beginners">
  <data key="d0">is</data>
</edge>
<edge source="THE PROPERTIES" target="4">
  <data key="d0">are described in</data>
</edge>
<edge source="LATENCY (MS)" target="READY MODEL PIPESWITCH MPS STOP-AND-START RESNET152 INCEPTIONV3 BERTBASE">
  <data key="d0">measured for</data>
</edge>
<edge source="LATENCY (MS)" target="5000 to 10000">
  <data key="d0">range</data>
</edge>
<edge source="LATENCY (MS)" target="5000 to 7500">
  <data key="d0">range</data>
</edge>
<edge source="LATENCY (MS)" target="7500 to 10000">
  <data key="d0">range</data>
</edge>
<edge source="PCIe VERSION" target="3.0">
  <data key="d0">is</data>
</edge>
<edge source="GPU COUNT" target="8">
  <data key="d0">is</data>
</edge>
<edge source="STOP-AND-START on P3.2XLARGE with RESNET152" target="6475.40 MS">
  <data key="d0">latency</data>
</edge>
<edge source="STOP-AND-START on P3.2XLARGE with INCEPTIONV3" target="7536.07 MS">
  <data key="d0">latency</data>
</edge>
<edge source="STOP-AND-START on P3.2XLARGE with BERTBASE" target="6371.32 MS">
  <data key="d0">latency</data>
</edge>
<edge source="STOP-AND-START on G4DN.2XLARGE with RESNET152" target="5486.74 MS">
  <data key="d0">latency</data>
</edge>
<edge source="STOP-AND-START on G4DN.2XLARGE with INCEPTIONV3" target="6558.76 MS">
  <data key="d0">latency</data>
</edge>
<edge source="STOP-AND-START on G4DN.2XLARGE with BERTBASE" target="5355.95 MS">
  <data key="d0">latency</data>
</edge>
<edge source="NVIDIA MPS on P3.2XLARGE with RESNET152" target="307.02 MS">
  <data key="d0">latency</data>
</edge>
<edge source="NVIDIA MPS on P3.2XLARGE with INCEPTIONV3" target="232.25 MS">
  <data key="d0">latency</data>
</edge>
<edge source="NVIDIA MPS on P3.2XLARGE with BERTBASE" target="204.52 MS">
  <data key="d0">latency</data>
</edge>
<edge source="NVIDIA MPS on G4DN.2XLARGE with RESNET152" target="259.20 MS">
  <data key="d0">latency</data>
</edge>
<edge source="NVIDIA MPS on G4DN.2XLARGE with INCEPTIONV3" target="193.05 MS">
  <data key="d0">latency</data>
</edge>
<edge source="NVIDIA MPS on G4DN.2XLARGE with BERTBASE" target="338.25 MS">
  <data key="d0">latency</data>
</edge>
<edge source="PIPESWITCH on P3.2XLARGE with RESNET152" target="6.01 MS">
  <data key="d0">latency</data>
</edge>
<edge source="PIPESWITCH on P3.2XLARGE with INCEPTIONV3" target="5.40 MS">
  <data key="d0">latency</data>
</edge>
<edge source="PIPESWITCH on P3.2XLARGE with BERTBASE" target="10.27 MS">
  <data key="d0">latency</data>
</edge>
<edge source="PIPESWITCH on G4DN.2XLARGE with RESNET152" target="5.57 MS">
  <data key="d0">latency</data>
</edge>
<edge source="PIPESWITCH on G4DN.2XLARGE with INCEPTIONV3" target="7.66 MS">
  <data key="d0">latency</data>
</edge>
<edge source="PIPESWITCH on G4DN.2XLARGE with BERTBASE" target="34.56 MS">
  <data key="d0">latency</data>
</edge>
<edge source="RESNET152" target="the text">
  <data key="d0">is mentioned in</data>
</edge>
<edge source="RESNET152" target="LATENCY (MS)">
  <data key="d0">measured by</data>
</edge>
<edge source="RESNET152" target="hundreds of layers">
  <data key="d0">has</data>
</edge>
<edge source="INCEPTIONV3" target="the text">
  <data key="d0">is mentioned in</data>
</edge>
<edge source="INCEPTIONV3" target="LATENCY (MS)">
  <data key="d0">measured by</data>
</edge>
<edge source="BERTBASE" target="the text">
  <data key="d0">is mentioned in</data>
</edge>
<edge source="BERTBASE" target="LATENCY (MS)">
  <data key="d0">measured by</data>
</edge>
<edge source="Latency" target="milliseconds (MS)">
  <data key="d0">is measured in</data>
</edge>
<edge source="Latency" target="milliseconds">
  <data key="d0">is measured in</data>
</edge>
<edge source="P3.2xlarge" target="NVIDIA V100">
  <data key="d0">uses</data>
</edge>
<edge source="Optimization" target="Pipeswitch per-layer pipeline grouped transmission no optimization">
  <data key="d0">types include</data>
</edge>
<edge source="PER-LAYER PIPELINE" target="pipeline">
  <data key="d0">is a type of</data>
</edge>
<edge source="PER-LAYER PIPELINE" target="transmission and computation">
  <data key="d0">overlaps</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="pipeline">
  <data key="d0">is a type of</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="NO OPTIMIZATION">
  <data key="d0">improves</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="THE LAYERS OF THE MODEL INTO ONE BIG TENSOR">
  <data key="d0">combines</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="ONE BIG TENSOR IN ONE GROUP">
  <data key="d0">transmits</data>
</edge>
<edge source="NO OPTIMIZATION" target="pipeline">
  <data key="d0">is a condition of</data>
</edge>
<edge source="NO OPTIMIZATION" target="the worst in most cases">
  <data key="d0">performs</data>
</edge>
<edge source="510 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">organized by</data>
</edge>
<edge source="ONE PROCESS" target="THE CUDA ENVIRONMENT">
  <data key="d0">reuses</data>
</edge>
<edge source="ONE PROCESS" target="THE OVERHEAD TO CLEAN THE ENVIRONMENT">
  <data key="d0">pays</data>
</edge>
<edge source="Models" target="RESNET152, INCEPTIONV3, BERTBASE">
  <data key="d0">include</data>
</edge>
<edge source="RESNET152 on P3.2XLARGE" target="3.62 MS">
  <data key="d0">has startup overhead</data>
</edge>
<edge source="INCEPTIONV3 on P3.2XLARGE" target="4.82 MS">
  <data key="d0">has startup overhead</data>
</edge>
<edge source="BERTBASE on P3.2XLARGE" target="3.62 MS">
  <data key="d0">has startup overhead</data>
</edge>
<edge source="RESNET152 on G4DN.2XLARGE" target="2.53 MS">
  <data key="d0">has startup overhead</data>
</edge>
<edge source="INCEPTIONV3 on G4DN.2XLARGE" target="5.49 MS">
  <data key="d0">has startup overhead</data>
</edge>
<edge source="BERTBASE on G4DN.2XLARGE" target="6.57 MS">
  <data key="d0">has startup overhead</data>
</edge>
<edge source="Table 4" target="the startup overhead for PipeSwitch to start computing the RST layer">
  <data key="d0">describes</data>
</edge>
<edge source="Table 4" target="task startup overhead for PipeSwitch">
  <data key="d0">shows</data>
</edge>
<edge source="task startup overhead for PipeSwitch" target="the difference between the time for ResNet152, InceptionV3, BERTBase">
  <data key="d0">is</data>
</edge>
<edge source="ResNet152" target="464 layers">
  <data key="d0">has</data>
</edge>
<edge source="ResNet152" target="eight p3.2xlarge instances">
  <data key="d0">runs on</data>
</edge>
<edge source="InceptionV3" target="189 layers">
  <data key="d0">has</data>
</edge>
<edge source="BERTBase" target="139 layers">
  <data key="d0">has</data>
</edge>
<edge source="Only Pruning 1" target="2.09 s, 0.30 s, 0.88 s">
  <data key="d0">has task startup overhead</data>
</edge>
<edge source="Only Pruning 2" target="3.44 h, 5.07 s, 24 h">
  <data key="d0">has task startup overhead</data>
</edge>
<edge source="No Pruning" target="24 h, 24 h, 24 h">
  <data key="d0">has task startup overhead</data>
</edge>
<edge source="Table 5" target="effectiveness of two pruning techniques">
  <data key="d0">shows</data>
</edge>
<edge source="ITS PERFORMANCE" target="THE READY MODEL">
  <data key="d0">is similar to</data>
</edge>
<edge source="ITS PERFORMANCE" target="NVIDIA MPS">
  <data key="d0">is similar to</data>
</edge>
<edge source="THE READY MODEL" target="preloaded">
  <data key="d0">is</data>
</edge>
<edge source="THE MODEL" target="THE HOST MEMORY">
  <data key="d0">is in</data>
</edge>
<edge source="THE TOTAL OVERHEAD" target="the difference between the latency of a mechanism and that of the ready model">
  <data key="d0">is</data>
</edge>
<edge source="STOP-AND-START" target="the worst">
  <data key="d0">performs</data>
</edge>
<edge source="the worst" target="several seconds">
  <data key="d0">takes</data>
</edge>
<edge source="the worst" target="the training task">
  <data key="d0">stop</data>
</edge>
<edge source="the worst" target="a new process for the new task">
  <data key="d0">initialize</data>
</edge>
<edge source="The main source of the overhead" target="CUDA context initialization and rst-time library loading operations in PyTorch">
  <data key="d0">is</data>
</edge>
<edge source="ANOTHER SOURCE" target="GPU MEMORY SWAPPING">
  <data key="d0">is</data>
</edge>
<edge source="computing BERT on T4" target="120ms">
  <data key="d0">takes</data>
</edge>
<edge source="relative overhead" target="acceptable">
  <data key="d0">is</data>
</edge>
<edge source="The startup overhead of PipeSwitch" target="only a few milliseconds">
  <data key="d0">is</data>
</edge>
<edge source="FIGURE 6(A)" target="the inference throughput">
  <data key="d0">shows</data>
</edge>
<edge source="THE DASHED LINE" target="THE UPPER BOUND">
  <data key="d0">is</data>
</edge>
<edge source="THE DASHED LINE" target="THE LOWER BOUND">
  <data key="d0">is</data>
</edge>
<edge source="THE UPPER BOUND" target="THE THROUGHPUT OF THE READY MODEL ASSUMING NO TASK SWITCHING">
  <data key="d0">is</data>
</edge>
<edge source="THE AVERAGE LATENCY OF THE READY MODEL" target="NO TASK SWITCHING">
  <data key="d0">assumes</data>
</edge>
<edge source="throughput of stop-and-start" target="zero for scheduling cycles smaller than 10 s">
  <data key="d0">is nearly</data>
</edge>
<edge source="MPS" target="around 100 batches per second">
  <data key="d0">keeps throughput</data>
</edge>
<edge source="MPS" target="about 80 ms average latency">
  <data key="d0">has</data>
</edge>
<edge source="MPS" target="several hundred milliseconds latency for the RST batch">
  <data key="d0">has</data>
</edge>
<edge source="FIGURE 6(B)" target="the average latency of the inference tasks">
  <data key="d0">shows</data>
</edge>
<edge source="THE ERROR BAR" target="THE MINIMUM AND MAXIMUM LATENCY">
  <data key="d0">indicates</data>
</edge>
<edge source="STOP- AND-START" target="poor latency">
  <data key="d0">has</data>
</edge>
<edge source="poor latency" target="several seconds overhead of RST BATCH">
  <data key="d0">is because of</data>
</edge>
<edge source="RST BATCH" target="several seconds overhead">
  <data key="d0">has</data>
</edge>
<edge source="PIPESWITCH MPS" target="1S 2S 5S 10S 30S">
  <data key="d0">has latency measurements</data>
</edge>
<edge source="LOWER BOUND" target="B">
  <data key="d0">is measured in</data>
</edge>
<edge source="LATENCY" target="0 200 400">
  <data key="d0">has lower bound</data>
</edge>
<edge source="Throughput and latency" target="different scheduling cycles">
  <data key="d0">are measured under</data>
</edge>
<edge source="Throughput and latency" target="ResNet">
  <data key="d0">are for</data>
</edge>
<edge source="ResNet" target="p3.2xlarge">
  <data key="d0">runs on</data>
</edge>
<edge source="Computation" target="once parameters are transmitted">
  <data key="d0">starts</data>
</edge>
<edge source="FIGURE 8" target="the total time measured by the client">
  <data key="d0">shows</data>
</edge>
<edge source="overlaps" target="layer">
  <data key="d0">occur at the granularity of</data>
</edge>
<edge source="PCIE overhead and synchronization overhead" target="for every layer">
  <data key="d0">occur</data>
</edge>
<edge source="models with many layers but relatively light computation such as ResNet152 and Inception" target="worse than grouped transmission">
  <data key="d0">can perform</data>
</edge>
<edge source="models with many layers but relatively light computation such as ResNet152 and Inception" target="sometimes even worse than no pipeline">
  <data key="d0">can perform</data>
</edge>
<edge source="this reduction" target="significant">
  <data key="d0">is</data>
</edge>
<edge source="this reduction" target="the optimizations on memory management and worker switching have already been applied">
  <data key="d0">is evaluated when</data>
</edge>
<edge source="TABLE 5" target="the running time of Algorithm 1">
  <data key="d0">shows</data>
</edge>
<edge source="TABLE 5" target="the effects of the two pruning techniques mentioned in 4.2">
  <data key="d0">shows</data>
</edge>
<edge source="both weighted and unweighted layers" target="the computation time">
  <data key="d0">contribute to</data>
</edge>
<edge source="no pruning" target="for all three models after running for 24 hours">
  <data key="d0">does not finish</data>
</edge>
<edge source="UNIED MEMORY MANAGEMENT" target="the effectiveness of UNIED MEMORY MANAGEMENT">
  <data key="d0">is used to evaluate</data>
</edge>
<edge source="Memory management" target="not unified">
  <data key="d0">is</data>
</edge>
<edge source="IPC" target="no optimization">
  <data key="d0">has</data>
</edge>
<edge source="the pages of the memory daemon" target="the main memory">
  <data key="d0">are not pinned to</data>
</edge>
<edge source="this experiment" target="all the optimizations on memory management are effective">
  <data key="d0">demonstrates</data>
</edge>
<edge source="Unified Memory Management Mechanism" target="PipeSwitch">
  <data key="d0">is used by</data>
</edge>
<edge source="IPC optimization" target="important">
  <data key="d0">is</data>
</edge>
<edge source="IPC optimization" target="latency by 1648 ms">
  <data key="d0">reduces</data>
</edge>
<edge source="Pinning the pages to the host memory" target="the latency with a few milliseconds">
  <data key="d0">can reduce</data>
</edge>
<edge source="two processes" target="the worst">
  <data key="d0">perform</data>
</edge>
<edge source="THE NEW PROCESS" target="A NEW CUDA ENVIRONMENT">
  <data key="d0">needs to create</data>
</edge>
<edge source="A NEW CUDA ENVIRONMENT" target="THE TOTAL TIME">
  <data key="d0">dominates</data>
</edge>
<edge source="Several algorithms and systems" target="executing and scheduling deep learning tasks on clusters">
  <data key="d0">have been designed for</data>
</edge>
<edge source="Deep learning tasks on clusters" target="both training and inference tasks">
  <data key="d0">include</data>
</edge>
<edge source="many techniques and systems" target="optimize communication and improve distributed training">
  <data key="d0">have been proposed to</data>
</edge>
<edge source="THE MOST RELEVANT ONES" target="PIPEDREAM 8">
  <data key="d0">are</data>
</edge>
<edge source="THE MOST RELEVANT ONES" target="BYTESCHEDULER 9">
  <data key="d0">are</data>
</edge>
<edge source="THE MOST RELEVANT ONES" target="POSEIDON 40">
  <data key="d0">are</data>
</edge>
<edge source="VDNN 43" target="GPU memory management module">
  <data key="d0">have</data>
</edge>
<edge source="SWAPADVISOR 44" target="GPU memory management module">
  <data key="d0">have</data>
</edge>
<edge source="VDNN 43 and SWAPADVISOR 44" target="memory management for a single training task of large models">
  <data key="d0">focus on</data>
</edge>
<edge source="memory management for a single training task of large models" target="PIPESWITCH">
  <data key="d0">are not directly comparable to</data>
</edge>
<edge source="CLUSTER MANAGERS 4548" target="GPUS to VMS or CONTAINERS at device granularity">
  <data key="d0">allocate</data>
</edge>
<edge source="Several solutions" target="to share a GPU at application granularity using techniques like library interception">
  <data key="d0">have been proposed</data>
</edge>
<edge source="Efforts on GPU optimization" target="the performance of running a single task">
  <data key="d0">aim to improve</data>
</edge>
<edge source="Efforts on GPU optimization" target="tensor fusion">
  <data key="d0">include</data>
</edge>
<edge source="Efforts on GPU optimization" target="kernel-level concurrency and scheduling">
  <data key="d0">include</data>
</edge>
<edge source="Madan Musu-Vathi and the anonymous reviewers" target="valuable feedback">
  <data key="d0">provide</data>
</edge>
<edge source="ZHIHAO BAI, ZHEN ZHANG AND XIN JIN" target="an AWS Machine Learning Research Award">
  <data key="d0">were supported in part by</data>
</edge>
<edge source="A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes" target="Large-scale cluster management at Google with Borg">
  <data key="d0">authored</data>
</edge>
<edge source="Large-scale cluster management at Google with Borg" target="EuroSys">
  <data key="d0">published in</data>
</edge>
<edge source="Large-scale cluster management at Google with Borg" target="2015">
  <data key="d0">published in year</data>
</edge>
<edge source="NEXUS" target="a GPU cluster engine">
  <data key="d0">is</data>
</edge>
<edge source="NEXUS" target="accelerating DNN-based video analysis">
  <data key="d0">purpose</data>
</edge>
<edge source="3 H. SHEN, L. CHEN, Y. JIN, L. ZHAO, B. KONG, M. PHILIPOSE, A. KRISHNAMURTHY, and R. SUNDARAM" target="NEXUS: A GPU cluster engine for accelerating DNN-based video analysis">
  <data key="d0">authored</data>
</edge>
<edge source="NEXUS paper" target="ACM SOSP">
  <data key="d0">published in</data>
</edge>
<edge source="NEXUS paper" target="2019">
  <data key="d0">published in year</data>
</edge>
<edge source="FRIED, J. BEHRENS, A. BELAY, AND H. BALAKRISHNAN" target="SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS">
  <data key="d0">authored</data>
</edge>
<edge source="SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS" target="USENIX NSDI">
  <data key="d0">published in</data>
</edge>
<edge source="SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS" target="2019">
  <data key="d0">published in year</data>
</edge>
<edge source="7 P. YU AND M. CHOWDHURY" target="SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS">
  <data key="d0">authored</data>
</edge>
<edge source="SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS" target="CONFERENCE ON MACHINE LEARNING AND SYSTEMS">
  <data key="d0">published in</data>
</edge>
<edge source="SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS" target="2020">
  <data key="d0">published in year</data>
</edge>
<edge source="D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia" target="Pipedream: Generalized Pipeline Parallelism for DNN Training">
  <data key="d0">authored</data>
</edge>
<edge source="Pipedream: Generalized Pipeline Parallelism for DNN Training" target="ACM SOSP">
  <data key="d0">published_in</data>
</edge>
<edge source="Pipedream: Generalized Pipeline Parallelism for DNN Training" target="2019">
  <data key="d0">published_year</data>
</edge>
<edge source="Y. PENG, Y. ZHU, Y. CHEN, Y. BAO, B. YI, C. LAN, C. WU, AND C. GUO" target="A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION">
  <data key="d0">authored</data>
</edge>
<edge source="A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION" target="ACM SOSP">
  <data key="d0">was published in</data>
</edge>
<edge source="A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION" target="2019">
  <data key="d0">was published in year</data>
</edge>
<edge source="TIRESIAS" target="a GPU cluster manager for distributed deep learning">
  <data key="d0">is</data>
</edge>
<edge source="TIRESIAS" target="USENIX NSDI">
  <data key="d0">was presented in</data>
</edge>
<edge source="TIRESIAS" target="2019">
  <data key="d0">was published in year</data>
</edge>
<edge source="TIRESIAS" target="J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo">
  <data key="d0">was authored by</data>
</edge>
<edge source="POSEIDON" target="an efficient communication architecture for distributed deep learning on GPU clusters">
  <data key="d0">is</data>
</edge>
<edge source="POSEIDON" target="USENIX ATC, 2017">
  <data key="d0">was presented in</data>
</edge>
<edge source="H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie, and E. P. Xing" target="POSEIDON">
  <data key="d0">are authors of</data>
</edge>
<edge source="AMAZON WEB SERVICES" target="12">
  <data key="d0">is</data>
</edge>
<edge source="HTTPS" target="AWS.AMAZON.COM">
  <data key="d0">is associated with</data>
</edge>
<edge source="HTTPS" target="CLOUD.GOOGLE.COM">
  <data key="d0">is associated with</data>
</edge>
<edge source="HTTPS" target="KUBERNETES.IO">
  <data key="d0">is</data>
</edge>
<edge source="HTTPS" target="GITHUB.COMNVIDIANVIDIA-DOCKER">
  <data key="d0">refers to</data>
</edge>
<edge source="MICROSOFT AZURE" target="a cloud computing service">
  <data key="d0">is</data>
</edge>
<edge source="GOOGLE CLOUD PLATFORM" target="14">
  <data key="d0">is</data>
</edge>
<edge source="HOROVOD" target="fast and easy distributed deep learning in TensorFlow">
  <data key="d0">is</data>
</edge>
<edge source="15 A. Sergeev and M. Del Balso" target="HOROVOD: Fast and Easy Distributed Deep Learning in TensorFlow">
  <data key="d0">are authors of</data>
</edge>
<edge source="HOROVOD paper" target="arXiv preprint arXiv:1802.05799">
  <data key="d0">was published in</data>
</edge>
<edge source="HOROVOD paper" target="2018">
  <data key="d0">was published in year</data>
</edge>
<edge source="SU" target="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER">
  <data key="d0">authored</data>
</edge>
<edge source="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER" target="USENIX OSDI">
  <data key="d0">published_in</data>
</edge>
<edge source="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER" target="2014">
  <data key="d0">published_year</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION" target="SUN">
  <data key="d0">author</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION" target="IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION">
  <data key="d0">published in</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION" target="2016">
  <data key="d0">publication year</data>
</edge>
<edge source="PHILLY" target="20 TRACES">
  <data key="d0">has quantity</data>
</edge>
<edge source="21" target="PYTORCH">
  <data key="d0">is</data>
</edge>
<edge source="C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna" target="Rethinking the Inception Architecture for Computer Vision">
  <data key="d0">authored</data>
</edge>
<edge source="Rethinking the Inception Architecture for Computer Vision" target="IEEE Conference on Computer Vision and Pattern Recognition">
  <data key="d0">was presented in</data>
</edge>
<edge source="Rethinking the Inception Architecture for Computer Vision" target="2016">
  <data key="d0">was published in year</data>
</edge>
<edge source="J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova" target="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding">
  <data key="d0">authored</data>
</edge>
<edge source="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" target="Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)">
  <data key="d0">was published in</data>
</edge>
<edge source="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" target="2019">
  <data key="d0">was published in year</data>
</edge>
<edge source="GANDIVA" target="introspective cluster scheduling for deep learning">
  <data key="d0">is</data>
</edge>
<edge source="GANDIVA" target="USENIX OSDI, 2018">
  <data key="d0">was presented in</data>
</edge>
<edge source="24 W. XIAO, R. BHARDWAJ, R. RAMJEE, M. SIVATHANU, N. KWATRA, Z. HAN, P. PATEL, X. PENG, H. ZHAO, Q. ZHANG, ET AL." target="GANDIVA">
  <data key="d0">are authors of</data>
</edge>
<edge source="25" target="TENSORFLOW">
  <data key="d0">is related to</data>
</edge>
<edge source="54" target="TENSORFLOW XLA">
  <data key="d0">is related to</data>
</edge>
<edge source="XLA" target="https://www.tensorflow.org">
  <data key="d0">is associated with</data>
</edge>
<edge source="26" target="MXNET">
  <data key="d0">is</data>
</edge>
<edge source="M. J. Freedman" target="SLAQ: Quality-Driven Scheduling for Distributed Machine Learning">
  <data key="d0">authored</data>
</edge>
<edge source="SLAQ: Quality-Driven Scheduling for Distributed Machine Learning" target="ACM Symposium on Cloud Computing">
  <data key="d0">published in</data>
</edge>
<edge source="SLAQ: Quality-Driven Scheduling for Distributed Machine Learning" target="2017">
  <data key="d0">published in year</data>
</edge>
<edge source="OPTIMUS" target="an efficient dynamic resource scheduler for deep learning clusters">
  <data key="d0">is</data>
</edge>
<edge source="OPTIMUS" target="EuroSys">
  <data key="d0">was presented in</data>
</edge>
<edge source="OPTIMUS" target="2018">
  <data key="d0">was published in year</data>
</edge>
<edge source="Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo" target="OPTIMUS">
  <data key="d0">are authors of</data>
</edge>
<edge source="THEMIS" target="fair and efficient GPU cluster scheduling">
  <data key="d0">is</data>
</edge>
<edge source="THEMIS" target="USENIX NSDI">
  <data key="d0">was presented in</data>
</edge>
<edge source="THEMIS" target="2020">
  <data key="d0">was presented in year</data>
</edge>
<edge source="THEMIS" target="K. Mahajan, A. Balasubramanian, A. Singhvi, S. Venkataraman, A. Akella, A. Phanishayee, and S. Chawla">
  <data key="d0">was authored by</data>
</edge>
<edge source="R. LIAW, R. BHARDWAJ, L. DUNLAP, Y. ZOU, J. E. GONZALEZ, I. STOICA, AND A. TUMANOV" target="HYPERSCHED: DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE">
  <data key="d0">authored</data>
</edge>
<edge source="HYPERSCHED: DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE" target="ACM SYMPOSIUM ON CLOUD COMPUTING">
  <data key="d0">published in</data>
</edge>
<edge source="HYPERSCHED: DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE" target="2019">
  <data key="d0">published in year</data>
</edge>
<edge source="CHET" target="an optimizing compiler for fully-homomorphic neural-network inferencing">
  <data key="d0">is</data>
</edge>
<edge source="CHET" target="ACM Conference on Programming Language Design and Implementation">
  <data key="d0">was presented in</data>
</edge>
<edge source="CHET" target="2019">
  <data key="d0">was presented in year</data>
</edge>
<edge source="R. Dathathri, O. Saarikivi, H. Chen, K. Laine, K. Lauter, S. Maleki, M. Musuvathi, and T. Mytkowicz" target="CHET">
  <data key="d0">are authors of</data>
</edge>
<edge source="TVM" target="an automated end-to-end optimizing compiler for deep learning">
  <data key="d0">is</data>
</edge>
<edge source="TVM" target="USENIX OSDI">
  <data key="d0">was presented in</data>
</edge>
<edge source="TVM" target="2018">
  <data key="d0">was presented in year</data>
</edge>
<edge source="T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, et al." target="TVM: An automated end-to-end optimizing compiler for deep learning">
  <data key="d0">are authors of</data>
</edge>
<edge source="GPIPE" target="33 Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al.">
  <data key="d0">is described in</data>
</edge>
<edge source="GPIPE" target="efficient training of giant neural networks using pipeline parallelism">
  <data key="d0">is about</data>
</edge>
<edge source="GPIPE" target="Advances in Neural Information Processing Systems">
  <data key="d0">was presented in</data>
</edge>
<edge source="GPIPE" target="2019">
  <data key="d0">was published in</data>
</edge>
<edge source="BLINK" target="fast and generic collectives for distributed ML">
  <data key="d0">is described as</data>
</edge>
<edge source="BLINK" target="Conference on Machine Learning and Systems">
  <data key="d0">was presented in</data>
</edge>
<edge source="BLINK" target="2020">
  <data key="d0">was presented in year</data>
</edge>
<edge source="Authors" target="G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica">
  <data key="d0">include</data>
</edge>
<edge source="Authors" target="M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler">
  <data key="d0">include</data>
</edge>
<edge source="Authors" target="V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan">
  <data key="d0">include</data>
</edge>
<edge source="NCCL" target="NVIDIA Collective Communications Library">
  <data key="d0">is</data>
</edge>
<edge source="36 J. LIU, J. WU, AND D. K. PANDA" target="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION OVER INNIBAND">
  <data key="d0">authored</data>
</edge>
<edge source="Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing" target="More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server">
  <data key="d0">authored</data>
</edge>
<edge source="More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server" target="Advances in Neural Information Processing Systems">
  <data key="d0">published in</data>
</edge>
<edge source="More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server" target="2013">
  <data key="d0">published in year</data>
</edge>
<edge source="A. AWAN, C.-H. CHU, H. SUBRAMONI, AND D. K. PANDA" target="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?">
  <data key="d0">authored</data>
</edge>
<edge source="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?" target="PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING">
  <data key="d0">published_in</data>
</edge>
<edge source="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?" target="2018">
  <data key="d0">published_year</data>
</edge>
<edge source="A. Vishnu, C. Siegel, T. Warfel, and V. Amatya" target="GossipGrad: Scalable Deep Learning Using Gossip Communication Based Asynchronous Gradient Descent">
  <data key="d0">authored</data>
</edge>
<edge source="41 Z. ZHANG, C. CHANG, H. LIN, Y. WANG, R. ARORA, AND X. JIN" target="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?">
  <data key="d0">authored</data>
</edge>
<edge source="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?" target="ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)">
  <data key="d0">published in</data>
</edge>
<edge source="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?" target="AUGUST 2020">
  <data key="d0">published in month and year</data>
</edge>
<edge source="42 Y. CHEN, Z. LIU, B. REN, AND X. JIN" target="ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS">
  <data key="d0">authored</data>
</edge>
<edge source="ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS" target="INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)">
  <data key="d0">published in</data>
</edge>
<edge source="INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)" target="JULY 2020">
  <data key="d0">date</data>
</edge>
<edge source="VDNN" target="Virtualized Deep Neural Networks for scalable, memory-efficient neural network design">
  <data key="d0">is</data>
</edge>
<edge source="VDNN" target="2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)">
  <data key="d0">was presented in</data>
</edge>
<edge source="VDNN" target="2016">
  <data key="d0">was published in year</data>
</edge>
<edge source="SWAPADVISOR" target="C.-C. Huang, G. Jin, and J. Li">
  <data key="d0">is authored by</data>
</edge>
<edge source="SWAPADVISOR" target="ACM ASPLOS">
  <data key="d0">was presented in</data>
</edge>
<edge source="SWAPADVISOR" target="2020">
  <data key="d0">was published in</data>
</edge>
<edge source="SWAPADVISOR" target="GPU memory limit via smart swapping">
  <data key="d0">pushes deep learning beyond</data>
</edge>
<edge source="MESOS" target="a platform for fine-grained resource sharing in the data center">
  <data key="d0">is</data>
</edge>
<edge source="47 B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H. Katz, S. Shenker, and I. Stoica" target="MESOS: A platform for fine-grained resource sharing in the data center">
  <data key="d0">authored</data>
</edge>
<edge source="MESOS paper" target="USENIX NSDI, 2011">
  <data key="d0">was published in</data>
</edge>
<edge source="V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al." target="Apache Hadoop YARN: Yet Another Resource Negotiator">
  <data key="d0">authored</data>
</edge>
<edge source="Apache Hadoop YARN: Yet Another Resource Negotiator" target="ACM Symposium on Cloud Computing">
  <data key="d0">was published in</data>
</edge>
<edge source="Apache Hadoop YARN: Yet Another Resource Negotiator" target="2013">
  <data key="d0">was published in year</data>
</edge>
<edge source="G. GIUNTA, R. MONTELLA, G. AGRILLO, AND G. COVIELLO" target="A GPGPU Transparent Virtualization Component for High Performance Computing Clouds">
  <data key="d0">authored</data>
</edge>
<edge source="A GPGPU Transparent Virtualization Component for High Performance Computing Clouds" target="European Conference on Parallel Processing">
  <data key="d0">was presented in</data>
</edge>
<edge source="European Conference on Parallel Processing" target="2010">
  <data key="d0">occurred in</data>
</edge>
<edge source="V. T. RAVI, M. BECCHI, G. AGRAWAL, AND S. CHAKRADHAR" target="SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK">
  <data key="d0">authored</data>
</edge>
<edge source="SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK" target="PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING">
  <data key="d0">was published in</data>
</edge>
<edge source="PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING" target="2011">
  <data key="d0">publication year</data>
</edge>
<edge source="GVIM" target="GPU-accelerated virtual machines">
  <data key="d0">is</data>
</edge>
<edge source="GVIM" target="Proceedings of the 3rd ACM Workshop on System-Level Virtualization for High Performance Computing">
  <data key="d0">was presented in</data>
</edge>
<edge source="GVIM" target="2009">
  <data key="d0">was published in</data>
</edge>
<edge source="J. DUATO, A. J. PENA, F. SILLA, R. MAYO, AND E. S. QUINTANA-ORT" target="RCUDA: REDUCING THE NUMBER OF GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS">
  <data key="d0">authored</data>
</edge>
<edge source="RCUDA: REDUCING THE NUMBER OF GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS" target="2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION">
  <data key="d0">was presented in</data>
</edge>
<edge source="2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION" target="2010">
  <data key="d0">occurred in</data>
</edge>
<edge source="VCUDA" target="GPU-accelerated high-performance computing in virtual machines">
  <data key="d0">is</data>
</edge>
<edge source="VCUDA" target="IEEE Transactions on Computers">
  <data key="d0">published in</data>
</edge>
<edge source="SUN and K. LI" target="VCUDA: GPU-accelerated high-performance computing in virtual machines">
  <data key="d0">authored</data>
</edge>
<edge source="MXNet" target="a flexible and efficient machine learning library">
  <data key="d0">is</data>
</edge>
<edge source="MXNet" target="heterogeneous distributed systems">
  <data key="d0">is designed for</data>
</edge>
<edge source="55 T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang" target="MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems">
  <data key="d0">are authors of</data>
</edge>
<edge source="MXNet paper" target="arXiv preprint arXiv:1512.01274">
  <data key="d0">was published in</data>
</edge>
<edge source="MXNet paper" target="2015">
  <data key="d0">was published in year</data>
</edge>
<edge source="56 C. Gregg, J. Dorn, K. Hazelwood, and K. Skadron" target="Fine-Grained Resource Sharing for Concurrent GPGPU Kernels">
  <data key="d0">authored</data>
</edge>
<edge source="Fine-Grained Resource Sharing for Concurrent GPGPU Kernels" target="4th USENIX Workshop on Hot Topics in Parallelism">
  <data key="d0">was presented at</data>
</edge>
<edge source="Fine-Grained Resource Sharing for Concurrent GPGPU Kernels" target="2012">
  <data key="d0">was presented in year</data>
</edge>
<edge source="57 S. PAI, M. J. THAZHUTHAVEETIL, AND R. GOVINDARAJAN" target="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS">
  <data key="d0">are authors of</data>
</edge>
<edge source="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS" target="ACM SIGARCH COMPUTER ARCHITECTURE NEWS">
  <data key="d0">published in</data>
</edge>
<edge source="TASO" target="58 Z. JIA, O. PADON, J. THOMAS, T. WARSZAWSKI, M. ZAHARIA, AND A. AIKEN, TASO: OPTIMIZING DEEP LEARNING COMPUTATION WITH AUTOMATIC GENERATION OF GRAPH SUBSTITUTIONS, IN ACM SOSP, 2019">
  <data key="d0">is described in</data>
</edge>
</graph></graphml>