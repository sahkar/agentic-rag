<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d0" for="edge" attr.name="relationship" attr.type="string"/>
<graph edgedefault="directed"><node id="THE DOMINANT PRACTICE TODAY"/>
<node id="DEDICATED GPU CLUSTERS FOR TRAINING AND INFERENCE SEPARATELY"/>
<node id="GPU CLUSTERS"/>
<node id="SHARED GPU CLUSTERS"/>
<node id="TRAINING AND INFERENCE"/>
<node id="GPUS"/>
<node id="CURRENT PRACTICE"/>
<node id="DEDICATED CLUSTERS FOR TRAINING AND INFERENCE SEPARATELY"/>
<node id="WE"/>
<node id="FINE-GRAINED TIME-SHARING GPU CLUSTERS"/>
<node id="DIFFERENT APPLICATIONS"/>
<node id="THE PEAK LOAD"/>
<node id="LIMITED SHARING BETWEEN APPLICATIONS AND TASK TYPES"/>
<node id="SERVICE-LEVEL OBJECTIVES (SLOS)"/>
<node id="STRICT"/>
<node id="PIPESWITCH"/>
<node id="A SYSTEM"/>
<node id="UNUSED CYCLES OF AN INFERENCE APPLICATION TO BE FILLED BY TRAINING OR OTHER INFERENCE APPLICATIONS"/>
<node id="GPU UTILIZATION"/>
<node id="SIGNIFICANTLY WITH PIPESWITCH"/>
<node id="IMPROVEMENT"/>
<node id="SLOS"/>
<node id="SINGLE-GPU TASKS FOR TRAINING AND INFERENCE"/>
<node id="MULTI-GPU INFERENCE TASKS"/>
<node id="PERFORMING PIPESWITCH ON EACH GPU WITH TRANSACTIONS"/>
<node id="SINGLE-GPU TRAINING"/>
<node id="ASYNCHRONOUS MULTI-GPU TRAINING"/>
<node id="DATA PARALLEL STRATEGIES"/>
<node id="PREEMPTING ONE GPU"/>
<node id="OTHER GPUS"/>
<node id="A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS"/>
<node id="A SINGLE GPU"/>
<node id="THEM OUT OF THE BOX"/>
<node id="ONE WAY TO SEAMLESSLY USE PIPESWITCH FOR SYNCHRONOUS MULTI-GPU TRAINING"/>
<node id="ELASTIC SYNCHRONOUS TRAINING"/>
<node id="THE DYNAMIC CHANGING OF THE NUMBER OF GPUS USED FOR TRAINING"/>
<node id="HIGH THROUGHPUT CLOSE TO THE UPPER BOUND"/>
<node id="NEAR 100 GPU UTILIZATION"/>
<node id="THE PERFORMANCE OF PIPESWITCH"/>
<node id="EXPERIMENTS"/>
<node id="A VARIETY OF DNN MODELS AND GPU CARDS"/>
<node id="THE AGILITY OF DL APPLICATIONS"/>
<node id="SO BY INTRODUCING PIPELINED CONTEXT SWITCHING"/>
<node id="THE KEY IDEA"/>
<node id="THE LAYERED STRUCTURE OF NEURAL NETWORK MODELS AND THEIR LAYER-BY-LAYER COMPUTATION PATTERN"/>
<node id="MODEL TRANSMISSION OVER THE PCIE AND TASK EXECUTION IN THE GPU WITH MODEL-AWARE GROUPING"/>
<node id="A PIPELINED MODEL TRANSMISSION MECHANISM"/>
<node id="PIPELINED MODEL TRANSMISSION MECHANISM"/>
<node id="MODEL TRANSMISSION OVER THE PCIE"/>
<node id="MODEL COMPUTATION IN THE GPU"/>
<node id="TRANSMITTING A TASK FROM CPU TO GPU"/>
<node id="PCIE BANDWIDTH"/>
<node id="UNIFIED MEMORY MANAGEMENT"/>
<node id="ACTIVE-STANDBY WORKER SWITCHING MECHANISMS"/>
<node id="THE PIPELINING"/>
<node id="PROCESS-LEVEL ISOLATION"/>
<node id="AN ACTIVE-STANDBY MECHANISM"/>
<node id="FAST WORKER SWITCHING"/>
<node id="A PIPESWITCH PROTOTYPE"/>
<node id="IT WITH PYTORCH"/>
<node id="THE MODEL STRUCTURE"/>
<node id="HOOKS FOR PYTORCH TO WAIT FOR TRANSMISSION OR SYNCHRONIZE THE EXECUTION"/>
<node id="SYSTEM PROTOTYPE FOR PIPESWITCH"/>
<node id="3600 LINES OF CODE"/>
<node id="C AND PYTHON"/>
<node id="PYTORCH 21"/>
<node id="DEEP LEARNING (DL)"/>
<node id="AN EMERGING FAMILY OF INTELLIGENT APPLICATIONS"/>
<node id="MANY DOMAINS"/>
<node id="RETAIL"/>
<node id="TRANSPORTATION"/>
<node id="FINANCE"/>
<node id="HEALTHCARE"/>
<node id="ONE OF THE MOST WIDELY-USED CLASSES OF ACCELERATORS FOR DL"/>
<node id="DL WORKLOADS"/>
<node id="THROUGHPUT-INTENSIVE TRAINING TASKS"/>
<node id="LATENCY-SENSITIVE INFERENCE TASKS"/>
<node id="INFERENCE TASKS"/>
<node id="TRAINING CLUSTERS UNDER ASH CROWDS"/>
<node id="TRAINING TASKS"/>
<node id="INFERENCE CLUSTERS WHEN THE INFERENCE LOAD IS LOW"/>
<node id="ASH CROWD"/>
<node id="WHEN AN APPLICATION SUDDENLY BECOMES POPULAR AND THE DEMAND GROWS BEYOND THE OPERATORS EXPECTATION"/>
<node id="TRAINING CLUSTER"/>
<node id="TRAINING TASKS FOR INFERENCE TASKS"/>
<node id="INFERENCE CLUSTERS"/>
<node id="STRICT SERVICE LEVEL OBJECTIVES (SLOS)"/>
<node id="OVER-PROVISIONED FOR THE PEAK LOAD"/>
<node id="USER REQUESTS"/>
<node id="STRICT SLOS"/>
<node id="PRODUCTION SYSTEMS"/>
<node id="EACH APPLICATION ON PER-GPU GRANULARITY"/>
<node id="PROVISIONING"/>
<node id="THE INTERFERENCE BETWEEN APPLICATIONS"/>
<node id="GPUS TO APPLICATIONS ON PER-GPU GRANULARITY"/>
<node id="VMS, CONTAINERS OR PROCESSES OF AN APPLICATION"/>
<node id="ALLOCATING GPUS TO APPLICATIONS"/>
<node id="THE INTERFERENCE BETWEEN DIFFERENT APPLICATIONS"/>
<node id="THE SLO REQUIREMENTS"/>
<node id="MULTIPLE DL APPLICATIONS"/>
<node id="THE SAME GPU SERVER"/>
<node id="PACKING MULTIPLE DL APPLICATIONS TO THE SAME GPU SERVER"/>
<node id="GPU UTILIZATION VIA TIME-SHARING"/>
<node id="OPERATING SYSTEMS"/>
<node id="HIGH CPU UTILIZATION"/>
<node id="TASK SCHEDULING AND CONTEXT SWITCHING"/>
<node id="THE IDEA OF NE-GRAINED CPU TIME-SHARING"/>
<node id="CLUSTER SCHEDULING"/>
<node id="NE-GRAINED TIME-SHARING"/>
<node id="BETTER UTILIZATION THAN PROVISIONING DEDICATED RESOURCES"/>
<node id="NECESSARY PROCESS-LEVEL ISOLATION"/>
<node id="CPU WORKLOADS"/>
<node id="SCHEDULING CYCLES"/>
<node id="ENABLED"/>
<node id="GOOGLE BORG 1"/>
<node id="ONLINE SERVICES AND BATCH JOBS"/>
<node id="20-30 MACHINES"/>
<node id="COMPARED WITH NOT PACKING THEM"/>
<node id="GPU"/>
<node id="HIGH OVERHEAD WHEN SWITCHING BETWEEN TASKS"/>
<node id="THE GAP"/>
<node id="THE PRECIOUS GPU MEMORY AND SLOW SWITCHING"/>
<node id="NAIVELY USING GPUS"/>
<node id="THE REQUIREMENTS OF DL INFERENCE THAT HAVE STRICT SLOS IN THE RANGE OF TENS TO HUNDREDS OF MILLISECONDS"/>
<node id="DNN MODEL (E.G., RESNET)"/>
<node id="STATE-OF-THE-ART TRICKS LIKE CUDA UNIFIED MEMORY 4 (6)"/>
<node id="MULTIPLE SECONDS DELAY"/>
<node id="CPU APPLICATIONS"/>
<node id="MILLISECONDS OR EVEN MICROSECONDS"/>
<node id="THE EXISTING SOLUTION"/>
<node id="SPATIALLY SHARE THE GPU MEMORY"/>
<node id="THIS APPROACH"/>
<node id="STRONG GPU MEMORY ISOLATION BETWEEN APPLICATIONS"/>
<node id="NVIDIA MULTIPLE PROCESS SHARING (MPS) 6"/>
<node id="MULTIPLE PROCESSES TO USE THE SAME GPU"/>
<node id="SALUS 7"/>
<node id="MULTI-PROCESS SUPPORT FROM NVIDIA"/>
<node id="INFERENCE PROCESS TO SHARE THE GPU WITH THE TRAINING PROCESS"/>
<node id="NVIDIA MPS 6"/>
<node id="OFFICIAL SUPPORT FOR SHARING A GPU BETWEEN MULTIPLE PROCESSES"/>
<node id="GPU MEMORY"/>
<node id="MUCH MORE LIMITED THAN HOST MEMORY"/>
<node id="PRELOAD MANY APPLICATIONS"/>
<node id="ONE SINGLE MEMORY-INTENSIVE TRAINING TASK"/>
<node id="ALL THE GPU MEMORY"/>
<node id="THE TRAINING TASK"/>
<node id="ITS GPU ENVIRONMENT"/>
<node id="THE GPU MEMORY"/>
<node id="THE ENTIRE GPU MEMORY"/>
<node id="WHEN INFERENCE TASKS COME"/>
<node id="MEMORY FOOTPRINTS OF INFERENCE TASKS"/>
<node id="INCREASING"/>
<node id="MODELS"/>
<node id="GETTING LARGER"/>
<node id="REQUEST BATCHING"/>
<node id="PREVALENTLY USED TO INCREASE THROUGHPUT"/>
<node id="THROUGHPUT 3"/>
<node id="GPU MEMORY REQUIREMENT OF INFERENCE APPLICATIONS"/>
<node id="A CONTEXT SWITCHING DESIGN"/>
<node id="THE SWITCHING OVERHEAD"/>
<node id="THE CONTENTS ON GPU MEMORY QUICKLY"/>
<node id="A BETTER APPROACH FOR EFFICIENTLY TIME-SHARING GPUS"/>
<node id="NO EXISTING SOLUTION"/>
<node id="SUCH CONTEXT SWITCHING ABSTRACTION FOR GPU"/>
<node id="SO BY INTRODUCING A NEW TECHNOLOGY CALLED PIPELINED CONTEXT SWITCHING"/>
<node id="PIPELINED CONTEXT SWITCHING"/>
<node id="THE CHARACTERISTICS OF DL APPLICATIONS"/>
<node id="MILLISECOND-SCALE OVERHEAD FOR SWITCHING TASKS ON GPUS"/>
<node id="A MAJOR CHALLENGE FAST GPU CONTEXT SWITCHING BETWEEN DIFFERENT PROCESSES"/>
<node id="APPLICATION"/>
<node id="THERE"/>
<node id="CONTEXT SWITCHING"/>
<node id="TASK SWITCHING OVERHEAD ON GPUS FOR DL APPLICATIONS"/>
<node id="DNN MODELS"/>
<node id="HOST MEMORY"/>
<node id="MUCH LARGER AND CHEAPER THAN GPU MEMORY"/>
<node id="THE MODELS"/>
<node id="TRAINING OR INFERENCE"/>
<node id="ENTERPRISES"/>
<node id="EITHER PRIVATELY OR PUBLICLY SHARED BY MULTIPLE USERS"/>
<node id="11 M. JEON, S. VENKATARAMAN, A. PHANISHAYEE, U. QIAN, W. XIAO, AND F. YANG"/>
<node id="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS"/>
<node id="USENIX ATC"/>
<node id="2019"/>
<node id="512 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="USENIX ASSOCIATION"/>
<node id="NUMBER OF APPLICATIONS THAT CAN BE MULTIPLEXED"/>
<node id="GPU MEMORY SIZE"/>
<node id="EACH APPLICATION"/>
<node id="ENTIRE GPU COMPUTE AND MEMORY RESOURCES DURING ITS TIME SLICE"/>
<node id="GPU-EFFICIENT MULTIPLEXING OF MANY DL APPLICATIONS ON GPU SERVERS VIA FINE-GRAINED TIME-SHARING"/>
<node id="MILLISECOND-SCALE LATENCIES AND HIGH THROUGHPUT AS DEDICATED SERVERS"/>
<node id="GPU-EFFICIENT FINE-GRAINED TIME-SHARING FOR MULTIPLE DL APPLICATIONS"/>
<node id="MILLISECOND-SCALE CONTEXT SWITCHING LATENCIES"/>
<node id="HIGH THROUGHPUT"/>
<node id="ALL THE IDEAS INTO OUR SYSTEM"/>
<node id="THE GAP OF GPU MEMORY SHARING AND SWITCHING"/>
<node id="THE DESIGN OF AN EFFICIENT TIME-SHARING GPU CLUSTER FOR DL WORKLOADS"/>
<node id="GPU-EFFICIENT MULTIPLEXING OF MULTIPLE DL APPLICATIONS ON GPU SERVERS"/>
<node id="GPU-EFFICIENT FINE-GRAINED TIME-SHARING"/>
<node id="MILLISECOND-SCALE TASK SWITCHING TIME"/>
<node id="DL APPLICATIONS ON TIME-SHARING GPUS TO MEET STRICT SLOS"/>
<node id="SUCH SMALL SWITCHING OVERHEAD"/>
<node id="DL APPLICATIONS TO SATISFY STRICT SLO REQUIREMENTS"/>
<node id="MILLISECOND-SCALE TASK SWITCHING OVERHEAD"/>
<node id="SLO REQUIREMENTS"/>
<node id="A MEASUREMENT STUDY"/>
<node id="THE MEASUREMENT STUDY"/>
<node id="THE TASK SWITCHING OVERHEAD"/>
<node id="THE OVERHEAD OF EACH COMPONENT"/>
<node id="THE SWITCHING OVERHEAD INTO FOUR COMPONENTS"/>
<node id="THE FOUR COMPONENTS"/>
<node id="OLD TASK CLEANING"/>
<node id="NEW TASK INITIALIZATION"/>
<node id="GPU MEMORY ALLOCATION"/>
<node id="MODEL TRANSMISSION VIA PCIE FROM CPU TO GPU"/>
<node id="INSTANCE TYPE"/>
<node id="G4DN.2XLARGE"/>
<node id="P3.2XLARGE"/>
<node id="GPU TYPE OF G4DN.2XLARGE"/>
<node id="NVIDIA T4"/>
<node id="GPU TYPE OF P3.2XLARGE"/>
<node id="NVIDIA V100"/>
<node id="TASK CLEANING TIME FOR G4DN.2XLARGE"/>
<node id="155 MS"/>
<node id="TASK CLEANING TIME FOR P3.2XLARGE"/>
<node id="165 MS"/>
<node id="TASK INITIALIZATION TIME FOR G4DN.2XLARGE"/>
<node id="5530 MS"/>
<node id="TASK INITIALIZATION TIME FOR P3.2XLARGE"/>
<node id="7290 MS"/>
<node id="MEMORY ALLOCATION TIME FOR G4DN.2XLARGE"/>
<node id="10 MS"/>
<node id="MEMORY ALLOCATION TIME FOR P3.2XLARGE"/>
<node id="13 MS"/>
<node id="MODEL TRANSMISSION TIME FOR G4DN.2XLARGE"/>
<node id="91 MS"/>
<node id="MODEL TRANSMISSION TIME FOR P3.2XLARGE"/>
<node id="81 MS"/>
<node id="TOTAL OVERHEAD FOR G4DN.2XLARGE"/>
<node id="5787 MS"/>
<node id="TOTAL OVERHEAD FOR P3.2XLARGE"/>
<node id="7551 MS"/>
<node id="INFERENCE TIME FOR G4DN.2XLARGE"/>
<node id="105 MS"/>
<node id="INFERENCE TIME FOR P3.2XLARGE"/>
<node id="32 MS"/>
<node id="EVERY COMPONENT"/>
<node id="A CONSIDERABLE AMOUNT OF TIME"/>
<node id="TIME"/>
<node id="TENS OF MILLISECONDS TO SECONDS"/>
<node id="ONE SOURCE OF THE OVERHEAD"/>
<node id="THE CONTENTIONS BOTH ON THE COMPUTATION AND MEMORY OF THE GPU"/>
<node id="WHEN AN INFERENCE TASK COMES"/>
<node id="A HOLISTIC APPROACH"/>
<node id="THE OVERHEAD OF ALL THE COMPONENTS"/>
<node id="OUR DESIGN"/>
<node id="A KEY OBSERVATION"/>
<node id="A LAYERED STRUCTURE"/>
<node id="A LAYER-BY-LAYER COMPUTATION PATTERN"/>
<node id="USUALLY DEEP"/>
<node id="MULTIPLE LAYERS STACKING ONE ON ANOTHER"/>
<node id="COMPUTATION OF DNN MODELS"/>
<node id="LAYER BY LAYER"/>
<node id="NO NEED TO WAIT FOR THE ENTIRE MODEL TO BE TRANSMITTED TO THE GPU BEFORE STARTING COMPUTATION"/>
<node id="A TASK"/>
<node id="THE ENTIRE MODEL TO BE TRANSMITTED TO THE GPU BEFORE BEGINNING THE COMPUTATION"/>
<node id="NAIVE PIPELINING ON PER-LAYER GRANULARITY"/>
<node id="HIGH OVERHEAD ON TENSOR TRANSMISSION AND SYNCHRONIZATION"/>
<node id="PIPELINING ON PER-LAYER GRANULARITY"/>
<node id="SYNCHRONIZATION FOR EVERY LAYER"/>
<node id="LAYERS INTO GROUPS"/>
<node id="AN OPTIMAL MODEL-AWARE GROUPING ALGORITHM"/>
<node id="THE BEST GROUPING STRATEGY FOR A GIVEN MODEL"/>
<node id="AN ALGORITHM TO FIND THE OPTIMAL GROUPING STRATEGY FOR A GIVEN MODEL"/>
<node id="THE COMPUTATION OF A DL TASK"/>
<node id="A SIMPLE, REGULAR PATTERN FOR MEMORY ALLOCATION"/>
<node id="A DL TASK"/>
<node id="TWO IMPORTANT TYPES OF DATA IN THE GPU MEMORY"/>
<node id="TWO IMPORTANT TYPES OF DATA"/>
<node id="THE DNN MODEL"/>
<node id="THE MODEL PARAMETERS"/>
<node id="THE INTERMEDIATE RESULTS"/>
<node id="THE DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT (E.G., CUDA UNI-ED MEMORY 4)"/>
<node id="AN OVERKILL"/>
<node id="UNNECESSARY OVERHEAD"/>
<node id="NVIDIA"/>
<node id="CUDA UNIFIED MEMORY 4"/>
<node id="MEMORY MOVEMENT BETWEEN THE HOST MEMORY AND THE GPU MEMORY"/>
<node id="MEMORY MOVEMENT"/>
<node id="APPLICATIONS"/>
<node id="DEDICATED MEMORY DAEMON"/>
<node id="OVERHEAD"/>
<node id="UNIFIED MEMORY MANAGEMENT WITH THE MEMORY DAEMON"/>
<node id="MINIMAL MEMORY FOOTPRINT"/>
<node id="EXTRA MEMORY COPIES"/>
<node id="THE DAEMON"/>
<node id="IT TO EACH TASK"/>
<node id="THE EXPENSIVE GPU MEMORY MANAGER"/>
<node id="THE MEMORY DAEMON"/>
<node id="CUDAMALLOC"/>
<node id="THE MEMORY TO THE WORKERS"/>
<node id="THE MEMORY AT RUNTIME"/>
<node id="THE DNN MODELS"/>
<node id="ONLY ONCE IN THE MEMORY DAEMON"/>
<node id="IN EVERY WORKER"/>
<node id="STORING THE DNN MODELS ONLY ONCE IN THE MEMORY DAEMON"/>
<node id="MEMORY FOOTPRINT"/>
<node id="THE MEMORY ALLOCATION FOR A DNN MODEL IS DETERMINISTIC"/>
<node id="EXTRA MEMORY COPIES BETWEEN THE DAEMON AND THE WORKERS"/>
<node id="THE IPC OVERHEAD"/>
<node id="NO UNIFIED MEMORY MANAGEMENT"/>
<node id="EACH WORKER TO KEEP A COPY FOR EACH DNN MODEL"/>
<node id="KEEPING A COPY FOR EACH DNN MODEL"/>
<node id="THE MEMORY FOOTPRINT"/>
<node id="EACH SERVER"/>
<node id="AN ACTIVE WORKER"/>
<node id="MULTIPLE STANDBY WORKERS"/>
<node id="A SERVER"/>
<node id="ONE OR MORE STANDBY WORKERS"/>
<node id="THE ACTIVE WORKER"/>
<node id="THE CURRENT TASK ON THE GPU"/>
<node id="THE STANDBY WORKERS"/>
<node id="THE CPU"/>
<node id="THE NEXT TASK"/>
<node id="THE WORKER THAT CURRENTLY EXECUTES A TASK IN THE GPU"/>
<node id="WORKER"/>
<node id="PROCESS"/>
<node id="TASKS"/>
<node id="ONE GPU"/>
<node id="ACTIVE WORKER"/>
<node id="CURRENT TASK"/>
<node id="CONTROLLER"/>
<node id="MEMORY DAEMON"/>
<node id="STANDBY WORKER"/>
<node id="TASK TO GPU"/>
<node id="TASK"/>
<node id="PIPELINED MODEL TRANSMISSION"/>
<node id="OUR MECHANISM"/>
<node id="OLD TASK CLEANING IN THE ACTIVE WORKER AND NEW TASK INITIALIZATION IN THE STANDBY WORKER"/>
<node id="WORKER SWITCHING OVERHEAD"/>
<node id="TABLE 2"/>
<node id="COMPARISON OF WORKER SWITCHING MECHANISMS"/>
<node id="AN ACTIVE AND STANDBY WORKER SWITCHING MECHANISM"/>
<node id="THE ACTIVE AND STANDBY WORKER SWITCHING MECHANISM"/>
<node id="THE OVERHEAD OF BOTH TASK CLEANING AND TASK INITIALIZATION"/>
<node id="SEPARATE WORKER PROCESSES"/>
<node id="US TO ADDRESS NEW TECHNICAL CHALLENGES ON MEMORY MANAGEMENT AND WORKER SWITCHING ACROSS DIFFERENT PROCESSES"/>
<node id="FAST TASK SWITCHING"/>
<node id="ALL OTHER COMPONENTS OF PIPESWITCH THE SAME"/>
<node id="THE FOLLOWING MECHANISMS DISCUSSED IN 4.4"/>
<node id="ACTIVE-STANDBY WORKER SWITCHING"/>
<node id="EVALUATE THE EFFECTIVENESS OF ACTIVE-STANDBY WORKER SWITCHING"/>
<node id="THE ACTIVE-STANDBY WORKER SWITCHING MECHANISM USED BY PIPESWITCH"/>
<node id="AN ACTIVE-STANDBY WORKER SWITCHING MECHANISM"/>
<node id="PARALLELIZE OLD TASK CLEANING AND NEW TASK INITIALIZATION"/>
<node id="MINIMAL OVERHEAD"/>
<node id="PIPELINING"/>
<node id="A CANONICAL TECHNIQUE"/>
<node id="COMPUTER SYSTEMS"/>
<node id="SYSTEM PERFORMANCE"/>
<node id="RESOURCE UTILIZATION"/>
<node id="TWO SOURCES OF SYSTEM OVERHEADS"/>
<node id="PRIOR WORK IN DL SYSTEMS SUCH AS PIPEDREAM 8 AND BYTESCHEDULER 9"/>
<node id="PIPELINING TO DISTRIBUTED TRAINING"/>
<node id="THESE SOLUTIONS"/>
<node id="INTER-BATCH PIPELINING"/>
<node id="COMPUTATION AND GRADIENT TRANSMISSION OF DIFFERENT BATCHES"/>
<node id="COMPUTATION AND GRADIENT TRANSMISSION"/>
<node id="TRAINING WORKLOADS OF THE SAME DNN MODEL"/>
<node id="INTRA-BATCH PIPELINING"/>
<node id="MODEL TRANSMISSION AND COMPUTATION"/>
<node id="OVERHEAD OF SWITCHING BETWEEN DIFFERENT DNN MODELS"/>
<node id="EITHER INFERENCE OR TRAINING"/>
<node id="NEW TECHNIQUES"/>
<node id="TRAINING"/>
<node id="INFERENCE"/>
<node id="SWITCHING OVERHEAD"/>
<node id="THREE KEY TECHNIQUES"/>
<node id="A SYSTEM PROTOTYPE"/>
<node id="INEFFICIENCIES IN TODAYS SHARED GPU CLUSTERS"/>
<node id="RUNNING DL WORKLOADS ON GPUS IN THE NE-GRAINED TIME-SHARING MODEL"/>
<node id="MULTIPLE DL APPLICATIONS ONTO THE SAME GPU"/>
<node id="PACKING MULTIPLE DL APPLICATIONS ONTO THE SAME GPU"/>
<node id="NE-GRAINED TIME-SHARING ABSTRACTION"/>
<node id="MORE FLEXIBLE FINE-GRAINED SCHEDULING"/>
<node id="DYNAMIC WORKLOADS"/>
<node id="DEDICATED PHYSICAL FORMS AND POWER SUPPLIES"/>
<node id="HIGH SPEED NETWORKS"/>
<node id="SPECIALIZED TASK SCHEDULERS"/>
<node id="500 14TH USENIX SYMPOSIUM"/>
<node id="OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="USENIX SYMPOSIUM"/>
<node id="QUESTION"/>
<node id="WHY BUILD A SHARED CLUSTER INSTEAD OF A DEDICATED ONE FOR EACH USER"/>
<node id="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="KUBERNETES"/>
<node id="14TH USENIX SYMPOSIUM"/>
<node id="THE MAIN REASON"/>
<node id="TO BRING DOWN THE COST"/>
<node id="THE DEMAND OF TRAINING"/>
<node id="WELL PREDICTABLE"/>
<node id="THE PROGRESS OF DIFFERENT DEVELOPERS"/>
<node id="THE DEMAND OF INFERENCE"/>
<node id="MORE PREDICTABLE"/>
<node id="AN INFERENCE TASK FOR A PARTICULAR APPLICATION"/>
<node id="A DAILY PERIODICAL PATTERN"/>
<node id="THE APPLICATION USAGE"/>
<node id="THE PATTERNS"/>
<node id="ACROSS DIFFERENT TASKS"/>
<node id="A SHARED CLUSTER"/>
<node id="THE RESOURCE UTILIZATION"/>
<node id="TIME-SHARING"/>
<node id="DIFFERENT TASKS"/>
<node id="SHARED CLUSTERS"/>
<node id="HIGH UTILIZATION"/>
<node id="TRAINING CLUSTERS"/>
<node id="POWERFUL GPUS"/>
<node id="RUN TRAINING TASKS"/>
<node id="OFTEN ELASTIC"/>
<node id="STRICT DEADLINES"/>
<node id="GPUS DESIGNED FOR INFERENCE TASKS"/>
<node id="NEW GPU HARDWARE"/>
<node id="UP TO 32GB GPU MEMORY"/>
<node id="15.7 TFLOPS (SINGLE-PRECISION)"/>
<node id="16GB GPU MEMORY"/>
<node id="8.1 TFLOPS (SINGLE-PRECISION)"/>
<node id="COMPARABLE PERFORMANCE WITH NVIDIA V100"/>
<node id="NEW ALGORITHMS AND SYSTEMS FOR DISTRIBUTED TRAINING"/>
<node id="MULTIPLE GPUS TO ACCELERATE TRAINING"/>
<node id="OUR INDUSTRY COLLABORATOR"/>
<node id="A LEADING ONLINE SERVICE PROVIDER"/>
<node id="THIS OBSERVATION"/>
<node id="THIS SERVICE PROVIDER"/>
<node id="MORE THAN 10K V100 GPUS FOR TRAINING"/>
<node id="AT LEAST 5 AS MANY T4 GPUS FOR INFERENCE"/>
<node id="THE COMPUTATION POWER ON BOTH SIDES"/>
<node id="THE SAME ORDER OF MAGNITUDE"/>
<node id="INFERENCE GPUS"/>
<node id="DURING LESS BUSY TIMES"/>
<node id="TRAINING MODELS"/>
<node id="DAILY UPDATES WITH LATEST DATA"/>
<node id="A GOOD EXAMPLE"/>
<node id="NE-TUNE BERT USING DAILY NEWS"/>
<node id="BORG-LIKE 1 SYSTEMS"/>
<node id="INFERENCE AND TRAINING WORKLOADS"/>
<node id="COMPLEMENTARY USAGE PATTERNS"/>
<node id="ONLINE INFERENCE SERVICES"/>
<node id="MORE IDLE DURING MIDNIGHT"/>
<node id="MANY TRAINING DEVELOPERS"/>
<node id="A TIME-CONSUMING JOB AT NIGHT"/>
<node id="INFERENCE LOADS ON DIFFERENT MODELS"/>
<node id="DIFFERENT PATTERNS"/>
<node id="TIME SHARING"/>
<node id="ANY SERVER"/>
<node id="ANY TASK"/>
<node id="LOW OVERHEAD TO SWITCH BETWEEN DIFFERENT APPLICATIONS"/>
<node id="A MODERN SERVER"/>
<node id="SEVERAL TB OF HOST MEMORY"/>
<node id="IT TO LOAD MANY APPLICATIONS"/>
<node id="TASK EXECUTION ON GPUS"/>
<node id="VERY LIMITED"/>
<node id="HIGH-END GPUS"/>
<node id="16 GB FOR T4"/>
<node id="32 GB FOR V100"/>
<node id="TASK EXECUTION"/>
<node id="STORING THE STATE OF IDLE APPLICATIONS"/>
<node id="DL TASKS"/>
<node id="A LARGE AMOUNT OF MEMORY ON A GPU"/>
<node id="ALL OF THE MEMORY ON A GPU"/>
<node id="DL APPLICATIONS"/>
<node id="LARGE MODELS"/>
<node id="LARGE AMOUNTS OF INTERMEDIATE RESULTS"/>
<node id="INTERMEDIATE RESULTS"/>
<node id="A LOT OF GPU MEMORY"/>
<node id="TRAINING TASKS WHICH ARE MEMORY-INTENSIVE"/>
<node id="MULTIPLE INFERENCE TASKS WHICH HAVE LARGE MODELS"/>
<node id="STATE-OF-THE-ART MODELS"/>
<node id="DEEPER AND LARGER"/>
<node id="IDLE APPLICATIONS"/>
<node id="LARGE MEMORY SPACE"/>
<node id="THE ACTIVE APPLICATION"/>
<node id="THE NUMBER OF APPLICATIONS THAT CAN BE SERVED BY A GPU SERVER"/>
<node id="ITS HOST MEMORY SIZE"/>
<node id="SWITCHING A TASK"/>
<node id="HEAVY MEMORY SWAPPING"/>
<node id="MANY ONLINE INFERENCE WORKLOADS"/>
<node id="NAIVE MEMORY SWAPPING BETWEEN THE HOST MEMORY AND THE GPU MEMORY"/>
<node id="REQUESTS TO BE HANDLED IN SMALL BATCHES FOR LOW LATENCY"/>
<node id="THE STRAWMAN SCENARIO"/>
<node id="A TRAINING TASK"/>
<node id="AN INFERENCE TASK"/>
<node id="THE RST INFERENCE BATCH"/>
<node id="SEVERAL SECONDS TO FINISH"/>
<node id="EXISTING SUPPORT SUCH AS NVIDIA MPS"/>
<node id="HUNDREDS OF MILLISECONDS OVERHEAD"/>
<node id="NVIDIA MPS"/>
<node id="LOWER OVERHEAD COMPARED TO STOP-AND-START"/>
<node id="SEVERAL HUNDRED MILLISECONDS OVERHEAD"/>
<node id="MPS FROM MEETING STRICT SLOS"/>
<node id="FIGURE 1"/>
<node id="PIPESWITCH ARCHITECTURE"/>
<node id="501 CONTROLLER"/>
<node id="GPU MEMORY DAEMON"/>
<node id="NEW TASK"/>
<node id="THROUGHPUT"/>
<node id="BATCHES PER SECOND"/>
<node id="PIPESWITCH MPS STOP-AND-START"/>
<node id="EIGHT P3.2XLARGE INSTANCES"/>
<node id="WELL-DENED STRUCTURES"/>
<node id="THE STRUCTURE AND COMPUTATION PATTERN OF DNN MODELS"/>
<node id="US TO HIGHLY OPTIMIZE TASK SWITCHING"/>
<node id="US TO ACHIEVE MILLISECOND-SCALE OVERHEAD"/>
<node id="PIPELINE"/>
<node id="FEASIBLE"/>
<node id="EFFECTIVE"/>
<node id="OTHER CHALLENGES"/>
<node id="MEMORY MANAGEMENT"/>
<node id="WORKER SWITCHING"/>
<node id="AN OVERVIEW OF THE ARCHITECTURE AND TASK EXECUTION"/>
<node id="THE ARCHITECTURE OF A PIPESWITCH SERVER"/>
<node id="PIPESWITCH PIPELINES MODEL"/>
<node id="TRANSMISSION AND TASK EXECUTION"/>
<node id="THIS SERVER"/>
<node id="FOUR TYPES OF COMPONENTS"/>
<node id="A CONTROLLER"/>
<node id="A MEMORY DAEMON"/>
<node id="THE CONTROLLER"/>
<node id="THE CENTRAL COMPONENT"/>
<node id="THE WORKERS"/>
<node id="THE TASKS"/>
<node id="MEMORY"/>
<node id="DAEMON"/>
<node id="THE SERVER"/>
<node id="THE DNN MODELS IN THE HOST MEMORY"/>
<node id="ALL COMPONENTS"/>
<node id="THE SLOS"/>
<node id="A STANDBY WORKER"/>
<node id="IDLE"/>
<node id="INITIALIZING A NEW TASK"/>
<node id="CLEANING ITS ENVIRONMENT FOR THE PREVIOUS TASK"/>
<node id="THE STANDBY WORKER"/>
<node id="THE NEW ACTIVE WORKER"/>
<node id="THE NEW TASK"/>
<node id="THE ENVIRONMENT FOR THE PREVIOUS TASK"/>
<node id="STANDBY WORKER FINISHES CLEANING PREVIOUS TASK"/>
<node id="WAIT"/>
<node id="WAITING"/>
<node id="NEW TASK STARTUP TIME"/>
<node id="A SET OF TASKS RECEIVED FROM THE CLIENTS"/>
<node id="A SCHEDULING POLICY"/>
<node id="WHICH TASK TO EXECUTE NEXT"/>
<node id="THE SCHEDULING"/>
<node id="PREEMPTIVE"/>
<node id="THE CURRENT TASK FOR THE NEXT ONE"/>
<node id="THE SCHEDULING POLICY"/>
<node id="CANONICAL SCHEDULING POLICIES"/>
<node id="RST COME RST SERVE (FCFS)"/>
<node id="EARLIEST DEADLINE RST (EDF)"/>
<node id="FAST CONTEXT SWITCHING"/>
<node id="THE SPECIFIC SCHEDULING ALGORITHM"/>
<node id="ORTHOGONAL TO THIS PAPER"/>
<node id="TRAINING TASK"/>
<node id="INFERENCE TASK"/>
<node id="STRICT LATENCY SLO"/>
<node id="THE CURRENT TASK TO FINISH"/>
<node id="THE CURRENT TASK"/>
<node id="THE ACTIVE WORKER TO STOP"/>
<node id="AN IDLE STANDBY WORKER"/>
<node id="ITS ENVIRONMENT FOR THE NEW TASK"/>
<node id="THE MEMORY TO THE STANDBY WORKER (4.3)"/>
<node id="THE MODEL USED BY THE NEW TASK FROM THE HOST MEMORY TO THE GPU MEMORY"/>
<node id="THE MODEL FROM THE HOST MEMORY TO THE GPU MEMORY"/>
<node id="THE TRANSMISSION"/>
<node id="THE EXTRA MEMORY COPY FROM THE MEMORY DAEMON TO THE WORKER"/>
<node id="MODEL"/>
<node id="RELEVANT GPU MEMORY HANDLERS"/>
<node id="ITS TASK"/>
<node id="MODEL TRANSMISSION"/>
<node id="GPU MEMORY HANDLERS"/>
<node id="GPU MEMORY HANDLERS TO WORKERS"/>
<node id="THE PRIMARY GOAL OF THIS PAPER"/>
<node id="A SET OF TECHNIQUES BASED ON THE CHARACTERISTICS OF DL APPLICATIONS"/>
<node id="THE SET OF TECHNIQUES"/>
<node id="THE TASK SWITCHING OVERHEAD IN THIS PROCESS"/>
<node id="INDIVIDUAL COMPONENTS"/>
<node id="END-TO-END EXPERIMENTS"/>
<node id="THE BENEFITS OF PIPESWITCH"/>
<node id="THE EFFECTIVENESS OF THE DESIGN CHOICES ON EACH COMPONENT"/>
<node id="THE MEA-502 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="SUREMENT"/>
<node id="A TYPICAL SCENARIO"/>
<node id="A TRAINING TASK RUNNING ON THE GPU"/>
<node id="RESNET152 17"/>
<node id="THE MEASUREMENT"/>
<node id="TWO TYPES OF INSTANCES ON AMAZON AWS"/>
<node id="G4DN.2XLARGE WITH NVIDIA T4"/>
<node id="P3.2XLARGE WITH NVIDIA V100"/>
<node id="MEASURING THE TIME TO START AND EXECUTE IT ON THE GPU"/>
<node id="THE NETWORK TIME"/>
<node id="THE TASK QUEUEING TIME"/>
<node id="TABLE 1"/>
<node id="THE RESULTS"/>
<node id="TOTAL TIMES TO START THE INFERENCE TASK ON THE GPUS"/>
<node id="5787 MS AND 7551 MS"/>
<node id="THE OVERHEAD INTO THE FOUR COMPONENTS"/>
<node id="CLEANING"/>
<node id="TASK CLEANING"/>
<node id="THE INFERENCE TASK"/>
<node id="ITS ENVIRONMENT"/>
<node id="PROCESS LAUNCHING"/>
<node id="PYTORCH CUDA RUNTIME LOADING"/>
<node id="CUDA CONTEXT INITIALIZATION"/>
<node id="ALLOCATION"/>
<node id="ITS NEURAL NETWORK MODEL"/>
<node id="THE MODEL"/>
<node id="THE HOST MEMORY"/>
<node id="TRANSMISSION"/>
<node id="GROUPED TRANSMISSION"/>
<node id="TEXT"/>
<node id="TASK SWITCHING"/>
<node id="CPU"/>
<node id="BETTER CPU THAN P3.2XLARGE"/>
<node id="BETTER CPU"/>
<node id="INTEL PLATINUM 8259CL"/>
<node id="CPU IN P3.2XLARGE"/>
<node id="INTEL XEON E5-2686 V4"/>
<node id="LOWER OVERHEAD ON T4"/>
<node id="TASK SWITCHING LARGELY DEPENDS ON CPU"/>
<node id="A STRAWMAN SOLUTION"/>
<node id="THE OLD TASK"/>
<node id="A STRAWMAN SOLUTION THAT SIMPLY STOPS THE OLD TASK AND STARTS THE NEW TASK"/>
<node id="ALL THE COMPONENTS"/>
<node id="CONSIDERABLE TIME COMPARED TO THE INFERENCE TIME"/>
<node id="ALL THE COMPONENTS SHOULD BE OPTIMIZED TO ACHIEVE MINIMAL SWITCHING OVERHEAD AND MEET THE SLOS"/>
<node id="THE PCIE BANDWIDTH"/>
<node id="THE PHYSICAL LIMIT ON HOW FAST AN ARBITRARY TASK CAN BE LOADED TO THE GPU"/>
<node id="THE CHARACTERISTICS OF DL APPLICATIONS TO CIRCUMVENT THIS PHYSICAL LIMIT"/>
<node id="THE COMPUTATION"/>
<node id="A FORWARD PASS FROM THE RST LAYER TO THE NAL LAYER"/>
<node id="MAKE A PREDICTION"/>
<node id="EACH ITERATION IN A TRAINING TASK"/>
<node id="A FORWARD PASS"/>
<node id="A BACKWARD PASS"/>
<node id="THE TASK"/>
<node id="THE COMPUTATION OF A LAYER"/>
<node id="THE LAYER IS LOADED IN THE GPU"/>
<node id="THE INPUT OF THE LAYER IS READY"/>
<node id="THE INPUT OF THE LAYER"/>
<node id="THE PREVIOUS LAYERS HAVE FINISHED THEIR COMPUTATION"/>
<node id="FIGURE 2"/>
<node id="THE ADVANTAGE OF PIPELINING OVER THE STRAWMAN SOLUTION"/>
<node id="THE KNOWLEDGE OF MODELS"/>
<node id="PIPELINING MECHANISM"/>
<node id="OPTIMAL MODEL-AWARE GROUPING IN PIPESWITCH"/>
<node id="MODEL-AWARE GROUPING"/>
<node id="THE BEST TRADE-OFF BETWEEN PIPELINE OVERHEAD AND EFFICIENCY"/>
<node id="PCIE"/>
<node id="PCIE GPU E0 E1 EN-1 E2 (B)"/>
<node id="MODEL TRANSMISSION AND TASK EXECUTION"/>
<node id="THE EXAMPLE"/>
<node id="ONLY A FORWARD PASS IN TASK EXECUTION"/>
<node id="ADDING HOOKS"/>
<node id="AUTOMATED"/>
<node id="A PART OF THE DNN FRAMEWORK"/>
<node id="DNN FRAMEWORK"/>
<node id="PYTORCH"/>
<node id="MODEL STRUCTURE INFORMATION"/>
<node id="TRANSPARENT TO USERS AND CLUSTER MANAGERS"/>
<node id="THE BASIC WAY FOR PIPELINING"/>
<node id="TO PIPELINE ON PER-LAYER GRANULARITY"/>
<node id="THE SYSTEM"/>
<node id="THE LAYERS TO THE GPU MEMORY ONE BY ONE"/>
<node id="THE COMPUTATION FOR A LAYER"/>
<node id="THE LAYER IS TRANSMITTED"/>
<node id="ONE"/>
<node id="THE OVERHEAD TO INVOKE MULTIPLE CALLS TO PCIE TO TRANSMIT THE DATA"/>
<node id="TRANSMISSION OVERHEAD"/>
<node id="DATA SIZE"/>
<node id="DIVIDING THE MODEL INTO MANY LAYERS"/>
<node id="SIGNIFICANT EXTRA OVERHEAD"/>
<node id="INVOKING A PCIE CALL FOR EACH LAYER"/>
<node id="SOME LAYERS"/>
<node id="VERY SMALL"/>
<node id="SYNCHRONIZATION OVERHEAD"/>
<node id="THE OTHER"/>
<node id="TRANSMISSION AND COMPUTATION"/>
<node id="THE COMPUTATION TO KNOW WHEN A LAYER IS READY TO COMPUTE"/>
<node id="MULTIPLE LAYERS INTO A GROUP"/>
<node id="PER-GROUP GRANULARITY"/>
<node id="PIPELINING OVERHEAD"/>
<node id="ONCE FOR EACH GROUP"/>
<node id="INSTEAD OF EACH LAYER"/>
<node id="GROUPING"/>
<node id="A TRADE-OFF BETWEEN PIPELINING EFFICIENCY AND PIPELINING OVERHEAD"/>
<node id="USING SMALL GROUPS"/>
<node id="MORE OVERLAP BETWEEN TRANSMISSION AND COMPUTATION"/>
<node id="PIPELINING EFFICIENCY"/>
<node id="MORE PIPELINING OVERHEAD"/>
<node id="USING BIG GROUPS"/>
<node id="MINIMAL PIPELINING OVERHEAD"/>
<node id="THE CHANCE FOR OVERLAPPING"/>
<node id="MODEL-AWARE"/>
<node id="DIFFERENT STRUCTURES"/>
<node id="THE NUMBER OF LAYERS"/>
<node id="THE SIZE OF EACH LAYER"/>
<node id="ALL POSSIBLE COMBINATIONS"/>
<node id="THE OPTIMAL GROUPING STRATEGY"/>
<node id="TWO PRUNING TECHNIQUES"/>
<node id="TWO INSIGHTS"/>
<node id="HUNDREDS OF LAYERS"/>
<node id="TIME COMPLEXITY FOR ENUMERATION"/>
<node id="EXPONENTIAL"/>
<node id="PCIE GPU"/>
<node id="LOWER BOUND OF F(GROUP(0, I), I1)"/>
<node id="GROUP(0, I)"/>
<node id="GROUP(I1, J)"/>
<node id="J, J1"/>
<node id="N-1"/>
<node id="CASE (A)"/>
<node id="LOWER BOUND CURRENT OPTIMAL TIME"/>
<node id="PRUNE"/>
<node id="FROM I TO J"/>
<node id="BATCH"/>
<node id="FROM LAYER I1 TO J"/>
<node id="FIGURE 3"/>
<node id="EXAMPLES FOR TWO PRUNING TECHNIQUES"/>
<node id="THE CASES THAT GROUP FROM LAYER (I 1) TO J J"/>
<node id="J J"/>
<node id="THE PROBLEM"/>
<node id="F(B,I)"/>
<node id="A FUNCTION"/>
<node id="THE TOTAL TIME OF THE OPTIMAL GROUPING STRATEGY FROM LAYER I TO N-1"/>
<node id="N"/>
<node id="B"/>
<node id="GROUPS FORMED BY LAYER 0 TO I-1"/>
<node id="THE FUNCTION"/>
<node id="ND THE OPTIMAL GROUPS FROM LAYER I1 TO N-1"/>
<node id="OPTGROUPS"/>
<node id="THE CURRENT STRATEGY IS BETTER"/>
<node id="ALL POSSIBLE COMBINATIONS INTO N CASES"/>
<node id="CASE I"/>
<node id="THE FIRST GROUP CONTAINS LAYER 0 TO I"/>
<node id="THIS FORMULA"/>
<node id="RECURSIVELY"/>
<node id="F(GROUP(0,I),I1)"/>
<node id="OUR RST INSIGHT"/>
<node id="IT IS NOT NECESSARY TO EXAMINE ALL THE N CASES"/>
<node id="THE RST GROUP"/>
<node id="TOO MANY LAYERS"/>
<node id="THE COMPUTATION OF THE RST GROUP"/>
<node id="DELAYED TOO MUCH"/>
<node id="THE DELAY"/>
<node id="COMPENSATE THE PIPELINE EFFICIENCY"/>
<node id="MULTIPLE LAYERS IN A GROUP BASED ON THE PROGRESS OF COMPUTATION"/>
<node id="PACKING MULTIPLE LAYERS IN A GROUP"/>
<node id="PIPELINE EFFICIENCY"/>
<node id="OTHER THAN THE RST GROUP"/>
<node id="SAFELY PACKING MULTIPLE LAYERS IN A GROUP"/>
<node id="T(I, J)"/>
<node id="TRANSMISSION TIME FOR A GROUP FROM LAYER I TO J"/>
<node id="E(I, J)"/>
<node id="EXECUTION TIME FOR A GROUP FROM LAYER I TO J"/>
<node id="SIZE OF LAYER I TO J AND PCIE BANDWIDTH"/>
<node id="THE OVERHEAD OF INVOKING MULTIPLE CALLS"/>
<node id="A LOWER BOUND FOR THE TOTAL TIME FOR EACH CASE IN EQUATION 1"/>
<node id="THE LOWER BOUND"/>
<node id="THE BEST CASE THAT ALL THE REMAINING LAYERS ARE COMBINED IN ONE GROUP FOR TRANSMISSION AND COMPUTATION"/>
<node id="THE COMPUTATION AND COMMUNICATION"/>
<node id="PERFECTLY OVERLAPPED"/>
<node id="ITS COMPUTATION"/>
<node id="RIGHT AFTER THE COMPUTATION OF THE FIRST GROUP FINISHES"/>
<node id="THE LOWER BOUND OF CASE I"/>
<node id="THE TOTAL TIME OF THE BEST GROUPING STRATEGY FOUND SO FAR"/>
<node id="IF THE LOWER BOUND OF CASE I IS ALREADY LARGER THAN THE TOTAL TIME OF THE BEST GROUPING STRATEGY FOUND SO FAR"/>
<node id="FIGURE 3(B)"/>
<node id="AN EXAMPLE FOR THIS INSIGHT"/>
<node id="THE TRANSMISSION OF THE SECOND GROUP INTO THE COMPUTATION OF THE RST GROUP"/>
<node id="THE LEAST NUMBER OF LAYERS TO GROUP"/>
<node id="THE FOLLOWING EQUATION"/>
<node id="JIS"/>
<node id="GROUPING FROM (I1) TO J"/>
<node id="HIGHER PIPELINE OVERHEAD"/>
<node id="ALGORITHM"/>
<node id="OFFLINE TO FIND THE STRATEGY"/>
<node id="RESULTING STRATEGY"/>
<node id="RESULTING STRATEGY FOR CONTEXT SWITCHING"/>
<node id="ALGORITHM 1"/>
<node id="THE PSEUDO CODE"/>
<node id="THE FUNCTION FINDOPTGROUPING"/>
<node id="THE OPTIMAL GROUPING STRATEGY BASED ON EQUATION 1"/>
<node id="EQUATION 1"/>
<node id="LINE 1-27"/>
<node id="THE GROUPS THAT HAVE ALREADY FORMED"/>
<node id="X"/>
<node id="THE RST LAYER THAT HAVE NOT FORMED A GROUP"/>
<node id="ALL LAYERS FROM X TO N1"/>
<node id="THE BEST GROUPING STRATEGY FROM LAYER X GIVEN B"/>
<node id="NONE"/>
<node id="THE ALGORITHM"/>
<node id="THE SECOND PRUNING INSIGHT"/>
<node id="THE RST GROUP FROM LAYER X (LINE 3-9)"/>
<node id="THE PROBLEM INTO K 1 CASES"/>
<node id="THE RST GROUP FROM LAYER X TO XI"/>
<node id="EQUATION 3 AND FIGURE 3(B)"/>
<node id="THIS INSIGHT"/>
<node id="ONE GROUP FROM LAYER 0 TO I"/>
<node id="MULTIPLE GROUPS FORMED BY PREVIOUS LAYERS"/>
<node id="B.DELAY TO DENOTE THE TIME TO WHICH THE GROUP CAN BE FORMED"/>
<node id="B.DELAY"/>
<node id="THE TIME TO WHICH THE GROUP CAN BE FORMED"/>
<node id="THE ALGORITHM NDS"/>
<node id="B.DELAY (LINE 4-9)"/>
<node id="THE ENUMERATION FOR I"/>
<node id="THE LAYERS FROM X TO J-1 (LINE 11)"/>
<node id="THE RST INSIGHT"/>
<node id="EXAMPLE IN EQUATION 2 AND FIGURE 3(A)"/>
<node id="A SPECIAL CASE"/>
<node id="0"/>
<node id="COMPUTATION FROM X"/>
<node id="ITS TRANSMISSION (T(X,I))"/>
<node id="COMPUTATION OF THE PREVIOUS GROUPS (B.DELAY)"/>
<node id="FIGURE 4"/>
<node id="LOWER BOUND"/>
<node id="CURRENT OPTIMAL TIME"/>
<node id="LINE 18-19"/>
<node id="THE TWO PRUNING TECHNIQUES"/>
<node id="MOST OF THE STRATEGIES"/>
<node id="M N X"/>
<node id="THE NUMBER OF LAYERS THE FUNCTION CONSIDERS"/>
<node id="INDUCTION ON M"/>
<node id="FINDOPTGROUPING(B,X)"/>
<node id="THE OPTIMAL GROUPING STRATEGY FROM LAYER X TO N 1"/>
<node id="PREVIOUS LAYERS"/>
<node id="GROUPS REPRESENTED BY B"/>
<node id="K"/>
<node id="SOME K 1"/>
<node id="M"/>
<node id="ANY M K"/>
<node id="THE OPTIMAL STRATEGY"/>
<node id="FINDOPTGROUPING(B GROUP(X,X I),X I 1)"/>
<node id="K I K LAYERS"/>
<node id="THE OPTIMAL GROUPING STRATEGY FOR CASE I"/>
<node id="THE ASSUMPTION"/>
<node id="1"/>
<node id="ONE LAYER"/>
<node id="LAYER X"/>
<node id="ONE GROUP"/>
<node id="THIS STRATEGY"/>
<node id="K1 LAYERS"/>
<node id="THE OPTIMAL STRATEGY FOR THIS CASE"/>
<node id="THE OPTIMAL GROUPING STRATEGY FOR M K 1"/>
<node id="THESE CASES"/>
<node id="EXCLUSIVE"/>
<node id="THE ENTIRE SEARCH SPACE"/>
<node id="THE OPTIMAL GROUPING STRATEGY FROM THESE CASES"/>
<node id="THE RST TECHNIQUE"/>
<node id="THE CASES"/>
<node id="THE LOWER BOUNDS"/>
<node id="NO BETTER THAN THE CURRENT FOUND OPTIMAL"/>
<node id="THIS TECHNIQUE"/>
<node id="THE OPTIMALITY"/>
<node id="THE SECOND TECHNIQUE"/>
<node id="THE CASE"/>
<node id="THEIR RST GROUPS"/>
<node id="LAYER X TO J J"/>
<node id="THE COMPUTATION TO AN EARLIER POINT THAN GROUPING FROM X TO AT LEAST J"/>
<node id="PRUNING THESE CASES"/>
<node id="OPTIMALITY"/>
<node id="A GIVEN LIST OF LAYERS"/>
<node id="LAYERS OR OPERATORS IN A DNN MODEL"/>
<node id="AN ARBITRARY COMPUTATION GRAPH"/>
<node id="MODELS LIKE RESNET AND INCEPTION"/>
<node id="TECHNICALLY NON-LINEAR DIRECTED ACYCLIC GRAPH (DAGS)"/>
<node id="EXECUTION ORDER"/>
<node id="ISSUED TO THE GPU ONE BY ONE"/>
<node id="LAYERSOPERATORS IN THE DAG"/>
<node id="THE GPU ONE BY ONE"/>
<node id="ANY SPECIAL ASSUMPTIONS ON THE EXECUTION ORDER"/>
<node id="GROUPING THE LAYERS"/>
<node id="HIGH PIPELINING EFFICIENCY AND LOW PIPELINING OVERHEAD"/>
<node id="ORDER"/>
<node id="THE RST TIME AN OPERATOR IS EXECUTED"/>
<node id="CORRECTNESS"/>
<node id="OPERATOR"/>
<node id="ONLY WHEN IT IS TRANSMITTED TO THE GPU AND THE INPUT IS READY"/>
<node id="OUR PIPELINED MODEL TRANSMISSION"/>
<node id="THE GENERAL CASE"/>
<node id="EVALUATE THE EFFECTIVENESS OF PIPELINED MODEL TRANSMISSION"/>
<node id="FIGURE 7"/>
<node id="EFFECTIVENESS OF PIPELINED MODEL TRANSMISSION"/>
<node id="UNIFIED MEMORY MANAGEMENT TASK EXECUTION IN A GPU"/>
<node id="A GPU"/>
<node id="ITS OWN MEMORY MANAGEMENT SYSTEM"/>
<node id="A MALLOC FUNCTION"/>
<node id="MALLOC FUNCTION"/>
<node id="CPUS FOR MEMORY ALLOCATION"/>
<node id="CUDAMALLOC FOR NVIDIA GPUS"/>
<node id="EACH TASK"/>
<node id="NATIVE CUDAMALLOCMANAGED FUNCTION FOR GPU MEMORY ALLOCATION"/>
<node id="MODEL TRANSMISSION TO CUDA UNIFIED MEMORY"/>
<node id="FUNCTIONS FOR ALLOCATING GPU MEMORY"/>
<node id="FUNCTIONS FOR SHARING THE GPU MEMORY TO WORKERS THROUGH CUDA IPC API"/>
<node id="FUNCTIONS FOR GETTING THE SHARED GPU MEMORY"/>
<node id="EACH WORKER"/>
<node id="CUDAMALLOC TO ALLOCATE GPU MEMORY"/>
<node id="THE MODEL TO GPU BY ITS OWN"/>
<node id="GPU MEMORY WITH CUDAMALLOCMANAGED"/>
<node id="CUDA"/>
<node id="THE MODEL TO GPU WHEN NEEDED"/>
<node id="THIS SOLUTION"/>
<node id="HIGH OVERHEAD FOR DL APPLICATIONS"/>
<node id="HIGH OVERHEAD"/>
<node id="TWO REASONS"/>
<node id="NATIVE CUDAMALLOC FUNCTION"/>
<node id="GENERAL-PURPOSE APPLICATIONS"/>
<node id="CUDA UNIFIED MEMORY"/>
<node id="NATIVE CUDAMALLOC FUNCTION AND CUDA UNIFIED MEMORY"/>
<node id="UNNECESSARY OVERHEAD FOR DL APPLICATIONS"/>
<node id="MORE THAN ONE HUNDRED MILLISECONDS OVERHEAD THAN PIPESWITCH"/>
<node id="TWO CHARACTERISTICS OF DL APPLICATIONS"/>
<node id="GPU MEMORY MANAGEMENT OVERHEAD"/>
<node id="THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT"/>
<node id="THESE CHARACTERISTICS"/>
<node id="TOO HEAVY-WEIGHT FOR DL APPLICATIONS THAT REQUIRE FAST TASK SWITCHING"/>
<node id="AMOUNT OF MEMORY ALLOCATED TO THE DNN MODEL"/>
<node id="FIXED"/>
<node id="THE WEIGHTS OF THE NEURAL NETWORK"/>
<node id="THE DNN STRUCTURE"/>
<node id="THE AMOUNT OF MEMORY NEEDED TO STORE THEM"/>
<node id="THE SAME"/>
<node id="THE MODEL FOR INFERENCE"/>
<node id="THE MODEL ITSELF"/>
<node id="A SIMPLE, REGULAR PATTERN"/>
<node id="MEMORY FRAGMENTATION"/>
<node id="OUTPUTS OF EACH LAYER"/>
<node id="NEXT LAYER"/>
<node id="THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS CANNOT BE IMMEDIATELY FREED"/>
<node id="THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS"/>
<node id="THE BACKWARD PASS"/>
<node id="THE WEIGHTS"/>
<node id="REVERSE ORDER"/>
<node id="THE FORWARD PASS"/>
<node id="RST-IN-LAST-OUT"/>
<node id="THE MEMORY ALLOCATION AND RELEASE"/>
<node id="A SIMPLE STACK-LIKE MECHANISM"/>
<node id="MEMORY ALLOCATION OVERHEAD"/>
<node id="MINIMIZED"/>
<node id="MINIMIZE MEMORY FOOTPRINT"/>
<node id="AVOID EXTRA MEMORY COPIES"/>
<node id="A MEMORY MANAGEMENT MECHANISM TAILORED FOR DL APPLICATIONS"/>
<node id="64-BIT INTEGER OFFSET FOR THE SHARED GPU MEMORY TO WORKERS"/>
<node id="223 MS"/>
<node id="THE MEMORY ALLOCATION OVERHEAD"/>
<node id="MEMORY POINTERS TO THE WORKERS"/>
<node id="PASSING MEMORY POINTERS TO THE WORKERS"/>
<node id="LIGHT-WEIGHT"/>
<node id="THAT EACH TIME ONLY ONE WORKER OWNS THE GPU MEMORY"/>
<node id="ONE WORKER"/>
<node id="MEMORY ISOLATION BETWEEN WORKERS"/>
<node id="A MEMORY POOL"/>
<node id="THE MEMORY TO STORE ITS MODEL AND INTERMEDIATE RESULTS"/>
<node id="THE MEMORY TO THE POOL"/>
<node id="THE MEMORY"/>
<node id="THE POOL AFTER THE INTERMEDIATE RESULTS ARE NO LONGER NEEDED"/>
<node id="THE MEMORY MANAGEMENT OF PIPESWITCH"/>
<node id="THAT OF PYTORCH"/>
<node id="GPU MEMORY BLOCKS TO PYTORCH GPU MEMORY POOL"/>
<node id="TENSORS ON THEM"/>
<node id="MEMORY MANAGEMENT IN PYTORCH"/>
<node id="MEMORY ALLOCATION FOR A TASK ITSELF"/>
<node id="REPLICATING THE MODELS IN EACH WORKER"/>
<node id="HIGH MEMORY FOOTPRINT"/>
<node id="THE NUMBER OF MODELS A SERVER CAN STORE"/>
<node id="THE TYPES OF TASKS THE SERVER CAN EXECUTE"/>
<node id="STORING THE MODELS IN A DEDICATE PROCESS"/>
<node id="EACH MODEL"/>
<node id="ONLY ONCE"/>
<node id="AN EXTRA MEMORY COPY FROM THIS PROCESS TO A WORKER TO START A TASK"/>
<node id="AN EXTRA MEMORY COPY"/>
<node id="THE TASK SWITCHING TIME"/>
<node id="THE MODELS IN THE MEMORY DAEMON"/>
<node id="ONE COPY OF EACH MODEL IN THE HOST MEMORY"/>
<node id="IPC"/>
<node id="A PROPERTY OF DL APPLICATIONS"/>
<node id="IPC APIS"/>
<node id="CUDAIPCOPENMEMHANDLE"/>
<node id="NVIDIA GPUS"/>
<node id="THE PERFORMANCE OF THESE IPC APIS"/>
<node id="THESE IPC APIS"/>
<node id="THE OVERHEAD"/>
<node id="THE PIPELINE"/>
<node id="THE IPCS FREQUENTLY"/>
<node id="THE IPCS"/>
<node id="SYNCHRONIZE MODEL TRANSMISSION AND TASK EXECUTION"/>
<node id="FOR EVERY PIPELINE GROUP"/>
<node id="ONLY ONCE FOR THE ENTIRE MODEL TRANSMISSION"/>
<node id="MEMORY ALLOCATION PROCESS FOR A NEURAL NETWORK MODEL"/>
<node id="DETERMINISTIC"/>
<node id="MEMORY DAEMON AND THE WORKER"/>
<node id="THE SAME ORDER TO ALLOCATE MEMORY FOR THE MODEL PARAMETERS"/>
<node id="THE MEMORY POINTERS FOR THE PARAMETERS"/>
<node id="THE NEURAL NETWORK MODEL"/>
<node id="KNOWN AND GIVEN"/>
<node id="THE SAME ORDER TO TRANSMIT THE MODEL AS THE WORKER WOULD"/>
<node id="THE USAGE OF EXPENSIVE GPU IPCS"/>
<node id="LATENCY"/>
<node id="CHEAP CPU IPCS"/>
<node id="THE WORKER"/>
<node id="WHICH PIPELINE GROUP"/>
<node id="NO PIN"/>
<node id="THE OS"/>
<node id="A MEMORY PAGE TO DISK"/>
<node id="THE PAGE"/>
<node id="INACTIVE FOR A CERTAIN AMOUNT OF TIME"/>
<node id="A PAGE IN THE HOST MEMORY TO BE PINNED (OR PAGE-LOCKED)"/>
<node id="A PAGE IN THE HOST MEMORY"/>
<node id="IN ORDER TO TRANSMIT THE DATA IN THE PAGE TO THE GPU MEMORY"/>
<node id="A TEMPORARY PINNED PAGE"/>
<node id="THE PAGES OF THE MEMORY DAEMON TO THE HOST MEMORY"/>
<node id="DESIRABLE"/>
<node id="ONE TASK CANNOT READ THE MEMORY OF ANOTHER TASK"/>
<node id="THE CRASHING OF ONE TASK DOES NOT AFFECT OTHER TASKS OR THE ENTIRE SYSTEM"/>
<node id="SEPARATE PROCESSES"/>
<node id="THE NAIVE SOLUTION"/>
<node id="A NAIVE SOLUTION"/>
<node id="THE NEW TASK AFTER THE CURRENT TASK IS STOPPED"/>
<node id="SEQUENTIAL EXECUTION"/>
<node id="LONG DELAY"/>
<node id="ANOTHER POSSIBLE SOLUTION"/>
<node id="TO LET THE CURRENT AND NEW TASKS SHARE THE SAME PROCESS WITH A WARM CUDA CONTEXT"/>
<node id="THE GPU ENVIRONMENT OF THE CURRENT TASK"/>
<node id="THE PROCESS OF THE OLD TASK"/>
<node id="THE GPU ENVIRONMENT"/>
<node id="ANOTHER PROCESS"/>
<node id="THE PROCESS"/>
<node id="THE ENVIRONMENT FOR THE NEW TASK"/>
<node id="A SEPARATE PROCESS"/>
<node id="ITS OWN GPU ENVIRONMENT"/>
<node id="GPU ENVIRONMENT"/>
<node id="CUDA CONTEXT"/>
<node id="WHEN IT IS RST CREATED"/>
<node id="A MAJOR JOB"/>
<node id="ASYNCHRONOUS CUDA FUNCTIONS QUEUED ON THE GPU"/>
<node id="A CURRENT TASK"/>
<node id="STOPPED"/>
<node id="SYNCHRONIZATION POINTS INTO TRAINING TASKS"/>
<node id="NUMBER OF QUEUED FUNCTIONS"/>
<node id="LIMITED"/>
<node id="QUICKLY CLEARED"/>
<node id="SYNCHRONIZATION POINTS"/>
<node id="SHORT"/>
<node id="PREEMPTED"/>
<node id="ANOTHER JOB"/>
<node id="ITS GPU MEMORY"/>
<node id="AN IMPORTANT PROPERTY OF THE CLEANING PROCEDURE"/>
<node id="THAT IT DOES NOT MODIFY THE CONTENT OF THE MEMORY"/>
<node id="THE METADATA"/>
<node id="GPU MEMORY POINTERS"/>
<node id="CLEANING PROCEDURE"/>
<node id="POINTERS POINTING TO THE TENSOR DATA"/>
<node id="ACTUAL DATA"/>
<node id="THE TASK CLEANING OF THE CURRENT TASK AND THE PIPELINED MODEL TRANSMISSION OF THE NEW TASK"/>
<node id="PARALLELIZING THE TASK CLEANING AND THE PIPELINED MODEL TRANSMISSION"/>
<node id="THE TASK CLEANING OVERHEAD"/>
<node id="THIS CHOICE"/>
<node id="PERFORMANCE"/>
<node id="A PROBLEM FOR A TRUSTED ENVIRONMENT"/>
<node id="A LATTER PROCESS"/>
<node id="THE MEMORY DATA OF A PREVIOUS PROCESS"/>
<node id="AN ADDITIONAL ZERO-OUT OPERATION"/>
<node id="ADDED"/>
<node id="HIGH MEMORY BANDWIDTH"/>
<node id="900GBS FOR V100"/>
<node id="SUB-MILLISECOND OVERHEAD"/>
<node id="ZEROING-OUT MOST MODELS LIKE RESNET-152"/>
<node id="RESNET-152"/>
<node id="AROUND 240MB"/>
<node id="NEW PROCESS"/>
<node id="ENTIRE GPU MEMORY"/>
<node id="THE DIFFERENCES BETWEEN THESE THREE SOLUTIONS"/>
<node id="THE CURRENT ACTIVE WORKER TO STOP"/>
<node id="THE GPU MEMORY ALLOCATED TO THE CURRENT ACTIVE WORKER"/>
<node id="THE GPU MEMORY TO THE NEW ACTIVE WORKER"/>
<node id="CURRENT ACTIVE WORKER TO STOP"/>
<node id="PARAMETERS OF THE NEW MODEL TO THE GPU"/>
<node id="ONLY ONE ACTIVE WORKER"/>
<node id="EXCLUSIVE OCCUPATION OF THE GPU"/>
<node id="TRADE-OFF"/>
<node id="NUMBER OF STANDBY WORKERS AND THEIR GPU MEMORY CONSUMPTION"/>
<node id="EVERY STANDBY WORKER"/>
<node id="ITS OWN CUDA CONTEXT"/>
<node id="A FEW HUNDRED MB GPU MEMORY"/>
<node id="ALWAYS AT LEAST ONE IDLE STANDBY WORKER"/>
<node id="TWO STANDBY WORKERS"/>
<node id="AT LEAST ONE IDLE WORKER"/>
<node id="THE WAITING TIME"/>
<node id="MODERATE GPU MEMORY CONSUMPTION"/>
<node id="A TRANSACTION"/>
<node id="A MODEL IS SWITCHED IN OR OUT ON ALL OF ITS GPUS"/>
<node id="INFERENCE ON THIS MODEL"/>
<node id="A PRODUCTION GPU TRAINING TRACE FROM MICROSOFT 19,20"/>
<node id="TASKS IN THIS TRACE"/>
<node id="111,883"/>
<node id="SINGLE-GPU TRAINING TASKS"/>
<node id="96,662"/>
<node id="86%"/>
<node id="THESE JOBS"/>
<node id="18 OF TOTAL GPU HOURS"/>
<node id="THE SHARE OF MULTI-GPU JOBS TO INCREASE IN THE FUTURE"/>
<node id="CURRENT TRAINING FRAMEWORKS"/>
<node id="MATURE SUPPORT OF ELASTIC TRAINING"/>
<node id="THESE SCHEDULING SOLUTIONS"/>
<node id="ORTHOGONAL AND COMPLEMENTARY TO PIPESWITCH"/>
<node id="HTTPS"/>
<node id="PYTORCH.ORG"/>
<node id="C AND PYTHON FUNCTIONS TO THE GPU MEMORY MANAGEMENT MODULE OF PYTORCH"/>
<node id="SHARED GPU MEMORY"/>
<node id="PYTORCH GPU MEMORY POOL"/>
<node id="DIFFERENT CUDA STREAMS"/>
<node id="ONLY ONE OF THESE CUDA STREAMS IS ACTIVE"/>
<node id="THE CONTROLLER PROCESS"/>
<node id="A TCP THREAD"/>
<node id="A SCHEDULER THREAD"/>
<node id="THE SCHEDULER AND THE MEMORY DAEMON"/>
<node id="FOR BETTER PERFORMANCE"/>
<node id="THE TCP THREAD"/>
<node id="TASK THROUGH TCP FROM CLIENTS"/>
<node id="THE TASK TO THE SCHEDULER THREAD"/>
<node id="THE SCHEDULER THREAD"/>
<node id="THE GPU MEMORY WITH WORKERS"/>
<node id="WORKERS"/>
<node id="THE TASK TO A WORKER"/>
<node id="PARAMETERS FOR THE CORRESPONDING MODEL TO THE GPU MEMORY"/>
<node id="THE USER"/>
<node id="THE MODEL IN THE SCHEDULER"/>
<node id="THE SCHEDULER"/>
<node id="THE MODEL FROM THE DISK TO THE CPU MEMORY"/>
<node id="PARAMETERS"/>
<node id="GROUPS"/>
<node id="A PIPELINE"/>
<node id="THE WORKER TO START COMPUTING THE CORRESPONDING LAYERS"/>
<node id="THE WORKER PROCESS"/>
<node id="TWO THREADS"/>
<node id="THE TERMINATION THREAD"/>
<node id="THE TERMINATION SIGNAL FROM THE CONTROLLER"/>
<node id="THE MAIN THREAD"/>
<node id="THE COMPUTATION FOR INFERENCE OR TRAINING"/>
<node id="USER TO REGISTER THE MODEL BEFORE STARTING A TASK"/>
<node id="THE HOOKS TO WAIT FOR PARAMETER TRANSMISSION OR TERMINATE ON NOTIFICATION"/>
<node id="THE MODEL STRUCTURES"/>
<node id="SMALL"/>
<node id="THE PARAMETERS"/>
<node id="ONCE IN THE MEMORY DAEMON"/>
<node id="THEIR PARAMETERS"/>
<node id="LOCATIONS IN THE SHARED GPU MEMORY"/>
<node id="DIFFERENT MODELS"/>
<node id="THE SAME GPU MEMORY LOCATION"/>
<node id="THE VALUE"/>
<node id="THE CONTROLLER TRANSFERS THE CORRESPONDING PARAMETERS TO THESE LOCATIONS"/>
<node id="THE SCHEDULER TO TRANSFER REQUIRED PARAMETERS FOR DNN MODELS"/>
<node id="INFERENCE OR TRAINING"/>
<node id="ALL EXPERIMENTS"/>
<node id="AWS"/>
<node id="TWO EC2 INSTANCE TYPES"/>
<node id="8 VCPUS (INTEL XEON E5-2686 V4)"/>
<node id="1 GPU (NVIDIA V100 WITH 16 GB GPU MEMORY)"/>
<node id="PCIE 3.0 16"/>
<node id="61 GB MEMORY"/>
<node id="8 VCPUS (INTEL PLATINUM 8259CL)"/>
<node id="1 GPU (NVIDIA T4 WITH 16 GB GPU MEMORY)"/>
<node id="PCIE 3.0 8"/>
<node id="32 GB MEMORY"/>
<node id="THE SOFTWARE ENVIRONMENT"/>
<node id="PYTORCH-1.3.0"/>
<node id="TORCHVISION-0.4.2"/>
<node id="SCIPY-1.3.2"/>
<node id="CUDA-10.1"/>
<node id="PYTORCH WITH OUR PLUGINS FOR ALL MECHANISMS"/>
<node id="PYTORCH WITH OUR PLUGINS"/>
<node id="BETTER RESULTS FOR STOP-AND-START"/>
<node id="NATIVE PYTORCH FROM PYTHON-PYPI USED IN TABLE 1"/>
<node id="INCEPTIONV3 22"/>
<node id="BERTBASE 23"/>
<node id="STANDARD BENCHMARKS FOR EVALUATING DL SYSTEMS"/>
<node id="REPRESENTATIVE CONFIGURATIONS FOR EACH MODEL"/>
<node id="THE EXPERIMENTS"/>
<node id="SINGLE-GPU INFERENCE AND TRAINING TASKS"/>
<node id="4.5"/>
<node id="THEIR MODELS TO THE HOST MEMORY"/>
<node id="FROM THE LATEST CHECKPOINT AFTER PREEMPTION"/>
<node id="THE CHECKPOINTING FREQUENCY OF TRAINING TASKS"/>
<node id="THE SCHEDULING CYCLE"/>
<node id="MINIMIZE CHECKPOINTING OVERHEAD"/>
<node id="DEFAULT BATCH SIZE FOR TRAINING"/>
<node id="32"/>
<node id="DEFAULT BATCH SIZE FOR INFERENCE"/>
<node id="8"/>
<node id="THROUGHPUT AND LATENCY AS EVALUATION METRICS"/>
<node id="THE END-TO-END LATENCY EXPERIENCED BY THE CLIENT"/>
<node id="FIGURE 5"/>
<node id="TOTAL LATENCY EXPERIENCED BY THE CLIENT FOR DIFFERENT MECHANISMS"/>
<node id="THE LATENCY EXPERIENCED BY THE CLIENT"/>
<node id="TABLE 3"/>
<node id="THE TOTAL OVERHEAD"/>
<node id="EACH NUMBER"/>
<node id="THE AVERAGE OF 100 RUNS"/>
<node id="FIGURE 6(B)"/>
<node id="MINIMUM AND MAXIMUM LATENCIES USING THE ERROR BAR"/>
<node id="LATENCY OF THE RST BATCH AND THOSE OF LATER BATCHES IN ONE SCHEDULING CYCLE"/>
<node id="SIGNIFICANTLY"/>
<node id="LATENCY DIFFERENCE"/>
<node id="6.1 END-TO-END EXPERIMENTS"/>
<node id="END-TO-END OVERHEAD"/>
<node id="A CLIENT"/>
<node id="AN INFERENCE TASK TO A GPU SERVER"/>
<node id="THE GPU SERVER"/>
<node id="A REPLY BACK TO THE CLIENT"/>
<node id="READY"/>
<node id="NO TRAINING TASK"/>
<node id="THE PROCESS WITH THE REQUIRED MODEL"/>
<node id="THE GPU"/>
<node id="THE LOWEST LATENCY WE CAN ACHIEVE FOR AN INFERENCE TASK"/>
<node id="EXISTING SYSTEMS LIKE GANDIVA 24"/>
<node id="GANDIVA 24"/>
<node id="SIMILAR SECOND-SCALE OVERHEAD"/>
<node id="MPS"/>
<node id="SEPARATE PROCESSES IN ADVANCE"/>
<node id="MEMORY SWAPPING"/>
<node id="UNIFIED MEMORY"/>
<node id="DEVBLOGS.NVIDIA.COM/UNIFIED-MEMORY-CUDA-BEGINNERS"/>
<node id="THE PROPERTIES"/>
<node id="4"/>
<node id="MILLISECONDS"/>
<node id="5000 TO 10000 MS"/>
<node id="STOP-AND-START"/>
<node id="RESNET152"/>
<node id="INCEPTIONV3"/>
<node id="BERTBASE"/>
<node id="0 20 40 60 80 100 MS"/>
<node id="PER-LAYER"/>
<node id="OPTIMIZATION"/>
<node id="NO OPTIMIZATION"/>
<node id="TECHNIQUE"/>
<node id="PER-LAYER PIPELINE"/>
<node id="CONDITION"/>
<node id="NO MEMORY MANAGEMENT"/>
<node id="NO IPC OPTIMIZATION"/>
<node id="NO PIN MEMORY"/>
<node id="0 100 200 300 400 MS"/>
<node id="6000 TO 8000 MS"/>
<node id="ONE PROCESS"/>
<node id="TWO PROCESSES"/>
<node id="5000 TO 7500 MS"/>
<node id="INSTANCE"/>
<node id="3.62 MS"/>
<node id="4.82 MS"/>
<node id="2.53 MS"/>
<node id="5.49 MS"/>
<node id="6.57 MS"/>
<node id="COMPUTING THE RST LAYER"/>
<node id="THE MODELS TO BE PRELOADED TO THE GPU"/>
<node id="SEVERAL LIMITATIONS DESCRIBED IN 2.2"/>
<node id="ITS PERFORMANCE"/>
<node id="THE READY MODEL WHEN THE MODEL IS PRELOADED"/>
<node id="NVIDIA MPS WHEN THE MODEL IS IN THE HOST MEMORY"/>
<node id="THE DIFFERENCE BETWEEN THE LATENCY OF A MECHANISM AND THAT OF THE READY MODEL"/>
<node id="THE WORST"/>
<node id="SEVERAL SECONDS"/>
<node id="THE MAIN SOURCE OF THE OVERHEAD"/>
<node id="CUDA CONTEXT INITIALIZATION AND RST-TIME LIBRARY LOADING OPERATIONS IN PYTORCH"/>
<node id="ANOTHER SOURCE"/>
<node id="GPU MEMORY SWAPPING"/>
<node id="THE BEST"/>
<node id="THE OVERHEAD OF PIPESWITCH FOR MOST CONFIGURATIONS"/>
<node id="UP TO 10MS"/>
<node id="BERT ON T4"/>
<node id="THE LARGE MODEL SIZE"/>
<node id="THE SMALLER PCIE BANDWIDTH ON T4 THAN THAT ON V100"/>
<node id="TIME TO COMPUTE BERT ON T4"/>
<node id="120MS"/>
<node id="THE RELATIVE OVERHEAD"/>
<node id="ACCEPTABLE"/>
<node id="READY MODEL"/>
<node id="COMPUTING"/>
<node id="THE STARTUP OVERHEAD OF PIPESWITCH"/>
<node id="ONLY A FEW MILLISECONDS"/>
<node id="ONLY A FEW MILLISECONDS OVERHEAD FOR TASK SWITCHING"/>
<node id="LOW LATENCY CLOSE TO THE LOWER BOUND"/>
<node id="THROUGHPUT AND END-TO-END LATENCY OF DIFFERENT MECHANISMS UNDER DIFFERENT SCHEDULING CYCLES"/>
<node id="SWITCHING"/>
<node id="EACH SCHEDULING CYCLE"/>
<node id="FIGURE 6(A)"/>
<node id="INFERENCE THROUGHPUT"/>
<node id="THE DASHED LINE"/>
<node id="THE UPPER BOUND"/>
<node id="THE THROUGHPUT OF THE READY MODEL ASSUMING NO TASK SWITCHING"/>
<node id="THE AVERAGE LATENCY OF THE READY MODEL"/>
<node id="NO TASK SWITCHING"/>
<node id="THROUGHPUT OF STOP-AND-START"/>
<node id="NEARLY ZERO FOR SCHEDULING CYCLES SMALLER THAN 10 S"/>
<node id="POOR THROUGHPUT AROUND 100 BATCHES PER SECOND"/>
<node id="THE RATIO TO THE UPPER BOUND"/>
<node id="THE AVERAGE LATENCY OF THE INFERENCE TASKS"/>
<node id="THE ERROR BAR"/>
<node id="THE MINIMUM AND MAXIMUM LATENCY"/>
<node id="STOP- AND-START"/>
<node id="POOR LATENCY"/>
<node id="RST BATCH"/>
<node id="SEVERAL SECONDS OVERHEAD"/>
<node id="RST BATCH HAS SEVERAL SECONDS OVERHEAD"/>
<node id="ABOUT 80 MS AVERAGE LATENCY"/>
<node id="SEVERAL HUNDRED MILLISECONDS LATENCY FOR THE RST BATCH"/>
<node id="7500 TO 10000 MS"/>
<node id="PIPESWITCH MPS"/>
<node id="DIFFERENT SCHEDULING CYCLES"/>
<node id="THROUGHPUT AND LATENCY"/>
<node id="RESNET"/>
<node id="COMPUTATION"/>
<node id="ONCE PARAMETERS ARE TRANSMITTED"/>
<node id="THE TOTAL TIME MEASURED BY THE CLIENT FOR AN INFERENCE TASK TO PREEMPT A TRAINING TASK AND FINISH ITS INFERENCE"/>
<node id="FIGURE 8"/>
<node id="THE TOTAL TIME MEASURED BY THE CLIENT"/>
<node id="THE WORST IN MOST CASES"/>
<node id="LAYERS OF THE MODEL INTO ONE BIG TENSOR"/>
<node id="ONE BIG TENSOR IN ONE GROUP"/>
<node id="TRANSMISSION AND COMPUTATION AT THE GRANULARITY OF LAYER"/>
<node id="MODELS WITH MANY LAYERS BUT RELATIVELY LIGHT COMPUTATION SUCH AS RESNET152 AND INCEPTION"/>
<node id="WORSE THAN GROUPED TRANSMISSION"/>
<node id="SOMETIMES EVEN WORSE THAN NO PIPELINE"/>
<node id="THIS REDUCTION"/>
<node id="SIGNIFICANT"/>
<node id="THE OPTIMIZATIONS ON MEMORY MANAGEMENT AND WORKER SWITCHING HAVE ALREADY BEEN APPLIED"/>
<node id="THAT TO MEET STRICT SLOS, IT IS IMPORTANT TO REDUCE ALL OVERHEADS FOR TASK SWITCHING"/>
<node id="ALL OVERHEADS FOR TASK SWITCHING"/>
<node id="NOT ONLY THE MOST SIGNIFICANT ONE"/>
<node id="NUMBER OF LAYERS"/>
<node id="BOTH WEIGHTED AND UNWEIGHTED LAYERS"/>
<node id="COMPUTATION TIME"/>
<node id="THE PARAMETER SIZE"/>
<node id="RUNNING TIME"/>
<node id="EACH LAYER"/>
<node id="ONLY SEVERAL SECONDS TO COMPUTE AN OPTIMAL GROUPING STRATEGY"/>
<node id="NO PRUNING"/>
<node id="FOR ALL THREE MODELS AFTER RUNNING FOR 24 HOURS"/>
<node id="UNIED MEMORY MANAGEMENT"/>
<node id="THE EFFECTIVENESS OF UNIED MEMORY MANAGEMENT"/>
<node id="NO UNIED MEMORY MANAGEMENT"/>
<node id="STATED"/>
<node id="EFFECTIVENESS OF UNIFIED MEMORY MANAGEMENT"/>
<node id="THE FOLLOWING VE MECHANISMS DISCUSSED IN 4.3"/>
<node id="THE PAGES OF THE MEMORY DAEMON"/>
<node id="THE MAIN MEMORY"/>
<node id="THIS EXPERIMENT"/>
<node id="ALL THE OPTIMIZATIONS ON MEMORY MANAGEMENT ARE EFFECTIVE"/>
<node id="UNIED MEMORY MANAGEMENT MECHANISM"/>
<node id="IPC OPTIMIZATION"/>
<node id="IMPORTANT"/>
<node id="LATENCY BY 1648 MS"/>
<node id="PINNING THE PAGES TO THE HOST MEMORY"/>
<node id="THE LATENCY WITH A FEW MILLISECONDS"/>
<node id="FIGURE 9"/>
<node id="EFFECTIVENESS OF ACTIVE-STANDBY SWITCHING"/>
<node id="A NEW PROCESS FOR THE NEW TASK"/>
<node id="THE NEW PROCESS"/>
<node id="A NEW CUDA ENVIRONMENT"/>
<node id="THE TOTAL TIME"/>
<node id="THE CUDA ENVIRONMENT"/>
<node id="THE OVERHEAD TO CLEAN THE ENVIRONMENT"/>
<node id="MANY FRAMEWORKS"/>
<node id="DEEP LEARNING"/>
<node id="FRAMEWORKS"/>
<node id="TENSORFLOW"/>
<node id="MXNET"/>
<node id="SEVERAL ALGORITHMS AND SYSTEMS"/>
<node id="EXECUTING AND SCHEDULING DEEP LEARNING TASKS ON CLUSTERS"/>
<node id="BOTH TRAINING AND INFERENCE TASKS"/>
<node id="HOW TO REALIZE A SCHEDULING DECISION"/>
<node id="THE SCHEDULER TO CHANGE THE RESOURCE ALLOCATION MORE OFTEN WITH MILLISECOND-SCALE TASK SWITCHING"/>
<node id="MANY TECHNIQUES AND SYSTEMS"/>
<node id="OPTIMIZE COMMUNICATION"/>
<node id="IMPROVE DISTRIBUTED TRAINING"/>
<node id="THE MOST RELEVANT ONES"/>
<node id="PIPEDREAM 8"/>
<node id="BYTESCHEDULER 9"/>
<node id="POSEIDON 40"/>
<node id="OTHER WORKS LIKE VDNN 43 AND SWAPADVISOR 44"/>
<node id="GPU MEMORY MANAGEMENT MODULE"/>
<node id="CLUSTER MANAGERS 4548"/>
<node id="GPUS TO VMS OR CONTAINERS AT DEVICE GRANULARITY"/>
<node id="SEVERAL SOLUTIONS"/>
<node id="SHARE A GPU AT APPLICATION GRANULARITY"/>
<node id="TECHNIQUES"/>
<node id="LIBRARY INTERCEPTION 6,4953"/>
<node id="DEEP LEARNING APPLICATIONS"/>
<node id="HUNDREDS OF KERNELS"/>
<node id="EFFORTS"/>
<node id="GPU OPTIMIZATION"/>
<node id="PERFORMANCE OF RUNNING A SINGLE TASK"/>
<node id="TENSOR FUSION"/>
<node id="KERNEL-LEVEL CONCURRENCY"/>
<node id="SCHEDULING"/>
<node id="OUR SHEPHERD MADAN MUSU-VATHI"/>
<node id="THE ANONYMOUS REVIEWERS"/>
<node id="VALUABLE FEEDBACK"/>
<node id="ZHIHAO BAI"/>
<node id="AN AWS MACHINE LEARNING RESEARCH AWARD"/>
<node id="ZHEN ZHANG"/>
<node id="XIN JIN"/>
<node id="A. VERMA, L. PEDROSA, M. KORUPOLU, D. OPPENHEIMER, E. TUNE, AND J. WILKES"/>
<node id="LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG"/>
<node id="EUROSYS"/>
<node id="2015"/>
<node id="DEAN AND L. A. BARROSO"/>
<node id="THE TAIL AT SCALE"/>
<node id="COMMUNICATIONS OF THE ACM"/>
<node id="VOL."/>
<node id="NEXUS"/>
<node id="A GPU CLUSTER ENGINE"/>
<node id="ACCELERATING DNN-BASED VIDEO ANALYSIS"/>
<node id="ACM SOSP"/>
<node id="H. SHEN"/>
<node id="L. CHEN"/>
<node id="Y. JIN"/>
<node id="L. ZHAO"/>
<node id="B. KONG"/>
<node id="M. PHILIPOSE"/>
<node id="A. KRISHNAMURTHY"/>
<node id="R. SUNDARAM"/>
<node id="FRIED, J. BEHRENS, A. BELAY, AND H. BAL-AKRISHNAN"/>
<node id="SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS"/>
<node id="SHENANGO"/>
<node id="USENIX NSDI"/>
<node id="CUDA MULTI-PROCESS SERVICE"/>
<node id="6"/>
<node id="CUDAMULTIPROCESSSERVICEOVERVIEW.PDF"/>
<node id="HTTPS://DOCS.NVIDIA.COM/DEPLOY.PDF"/>
<node id="7 P. YU AND M. CHOWDHURY"/>
<node id="SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS"/>
<node id="CONFERENCE ON MACHINE LEARNING AND SYSTEMS"/>
<node id="2020"/>
<node id="PIPEDREAM"/>
<node id="GENERALIZED PIPELINE PARALLELISM FOR DNN TRAINING"/>
<node id="D. NARAYANAN"/>
<node id="A. HARLAP"/>
<node id="A. PHANISHAYEE"/>
<node id="V. SESHADRI"/>
<node id="N. R. DEVANUR"/>
<node id="G. R. GANGER"/>
<node id="P. B. GIBBONS"/>
<node id="M. ZAHARIA"/>
<node id="9 Y. PENG, Y. ZHU, Y. CHEN, Y. BAO, B. YI, C. LAN, C. WU, AND C. GUO"/>
<node id="A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION"/>
<node id="TIRESIAS"/>
<node id="A GPU CLUSTER MANAGER FOR DISTRIBUTED DEEP LEARNING"/>
<node id="J. GU, M. CHOWDHURY, K. G. SHIN, Y. ZHU, M. JEON, J. QIAN, H. LIU, AND C. GUO"/>
<node id="POSEIDON"/>
<node id="AN EFFICIENT COMMUNICATION ARCHITECTURE"/>
<node id="DISTRIBUTED DEEP LEARNING ON GPU CLUSTERS"/>
<node id="2017"/>
<node id="H. ZHANG, Z. ZHENG, S. XU, W. DAI, Q. HO, X. LIANG, Z. HU, J. WEI, P. XIE, AND E. P. XING"/>
<node id="AMAZON WEB SERVICES"/>
<node id="12"/>
<node id="AWS.AMAZON.COM"/>
<node id="MICROSOFT"/>
<node id="AZURE"/>
<node id="AZURE.MICROSOFT.COM"/>
<node id="GOOGLE CLOUD PLATFORM"/>
<node id="14"/>
<node id="CLOUD.GOOGLE.COM"/>
<node id="HOROVOD"/>
<node id="FAST AND EASY DISTRIBUTED DEEP LEARNING FRAMEWORK"/>
<node id="15 A. SERGEEV AND M. DEL BALSO"/>
<node id="HOROVOD PAPER"/>
<node id="ARXIV PREPRINT ARXIV:1802.05799"/>
<node id="2018"/>
<node id="SU"/>
<node id="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER"/>
<node id="USENIX OSDI"/>
<node id="2014"/>
<node id="DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION"/>
<node id="SUN"/>
<node id="IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"/>
<node id="2016"/>
<node id="NVIDIA DATA CENTER DEEP LEARNING PRODUCT"/>
<node id="DEVELOPER.NVIDIA.COMDEEP-LEARNING-PERFORMANCE-TRAINING-INFERENCE"/>
<node id="PHILLY"/>
<node id="20 TRACES"/>
<node id="HTTPS:GITHUB.COM/MSR-FIDDLE"/>
<node id="PHILLY-TRACES"/>
<node id="21"/>
<node id="22 C. SZEGEDY, V. VANHOUCKE, S. IOFFE, J. SHLENS, AND Z. WOJNA"/>
<node id="RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION"/>
<node id="J. DEVLIN, M.-W. CHANG, K. LEE, AND K. TOUTANOVA"/>
<node id="BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING"/>
<node id="PROCEEDINGS OF THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, VOLUME 1 (LONG AND SHORT PAPERS)"/>
<node id="GANDIVA"/>
<node id="INTROSPECTIVE CLUSTER SCHEDULING FOR DEEP LEARNING"/>
<node id="24 W. XIAO, R. BHARDWAJ, R. RAMJEE, M. SIVATHANU, N. KWATRA, Z. HAN, P. PATEL, X. PENG, H. ZHAO, Q. ZHANG, ET AL."/>
<node id="25"/>
<node id="TENSORFLOW XLA"/>
<node id="54"/>
<node id="WWW.TENSORFLOW.ORG"/>
<node id="HTTPS://WWW.TENSORFLOW.ORG"/>
<node id="XLA"/>
<node id="26"/>
<node id="MXNET.APACHE.ORG"/>
<node id="SLAQ"/>
<node id="QUALITY-DRIVEN SCHEDULING FOR DISTRIBUTED MACHINE LEARNING"/>
<node id="M. J. FREEDMAN"/>
<node id="ACM SYMPOSIUM ON CLOUD COMPUTING"/>
<node id="OPTIMUS"/>
<node id="AN EFFICIENT DYNAMIC RESOURCE SCHEDULER FOR DEEP LEARNING CLUSTERS"/>
<node id="Y. PENG, Y. BAO, Y. CHEN, C. WU, AND C. GUO"/>
<node id="THEMIS"/>
<node id="FAIR AND EFFICIENT GPU CLUSTER SCHEDULING"/>
<node id="K. MAHAJAN, A. BALASUBRAMANIAN, A. SINGHVI, S. VENKATARAMAN, A. AKELLA, A. PHANISHAYEE, AND S. CHAWLA"/>
<node id="HYPERSCHED"/>
<node id="DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE"/>
<node id="R. LIAW"/>
<node id="R. BHARDWAJ"/>
<node id="L. DUNLAP"/>
<node id="Y. ZOU"/>
<node id="J. E. GONZALEZ"/>
<node id="I. STOICA"/>
<node id="A. TUMANOV"/>
<node id="CHET"/>
<node id="AN OPTIMIZING COMPILER FOR FULLY-HOMOMORPHIC NEURAL-NETWORK INFERENCING"/>
<node id="ACM CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION"/>
<node id="R. DATHATHRI"/>
<node id="O. SAARIKIVI"/>
<node id="H. CHEN"/>
<node id="K. LAINE"/>
<node id="K. LAUTER"/>
<node id="S. MALEKI"/>
<node id="M. MUSUVATHI"/>
<node id="T. MYTKOWICZ"/>
<node id="TVM"/>
<node id="AN AUTOMATED END-TO-END OPTIMIZING COMPILER FOR DEEP LEARNING"/>
<node id="T. CHEN, T. MOREAU, Z. JIANG, L. ZHENG, E. YAN, H. SHEN, M. COWAN, L. WANG, Y. HU, L. CEZE, ET AL."/>
<node id="GPIPE"/>
<node id="EFFICIENT TRAINING OF GIANT NEURAL NETWORKS USING PIPELINE PARALLELISM"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS"/>
<node id="Y. HUANG ET AL."/>
<node id="BLINK"/>
<node id="FAST AND GENERIC COLLECTIVES FOR DISTRIBUTED ML"/>
<node id="G. WANG, S. VENKATARAMAN, A. PHANISHAYEE, J. THELIN, N. DEVANUR, AND I. STOICA"/>
<node id="NVIDIA COLLECTIVE COMMUNICATIONS LIBRARY"/>
<node id="NCCL"/>
<node id="DEVELOPER.NVIDIA.COM/NCCL"/>
<node id="36 J. LIU, J. WU, AND D. K. PANDA"/>
<node id="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION OVER INNIBAND"/>
<node id="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION"/>
<node id="OVER INNIBAND"/>
<node id="37 Q. HO, J. CIPAR, H. CUI, S. LEE, J. K. KIM, P. B. GIBBONS, G. A. GIBSON, G. GANGER, AND E. P. XING"/>
<node id="MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER"/>
<node id="2013"/>
<node id="A. AWAN, C.-H. CHU, H. SUBRAMONI, AND D. K. PANDA"/>
<node id="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?"/>
<node id="PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING"/>
<node id="DAILY, A. VISHNU, C. SIEGEL, T. WARFEL, AND V. AMATYA"/>
<node id="GOSSIPGRAD: SCALABLE DEEP LEARNING USING GOSSIP COMMUNICATION BASED ASYNCHRONOUS GRADIENT DESCENT"/>
<node id="GOSSIPGRAD"/>
<node id="SCALABLE DEEP LEARNING USING GOSSIP COMMUNICATION BASED ASYNCHRONOUS GRADIENT DESCENT"/>
<node id="CORR, VOL."/>
<node id="41 Z. ZHANG, C. CHANG, H. LIN, Y. WANG, R. ARORA, AND X. JIN"/>
<node id="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?"/>
<node id="ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)"/>
<node id="AUGUST 2020"/>
<node id="42 Y. CHEN, Z. LIU, B. REN, AND X. JIN"/>
<node id="ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS"/>
<node id="INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)"/>
<node id="JULY 2020"/>
<node id="VDNN"/>
<node id="VIRTUALIZED DEEP NEURAL NETWORKS FOR SCALABLE, MEMORY-EFFICIENT NEURAL NETWORK DESIGN"/>
<node id="2016 49TH ANNUAL IEEEACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO)"/>
<node id="PAPER"/>
<node id="43 M. RHU, N. GIMELSHEIN, J. CLEMONS, A. ZULQAR, AND S. W. KECKLER"/>
<node id="SYMPOSIUM"/>
<node id="SWAPADVISOR"/>
<node id="44 C.-C. HUANG, G. JIN, AND J. LI"/>
<node id="ACM ASPLOS"/>
<node id="PUSHING DEEP LEARNING BEYOND THE GPU MEMORY LIMIT VIA SMART SWAPPING"/>
<node id="KUBERNETES.IO"/>
<node id="NVIDIA CONTAINER RUNTIME"/>
<node id="DOCKER"/>
<node id="GITHUB.COM/NVIDIA/NVIDIA-DOCKER"/>
<node id="MESOS"/>
<node id="A PLATFORM FOR NE-GRAINED RESOURCE SHARING IN THE DATA CENTER"/>
<node id="47 B. HINDMAN, A. KONWINSKI, M. ZAHARIA, A. GHODSI, A. D. JOSEPH, R. H. KATZ, S. SHENKER, AND I. STOICA"/>
<node id="MESOS: A PLATFORM FOR NE-GRAINED RESOURCE SHARING IN THE DATA CENTER"/>
<node id="2011"/>
<node id="APACHE HADOOP YARN"/>
<node id="YET ANOTHER RESOURCE NEGOTIATOR"/>
<node id="V. K. VAVILAPALLI, A. C. MURTHY, C. DOUGLAS, S. AGARWAL, M. KONAR, R. EVANS, T. GRAVES, J. LOWE, H. SHAH, S. SETH, ET AL."/>
<node id="APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR"/>
<node id="G. GIUNTA, R. MONTELLA, G. AGRILLO, AND G. COVIELLO"/>
<node id="A GPGPU TRANSPARENT VIRTUALIZATION COMPONENT FOR HIGH PERFORMANCE COMPUTING CLOUDS"/>
<node id="EUROPEAN CONFERENCE ON PARALLEL PROCESSING"/>
<node id="2010"/>
<node id="V. T. RAVI, M. BECCHI, G. AGRAWAL, AND S. CHAKRADHAR"/>
<node id="SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK"/>
<node id="PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING"/>
<node id="GVIM"/>
<node id="GPU-ACCELERATED VIRTUAL MACHINES"/>
<node id="PROCEEDINGS OF THE 3RD ACM WORKSHOP ON SYSTEM-LEVEL VIRTUALIZATION FOR HIGH PERFORMANCE COMPUTING"/>
<node id="2009"/>
<node id="50 V. GUPTA, A. GAVRILOVSKA, K. SCHWAN, H. KHARCHE, N. TOLIA, V. TALWAR, AND P. RANGANATHAN"/>
<node id="RCUDA"/>
<node id="THE NUMBER OF GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS"/>
<node id="51 J. DUATO, A. J. PENA, F. SILLA, R. MAYO, AND E. S. QUINTANA-ORT"/>
<node id="2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION"/>
<node id="VCUDA"/>
<node id="GPU-ACCELERATED HIGH-PERFORMANCE COMPUTING IN VIRTUAL MACHINES"/>
<node id="SUN AND K. LI"/>
<node id="IEEE TRANSACTIONS ON COMPUTERS"/>
<node id="A FLEXIBLE AND EFFICIENT MACHINE LEARNING LIBRARY"/>
<node id="HETEROGENEOUS DISTRIBUTED SYSTEMS"/>
<node id="55 T. CHEN, M. LI, Y. LI, M. LIN, N. WANG, M. WANG, T. XIAO, B. XU, C. ZHANG, AND Z. ZHANG"/>
<node id="ARXIV PREPRINT ARXIV:1512.01274"/>
<node id="56 C. GREGG, J. DORN, K. HAZELWOOD, AND K. SKADRON"/>
<node id="FINE-GRAINED RESOURCE SHARING FOR CONCURRENT GPGPU KERNELS"/>
<node id="4TH USENIX WORKSHOP ON HOT TOPICS IN PARALLELISM"/>
<node id="2012"/>
<node id="57 S. PAI, M. J. THAZHUTHAVEETIL, AND R. GOVINDARAJAN"/>
<node id="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS"/>
<node id="ACM SIGARCH COMPUTER ARCHITECTURE NEWS"/>
<node id="TASO"/>
<node id="DEEP LEARNING COMPUTATION"/>
<node id="AUTOMATIC GENERATION OF GRAPH SUBSTITUTIONS"/>
<node id="Z. JIA, O. PADON, J. THOMAS, T. WARSZAWSKI, M. ZAHARIA, AND A. AIKEN"/>
<edge source="THE DOMINANT PRACTICE TODAY" target="DEDICATED GPU CLUSTERS FOR TRAINING AND INFERENCE SEPARATELY">
  <data key="d0">IS TO PROVISION</data>
</edge>
<edge source="GPU CLUSTERS" target="SHARED GPU CLUSTERS">
  <data key="d0">ARE</data>
</edge>
<edge source="GPU CLUSTERS" target="DIFFERENT APPLICATIONS">
  <data key="d0">CAN BE SHARED ACROSS</data>
</edge>
<edge source="GPU CLUSTERS" target="THE PEAK LOAD">
  <data key="d0">ARE OFTEN OVER-PROVISIONED BASED ON</data>
</edge>
<edge source="GPU CLUSTERS" target="LIMITED SHARING BETWEEN APPLICATIONS AND TASK TYPES">
  <data key="d0">HAVE</data>
</edge>
<edge source="GPU CLUSTERS" target="EITHER PRIVATELY OR PUBLICLY SHARED BY MULTIPLE USERS">
  <data key="d0">ARE</data>
</edge>
<edge source="GPU CLUSTERS" target="DEDICATED PHYSICAL FORMS AND POWER SUPPLIES">
  <data key="d0">ARE DESIGNED WITH</data>
</edge>
<edge source="GPU CLUSTERS" target="HIGH SPEED NETWORKS">
  <data key="d0">ARE DESIGNED WITH</data>
</edge>
<edge source="GPU CLUSTERS" target="SPECIALIZED TASK SCHEDULERS">
  <data key="d0">ARE DESIGNED WITH</data>
</edge>
<edge source="TRAINING AND INFERENCE" target="GPUS">
  <data key="d0">USE</data>
</edge>
<edge source="TRAINING AND INFERENCE" target="EIGHT P3.2XLARGE INSTANCES">
  <data key="d0">PERFORMED ON</data>
</edge>
<edge source="GPUS" target="ONE OF THE MOST WIDELY-USED CLASSES OF ACCELERATORS FOR DL">
  <data key="d0">ARE</data>
</edge>
<edge source="GPUS" target="VMS, CONTAINERS OR PROCESSES OF AN APPLICATION">
  <data key="d0">ARE BOUND TO</data>
</edge>
<edge source="GPUS" target="A PAGE IN THE HOST MEMORY TO BE PINNED (OR PAGE-LOCKED)">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="CURRENT PRACTICE" target="DEDICATED CLUSTERS FOR TRAINING AND INFERENCE SEPARATELY">
  <data key="d0">IS TO BUILD</data>
</edge>
<edge source="WE" target="FINE-GRAINED TIME-SHARING GPU CLUSTERS">
  <data key="d0">ENVISION TO BUILD</data>
</edge>
<edge source="WE" target="THE PERFORMANCE OF PIPESWITCH">
  <data key="d0">DEMONSTRATE</data>
</edge>
<edge source="WE" target="SO BY INTRODUCING PIPELINED CONTEXT SWITCHING">
  <data key="d0">ACHIEVE</data>
</edge>
<edge source="WE" target="A PIPELINED MODEL TRANSMISSION MECHANISM">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="UNIFIED MEMORY MANAGEMENT">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="ACTIVE-STANDBY WORKER SWITCHING MECHANISMS">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="AN ACTIVE-STANDBY MECHANISM">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="A PIPESWITCH PROTOTYPE">
  <data key="d0">HAVE BUILT</data>
</edge>
<edge source="WE" target="IT WITH PYTORCH">
  <data key="d0">INTEGRATE</data>
</edge>
<edge source="WE" target="SO BY INTRODUCING A NEW TECHNOLOGY CALLED PIPELINED CONTEXT SWITCHING">
  <data key="d0">ACHIEVE</data>
</edge>
<edge source="WE" target="A MAJOR CHALLENGE FAST GPU CONTEXT SWITCHING BETWEEN DIFFERENT PROCESSES">
  <data key="d0">FACE</data>
</edge>
<edge source="WE" target="PIPELINED CONTEXT SWITCHING">
  <data key="d0">INTRODUCE</data>
</edge>
<edge source="WE" target="A MEASUREMENT STUDY">
  <data key="d0">PERFORM</data>
</edge>
<edge source="WE" target="THE OVERHEAD OF EACH COMPONENT">
  <data key="d0">ANALYZE</data>
</edge>
<edge source="WE" target="THE SWITCHING OVERHEAD INTO FOUR COMPONENTS">
  <data key="d0">DIVIDE</data>
</edge>
<edge source="WE" target="A HOLISTIC APPROACH">
  <data key="d0">TAKE</data>
</edge>
<edge source="WE" target="THE CHARACTERISTICS OF DL APPLICATIONS">
  <data key="d0">EXPLOIT</data>
</edge>
<edge source="WE" target="THE OVERHEAD OF ALL THE COMPONENTS">
  <data key="d0">MINIMIZE</data>
</edge>
<edge source="WE" target="LAYERS INTO GROUPS">
  <data key="d0">DIVIDE</data>
</edge>
<edge source="WE" target="AN OPTIMAL MODEL-AWARE GROUPING ALGORITHM">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="AN ALGORITHM TO FIND THE OPTIMAL GROUPING STRATEGY FOR A GIVEN MODEL">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="UNIFIED MEMORY MANAGEMENT WITH THE MEMORY DAEMON">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="THE MEMORY ALLOCATION FOR A DNN MODEL IS DETERMINISTIC">
  <data key="d0">EXPLOIT</data>
</edge>
<edge source="WE" target="EXTRA MEMORY COPIES BETWEEN THE DAEMON AND THE WORKERS">
  <data key="d0">ELIMINATE</data>
</edge>
<edge source="WE" target="THE IPC OVERHEAD">
  <data key="d0">MINIMIZE</data>
</edge>
<edge source="WE" target="AN ACTIVE AND STANDBY WORKER SWITCHING MECHANISM">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="ALL OTHER COMPONENTS OF PIPESWITCH THE SAME">
  <data key="d0">KEEP</data>
</edge>
<edge source="WE" target="THE FOLLOWING MECHANISMS DISCUSSED IN 4.4">
  <data key="d0">COMPARE</data>
</edge>
<edge source="WE" target="NEW TECHNIQUES">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="A SYSTEM PROTOTYPE">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="INEFFICIENCIES IN TODAYS SHARED GPU CLUSTERS">
  <data key="d0">IDENTIFY</data>
</edge>
<edge source="WE" target="RUNNING DL WORKLOADS ON GPUS IN THE NE-GRAINED TIME-SHARING MODEL">
  <data key="d0">MOTIVATE</data>
</edge>
<edge source="WE" target="MULTIPLE DL APPLICATIONS ONTO THE SAME GPU">
  <data key="d0">PROPOSE TO PACK</data>
</edge>
<edge source="WE" target="THE STRAWMAN SCENARIO">
  <data key="d0">TEST</data>
</edge>
<edge source="WE" target="A TRAINING TASK">
  <data key="d0">STOP</data>
</edge>
<edge source="WE" target="AN INFERENCE TASK">
  <data key="d0">START</data>
</edge>
<edge source="WE" target="OTHER CHALLENGES">
  <data key="d0">WILL NEED TO RESOLVE</data>
</edge>
<edge source="WE" target="AN OVERVIEW OF THE ARCHITECTURE AND TASK EXECUTION">
  <data key="d0">PROVIDE</data>
</edge>
<edge source="WE" target="FAST CONTEXT SWITCHING">
  <data key="d0">FOCUS ON</data>
</edge>
<edge source="WE" target="END-TO-END EXPERIMENTS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="THE EFFECTIVENESS OF THE DESIGN CHOICES ON EACH COMPONENT">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="OUR DESIGN">
  <data key="d0">DESCRIBE</data>
</edge>
<edge source="WE" target="MEASURING THE TIME TO START AND EXECUTE IT ON THE GPU">
  <data key="d0">FOCUS ON</data>
</edge>
<edge source="WE" target="THE NETWORK TIME">
  <data key="d0">EXCLUDE</data>
</edge>
<edge source="WE" target="THE TASK QUEUEING TIME">
  <data key="d0">EXCLUDE</data>
</edge>
<edge source="WE" target="THE OVERHEAD INTO THE FOUR COMPONENTS">
  <data key="d0">BREAK DOWN</data>
</edge>
<edge source="WE" target="ALL THE COMPONENTS SHOULD BE OPTIMIZED TO ACHIEVE MINIMAL SWITCHING OVERHEAD AND MEET THE SLOS">
  <data key="d0">EMPHASIZE</data>
</edge>
<edge source="WE" target="THE CHARACTERISTICS OF DL APPLICATIONS TO CIRCUMVENT THIS PHYSICAL LIMIT">
  <data key="d0">EXPLOIT</data>
</edge>
<edge source="WE" target="MULTIPLE LAYERS INTO A GROUP">
  <data key="d0">COMBINE</data>
</edge>
<edge source="WE" target="ALL POSSIBLE COMBINATIONS">
  <data key="d0">CAN ENUMERATE</data>
</edge>
<edge source="WE" target="TWO PRUNING TECHNIQUES">
  <data key="d0">INTRODUCE</data>
</edge>
<edge source="WE" target="THE CASES THAT GROUP FROM LAYER (I 1) TO J J">
  <data key="d0">CAN PRUNE</data>
</edge>
<edge source="WE" target="J J">
  <data key="d0">ONLY SEARCH FOR</data>
</edge>
<edge source="WE" target="THE PROBLEM">
  <data key="d0">FORMULATE</data>
</edge>
<edge source="WE" target="ALL POSSIBLE COMBINATIONS INTO N CASES">
  <data key="d0">DIVIDE</data>
</edge>
<edge source="WE" target="MULTIPLE LAYERS IN A GROUP BASED ON THE PROGRESS OF COMPUTATION">
  <data key="d0">CAN PACK</data>
</edge>
<edge source="WE" target="A LOWER BOUND FOR THE TOTAL TIME FOR EACH CASE IN EQUATION 1">
  <data key="d0">COMPUTE</data>
</edge>
<edge source="WE" target="THE TRANSMISSION OF THE SECOND GROUP INTO THE COMPUTATION OF THE RST GROUP">
  <data key="d0">CAN HIDE</data>
</edge>
<edge source="WE" target="B.DELAY TO DENOTE THE TIME TO WHICH THE GROUP CAN BE FORMED">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="INDUCTION ON M">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="FUNCTIONS FOR ALLOCATING GPU MEMORY">
  <data key="d0">ADD</data>
</edge>
<edge source="WE" target="FUNCTIONS FOR SHARING THE GPU MEMORY TO WORKERS THROUGH CUDA IPC API">
  <data key="d0">ADD</data>
</edge>
<edge source="WE" target="FUNCTIONS FOR GETTING THE SHARED GPU MEMORY">
  <data key="d0">ADD</data>
</edge>
<edge source="WE" target="TWO CHARACTERISTICS OF DL APPLICATIONS">
  <data key="d0">EXPLOIT</data>
</edge>
<edge source="WE" target="A MEMORY MANAGEMENT MECHANISM TAILORED FOR DL APPLICATIONS">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="A PROPERTY OF DL APPLICATIONS">
  <data key="d0">LEVERAGE</data>
</edge>
<edge source="WE" target="THE PERFORMANCE OF THESE IPC APIS">
  <data key="d0">HAVE MEASURED</data>
</edge>
<edge source="WE" target="THE PAGES OF THE MEMORY DAEMON TO THE HOST MEMORY">
  <data key="d0">PIN</data>
</edge>
<edge source="WE" target="SEPARATE PROCESSES">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="SYNCHRONIZATION POINTS INTO TRAINING TASKS">
  <data key="d0">INSERT</data>
</edge>
<edge source="WE" target="THE TASK CLEANING OF THE CURRENT TASK AND THE PIPELINED MODEL TRANSMISSION OF THE NEW TASK">
  <data key="d0">CAN PARALLELIZE</data>
</edge>
<edge source="WE" target="A PRODUCTION GPU TRAINING TRACE FROM MICROSOFT 19,20">
  <data key="d0">HAVE ANALYZED</data>
</edge>
<edge source="WE" target="THE SHARE OF MULTI-GPU JOBS TO INCREASE IN THE FUTURE">
  <data key="d0">EXPECT</data>
</edge>
<edge source="WE" target="C AND PYTHON FUNCTIONS TO THE GPU MEMORY MANAGEMENT MODULE OF PYTORCH">
  <data key="d0">ADD</data>
</edge>
<edge source="WE" target="TWO EC2 INSTANCE TYPES">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="PYTORCH WITH OUR PLUGINS FOR ALL MECHANISMS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="REPRESENTATIVE CONFIGURATIONS FOR EACH MODEL">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="SINGLE-GPU INFERENCE AND TRAINING TASKS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="THROUGHPUT AND LATENCY AS EVALUATION METRICS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="THE END-TO-END LATENCY EXPERIENCED BY THE CLIENT">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="SEPARATE PROCESSES IN ADVANCE">
  <data key="d0">INITIALIZE</data>
</edge>
<edge source="WE" target="THROUGHPUT AND END-TO-END LATENCY OF DIFFERENT MECHANISMS UNDER DIFFERENT SCHEDULING CYCLES">
  <data key="d0">COMPARE</data>
</edge>
<edge source="WE" target="RESNET152">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="TRAINING AND INFERENCE">
  <data key="d0">SWITCH BETWEEN</data>
</edge>
<edge source="WE" target="THAT TO MEET STRICT SLOS, IT IS IMPORTANT TO REDUCE ALL OVERHEADS FOR TASK SWITCHING">
  <data key="d0">WOULD LIKE TO EMPHASIZE</data>
</edge>
<edge source="WE" target="THE PARAMETER SIZE">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="RUNNING TIME">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="THE FOLLOWING VE MECHANISMS DISCUSSED IN 4.3">
  <data key="d0">COMPARE</data>
</edge>
<edge source="WE" target="OUR SHEPHERD MADAN MUSU-VATHI">
  <data key="d0">THANK</data>
</edge>
<edge source="WE" target="THE ANONYMOUS REVIEWERS">
  <data key="d0">THANK</data>
</edge>
<edge source="DIFFERENT APPLICATIONS" target="TRAINING AND INFERENCE">
  <data key="d0">INCLUDING</data>
</edge>
<edge source="SERVICE-LEVEL OBJECTIVES (SLOS)" target="STRICT">
  <data key="d0">NEED TO BE MET</data>
</edge>
<edge source="PIPESWITCH" target="A SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="PIPESWITCH" target="UNUSED CYCLES OF AN INFERENCE APPLICATION TO BE FILLED BY TRAINING OR OTHER INFERENCE APPLICATIONS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PIPESWITCH" target="GPU UTILIZATION">
  <data key="d0">CAN INCREASE</data>
</edge>
<edge source="PIPESWITCH" target="SINGLE-GPU TASKS FOR TRAINING AND INFERENCE">
  <data key="d0">IS FOCUSED ON</data>
</edge>
<edge source="PIPESWITCH" target="SINGLE-GPU TRAINING">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="PIPESWITCH" target="ASYNCHRONOUS MULTI-GPU TRAINING">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="PIPESWITCH" target="A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS">
  <data key="d0">IS APPLICABLE TO</data>
</edge>
<edge source="PIPESWITCH" target="THEM OUT OF THE BOX">
  <data key="d0">IS APPLICABLE TO</data>
</edge>
<edge source="PIPESWITCH" target="HIGH THROUGHPUT CLOSE TO THE UPPER BOUND">
  <data key="d0">HAS</data>
</edge>
<edge source="PIPESWITCH" target="NEAR 100 GPU UTILIZATION">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="PIPESWITCH" target="THE AGILITY OF DL APPLICATIONS">
  <data key="d0">CAN IMPROVE</data>
</edge>
<edge source="PIPESWITCH" target="THE MODEL STRUCTURE">
  <data key="d0">DOES NOT MODIFY</data>
</edge>
<edge source="PIPESWITCH" target="HOOKS FOR PYTORCH TO WAIT FOR TRANSMISSION OR SYNCHRONIZE THE EXECUTION">
  <data key="d0">ADDS</data>
</edge>
<edge source="PIPESWITCH" target="GPU-EFFICIENT MULTIPLEXING OF MANY DL APPLICATIONS ON GPU SERVERS VIA FINE-GRAINED TIME-SHARING">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PIPESWITCH" target="MILLISECOND-SCALE LATENCIES AND HIGH THROUGHPUT AS DEDICATED SERVERS">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="PIPESWITCH" target="GPU-EFFICIENT FINE-GRAINED TIME-SHARING FOR MULTIPLE DL APPLICATIONS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PIPESWITCH" target="MILLISECOND-SCALE CONTEXT SWITCHING LATENCIES">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="PIPESWITCH" target="HIGH THROUGHPUT">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="PIPESWITCH" target="ALL THE IDEAS INTO OUR SYSTEM">
  <data key="d0">COMBINES</data>
</edge>
<edge source="PIPESWITCH" target="THE GAP OF GPU MEMORY SHARING AND SWITCHING">
  <data key="d0">CLOSES</data>
</edge>
<edge source="PIPESWITCH" target="THE DESIGN OF AN EFFICIENT TIME-SHARING GPU CLUSTER FOR DL WORKLOADS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PIPESWITCH" target="GPU-EFFICIENT MULTIPLEXING OF MULTIPLE DL APPLICATIONS ON GPU SERVERS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PIPESWITCH" target="GPU-EFFICIENT FINE-GRAINED TIME-SHARING">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PIPESWITCH" target="MILLISECOND-SCALE TASK SWITCHING TIME">
  <data key="d0">IS ABLE TO ACHIEVE</data>
</edge>
<edge source="PIPESWITCH" target="DL APPLICATIONS ON TIME-SHARING GPUS TO MEET STRICT SLOS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PIPESWITCH" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">AIMS TO ENSURE</data>
</edge>
<edge source="PIPESWITCH" target="SEPARATE WORKER PROCESSES">
  <data key="d0">ENFORCES WITH</data>
</edge>
<edge source="PIPESWITCH" target="US TO ADDRESS NEW TECHNICAL CHALLENGES ON MEMORY MANAGEMENT AND WORKER SWITCHING ACROSS DIFFERENT PROCESSES">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="PIPESWITCH" target="FAST TASK SWITCHING">
  <data key="d0">AIMS TO PROVIDE</data>
</edge>
<edge source="PIPESWITCH" target="AN ACTIVE WORKER">
  <data key="d0">HAS</data>
</edge>
<edge source="PIPESWITCH" target="MULTIPLE STANDBY WORKERS">
  <data key="d0">HAS</data>
</edge>
<edge source="PIPESWITCH" target="THE ACTIVE-STANDBY WORKER SWITCHING MECHANISM USED BY PIPESWITCH">
  <data key="d0">IS</data>
</edge>
<edge source="PIPESWITCH" target="AN ACTIVE-STANDBY WORKER SWITCHING MECHANISM">
  <data key="d0">USES</data>
</edge>
<edge source="PIPESWITCH" target="MINIMAL OVERHEAD">
  <data key="d0">INCURS</data>
</edge>
<edge source="PIPESWITCH" target="INTRA-BATCH PIPELINING">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="PIPESWITCH" target="THE KNOWLEDGE OF MODELS">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="PIPESWITCH" target="MODEL-AWARE GROUPING">
  <data key="d0">USES</data>
</edge>
<edge source="PIPESWITCH" target="THE BEST TRADE-OFF BETWEEN PIPELINE OVERHEAD AND EFFICIENCY">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="PIPESWITCH" target="A PART OF THE DNN FRAMEWORK">
  <data key="d0">CAN BE IMPLEMENTED AS</data>
</edge>
<edge source="PIPESWITCH" target="MODEL STRUCTURE INFORMATION">
  <data key="d0">CAN GATHER</data>
</edge>
<edge source="PIPESWITCH" target="TRANSPARENT TO USERS AND CLUSTER MANAGERS">
  <data key="d0">REMAINS</data>
</edge>
<edge source="PIPESWITCH" target="RESULTING STRATEGY FOR CONTEXT SWITCHING">
  <data key="d0">USES</data>
</edge>
<edge source="PIPESWITCH" target="DEDICATED MEMORY DAEMON">
  <data key="d0">USES</data>
</edge>
<edge source="PIPESWITCH" target="64-BIT INTEGER OFFSET FOR THE SHARED GPU MEMORY TO WORKERS">
  <data key="d0">SENDS</data>
</edge>
<edge source="PIPESWITCH" target="223 MS">
  <data key="d0">SAVES</data>
</edge>
<edge source="PIPESWITCH" target="THE MEMORY ALLOCATION OVERHEAD">
  <data key="d0">ELIMINATES</data>
</edge>
<edge source="PIPESWITCH" target="GPU MEMORY BLOCKS TO PYTORCH GPU MEMORY POOL">
  <data key="d0">INSERTS</data>
</edge>
<edge source="PIPESWITCH" target="THE MODELS IN THE MEMORY DAEMON">
  <data key="d0">STORES</data>
</edge>
<edge source="PIPESWITCH" target="TECHNIQUE">
  <data key="d0">IS A</data>
</edge>
<edge source="PIPESWITCH" target="NO MEMORY MANAGEMENT">
  <data key="d0">HAS</data>
</edge>
<edge source="PIPESWITCH" target="NO IPC OPTIMIZATION">
  <data key="d0">HAS</data>
</edge>
<edge source="PIPESWITCH" target="NO PIN MEMORY">
  <data key="d0">HAS</data>
</edge>
<edge source="PIPESWITCH" target="CUDA UNIFIED MEMORY">
  <data key="d0">USES</data>
</edge>
<edge source="PIPESWITCH" target="6000 TO 8000 MS">
  <data key="d0">HAS LATENCY RANGE</data>
</edge>
<edge source="PIPESWITCH" target="ONE PROCESS">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="PIPESWITCH" target="TWO PROCESSES">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="PIPESWITCH" target="RESNET152">
  <data key="d0">RUNS</data>
</edge>
<edge source="PIPESWITCH" target="INCEPTIONV3">
  <data key="d0">RUNS</data>
</edge>
<edge source="PIPESWITCH" target="BERTBASE">
  <data key="d0">RUNS</data>
</edge>
<edge source="PIPESWITCH" target="COMPUTING THE RST LAYER">
  <data key="d0">TO START</data>
</edge>
<edge source="PIPESWITCH" target="THE BEST">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="PIPESWITCH" target="THE LOWER BOUND">
  <data key="d0">IS CLOSE TO</data>
</edge>
<edge source="PIPESWITCH" target="ONLY A FEW MILLISECONDS OVERHEAD FOR TASK SWITCHING">
  <data key="d0">INCURS</data>
</edge>
<edge source="PIPESWITCH" target="LOW LATENCY CLOSE TO THE LOWER BOUND">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="PIPESWITCH" target="HOW TO REALIZE A SCHEDULING DECISION">
  <data key="d0">FOCUSES ON</data>
</edge>
<edge source="PIPESWITCH" target="THE SCHEDULER TO CHANGE THE RESOURCE ALLOCATION MORE OFTEN WITH MILLISECOND-SCALE TASK SWITCHING">
  <data key="d0">ENABLES</data>
</edge>
<edge source="GPU UTILIZATION" target="SIGNIFICANTLY WITH PIPESWITCH">
  <data key="d0">CAN BE IMPROVED</data>
</edge>
<edge source="GPU UTILIZATION" target="DYNAMIC WORKLOADS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="GPU UTILIZATION" target="THE RATIO TO THE UPPER BOUND">
  <data key="d0">IS DEFINED AS</data>
</edge>
<edge source="IMPROVEMENT" target="SLOS">
  <data key="d0">DOES NOT SACRIFICE</data>
</edge>
<edge source="MULTI-GPU INFERENCE TASKS" target="PERFORMING PIPESWITCH ON EACH GPU WITH TRANSACTIONS">
  <data key="d0">CAN BE SUPPORTED BY</data>
</edge>
<edge source="ASYNCHRONOUS MULTI-GPU TRAINING" target="DATA PARALLEL STRATEGIES">
  <data key="d0">IS FOR</data>
</edge>
<edge source="PREEMPTING ONE GPU" target="OTHER GPUS">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="A SIGNIFICANT FRACTION OF TASKS IN REAL-WORLD WORKLOADS" target="A SINGLE GPU">
  <data key="d0">USE</data>
</edge>
<edge source="ONE WAY TO SEAMLESSLY USE PIPESWITCH FOR SYNCHRONOUS MULTI-GPU TRAINING" target="ELASTIC SYNCHRONOUS TRAINING">
  <data key="d0">IS TO USE</data>
</edge>
<edge source="ELASTIC SYNCHRONOUS TRAINING" target="THE DYNAMIC CHANGING OF THE NUMBER OF GPUS USED FOR TRAINING">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="EXPERIMENTS" target="A VARIETY OF DNN MODELS AND GPU CARDS">
  <data key="d0">ARE ON</data>
</edge>
<edge source="THE KEY IDEA" target="THE LAYERED STRUCTURE OF NEURAL NETWORK MODELS AND THEIR LAYER-BY-LAYER COMPUTATION PATTERN">
  <data key="d0">IS TO LEVERAGE</data>
</edge>
<edge source="THE KEY IDEA" target="MODEL TRANSMISSION OVER THE PCIE AND TASK EXECUTION IN THE GPU WITH MODEL-AWARE GROUPING">
  <data key="d0">IS TO PIPELINE</data>
</edge>
<edge source="PIPELINED MODEL TRANSMISSION MECHANISM" target="MODEL TRANSMISSION OVER THE PCIE">
  <data key="d0">PIPELINES</data>
</edge>
<edge source="PIPELINED MODEL TRANSMISSION MECHANISM" target="MODEL COMPUTATION IN THE GPU">
  <data key="d0">PIPELINES</data>
</edge>
<edge source="TRANSMITTING A TASK FROM CPU TO GPU" target="PCIE BANDWIDTH">
  <data key="d0">IS BOUNDED BY</data>
</edge>
<edge source="UNIFIED MEMORY MANAGEMENT" target="DEDICATED MEMORY DAEMON">
  <data key="d0">HAS</data>
</edge>
<edge source="ACTIVE-STANDBY WORKER SWITCHING MECHANISMS" target="THE PIPELINING">
  <data key="d0">ACCOMPANY</data>
</edge>
<edge source="ACTIVE-STANDBY WORKER SWITCHING MECHANISMS" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">ENSURE</data>
</edge>
<edge source="PROCESS-LEVEL ISOLATION" target="DESIRABLE">
  <data key="d0">IS</data>
</edge>
<edge source="PROCESS-LEVEL ISOLATION" target="ONE TASK CANNOT READ THE MEMORY OF ANOTHER TASK">
  <data key="d0">ENSURES</data>
</edge>
<edge source="PROCESS-LEVEL ISOLATION" target="THE CRASHING OF ONE TASK DOES NOT AFFECT OTHER TASKS OR THE ENTIRE SYSTEM">
  <data key="d0">ENSURES</data>
</edge>
<edge source="AN ACTIVE-STANDBY MECHANISM" target="FAST WORKER SWITCHING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="AN ACTIVE-STANDBY MECHANISM" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="SYSTEM PROTOTYPE FOR PIPESWITCH" target="3600 LINES OF CODE">
  <data key="d0">HAS</data>
</edge>
<edge source="SYSTEM PROTOTYPE FOR PIPESWITCH" target="PYTORCH 21">
  <data key="d0">IS INTEGRATED WITH</data>
</edge>
<edge source="3600 LINES OF CODE" target="C AND PYTHON">
  <data key="d0">ARE IN</data>
</edge>
<edge source="DEEP LEARNING (DL)" target="AN EMERGING FAMILY OF INTELLIGENT APPLICATIONS">
  <data key="d0">POWERS</data>
</edge>
<edge source="AN EMERGING FAMILY OF INTELLIGENT APPLICATIONS" target="MANY DOMAINS">
  <data key="d0">EXISTS IN</data>
</edge>
<edge source="MANY DOMAINS" target="RETAIL">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MANY DOMAINS" target="TRANSPORTATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MANY DOMAINS" target="FINANCE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MANY DOMAINS" target="HEALTHCARE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="DL WORKLOADS" target="THROUGHPUT-INTENSIVE TRAINING TASKS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="DL WORKLOADS" target="LATENCY-SENSITIVE INFERENCE TASKS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="DL WORKLOADS" target="WELL-DENED STRUCTURES">
  <data key="d0">HAVE</data>
</edge>
<edge source="INFERENCE TASKS" target="TRAINING CLUSTERS UNDER ASH CROWDS">
  <data key="d0">CANNOT BE SERVED WITH</data>
</edge>
<edge source="INFERENCE TASKS" target="SHORT">
  <data key="d0">ARE</data>
</edge>
<edge source="INFERENCE TASKS" target="PREEMPTED">
  <data key="d0">ARE NOT</data>
</edge>
<edge source="TRAINING TASKS" target="INFERENCE CLUSTERS WHEN THE INFERENCE LOAD IS LOW">
  <data key="d0">CANNOT UTILIZE</data>
</edge>
<edge source="TRAINING TASKS" target="OFTEN ELASTIC">
  <data key="d0">ARE</data>
</edge>
<edge source="TRAINING TASKS" target="STRICT DEADLINES">
  <data key="d0">DO NOT HAVE</data>
</edge>
<edge source="TRAINING TASKS" target="THEIR MODELS TO THE HOST MEMORY">
  <data key="d0">CHECK-POINT</data>
</edge>
<edge source="TRAINING TASKS" target="FROM THE LATEST CHECKPOINT AFTER PREEMPTION">
  <data key="d0">RESTART</data>
</edge>
<edge source="ASH CROWD" target="WHEN AN APPLICATION SUDDENLY BECOMES POPULAR AND THE DEMAND GROWS BEYOND THE OPERATORS EXPECTATION">
  <data key="d0">IS</data>
</edge>
<edge source="TRAINING CLUSTER" target="TRAINING TASKS FOR INFERENCE TASKS">
  <data key="d0">CANNOT PREEMPT</data>
</edge>
<edge source="INFERENCE CLUSTERS" target="THE PEAK LOAD">
  <data key="d0">ARE OFTEN OVER-PROVISIONED FOR</data>
</edge>
<edge source="INFERENCE CLUSTERS" target="STRICT SERVICE LEVEL OBJECTIVES (SLOS)">
  <data key="d0">ARE OVER-PROVISIONED IN ORDER TO MEET</data>
</edge>
<edge source="INFERENCE CLUSTERS" target="OVER-PROVISIONED FOR THE PEAK LOAD">
  <data key="d0">ARE</data>
</edge>
<edge source="INFERENCE CLUSTERS" target="USER REQUESTS">
  <data key="d0">SERVE</data>
</edge>
<edge source="INFERENCE CLUSTERS" target="STRICT SLOS">
  <data key="d0">NEED TO MEET</data>
</edge>
<edge source="INFERENCE CLUSTERS" target="HIGH UTILIZATION">
  <data key="d0">ARE NOT ALWAYS RUNNING AT</data>
</edge>
<edge source="INFERENCE CLUSTERS" target="TRAINING">
  <data key="d0">CANNOT BE UTILIZED BY</data>
</edge>
<edge source="STRICT SLOS" target="REQUESTS TO BE HANDLED IN SMALL BATCHES FOR LOW LATENCY">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="PRODUCTION SYSTEMS" target="EACH APPLICATION ON PER-GPU GRANULARITY">
  <data key="d0">ARE PROVISIONED TO</data>
</edge>
<edge source="PRODUCTION SYSTEMS" target="GPUS TO APPLICATIONS ON PER-GPU GRANULARITY">
  <data key="d0">ALLOCATE</data>
</edge>
<edge source="PROVISIONING" target="THE INTERFERENCE BETWEEN APPLICATIONS">
  <data key="d0">LIMITS</data>
</edge>
<edge source="ALLOCATING GPUS TO APPLICATIONS" target="THE INTERFERENCE BETWEEN DIFFERENT APPLICATIONS">
  <data key="d0">LIMITS</data>
</edge>
<edge source="ALLOCATING GPUS TO APPLICATIONS" target="THE SLO REQUIREMENTS">
  <data key="d0">SATISFIES</data>
</edge>
<edge source="MULTIPLE DL APPLICATIONS" target="THE SAME GPU SERVER">
  <data key="d0">SHOULD BE ABLE TO BE PACKED TO</data>
</edge>
<edge source="PACKING MULTIPLE DL APPLICATIONS TO THE SAME GPU SERVER" target="GPU UTILIZATION VIA TIME-SHARING">
  <data key="d0">MAXIMIZES</data>
</edge>
<edge source="OPERATING SYSTEMS" target="HIGH CPU UTILIZATION">
  <data key="d0">ACHIEVE</data>
</edge>
<edge source="HIGH CPU UTILIZATION" target="TASK SCHEDULING AND CONTEXT SWITCHING">
  <data key="d0">ACHIEVE VIA</data>
</edge>
<edge source="THE IDEA OF NE-GRAINED CPU TIME-SHARING" target="CLUSTER SCHEDULING">
  <data key="d0">HAS BEEN FURTHER EXTENDED TO</data>
</edge>
<edge source="NE-GRAINED TIME-SHARING" target="BETTER UTILIZATION THAN PROVISIONING DEDICATED RESOURCES">
  <data key="d0">CAN PROVIDE</data>
</edge>
<edge source="NE-GRAINED TIME-SHARING" target="NECESSARY PROCESS-LEVEL ISOLATION">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="CPU WORKLOADS" target="NE-GRAINED TIME-SHARING">
  <data key="d0">ARE SIMILAR TO</data>
</edge>
<edge source="SCHEDULING CYCLES" target="ENABLED">
  <data key="d0">ARE</data>
</edge>
<edge source="GOOGLE BORG 1" target="ONLINE SERVICES AND BATCH JOBS">
  <data key="d0">PACKS</data>
</edge>
<edge source="GOOGLE BORG 1" target="20-30 MACHINES">
  <data key="d0">SAVES</data>
</edge>
<edge source="20-30 MACHINES" target="COMPARED WITH NOT PACKING THEM">
  <data key="d0">ARE SAVED</data>
</edge>
<edge source="GPU" target="HIGH OVERHEAD WHEN SWITCHING BETWEEN TASKS">
  <data key="d0">HAS</data>
</edge>
<edge source="GPU" target="DNN MODEL (E.G., RESNET)">
  <data key="d0">SWITCHES TO</data>
</edge>
<edge source="GPU" target="THE MODELS">
  <data key="d0">CAN QUICKLY CONTEXT-SWITCH BETWEEN</data>
</edge>
<edge source="GPU" target="HIGH MEMORY BANDWIDTH">
  <data key="d0">HAS</data>
</edge>
<edge source="GPU" target="NVIDIA T4">
  <data key="d0">IS</data>
</edge>
<edge source="GPU" target="PCIE 3.0 8">
  <data key="d0">USES</data>
</edge>
<edge source="THE GAP" target="THE PRECIOUS GPU MEMORY AND SLOW SWITCHING">
  <data key="d0">IS</data>
</edge>
<edge source="NAIVELY USING GPUS" target="THE REQUIREMENTS OF DL INFERENCE THAT HAVE STRICT SLOS IN THE RANGE OF TENS TO HUNDREDS OF MILLISECONDS">
  <data key="d0">WILL NOT SATISFY</data>
</edge>
<edge source="DNN MODEL (E.G., RESNET)" target="GPU">
  <data key="d0">HAS NOT BEEN PRELOADED ONTO</data>
</edge>
<edge source="STATE-OF-THE-ART TRICKS LIKE CUDA UNIFIED MEMORY 4 (6)" target="MULTIPLE SECONDS DELAY">
  <data key="d0">DO NOT PREVENT</data>
</edge>
<edge source="CPU APPLICATIONS" target="MILLISECONDS OR EVEN MICROSECONDS">
  <data key="d0">CAN BE SWITCHED IN</data>
</edge>
<edge source="THE EXISTING SOLUTION" target="SPATIALLY SHARE THE GPU MEMORY">
  <data key="d0">IS TO</data>
</edge>
<edge source="THIS APPROACH" target="STRONG GPU MEMORY ISOLATION BETWEEN APPLICATIONS">
  <data key="d0">DOES NOT PROVIDE</data>
</edge>
<edge source="NVIDIA MULTIPLE PROCESS SHARING (MPS) 6" target="MULTIPLE PROCESSES TO USE THE SAME GPU">
  <data key="d0">ALLOW</data>
</edge>
<edge source="SALUS 7" target="MULTIPLE PROCESSES TO USE THE SAME GPU">
  <data key="d0">ALLOW</data>
</edge>
<edge source="SALUS 7" target="TRAINING TASKS WHICH ARE MEMORY-INTENSIVE">
  <data key="d0">CANNOT SUPPORT</data>
</edge>
<edge source="SALUS 7" target="MULTIPLE INFERENCE TASKS WHICH HAVE LARGE MODELS">
  <data key="d0">CANNOT SUPPORT</data>
</edge>
<edge source="SALUS 7" target="THE MODELS TO BE PRELOADED TO THE GPU">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="SALUS 7" target="SEVERAL LIMITATIONS DESCRIBED IN 2.2">
  <data key="d0">HAS</data>
</edge>
<edge source="MULTI-PROCESS SUPPORT FROM NVIDIA" target="INFERENCE PROCESS TO SHARE THE GPU WITH THE TRAINING PROCESS">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="NVIDIA MPS 6" target="OFFICIAL SUPPORT FOR SHARING A GPU BETWEEN MULTIPLE PROCESSES">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="GPU MEMORY" target="MUCH MORE LIMITED THAN HOST MEMORY">
  <data key="d0">IS</data>
</edge>
<edge source="GPU MEMORY" target="PRELOAD MANY APPLICATIONS">
  <data key="d0">CANNOT</data>
</edge>
<edge source="GPU MEMORY" target="VERY LIMITED">
  <data key="d0">IS</data>
</edge>
<edge source="GPU MEMORY" target="TASK EXECUTION">
  <data key="d0">IS PURPOSED FOR</data>
</edge>
<edge source="GPU MEMORY" target="STORING THE STATE OF IDLE APPLICATIONS">
  <data key="d0">IS NOT FOR</data>
</edge>
<edge source="GPU MEMORY" target="PIPESWITCH">
  <data key="d0">IS MANAGED BY</data>
</edge>
<edge source="ONE SINGLE MEMORY-INTENSIVE TRAINING TASK" target="ALL THE GPU MEMORY">
  <data key="d0">MAY CONSUME</data>
</edge>
<edge source="THE TRAINING TASK" target="ITS GPU ENVIRONMENT">
  <data key="d0">STOPS AND CLEANS</data>
</edge>
<edge source="THE TRAINING TASK" target="THE GPU MEMORY">
  <data key="d0">FREES</data>
</edge>
<edge source="THE TRAINING TASK" target="THE ENTIRE GPU MEMORY">
  <data key="d0">OCCUPIES</data>
</edge>
<edge source="THE TRAINING TASK" target="WHEN INFERENCE TASKS COME">
  <data key="d0">DOES NOT STOP</data>
</edge>
<edge source="THE TRAINING TASK" target="WHEN AN INFERENCE TASK COMES">
  <data key="d0">DO NOT STOP</data>
</edge>
<edge source="MEMORY FOOTPRINTS OF INFERENCE TASKS" target="INCREASING">
  <data key="d0">ARE</data>
</edge>
<edge source="MODELS" target="GETTING LARGER">
  <data key="d0">ARE</data>
</edge>
<edge source="MODELS" target="DAILY UPDATES WITH LATEST DATA">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="MODELS" target="DIFFERENT STRUCTURES">
  <data key="d0">HAVE</data>
</edge>
<edge source="MODELS" target="RESNET152">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MODELS" target="INCEPTIONV3">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MODELS" target="BERTBASE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="REQUEST BATCHING" target="PREVALENTLY USED TO INCREASE THROUGHPUT">
  <data key="d0">IS</data>
</edge>
<edge source="REQUEST BATCHING" target="THROUGHPUT 3">
  <data key="d0">IS USED TO INCREASE</data>
</edge>
<edge source="THROUGHPUT 3" target="GPU MEMORY REQUIREMENT OF INFERENCE APPLICATIONS">
  <data key="d0">INCREASES</data>
</edge>
<edge source="A CONTEXT SWITCHING DESIGN" target="THE SWITCHING OVERHEAD">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="A CONTEXT SWITCHING DESIGN" target="THE CONTENTS ON GPU MEMORY QUICKLY">
  <data key="d0">SWITCHES</data>
</edge>
<edge source="A CONTEXT SWITCHING DESIGN" target="A BETTER APPROACH FOR EFFICIENTLY TIME-SHARING GPUS">
  <data key="d0">IS</data>
</edge>
<edge source="NO EXISTING SOLUTION" target="SUCH CONTEXT SWITCHING ABSTRACTION FOR GPU">
  <data key="d0">OFFERS</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="THE CHARACTERISTICS OF DL APPLICATIONS">
  <data key="d0">EXPLOITS</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="MILLISECOND-SCALE OVERHEAD FOR SWITCHING TASKS ON GPUS">
  <data key="d0">ACHIEVE</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="TASK SWITCHING OVERHEAD ON GPUS FOR DL APPLICATIONS">
  <data key="d0">MINIMIZE</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="PIPELINED MODEL TRANSMISSION">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="UNIFIED MEMORY MANAGEMENT">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="ACTIVE-STANDBY WORKER SWITCHING">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="SWITCHING OVERHEAD">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">ENFORCES</data>
</edge>
<edge source="PIPELINED CONTEXT SWITCHING" target="THREE KEY TECHNIQUES">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="APPLICATION" target="GPU">
  <data key="d0">IS LOADED IN</data>
</edge>
<edge source="THERE" target="CONTEXT SWITCHING">
  <data key="d0">IS NO NEED FOR</data>
</edge>
<edge source="THERE" target="NO NEED TO WAIT FOR THE ENTIRE MODEL TO BE TRANSMITTED TO THE GPU BEFORE STARTING COMPUTATION">
  <data key="d0">IS</data>
</edge>
<edge source="THERE" target="ALWAYS AT LEAST ONE IDLE STANDBY WORKER">
  <data key="d0">IS</data>
</edge>
<edge source="THERE" target="NO TRAINING TASK">
  <data key="d0">IS</data>
</edge>
<edge source="DNN MODELS" target="HOST MEMORY">
  <data key="d0">CAN BE HELD IN</data>
</edge>
<edge source="DNN MODELS" target="A LAYERED STRUCTURE">
  <data key="d0">HAVE</data>
</edge>
<edge source="DNN MODELS" target="A LAYER-BY-LAYER COMPUTATION PATTERN">
  <data key="d0">HAVE</data>
</edge>
<edge source="DNN MODELS" target="USUALLY DEEP">
  <data key="d0">ARE</data>
</edge>
<edge source="DNN MODELS" target="MULTIPLE LAYERS STACKING ONE ON ANOTHER">
  <data key="d0">CONSIST OF</data>
</edge>
<edge source="DNN MODELS" target="EITHER INFERENCE OR TRAINING">
  <data key="d0">CAN BE</data>
</edge>
<edge source="HOST MEMORY" target="MUCH LARGER AND CHEAPER THAN GPU MEMORY">
  <data key="d0">IS</data>
</edge>
<edge source="THE MODELS" target="TRAINING OR INFERENCE">
  <data key="d0">ARE USED FOR</data>
</edge>
<edge source="THE MODELS" target="DIFFERENT CUDA STREAMS">
  <data key="d0">ARE ATTACHED TO</data>
</edge>
<edge source="THE MODELS" target="RESNET152 17">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE MODELS" target="INCEPTIONV3 22">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE MODELS" target="BERTBASE 23">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ENTERPRISES" target="GPU CLUSTERS">
  <data key="d0">BUILD</data>
</edge>
<edge source="11 M. JEON, S. VENKATARAMAN, A. PHANISHAYEE, U. QIAN, W. XIAO, AND F. YANG" target="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS" target="USENIX ATC">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="ANALYSIS OF LARGE-SCALE MULTI-TENANT GPU CLUSTERS FOR DNN TRAINING WORKLOADS" target="2019">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="512 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">IS ORGANIZED BY</data>
</edge>
<edge source="USENIX ASSOCIATION" target="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">HOSTS</data>
</edge>
<edge source="NUMBER OF APPLICATIONS THAT CAN BE MULTIPLEXED" target="GPU MEMORY SIZE">
  <data key="d0">IS NOT LIMITED BY</data>
</edge>
<edge source="EACH APPLICATION" target="ENTIRE GPU COMPUTE AND MEMORY RESOURCES DURING ITS TIME SLICE">
  <data key="d0">IS ABLE TO USE</data>
</edge>
<edge source="GPU-EFFICIENT FINE-GRAINED TIME-SHARING" target="MULTIPLE DL APPLICATIONS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="SUCH SMALL SWITCHING OVERHEAD" target="DL APPLICATIONS TO SATISFY STRICT SLO REQUIREMENTS">
  <data key="d0">IS CRITICAL FOR</data>
</edge>
<edge source="MILLISECOND-SCALE TASK SWITCHING OVERHEAD" target="SLO REQUIREMENTS">
  <data key="d0">SATISFY</data>
</edge>
<edge source="THE MEASUREMENT STUDY" target="THE TASK SWITCHING OVERHEAD">
  <data key="d0">PROLE</data>
</edge>
<edge source="THE TASK SWITCHING OVERHEAD" target="INDIVIDUAL COMPONENTS">
  <data key="d0">IS BROKEN DOWN TO</data>
</edge>
<edge source="THE FOUR COMPONENTS" target="OLD TASK CLEANING">
  <data key="d0">ARE</data>
</edge>
<edge source="THE FOUR COMPONENTS" target="NEW TASK INITIALIZATION">
  <data key="d0">ARE</data>
</edge>
<edge source="THE FOUR COMPONENTS" target="GPU MEMORY ALLOCATION">
  <data key="d0">ARE</data>
</edge>
<edge source="THE FOUR COMPONENTS" target="MODEL TRANSMISSION VIA PCIE FROM CPU TO GPU">
  <data key="d0">ARE</data>
</edge>
<edge source="INSTANCE TYPE" target="G4DN.2XLARGE">
  <data key="d0">IS</data>
</edge>
<edge source="INSTANCE TYPE" target="P3.2XLARGE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="G4DN.2XLARGE" target="BETTER CPU THAN P3.2XLARGE">
  <data key="d0">IS EQUIPPED WITH</data>
</edge>
<edge source="G4DN.2XLARGE" target="8 VCPUS (INTEL PLATINUM 8259CL)">
  <data key="d0">IS CONFIGURED WITH</data>
</edge>
<edge source="G4DN.2XLARGE" target="1 GPU (NVIDIA T4 WITH 16 GB GPU MEMORY)">
  <data key="d0">HAS</data>
</edge>
<edge source="G4DN.2XLARGE" target="PCIE 3.0 8">
  <data key="d0">HAS</data>
</edge>
<edge source="G4DN.2XLARGE" target="32 GB MEMORY">
  <data key="d0">HAS</data>
</edge>
<edge source="G4DN.2XLARGE" target="NVIDIA T4">
  <data key="d0">HAS GPU</data>
</edge>
<edge source="G4DN.2XLARGE" target="2.53 MS">
  <data key="d0">HAS STARTUP OVERHEAD</data>
</edge>
<edge source="G4DN.2XLARGE" target="5.49 MS">
  <data key="d0">HAS STARTUP OVERHEAD</data>
</edge>
<edge source="G4DN.2XLARGE" target="6.57 MS">
  <data key="d0">HAS STARTUP OVERHEAD</data>
</edge>
<edge source="P3.2XLARGE" target="8 VCPUS (INTEL XEON E5-2686 V4)">
  <data key="d0">IS CONFIGURED WITH</data>
</edge>
<edge source="P3.2XLARGE" target="1 GPU (NVIDIA V100 WITH 16 GB GPU MEMORY)">
  <data key="d0">HAS</data>
</edge>
<edge source="P3.2XLARGE" target="PCIE 3.0 16">
  <data key="d0">HAS</data>
</edge>
<edge source="P3.2XLARGE" target="61 GB MEMORY">
  <data key="d0">HAS</data>
</edge>
<edge source="P3.2XLARGE" target="NVIDIA V100">
  <data key="d0">HAS</data>
</edge>
<edge source="P3.2XLARGE" target="3.62 MS">
  <data key="d0">HAS STARTUP OVERHEAD</data>
</edge>
<edge source="GPU TYPE OF G4DN.2XLARGE" target="NVIDIA T4">
  <data key="d0">IS</data>
</edge>
<edge source="NVIDIA T4" target="16GB GPU MEMORY">
  <data key="d0">HAS</data>
</edge>
<edge source="NVIDIA T4" target="8.1 TFLOPS (SINGLE-PRECISION)">
  <data key="d0">HAS</data>
</edge>
<edge source="NVIDIA T4" target="COMPARABLE PERFORMANCE WITH NVIDIA V100">
  <data key="d0">HAS</data>
</edge>
<edge source="NVIDIA T4" target="PCIE 3.0 8">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="GPU TYPE OF P3.2XLARGE" target="NVIDIA V100">
  <data key="d0">IS</data>
</edge>
<edge source="NVIDIA V100" target="UP TO 32GB GPU MEMORY">
  <data key="d0">HAS</data>
</edge>
<edge source="NVIDIA V100" target="15.7 TFLOPS (SINGLE-PRECISION)">
  <data key="d0">HAS</data>
</edge>
<edge source="NVIDIA V100" target="PCIE 3.0 16">
  <data key="d0">USES INTERFACE</data>
</edge>
<edge source="TASK CLEANING TIME FOR G4DN.2XLARGE" target="155 MS">
  <data key="d0">IS</data>
</edge>
<edge source="TASK CLEANING TIME FOR P3.2XLARGE" target="165 MS">
  <data key="d0">IS</data>
</edge>
<edge source="TASK INITIALIZATION TIME FOR G4DN.2XLARGE" target="5530 MS">
  <data key="d0">IS</data>
</edge>
<edge source="TASK INITIALIZATION TIME FOR P3.2XLARGE" target="7290 MS">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY ALLOCATION TIME FOR G4DN.2XLARGE" target="10 MS">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY ALLOCATION TIME FOR P3.2XLARGE" target="13 MS">
  <data key="d0">IS</data>
</edge>
<edge source="MODEL TRANSMISSION TIME FOR G4DN.2XLARGE" target="91 MS">
  <data key="d0">IS</data>
</edge>
<edge source="MODEL TRANSMISSION TIME FOR P3.2XLARGE" target="81 MS">
  <data key="d0">IS</data>
</edge>
<edge source="TOTAL OVERHEAD FOR G4DN.2XLARGE" target="5787 MS">
  <data key="d0">IS</data>
</edge>
<edge source="TOTAL OVERHEAD FOR P3.2XLARGE" target="7551 MS">
  <data key="d0">IS</data>
</edge>
<edge source="INFERENCE TIME FOR G4DN.2XLARGE" target="105 MS">
  <data key="d0">IS</data>
</edge>
<edge source="INFERENCE TIME FOR P3.2XLARGE" target="32 MS">
  <data key="d0">IS</data>
</edge>
<edge source="EVERY COMPONENT" target="A CONSIDERABLE AMOUNT OF TIME">
  <data key="d0">TAKES</data>
</edge>
<edge source="TIME" target="TENS OF MILLISECONDS TO SECONDS">
  <data key="d0">VARIES FROM</data>
</edge>
<edge source="ONE SOURCE OF THE OVERHEAD" target="THE CONTENTIONS BOTH ON THE COMPUTATION AND MEMORY OF THE GPU">
  <data key="d0">IS</data>
</edge>
<edge source="OUR DESIGN" target="A KEY OBSERVATION">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="OUR DESIGN" target="THE OVERHEAD OF EACH COMPONENT">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="COMPUTATION OF DNN MODELS" target="LAYER BY LAYER">
  <data key="d0">TAKES PLACE</data>
</edge>
<edge source="A TASK" target="THE ENTIRE MODEL TO BE TRANSMITTED TO THE GPU BEFORE BEGINNING THE COMPUTATION">
  <data key="d0">DOES NOT NEED TO WAIT FOR</data>
</edge>
<edge source="NAIVE PIPELINING ON PER-LAYER GRANULARITY" target="HIGH OVERHEAD ON TENSOR TRANSMISSION AND SYNCHRONIZATION">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="PIPELINING ON PER-LAYER GRANULARITY" target="SYNCHRONIZATION FOR EVERY LAYER">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="AN OPTIMAL MODEL-AWARE GROUPING ALGORITHM" target="THE BEST GROUPING STRATEGY FOR A GIVEN MODEL">
  <data key="d0">FIND</data>
</edge>
<edge source="THE COMPUTATION OF A DL TASK" target="LAYER BY LAYER">
  <data key="d0">IS</data>
</edge>
<edge source="THE COMPUTATION OF A DL TASK" target="A SIMPLE, REGULAR PATTERN FOR MEMORY ALLOCATION">
  <data key="d0">HAS</data>
</edge>
<edge source="A DL TASK" target="TWO IMPORTANT TYPES OF DATA IN THE GPU MEMORY">
  <data key="d0">STORES</data>
</edge>
<edge source="TWO IMPORTANT TYPES OF DATA" target="THE DNN MODEL">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="TWO IMPORTANT TYPES OF DATA" target="THE INTERMEDIATE RESULTS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE DNN MODEL" target="THE MODEL PARAMETERS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE DNN MODEL" target="RESNET152 17">
  <data key="d0">IS</data>
</edge>
<edge source="THE MODEL PARAMETERS" target="THE WEIGHTS OF THE NEURAL NETWORK">
  <data key="d0">ARE</data>
</edge>
<edge source="THE INTERMEDIATE RESULTS" target="RST-IN-LAST-OUT">
  <data key="d0">ARE</data>
</edge>
<edge source="THE DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT (E.G., CUDA UNI-ED MEMORY 4)" target="AN OVERKILL">
  <data key="d0">IS</data>
</edge>
<edge source="THE DEFAULT GENERAL-PURPOSE GPU MEMORY MANAGEMENT (E.G., CUDA UNI-ED MEMORY 4)" target="UNNECESSARY OVERHEAD">
  <data key="d0">INCURS</data>
</edge>
<edge source="NVIDIA" target="CUDA UNIFIED MEMORY 4">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="NVIDIA" target="MPS">
  <data key="d0">HAS</data>
</edge>
<edge source="CUDA UNIFIED MEMORY 4" target="MEMORY MOVEMENT BETWEEN THE HOST MEMORY AND THE GPU MEMORY">
  <data key="d0">HANDLES</data>
</edge>
<edge source="MEMORY MOVEMENT" target="APPLICATIONS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="DEDICATED MEMORY DAEMON" target="OVERHEAD">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="DEDICATED MEMORY DAEMON" target="GPU MEMORY">
  <data key="d0">MANAGES</data>
</edge>
<edge source="OVERHEAD" target="MINIMIZED">
  <data key="d0">SHOULD BE</data>
</edge>
<edge source="UNIFIED MEMORY MANAGEMENT WITH THE MEMORY DAEMON" target="MINIMAL MEMORY FOOTPRINT">
  <data key="d0">ACHIEVE</data>
</edge>
<edge source="UNIFIED MEMORY MANAGEMENT WITH THE MEMORY DAEMON" target="EXTRA MEMORY COPIES">
  <data key="d0">ELIMINATE</data>
</edge>
<edge source="THE DAEMON" target="THE GPU MEMORY">
  <data key="d0">PRE-ALLOCATES</data>
</edge>
<edge source="THE DAEMON" target="IT TO EACH TASK">
  <data key="d0">RE-ALLOCATES</data>
</edge>
<edge source="THE DAEMON" target="THE EXPENSIVE GPU MEMORY MANAGER">
  <data key="d0">DOES NOT INVOLVE</data>
</edge>
<edge source="THE DAEMON" target="THAT EACH TIME ONLY ONE WORKER OWNS THE GPU MEMORY">
  <data key="d0">ENSURES</data>
</edge>
<edge source="THE DAEMON" target="MEMORY ISOLATION BETWEEN WORKERS">
  <data key="d0">GUARANTEES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="CUDAMALLOC">
  <data key="d0">USES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE MEMORY TO THE WORKERS">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE MEMORY AT RUNTIME">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE GPU MEMORY">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE DNN MODELS">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE MEMORY TO THE STANDBY WORKER (4.3)">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE MODEL USED BY THE NEW TASK FROM THE HOST MEMORY TO THE GPU MEMORY">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE MODEL FROM THE HOST MEMORY TO THE GPU MEMORY">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="THE MEMORY DAEMON" target="GPU MEMORY ALLOCATION">
  <data key="d0">HANDLES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="MODEL TRANSMISSION">
  <data key="d0">HANDLES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="GPU MEMORY HANDLERS">
  <data key="d0">CREATES</data>
</edge>
<edge source="THE MEMORY DAEMON" target="GPU MEMORY HANDLERS TO WORKERS">
  <data key="d0">SENDS</data>
</edge>
<edge source="THE MEMORY DAEMON" target="MEMORY POINTERS TO THE WORKERS">
  <data key="d0">ONLY NEEDS TO PASS</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE SAME ORDER TO TRANSMIT THE MODEL AS THE WORKER WOULD">
  <data key="d0">ONLY NEEDS TO USE</data>
</edge>
<edge source="THE MEMORY DAEMON" target="THE USAGE OF EXPENSIVE GPU IPCS">
  <data key="d0">CAN MINIMIZE</data>
</edge>
<edge source="CUDAMALLOC" target="THE GPU MEMORY">
  <data key="d0">OBTAINS</data>
</edge>
<edge source="THE DNN MODELS" target="ONLY ONCE IN THE MEMORY DAEMON">
  <data key="d0">ARE STORED</data>
</edge>
<edge source="THE DNN MODELS" target="IN EVERY WORKER">
  <data key="d0">ARE NOT STORED</data>
</edge>
<edge source="STORING THE DNN MODELS ONLY ONCE IN THE MEMORY DAEMON" target="MEMORY FOOTPRINT">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="NO UNIFIED MEMORY MANAGEMENT" target="EACH WORKER TO KEEP A COPY FOR EACH DNN MODEL">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="KEEPING A COPY FOR EACH DNN MODEL" target="THE MEMORY FOOTPRINT">
  <data key="d0">INCREASES</data>
</edge>
<edge source="EACH SERVER" target="AN ACTIVE WORKER">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="EACH SERVER" target="MULTIPLE STANDBY WORKERS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="A SERVER" target="ONE OR MORE STANDBY WORKERS">
  <data key="d0">HAS</data>
</edge>
<edge source="A SERVER" target="A TRAINING TASK RUNNING ON THE GPU">
  <data key="d0">STOPS</data>
</edge>
<edge source="A SERVER" target="AN INFERENCE TASK">
  <data key="d0">STARTS</data>
</edge>
<edge source="THE ACTIVE WORKER" target="THE CURRENT TASK ON THE GPU">
  <data key="d0">EXECUTES</data>
</edge>
<edge source="THE ACTIVE WORKER" target="THE WORKER THAT CURRENTLY EXECUTES A TASK IN THE GPU">
  <data key="d0">IS</data>
</edge>
<edge source="THE ACTIVE WORKER" target="A STANDBY WORKER">
  <data key="d0">BECOMES</data>
</edge>
<edge source="THE ACTIVE WORKER" target="THE ENVIRONMENT FOR THE PREVIOUS TASK">
  <data key="d0">CLEANS</data>
</edge>
<edge source="THE STANDBY WORKERS" target="THE CPU">
  <data key="d0">STAY ON</data>
</edge>
<edge source="THE STANDBY WORKERS" target="THE NEXT TASK">
  <data key="d0">WAIT FOR</data>
</edge>
<edge source="WORKER" target="PROCESS">
  <data key="d0">IS</data>
</edge>
<edge source="WORKER" target="MODEL">
  <data key="d0">CAN ACCESS</data>
</edge>
<edge source="WORKER" target="ITS TASK">
  <data key="d0">CAN EXECUTE</data>
</edge>
<edge source="WORKER" target="USER TO REGISTER THE MODEL BEFORE STARTING A TASK">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="WORKER" target="THE MODELS">
  <data key="d0">CAN LOAD</data>
</edge>
<edge source="WORKER" target="THE HOOKS TO WAIT FOR PARAMETER TRANSMISSION OR TERMINATE ON NOTIFICATION">
  <data key="d0">CAN ADD</data>
</edge>
<edge source="PROCESS" target="TASKS">
  <data key="d0">EXECUTES</data>
</edge>
<edge source="TASKS" target="ONE GPU">
  <data key="d0">EXECUTES ON</data>
</edge>
<edge source="ACTIVE WORKER" target="CURRENT TASK">
  <data key="d0">COMPLETES OR STOPS</data>
</edge>
<edge source="ACTIVE WORKER" target="ACTIVE WORKER">
  <data key="d0">IS</data>
</edge>
<edge source="CURRENT TASK" target="TRAINING TASK">
  <data key="d0">IS</data>
</edge>
<edge source="CONTROLLER" target="MEMORY DAEMON">
  <data key="d0">AND</data>
</edge>
<edge source="CONTROLLER" target="STANDBY WORKER">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="CONTROLLER" target="CURRENT TASK">
  <data key="d0">CAN PREEMPT</data>
</edge>
<edge source="CONTROLLER" target="CURRENT ACTIVE WORKER TO STOP">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="CONTROLLER" target="PARAMETERS OF THE NEW MODEL TO THE GPU">
  <data key="d0">TRANSFERS</data>
</edge>
<edge source="CONTROLLER" target="ONLY ONE OF THESE CUDA STREAMS IS ACTIVE">
  <data key="d0">GUARANTEES</data>
</edge>
<edge source="MEMORY DAEMON" target="WORKER">
  <data key="d0">NEEDS TO NOTIFY</data>
</edge>
<edge source="MEMORY DAEMON" target="RELEVANT GPU MEMORY HANDLERS">
  <data key="d0">NEEDS TO EXPORT</data>
</edge>
<edge source="STANDBY WORKER" target="TASK TO GPU">
  <data key="d0">LOADS</data>
</edge>
<edge source="STANDBY WORKER" target="STANDBY WORKER">
  <data key="d0">IS</data>
</edge>
<edge source="TASK" target="PIPELINED MODEL TRANSMISSION">
  <data key="d0">EXECUTES WITH</data>
</edge>
<edge source="TASK" target="CLEANING">
  <data key="d0">IS</data>
</edge>
<edge source="TASK" target="GPU">
  <data key="d0">EXECUTES ON</data>
</edge>
<edge source="PIPELINED MODEL TRANSMISSION" target="EVALUATE THE EFFECTIVENESS OF PIPELINED MODEL TRANSMISSION">
  <data key="d0">IS USED TO</data>
</edge>
<edge source="OUR MECHANISM" target="OLD TASK CLEANING IN THE ACTIVE WORKER AND NEW TASK INITIALIZATION IN THE STANDBY WORKER">
  <data key="d0">PARALLELIZES</data>
</edge>
<edge source="OUR MECHANISM" target="WORKER SWITCHING OVERHEAD">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="TABLE 2" target="COMPARISON OF WORKER SWITCHING MECHANISMS">
  <data key="d0">IS</data>
</edge>
<edge source="TABLE 2" target="THE DIFFERENCES BETWEEN THESE THREE SOLUTIONS">
  <data key="d0">SUMMARIZES</data>
</edge>
<edge source="THE ACTIVE AND STANDBY WORKER SWITCHING MECHANISM" target="THE OVERHEAD OF BOTH TASK CLEANING AND TASK INITIALIZATION">
  <data key="d0">HIDES</data>
</edge>
<edge source="THE ACTIVE AND STANDBY WORKER SWITCHING MECHANISM" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">ENSURES</data>
</edge>
<edge source="FAST TASK SWITCHING" target="MORE FLEXIBLE FINE-GRAINED SCHEDULING">
  <data key="d0">ENABLES</data>
</edge>
<edge source="ACTIVE-STANDBY WORKER SWITCHING" target="EVALUATE THE EFFECTIVENESS OF ACTIVE-STANDBY WORKER SWITCHING">
  <data key="d0">IS USED TO</data>
</edge>
<edge source="AN ACTIVE-STANDBY WORKER SWITCHING MECHANISM" target="PARALLELIZE OLD TASK CLEANING AND NEW TASK INITIALIZATION">
  <data key="d0">TO</data>
</edge>
<edge source="PIPELINING" target="A CANONICAL TECHNIQUE">
  <data key="d0">IS</data>
</edge>
<edge source="PIPELINING" target="COMPUTER SYSTEMS">
  <data key="d0">IS WIDELY USED IN</data>
</edge>
<edge source="PIPELINING" target="SYSTEM PERFORMANCE">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="PIPELINING" target="RESOURCE UTILIZATION">
  <data key="d0">MAXIMIZES</data>
</edge>
<edge source="PIPELINING" target="TWO SOURCES OF SYSTEM OVERHEADS">
  <data key="d0">BRINGS</data>
</edge>
<edge source="PIPELINING" target="PER-GROUP GRANULARITY">
  <data key="d0">IS PERFORMED ON</data>
</edge>
<edge source="RESOURCE UTILIZATION" target="TIME-SHARING">
  <data key="d0">INCREASE VIA</data>
</edge>
<edge source="PRIOR WORK IN DL SYSTEMS SUCH AS PIPEDREAM 8 AND BYTESCHEDULER 9" target="PIPELINING TO DISTRIBUTED TRAINING">
  <data key="d0">HAS APPLIED</data>
</edge>
<edge source="THESE SOLUTIONS" target="INTER-BATCH PIPELINING">
  <data key="d0">FOCUS ON</data>
</edge>
<edge source="THESE SOLUTIONS" target="PIPESWITCH">
  <data key="d0">ARE COMPLEMENTARY TO</data>
</edge>
<edge source="INTER-BATCH PIPELINING" target="COMPUTATION AND GRADIENT TRANSMISSION OF DIFFERENT BATCHES">
  <data key="d0">OVERLAPS</data>
</edge>
<edge source="COMPUTATION AND GRADIENT TRANSMISSION" target="TRAINING WORKLOADS OF THE SAME DNN MODEL">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="INTRA-BATCH PIPELINING" target="MODEL TRANSMISSION AND COMPUTATION">
  <data key="d0">OVERLAPS</data>
</edge>
<edge source="INTRA-BATCH PIPELINING" target="OVERHEAD OF SWITCHING BETWEEN DIFFERENT DNN MODELS">
  <data key="d0">REDUCES</data>
</edge>
<edge source="NEW TECHNIQUES" target="TRAINING">
  <data key="d0">SUPPORT</data>
</edge>
<edge source="NEW TECHNIQUES" target="INFERENCE">
  <data key="d0">SUPPORT</data>
</edge>
<edge source="TRAINING" target="INFERENCE">
  <data key="d0">HAS NO SHARING WITH</data>
</edge>
<edge source="TRAINING" target="DL TASKS">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="INFERENCE" target="STRICT SLOS">
  <data key="d0">HAS</data>
</edge>
<edge source="THREE KEY TECHNIQUES" target="PIPELINED MODEL TRANSMISSION">
  <data key="d0">ARE</data>
</edge>
<edge source="THREE KEY TECHNIQUES" target="UNIFIED MEMORY MANAGEMENT">
  <data key="d0">ARE</data>
</edge>
<edge source="THREE KEY TECHNIQUES" target="ACTIVE-STANDBY WORKER SWITCHING">
  <data key="d0">ARE</data>
</edge>
<edge source="PACKING MULTIPLE DL APPLICATIONS ONTO THE SAME GPU" target="NE-GRAINED TIME-SHARING ABSTRACTION">
  <data key="d0">IS DONE VIA</data>
</edge>
<edge source="NE-GRAINED TIME-SHARING ABSTRACTION" target="GPU UTILIZATION">
  <data key="d0">MAXIMIZES</data>
</edge>
<edge source="MORE FLEXIBLE FINE-GRAINED SCHEDULING" target="GPU UTILIZATION">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="500 14TH USENIX SYMPOSIUM" target="OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS ON</data>
</edge>
<edge source="USENIX SYMPOSIUM" target="USENIX ASSOCIATION">
  <data key="d0">IS ORGANIZED BY</data>
</edge>
<edge source="USENIX SYMPOSIUM" target="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS</data>
</edge>
<edge source="QUESTION" target="WHY BUILD A SHARED CLUSTER INSTEAD OF A DEDICATED ONE FOR EACH USER">
  <data key="d0">IS</data>
</edge>
<edge source="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="KUBERNETES">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="14TH USENIX SYMPOSIUM" target="OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS ON</data>
</edge>
<edge source="14TH USENIX SYMPOSIUM" target="USENIX ASSOCIATION">
  <data key="d0">IS ORGANIZED BY</data>
</edge>
<edge source="THE MAIN REASON" target="TO BRING DOWN THE COST">
  <data key="d0">IS</data>
</edge>
<edge source="THE DEMAND OF TRAINING" target="WELL PREDICTABLE">
  <data key="d0">IS NOT</data>
</edge>
<edge source="THE DEMAND OF TRAINING" target="THE PROGRESS OF DIFFERENT DEVELOPERS">
  <data key="d0">WOULD DEPEND ON</data>
</edge>
<edge source="THE DEMAND OF INFERENCE" target="MORE PREDICTABLE">
  <data key="d0">IS</data>
</edge>
<edge source="AN INFERENCE TASK FOR A PARTICULAR APPLICATION" target="A DAILY PERIODICAL PATTERN">
  <data key="d0">HAS</data>
</edge>
<edge source="A DAILY PERIODICAL PATTERN" target="THE APPLICATION USAGE">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="THE PATTERNS" target="ACROSS DIFFERENT TASKS">
  <data key="d0">CAN VARY</data>
</edge>
<edge source="A SHARED CLUSTER" target="THE RESOURCE UTILIZATION">
  <data key="d0">WOULD INCREASE</data>
</edge>
<edge source="A SHARED CLUSTER" target="DIFFERENT TASKS">
  <data key="d0">IS USED BY</data>
</edge>
<edge source="SHARED CLUSTERS" target="TRAINING AND INFERENCE">
  <data key="d0">ARE NOT SHARED BETWEEN</data>
</edge>
<edge source="TRAINING CLUSTERS" target="POWERFUL GPUS">
  <data key="d0">ARE EQUIPPED WITH</data>
</edge>
<edge source="TRAINING CLUSTERS" target="RUN TRAINING TASKS">
  <data key="d0">ARE USED TO</data>
</edge>
<edge source="GPUS DESIGNED FOR INFERENCE TASKS" target="TRAINING TASKS">
  <data key="d0">MIGHT BE TOO WIMPY FOR</data>
</edge>
<edge source="NEW GPU HARDWARE" target="NVIDIA T4">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="NEW ALGORITHMS AND SYSTEMS FOR DISTRIBUTED TRAINING" target="MULTIPLE GPUS TO ACCELERATE TRAINING">
  <data key="d0">ENABLE</data>
</edge>
<edge source="OUR INDUSTRY COLLABORATOR" target="A LEADING ONLINE SERVICE PROVIDER">
  <data key="d0">IS</data>
</edge>
<edge source="OUR INDUSTRY COLLABORATOR" target="THIS OBSERVATION">
  <data key="d0">CONFIRMS</data>
</edge>
<edge source="THIS SERVICE PROVIDER" target="MORE THAN 10K V100 GPUS FOR TRAINING">
  <data key="d0">RUNS</data>
</edge>
<edge source="THIS SERVICE PROVIDER" target="AT LEAST 5 AS MANY T4 GPUS FOR INFERENCE">
  <data key="d0">RUNS</data>
</edge>
<edge source="THE COMPUTATION POWER ON BOTH SIDES" target="THE SAME ORDER OF MAGNITUDE">
  <data key="d0">IS WITHIN</data>
</edge>
<edge source="INFERENCE GPUS" target="DURING LESS BUSY TIMES">
  <data key="d0">ARE UTILIZED</data>
</edge>
<edge source="INFERENCE GPUS" target="TRAINING MODELS">
  <data key="d0">ARE USED FOR</data>
</edge>
<edge source="A GOOD EXAMPLE" target="NE-TUNE BERT USING DAILY NEWS">
  <data key="d0">IS TO</data>
</edge>
<edge source="BORG-LIKE 1 SYSTEMS" target="GPU UTILIZATION">
  <data key="d0">IMPROVE</data>
</edge>
<edge source="INFERENCE AND TRAINING WORKLOADS" target="COMPLEMENTARY USAGE PATTERNS">
  <data key="d0">HAVE</data>
</edge>
<edge source="ONLINE INFERENCE SERVICES" target="MORE IDLE DURING MIDNIGHT">
  <data key="d0">ARE OFTEN</data>
</edge>
<edge source="MANY TRAINING DEVELOPERS" target="A TIME-CONSUMING JOB AT NIGHT">
  <data key="d0">WOULD START</data>
</edge>
<edge source="INFERENCE LOADS ON DIFFERENT MODELS" target="DIFFERENT PATTERNS">
  <data key="d0">HAVE</data>
</edge>
<edge source="DIFFERENT PATTERNS" target="TIME SHARING">
  <data key="d0">BENEFIT FROM</data>
</edge>
<edge source="ANY SERVER" target="ANY TASK">
  <data key="d0">WOULD BE ABLE TO RUN</data>
</edge>
<edge source="ANY SERVER" target="LOW OVERHEAD TO SWITCH BETWEEN DIFFERENT APPLICATIONS">
  <data key="d0">WOULD HAVE</data>
</edge>
<edge source="A MODERN SERVER" target="SEVERAL TB OF HOST MEMORY">
  <data key="d0">CAN BE EQUIPPED WITH</data>
</edge>
<edge source="SEVERAL TB OF HOST MEMORY" target="IT TO LOAD MANY APPLICATIONS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="TASK EXECUTION ON GPUS" target="GPU MEMORY">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="HIGH-END GPUS" target="16 GB FOR T4">
  <data key="d0">HAVE</data>
</edge>
<edge source="HIGH-END GPUS" target="32 GB FOR V100">
  <data key="d0">HAVE</data>
</edge>
<edge source="DL TASKS" target="A LARGE AMOUNT OF MEMORY ON A GPU">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="DL TASKS" target="ALL OF THE MEMORY ON A GPU">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="DL APPLICATIONS" target="LARGE MODELS">
  <data key="d0">HAVE</data>
</edge>
<edge source="DL APPLICATIONS" target="LARGE AMOUNTS OF INTERMEDIATE RESULTS">
  <data key="d0">GENERATE</data>
</edge>
<edge source="LARGE MODELS" target="HUNDREDS OF LAYERS">
  <data key="d0">CAN HAVE</data>
</edge>
<edge source="INTERMEDIATE RESULTS" target="A LOT OF GPU MEMORY">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="INTERMEDIATE RESULTS" target="A SIMPLE, REGULAR PATTERN">
  <data key="d0">CHANGE IN</data>
</edge>
<edge source="INTERMEDIATE RESULTS" target="MEMORY FRAGMENTATION">
  <data key="d0">DO NOT CAUSE</data>
</edge>
<edge source="INTERMEDIATE RESULTS" target="OUTPUTS OF EACH LAYER">
  <data key="d0">ARE</data>
</edge>
<edge source="INTERMEDIATE RESULTS" target="NEXT LAYER">
  <data key="d0">ARE USED BY</data>
</edge>
<edge source="STATE-OF-THE-ART MODELS" target="DEEPER AND LARGER">
  <data key="d0">ARE GETTING</data>
</edge>
<edge source="IDLE APPLICATIONS" target="LARGE MEMORY SPACE">
  <data key="d0">CAN OCCUPY</data>
</edge>
<edge source="THE ACTIVE APPLICATION" target="THE ENTIRE GPU MEMORY">
  <data key="d0">SHOULD BE ABLE TO UTILIZE</data>
</edge>
<edge source="THE NUMBER OF APPLICATIONS THAT CAN BE SERVED BY A GPU SERVER" target="ITS HOST MEMORY SIZE">
  <data key="d0">SHOULD ONLY BE LIMITED BY</data>
</edge>
<edge source="SWITCHING A TASK" target="HEAVY MEMORY SWAPPING">
  <data key="d0">WOULD REQUIRE</data>
</edge>
<edge source="MANY ONLINE INFERENCE WORKLOADS" target="STRICT SLOS">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="NAIVE MEMORY SWAPPING BETWEEN THE HOST MEMORY AND THE GPU MEMORY" target="STRICT SLOS">
  <data key="d0">CANNOT MEET</data>
</edge>
<edge source="A TRAINING TASK" target="THE MODEL PARAMETERS">
  <data key="d0">UPDATES</data>
</edge>
<edge source="A TRAINING TASK" target="THE DNN STRUCTURE">
  <data key="d0">DOES NOT UPDATE</data>
</edge>
<edge source="A TRAINING TASK" target="THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS CANNOT BE IMMEDIATELY FREED">
  <data key="d0">DIFFERS IN THAT</data>
</edge>
<edge source="AN INFERENCE TASK" target="A FORWARD PASS FROM THE RST LAYER TO THE NAL LAYER">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="AN INFERENCE TASK" target="MAKE A PREDICTION">
  <data key="d0">PURPOSE</data>
</edge>
<edge source="AN INFERENCE TASK" target="ONLY A FORWARD PASS IN TASK EXECUTION">
  <data key="d0">HAS</data>
</edge>
<edge source="THE RST INFERENCE BATCH" target="SEVERAL SECONDS TO FINISH">
  <data key="d0">WOULD REQUIRE</data>
</edge>
<edge source="EXISTING SUPPORT SUCH AS NVIDIA MPS" target="DL WORKLOADS">
  <data key="d0">IS NOT OPTIMIZED FOR</data>
</edge>
<edge source="EXISTING SUPPORT SUCH AS NVIDIA MPS" target="HUNDREDS OF MILLISECONDS OVERHEAD">
  <data key="d0">INCURS</data>
</edge>
<edge source="NVIDIA MPS" target="LOWER OVERHEAD COMPARED TO STOP-AND-START">
  <data key="d0">HAS</data>
</edge>
<edge source="NVIDIA MPS" target="SEVERAL HUNDRED MILLISECONDS OVERHEAD">
  <data key="d0">INCURS</data>
</edge>
<edge source="SEVERAL HUNDRED MILLISECONDS OVERHEAD" target="MPS FROM MEETING STRICT SLOS">
  <data key="d0">PREVENTS</data>
</edge>
<edge source="FIGURE 1" target="PIPESWITCH ARCHITECTURE">
  <data key="d0">SHOWS</data>
</edge>
<edge source="FIGURE 1" target="THE ARCHITECTURE OF A PIPESWITCH SERVER">
  <data key="d0">SHOWS</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="501 CONTROLLER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="ACTIVE WORKER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="GPU MEMORY DAEMON">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="STANDBY WORKER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PIPESWITCH ARCHITECTURE" target="NEW TASK">
  <data key="d0">HANDLES</data>
</edge>
<edge source="NEW TASK" target="STANDBY WORKER FINISHES CLEANING PREVIOUS TASK">
  <data key="d0">ARRIVES BEFORE</data>
</edge>
<edge source="NEW TASK" target="WAIT">
  <data key="d0">NEEDS TO</data>
</edge>
<edge source="THROUGHPUT" target="BATCHES PER SECOND">
  <data key="d0">MEASURED IN</data>
</edge>
<edge source="THROUGHPUT" target="PIPESWITCH MPS STOP-AND-START">
  <data key="d0">HAS UPPER BOUND</data>
</edge>
<edge source="THROUGHPUT" target="EIGHT P3.2XLARGE INSTANCES">
  <data key="d0">MEASURED FOR</data>
</edge>
<edge source="THROUGHPUT" target="DIFFERENT SCHEDULING CYCLES">
  <data key="d0">IS MEASURED UNDER</data>
</edge>
<edge source="THE STRUCTURE AND COMPUTATION PATTERN OF DNN MODELS" target="US TO HIGHLY OPTIMIZE TASK SWITCHING">
  <data key="d0">ALLOW</data>
</edge>
<edge source="THE STRUCTURE AND COMPUTATION PATTERN OF DNN MODELS" target="US TO ACHIEVE MILLISECOND-SCALE OVERHEAD">
  <data key="d0">ALLOW</data>
</edge>
<edge source="PIPELINE" target="FEASIBLE">
  <data key="d0">IS</data>
</edge>
<edge source="PIPELINE" target="EFFECTIVE">
  <data key="d0">IS</data>
</edge>
<edge source="PIPELINE" target="PER-LAYER">
  <data key="d0">HAS TYPE</data>
</edge>
<edge source="PIPELINE" target="GROUPED TRANSMISSION">
  <data key="d0">HAS TYPE</data>
</edge>
<edge source="OTHER CHALLENGES" target="MEMORY MANAGEMENT">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="OTHER CHALLENGES" target="WORKER SWITCHING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="PIPESWITCH PIPELINES MODEL" target="TRANSMISSION AND TASK EXECUTION">
  <data key="d0">MODEL</data>
</edge>
<edge source="THIS SERVER" target="FOUR TYPES OF COMPONENTS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="FOUR TYPES OF COMPONENTS" target="A CONTROLLER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FOUR TYPES OF COMPONENTS" target="A MEMORY DAEMON">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FOUR TYPES OF COMPONENTS" target="AN ACTIVE WORKER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FOUR TYPES OF COMPONENTS" target="MULTIPLE STANDBY WORKERS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE CONTROLLER" target="THE CENTRAL COMPONENT">
  <data key="d0">IS</data>
</edge>
<edge source="THE CONTROLLER" target="A SET OF TASKS RECEIVED FROM THE CLIENTS">
  <data key="d0">QUEUES</data>
</edge>
<edge source="THE CONTROLLER" target="THE CURRENT TASK FOR THE NEXT ONE">
  <data key="d0">CAN PREEMPT</data>
</edge>
<edge source="THE CONTROLLER" target="THE SCHEDULING POLICY">
  <data key="d0">PREEMPTS BASED ON</data>
</edge>
<edge source="THE CONTROLLER" target="THE CURRENT TASK TO FINISH">
  <data key="d0">WAITS FOR</data>
</edge>
<edge source="THE CONTROLLER" target="THE CURRENT TASK">
  <data key="d0">PREEMPTS</data>
</edge>
<edge source="THE CONTROLLER" target="THE ACTIVE WORKER TO STOP">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="THE CONTROLLER" target="AN IDLE STANDBY WORKER">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="THE CONTROLLER" target="A TASK">
  <data key="d0">SCHEDULES</data>
</edge>
<edge source="THE CONTROLLER" target="THE CURRENT ACTIVE WORKER TO STOP">
  <data key="d0">SIGNALS</data>
</edge>
<edge source="THE CONTROLLER" target="THE GPU MEMORY ALLOCATED TO THE CURRENT ACTIVE WORKER">
  <data key="d0">DELETES</data>
</edge>
<edge source="THE CONTROLLER" target="THE GPU MEMORY TO THE NEW ACTIVE WORKER">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THE CONTROLLER" target="ONLY ONE ACTIVE WORKER">
  <data key="d0">ENSURES</data>
</edge>
<edge source="THE CONTROLLER" target="THE MODEL FROM THE DISK TO THE CPU MEMORY">
  <data key="d0">LOADS</data>
</edge>
<edge source="THE CONTROLLER" target="THE WORKER TO START COMPUTING THE CORRESPONDING LAYERS">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="THE WORKERS" target="THE TASKS">
  <data key="d0">EXECUTE</data>
</edge>
<edge source="MEMORY" target="DAEMON">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY" target="ALLOCATION">
  <data key="d0">HAS</data>
</edge>
<edge source="THE SERVER" target="THE DNN MODELS IN THE HOST MEMORY">
  <data key="d0">STORES</data>
</edge>
<edge source="THE SERVER" target="ONE COPY OF EACH MODEL IN THE HOST MEMORY">
  <data key="d0">ONLY NEEDS TO KEEP</data>
</edge>
<edge source="ALL COMPONENTS" target="THE SLOS">
  <data key="d0">SHOULD BE OPTIMIZED TO MEET</data>
</edge>
<edge source="A STANDBY WORKER" target="IDLE">
  <data key="d0">IS</data>
</edge>
<edge source="A STANDBY WORKER" target="INITIALIZING A NEW TASK">
  <data key="d0">IS</data>
</edge>
<edge source="A STANDBY WORKER" target="CLEANING ITS ENVIRONMENT FOR THE PREVIOUS TASK">
  <data key="d0">IS</data>
</edge>
<edge source="THE STANDBY WORKER" target="THE NEW ACTIVE WORKER">
  <data key="d0">BECOMES</data>
</edge>
<edge source="THE NEW ACTIVE WORKER" target="THE NEW TASK">
  <data key="d0">EXECUTES</data>
</edge>
<edge source="THE NEW TASK" target="THE GPU ENVIRONMENT OF THE CURRENT TASK">
  <data key="d0">CAN REUSE</data>
</edge>
<edge source="WAITING" target="NEW TASK STARTUP TIME">
  <data key="d0">INCREASES</data>
</edge>
<edge source="A SCHEDULING POLICY" target="WHICH TASK TO EXECUTE NEXT">
  <data key="d0">DECIDES</data>
</edge>
<edge source="THE SCHEDULING" target="PREEMPTIVE">
  <data key="d0">IS</data>
</edge>
<edge source="CANONICAL SCHEDULING POLICIES" target="RST COME RST SERVE (FCFS)">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CANONICAL SCHEDULING POLICIES" target="EARLIEST DEADLINE RST (EDF)">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE SPECIFIC SCHEDULING ALGORITHM" target="ORTHOGONAL TO THIS PAPER">
  <data key="d0">IS</data>
</edge>
<edge source="INFERENCE TASK" target="STRICT LATENCY SLO">
  <data key="d0">HAS</data>
</edge>
<edge source="INFERENCE TASK" target="THE SERVER">
  <data key="d0">HAS ARRIVED AT</data>
</edge>
<edge source="INFERENCE TASK" target="THE MODEL FOR INFERENCE">
  <data key="d0">USES</data>
</edge>
<edge source="INFERENCE TASK" target="THE MODEL ITSELF">
  <data key="d0">DOES NOT CHANGE</data>
</edge>
<edge source="AN IDLE STANDBY WORKER" target="ITS ENVIRONMENT FOR THE NEW TASK">
  <data key="d0">INITIALIZES</data>
</edge>
<edge source="THE TRANSMISSION" target="THE EXTRA MEMORY COPY FROM THE MEMORY DAEMON TO THE WORKER">
  <data key="d0">ELIMINATES</data>
</edge>
<edge source="THE TRANSMISSION" target="THE COMPUTATION OF THE RST GROUP">
  <data key="d0">NISHES NO LATER THAN</data>
</edge>
<edge source="MODEL" target="GPU">
  <data key="d0">TRANSMITS TO</data>
</edge>
<edge source="MODEL" target="TRANSMISSION">
  <data key="d0">IS</data>
</edge>
<edge source="MODEL" target="PCIE">
  <data key="d0">TRANSMITS OVER</data>
</edge>
<edge source="MODEL" target="READY">
  <data key="d0">IS</data>
</edge>
<edge source="MODEL" target="PIPESWITCH">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MODEL" target="MPS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MODEL" target="STOP-AND-START">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MODEL" target="RESNET152">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MODEL" target="INCEPTIONV3">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MODEL" target="BERTBASE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="RELEVANT GPU MEMORY HANDLERS" target="WORKER">
  <data key="d0">ARE EXPORTED TO</data>
</edge>
<edge source="THE PRIMARY GOAL OF THIS PAPER" target="A SET OF TECHNIQUES BASED ON THE CHARACTERISTICS OF DL APPLICATIONS">
  <data key="d0">IS TO DESIGN</data>
</edge>
<edge source="THE SET OF TECHNIQUES" target="THE TASK SWITCHING OVERHEAD IN THIS PROCESS">
  <data key="d0">IS TO MINIMIZE</data>
</edge>
<edge source="END-TO-END EXPERIMENTS" target="THE BENEFITS OF PIPESWITCH">
  <data key="d0">DEMONSTRATE</data>
</edge>
<edge source="THE MEA-502 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION" target="USENIX ASSOCIATION">
  <data key="d0">IS ORGANIZED BY</data>
</edge>
<edge source="SUREMENT" target="A TYPICAL SCENARIO">
  <data key="d0">CONSIDERS</data>
</edge>
<edge source="RESNET152 17" target="STANDARD BENCHMARKS FOR EVALUATING DL SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="THE MEASUREMENT" target="TWO TYPES OF INSTANCES ON AMAZON AWS">
  <data key="d0">COVERS</data>
</edge>
<edge source="TWO TYPES OF INSTANCES ON AMAZON AWS" target="G4DN.2XLARGE WITH NVIDIA T4">
  <data key="d0">ARE</data>
</edge>
<edge source="TWO TYPES OF INSTANCES ON AMAZON AWS" target="P3.2XLARGE WITH NVIDIA V100">
  <data key="d0">ARE</data>
</edge>
<edge source="TABLE 1" target="THE RESULTS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="TOTAL TIMES TO START THE INFERENCE TASK ON THE GPUS" target="5787 MS AND 7551 MS">
  <data key="d0">ARE</data>
</edge>
<edge source="TASK CLEANING" target="TIME">
  <data key="d0">TAKES</data>
</edge>
<edge source="THE INFERENCE TASK" target="ITS ENVIRONMENT">
  <data key="d0">CREATES AND INITIALIZES</data>
</edge>
<edge source="THE INFERENCE TASK" target="GPU MEMORY">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THE INFERENCE TASK" target="ITS NEURAL NETWORK MODEL">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THE INFERENCE TASK" target="THE MODEL">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="THE INFERENCE TASK" target="THE HOST MEMORY">
  <data key="d0">TRANSMITS FROM</data>
</edge>
<edge source="THE INFERENCE TASK" target="THE GPU MEMORY">
  <data key="d0">TRANSMITS TO</data>
</edge>
<edge source="ITS ENVIRONMENT" target="PROCESS LAUNCHING">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ITS ENVIRONMENT" target="PYTORCH CUDA RUNTIME LOADING">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ITS ENVIRONMENT" target="CUDA CONTEXT INITIALIZATION">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="TEXT">
  <data key="d0">IS</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="TECHNIQUE">
  <data key="d0">IS A</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="NO OPTIMIZATION">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="LAYERS OF THE MODEL INTO ONE BIG TENSOR">
  <data key="d0">COMBINES</data>
</edge>
<edge source="GROUPED TRANSMISSION" target="ONE BIG TENSOR IN ONE GROUP">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="TASK SWITCHING" target="CPU">
  <data key="d0">DEPENDS ON</data>
</edge>
<edge source="TASK SWITCHING" target="SIMILAR SECOND-SCALE OVERHEAD">
  <data key="d0">REPORTED</data>
</edge>
<edge source="TASK SWITCHING" target="SEVERAL SECONDS">
  <data key="d0">TAKES</data>
</edge>
<edge source="BETTER CPU" target="INTEL PLATINUM 8259CL">
  <data key="d0">IS</data>
</edge>
<edge source="CPU IN P3.2XLARGE" target="INTEL XEON E5-2686 V4">
  <data key="d0">IS</data>
</edge>
<edge source="LOWER OVERHEAD ON T4" target="TASK SWITCHING LARGELY DEPENDS ON CPU">
  <data key="d0">IS BECAUSE</data>
</edge>
<edge source="A STRAWMAN SOLUTION" target="THE OLD TASK">
  <data key="d0">STOPS</data>
</edge>
<edge source="A STRAWMAN SOLUTION" target="THE NEW TASK">
  <data key="d0">STARTS</data>
</edge>
<edge source="A STRAWMAN SOLUTION THAT SIMPLY STOPS THE OLD TASK AND STARTS THE NEW TASK" target="SLOS">
  <data key="d0">WOULD VIOLATE</data>
</edge>
<edge source="ALL THE COMPONENTS" target="CONSIDERABLE TIME COMPARED TO THE INFERENCE TIME">
  <data key="d0">TAKE</data>
</edge>
<edge source="THE PCIE BANDWIDTH" target="THE PHYSICAL LIMIT ON HOW FAST AN ARBITRARY TASK CAN BE LOADED TO THE GPU">
  <data key="d0">IS</data>
</edge>
<edge source="THE COMPUTATION" target="LAYER BY LAYER">
  <data key="d0">IS PERFORMED</data>
</edge>
<edge source="EACH ITERATION IN A TRAINING TASK" target="A FORWARD PASS">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="EACH ITERATION IN A TRAINING TASK" target="A BACKWARD PASS">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="THE TASK" target="THE COMPUTATION OF A LAYER">
  <data key="d0">CAN START</data>
</edge>
<edge source="THE COMPUTATION OF A LAYER" target="THE LAYER IS LOADED IN THE GPU">
  <data key="d0">STARTS AS SOON AS</data>
</edge>
<edge source="THE COMPUTATION OF A LAYER" target="THE INPUT OF THE LAYER IS READY">
  <data key="d0">STARTS AS SOON AS</data>
</edge>
<edge source="THE INPUT OF THE LAYER" target="THE PREVIOUS LAYERS HAVE FINISHED THEIR COMPUTATION">
  <data key="d0">IS READY WHEN</data>
</edge>
<edge source="FIGURE 2" target="THE ADVANTAGE OF PIPELINING OVER THE STRAWMAN SOLUTION">
  <data key="d0">ILLUSTRATES</data>
</edge>
<edge source="PIPELINING MECHANISM" target="OPTIMAL MODEL-AWARE GROUPING IN PIPESWITCH">
  <data key="d0">HAS</data>
</edge>
<edge source="PCIE GPU E0 E1 EN-1 E2 (B)" target="MODEL TRANSMISSION AND TASK EXECUTION">
  <data key="d0">PIPELINES</data>
</edge>
<edge source="THE EXAMPLE" target="AN INFERENCE TASK">
  <data key="d0">SHOWS</data>
</edge>
<edge source="ADDING HOOKS" target="AUTOMATED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="DNN FRAMEWORK" target="PYTORCH">
  <data key="d0">EXAMPLES</data>
</edge>
<edge source="PYTORCH" target="TENSORS ON THEM">
  <data key="d0">CREATES</data>
</edge>
<edge source="THE BASIC WAY FOR PIPELINING" target="TO PIPELINE ON PER-LAYER GRANULARITY">
  <data key="d0">IS</data>
</edge>
<edge source="THE SYSTEM" target="THE LAYERS TO THE GPU MEMORY ONE BY ONE">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="THE COMPUTATION FOR A LAYER" target="THE LAYER IS TRANSMITTED">
  <data key="d0">IS BLOCKED BEFORE</data>
</edge>
<edge source="ONE" target="THE OVERHEAD TO INVOKE MULTIPLE CALLS TO PCIE TO TRANSMIT THE DATA">
  <data key="d0">IS</data>
</edge>
<edge source="ONE" target="P3.2XLARGE">
  <data key="d0">IS</data>
</edge>
<edge source="ONE" target="PROCESS">
  <data key="d0">IS</data>
</edge>
<edge source="TRANSMISSION OVERHEAD" target="DATA SIZE">
  <data key="d0">IS DOMINATED BY</data>
</edge>
<edge source="DIVIDING THE MODEL INTO MANY LAYERS" target="SIGNIFICANT EXTRA OVERHEAD">
  <data key="d0">CAUSES</data>
</edge>
<edge source="INVOKING A PCIE CALL FOR EACH LAYER" target="SIGNIFICANT EXTRA OVERHEAD">
  <data key="d0">CAUSES</data>
</edge>
<edge source="SOME LAYERS" target="VERY SMALL">
  <data key="d0">CAN BE</data>
</edge>
<edge source="SYNCHRONIZATION OVERHEAD" target="THE OTHER">
  <data key="d0">IS</data>
</edge>
<edge source="SYNCHRONIZATION OVERHEAD" target="TRANSMISSION AND COMPUTATION">
  <data key="d0">IS BETWEEN</data>
</edge>
<edge source="SYNCHRONIZATION OVERHEAD" target="THE COMPUTATION TO KNOW WHEN A LAYER IS READY TO COMPUTE">
  <data key="d0">IS NECESSARY FOR</data>
</edge>
<edge source="PIPELINING OVERHEAD" target="ONCE FOR EACH GROUP">
  <data key="d0">IS PAID</data>
</edge>
<edge source="PIPELINING OVERHEAD" target="INSTEAD OF EACH LAYER">
  <data key="d0">IS PAID</data>
</edge>
<edge source="GROUPING" target="A TRADE-OFF BETWEEN PIPELINING EFFICIENCY AND PIPELINING OVERHEAD">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="GROUPING" target="MODEL-AWARE">
  <data key="d0">MUST BE</data>
</edge>
<edge source="USING SMALL GROUPS" target="MORE OVERLAP BETWEEN TRANSMISSION AND COMPUTATION">
  <data key="d0">ENABLES</data>
</edge>
<edge source="USING SMALL GROUPS" target="MORE PIPELINING OVERHEAD">
  <data key="d0">PAYS</data>
</edge>
<edge source="MORE OVERLAP BETWEEN TRANSMISSION AND COMPUTATION" target="PIPELINING EFFICIENCY">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="USING BIG GROUPS" target="MINIMAL PIPELINING OVERHEAD">
  <data key="d0">HAS</data>
</edge>
<edge source="USING BIG GROUPS" target="THE CHANCE FOR OVERLAPPING">
  <data key="d0">REDUCES</data>
</edge>
<edge source="DIFFERENT STRUCTURES" target="THE NUMBER OF LAYERS">
  <data key="d0">ARE IN TERMS OF</data>
</edge>
<edge source="DIFFERENT STRUCTURES" target="THE SIZE OF EACH LAYER">
  <data key="d0">ARE IN TERMS OF</data>
</edge>
<edge source="ALL POSSIBLE COMBINATIONS" target="THE OPTIMAL GROUPING STRATEGY">
  <data key="d0">ARE USED TO FIND</data>
</edge>
<edge source="TWO PRUNING TECHNIQUES" target="TWO INSIGHTS">
  <data key="d0">ARE BASED ON</data>
</edge>
<edge source="TIME COMPLEXITY FOR ENUMERATION" target="EXPONENTIAL">
  <data key="d0">IS</data>
</edge>
<edge source="PCIE GPU" target="LOWER BOUND OF F(GROUP(0, I), I1)">
  <data key="d0">HAS</data>
</edge>
<edge source="GROUP(0, I)" target="GROUP(I1, J)">
  <data key="d0">IS RELATED TO</data>
</edge>
<edge source="J, J1" target="N-1">
  <data key="d0">RANGE</data>
</edge>
<edge source="CASE (A)" target="LOWER BOUND CURRENT OPTIMAL TIME">
  <data key="d0">IS PRUNED IF</data>
</edge>
<edge source="PRUNE" target="FROM I TO J">
  <data key="d0">THE CASES THAT GROUP</data>
</edge>
<edge source="BATCH" target="FROM LAYER I1 TO J">
  <data key="d0">AT LEAST</data>
</edge>
<edge source="FIGURE 3" target="EXAMPLES FOR TWO PRUNING TECHNIQUES">
  <data key="d0">SHOWS</data>
</edge>
<edge source="F(B,I)" target="A FUNCTION">
  <data key="d0">IS</data>
</edge>
<edge source="F(B,I)" target="THE TOTAL TIME OF THE OPTIMAL GROUPING STRATEGY FROM LAYER I TO N-1">
  <data key="d0">RETURNS</data>
</edge>
<edge source="N" target="THE NUMBER OF LAYERS">
  <data key="d0">IS</data>
</edge>
<edge source="B" target="GROUPS FORMED BY LAYER 0 TO I-1">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="B" target="THE GROUPS THAT HAVE ALREADY FORMED">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="B" target="NONE">
  <data key="d0">IS INITIALIZED TO</data>
</edge>
<edge source="B" target="ONE GROUP FROM LAYER 0 TO I">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="B" target="MULTIPLE GROUPS FORMED BY PREVIOUS LAYERS">
  <data key="d0">CAN CONTAIN</data>
</edge>
<edge source="THE FUNCTION" target="ND THE OPTIMAL GROUPS FROM LAYER I1 TO N-1">
  <data key="d0">APPLIES ITSELF RECURSIVELY</data>
</edge>
<edge source="THE FUNCTION" target="OPTGROUPS">
  <data key="d0">UPDATES</data>
</edge>
<edge source="THE FUNCTION" target="ONE LAYER">
  <data key="d0">ONLY EXAMINES</data>
</edge>
<edge source="OPTGROUPS" target="THE CURRENT STRATEGY IS BETTER">
  <data key="d0">IS UPDATED IF</data>
</edge>
<edge source="OPTGROUPS" target="THE BEST GROUPING STRATEGY FROM LAYER X GIVEN B">
  <data key="d0">STORE</data>
</edge>
<edge source="CASE I" target="THE FIRST GROUP CONTAINS LAYER 0 TO I">
  <data key="d0">MEANS</data>
</edge>
<edge source="CASE I" target="IF THE LOWER BOUND OF CASE I IS ALREADY LARGER THAN THE TOTAL TIME OF THE BEST GROUPING STRATEGY FOUND SO FAR">
  <data key="d0">CAN BE PRUNED</data>
</edge>
<edge source="CASE I" target="THE RST GROUP FROM LAYER X TO XI">
  <data key="d0">FORMS</data>
</edge>
<edge source="CASE I" target="LINE 18-19">
  <data key="d0">IS PRUNED</data>
</edge>
<edge source="THIS FORMULA" target="RECURSIVELY">
  <data key="d0">CAN BE APPLIED</data>
</edge>
<edge source="THIS FORMULA" target="F(GROUP(0,I),I1)">
  <data key="d0">CAN BE APPLIED TO COMPUTE</data>
</edge>
<edge source="OUR RST INSIGHT" target="IT IS NOT NECESSARY TO EXAMINE ALL THE N CASES">
  <data key="d0">IS</data>
</edge>
<edge source="THE RST GROUP" target="TOO MANY LAYERS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="THE RST GROUP" target="ALL LAYERS FROM X TO N1">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="THE COMPUTATION OF THE RST GROUP" target="DELAYED TOO MUCH">
  <data key="d0">WOULD BE</data>
</edge>
<edge source="THE DELAY" target="COMPENSATE THE PIPELINE EFFICIENCY">
  <data key="d0">WOULD</data>
</edge>
<edge source="PACKING MULTIPLE LAYERS IN A GROUP" target="PIPELINE EFFICIENCY">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="OTHER THAN THE RST GROUP" target="SAFELY PACKING MULTIPLE LAYERS IN A GROUP">
  <data key="d0">IS EXCLUDED FROM</data>
</edge>
<edge source="T(I, J)" target="TRANSMISSION TIME FOR A GROUP FROM LAYER I TO J">
  <data key="d0">IS</data>
</edge>
<edge source="T(I, J)" target="SIZE OF LAYER I TO J AND PCIE BANDWIDTH">
  <data key="d0">IS CALCULATED BASED ON</data>
</edge>
<edge source="E(I, J)" target="EXECUTION TIME FOR A GROUP FROM LAYER I TO J">
  <data key="d0">IS</data>
</edge>
<edge source="E(I, J)" target="GPU">
  <data key="d0">IS PROFILED ON</data>
</edge>
<edge source="THE OVERHEAD OF INVOKING MULTIPLE CALLS" target="T(I, J)">
  <data key="d0">IS INCLUDED IN</data>
</edge>
<edge source="THE LOWER BOUND" target="THE BEST CASE THAT ALL THE REMAINING LAYERS ARE COMBINED IN ONE GROUP FOR TRANSMISSION AND COMPUTATION">
  <data key="d0">CONSIDERS</data>
</edge>
<edge source="THE LOWER BOUND" target="THE LOWEST LATENCY WE CAN ACHIEVE FOR AN INFERENCE TASK">
  <data key="d0">IS</data>
</edge>
<edge source="THE LOWER BOUND" target="THE AVERAGE LATENCY OF THE READY MODEL">
  <data key="d0">IS</data>
</edge>
<edge source="THE COMPUTATION AND COMMUNICATION" target="PERFECTLY OVERLAPPED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="ITS COMPUTATION" target="RIGHT AFTER THE COMPUTATION OF THE FIRST GROUP FINISHES">
  <data key="d0">CAN HAPPEN</data>
</edge>
<edge source="THE LOWER BOUND OF CASE I" target="THE TOTAL TIME OF THE BEST GROUPING STRATEGY FOUND SO FAR">
  <data key="d0">IS LARGER THAN</data>
</edge>
<edge source="FIGURE 3(B)" target="AN EXAMPLE FOR THIS INSIGHT">
  <data key="d0">SHOWS</data>
</edge>
<edge source="THE LEAST NUMBER OF LAYERS TO GROUP" target="THE FOLLOWING EQUATION">
  <data key="d0">CAN BE COMPUTED USING</data>
</edge>
<edge source="JIS" target="GROUPING FROM (I1) TO J">
  <data key="d0">IS NO BETTER THAN</data>
</edge>
<edge source="JIS" target="HIGHER PIPELINE OVERHEAD">
  <data key="d0">HAS</data>
</edge>
<edge source="GROUPING FROM (I1) TO J" target="PIPELINE EFFICIENCY">
  <data key="d0">DOES NOT INCREASE</data>
</edge>
<edge source="ALGORITHM" target="OFFLINE TO FIND THE STRATEGY">
  <data key="d0">RUNS</data>
</edge>
<edge source="RESULTING STRATEGY" target="PIPESWITCH">
  <data key="d0">IS USED BY</data>
</edge>
<edge source="ALGORITHM 1" target="THE PSEUDO CODE">
  <data key="d0">SHOWS</data>
</edge>
<edge source="ALGORITHM 1" target="OPTIMALITY">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="ALGORITHM 1" target="A GIVEN LIST OF LAYERS">
  <data key="d0">ACHIEVES OPTIMALITY FOR</data>
</edge>
<edge source="ALGORITHM 1" target="ANY SPECIAL ASSUMPTIONS ON THE EXECUTION ORDER">
  <data key="d0">DOES NOT HAVE</data>
</edge>
<edge source="ALGORITHM 1" target="ONLY SEVERAL SECONDS TO COMPUTE AN OPTIMAL GROUPING STRATEGY">
  <data key="d0">TAKES</data>
</edge>
<edge source="THE FUNCTION FINDOPTGROUPING" target="THE OPTIMAL GROUPING STRATEGY">
  <data key="d0">FINDS</data>
</edge>
<edge source="THE FUNCTION FINDOPTGROUPING" target="THE OPTIMAL GROUPING STRATEGY BASED ON EQUATION 1">
  <data key="d0">FINDS</data>
</edge>
<edge source="EQUATION 1" target="LINE 1-27">
  <data key="d0">IS LOCATED AT</data>
</edge>
<edge source="X" target="THE RST LAYER THAT HAVE NOT FORMED A GROUP">
  <data key="d0">IS</data>
</edge>
<edge source="X" target="0">
  <data key="d0">IS</data>
</edge>
<edge source="THE ALGORITHM" target="THE SECOND PRUNING INSIGHT">
  <data key="d0">APPLIES</data>
</edge>
<edge source="THE ALGORITHM" target="THE RST GROUP FROM LAYER X (LINE 3-9)">
  <data key="d0">FORMS</data>
</edge>
<edge source="THE ALGORITHM" target="THE PROBLEM INTO K 1 CASES">
  <data key="d0">DIVIDES</data>
</edge>
<edge source="THE ALGORITHM" target="THE RST INSIGHT">
  <data key="d0">APPLIES</data>
</edge>
<edge source="THE ALGORITHM" target="THE LOWER BOUND">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="THE ALGORITHM" target="TWO PRUNING TECHNIQUES">
  <data key="d0">USES</data>
</edge>
<edge source="THE ALGORITHM" target="K1 LAYERS">
  <data key="d0">CONSIDERS</data>
</edge>
<edge source="THE ALGORITHM" target="THE OPTIMAL GROUPING STRATEGY FOR M K 1">
  <data key="d0">OUTPUTS</data>
</edge>
<edge source="THE ALGORITHM" target="THE OPTIMAL GROUPING STRATEGY FROM THESE CASES">
  <data key="d0">CHOOSES</data>
</edge>
<edge source="EQUATION 3 AND FIGURE 3(B)" target="THIS INSIGHT">
  <data key="d0">ILLUSTRATE</data>
</edge>
<edge source="B.DELAY" target="THE TIME TO WHICH THE GROUP CAN BE FORMED">
  <data key="d0">DENOTES</data>
</edge>
<edge source="THE ALGORITHM NDS" target="B.DELAY (LINE 4-9)">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="THE ENUMERATION FOR I" target="THE LAYERS FROM X TO J-1 (LINE 11)">
  <data key="d0">CAN SKIP</data>
</edge>
<edge source="EXAMPLE IN EQUATION 2 AND FIGURE 3(A)" target="A SPECIAL CASE">
  <data key="d0">IS</data>
</edge>
<edge source="COMPUTATION FROM X" target="ITS TRANSMISSION (T(X,I))">
  <data key="d0">HAS TO WAIT FOR</data>
</edge>
<edge source="COMPUTATION FROM X" target="COMPUTATION OF THE PREVIOUS GROUPS (B.DELAY)">
  <data key="d0">HAS TO WAIT FOR</data>
</edge>
<edge source="COMPUTATION FROM X" target="FIGURE 4">
  <data key="d0">IS SHOWN IN</data>
</edge>
<edge source="LOWER BOUND" target="CURRENT OPTIMAL TIME">
  <data key="d0">IS BIGGER THAN</data>
</edge>
<edge source="THE TWO PRUNING TECHNIQUES" target="MOST OF THE STRATEGIES">
  <data key="d0">ARE ABLE TO PRUNE</data>
</edge>
<edge source="M N X" target="THE NUMBER OF LAYERS THE FUNCTION CONSIDERS">
  <data key="d0">IS</data>
</edge>
<edge source="FINDOPTGROUPING(B,X)" target="THE OPTIMAL GROUPING STRATEGY FROM LAYER X TO N 1">
  <data key="d0">OUTPUTS</data>
</edge>
<edge source="FINDOPTGROUPING(B,X)" target="THE OPTIMAL STRATEGY">
  <data key="d0">OUTPUTS</data>
</edge>
<edge source="PREVIOUS LAYERS" target="GROUPS REPRESENTED BY B">
  <data key="d0">HAVE FORMED</data>
</edge>
<edge source="K" target="SOME K 1">
  <data key="d0">IS</data>
</edge>
<edge source="M" target="ANY M K">
  <data key="d0">IS</data>
</edge>
<edge source="M" target="1">
  <data key="d0">EQUALS</data>
</edge>
<edge source="FINDOPTGROUPING(B GROUP(X,X I),X I 1)" target="K I K LAYERS">
  <data key="d0">ONLY CONSIDERS</data>
</edge>
<edge source="FINDOPTGROUPING(B GROUP(X,X I),X I 1)" target="THE OPTIMAL GROUPING STRATEGY FOR CASE I">
  <data key="d0">OUTPUTS</data>
</edge>
<edge source="THE OPTIMAL GROUPING STRATEGY FOR CASE I" target="THE ASSUMPTION">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="LAYER X" target="ONE GROUP">
  <data key="d0">IS</data>
</edge>
<edge source="THIS STRATEGY" target="THE OPTIMAL STRATEGY">
  <data key="d0">IS</data>
</edge>
<edge source="THE OPTIMAL STRATEGY FOR THIS CASE" target="ONE GROUP">
  <data key="d0">IS</data>
</edge>
<edge source="THESE CASES" target="EXCLUSIVE">
  <data key="d0">ARE</data>
</edge>
<edge source="THESE CASES" target="THE ENTIRE SEARCH SPACE">
  <data key="d0">COVER</data>
</edge>
<edge source="THESE CASES" target="THE COMPUTATION TO AN EARLIER POINT THAN GROUPING FROM X TO AT LEAST J">
  <data key="d0">CANNOT ADVANCE</data>
</edge>
<edge source="THE RST TECHNIQUE" target="THE CASES">
  <data key="d0">PRUNES</data>
</edge>
<edge source="THE LOWER BOUNDS" target="NO BETTER THAN THE CURRENT FOUND OPTIMAL">
  <data key="d0">ARE</data>
</edge>
<edge source="THIS TECHNIQUE" target="THE OPTIMALITY">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="THE SECOND TECHNIQUE" target="THE CASE">
  <data key="d0">PRUNES</data>
</edge>
<edge source="THEIR RST GROUPS" target="LAYER X TO J J">
  <data key="d0">ARE FROM</data>
</edge>
<edge source="PRUNING THESE CASES" target="THE OPTIMALITY">
  <data key="d0">DO NOT AFFECT</data>
</edge>
<edge source="LAYERS OR OPERATORS IN A DNN MODEL" target="AN ARBITRARY COMPUTATION GRAPH">
  <data key="d0">CAN BE CONNECTED AS</data>
</edge>
<edge source="MODELS LIKE RESNET AND INCEPTION" target="TECHNICALLY NON-LINEAR DIRECTED ACYCLIC GRAPH (DAGS)">
  <data key="d0">ARE</data>
</edge>
<edge source="EXECUTION ORDER" target="ISSUED TO THE GPU ONE BY ONE">
  <data key="d0">IS</data>
</edge>
<edge source="LAYERSOPERATORS IN THE DAG" target="THE GPU ONE BY ONE">
  <data key="d0">ARE ISSUED TO</data>
</edge>
<edge source="GROUPING THE LAYERS" target="HIGH PIPELINING EFFICIENCY AND LOW PIPELINING OVERHEAD">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="ORDER" target="THE RST TIME AN OPERATOR IS EXECUTED">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="ORDER" target="CORRECTNESS">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="OPERATOR" target="ONLY WHEN IT IS TRANSMITTED TO THE GPU AND THE INPUT IS READY">
  <data key="d0">CAN BE EXECUTED</data>
</edge>
<edge source="OUR PIPELINED MODEL TRANSMISSION" target="THE GENERAL CASE">
  <data key="d0">IS APPLICABLE TO</data>
</edge>
<edge source="FIGURE 7" target="EFFECTIVENESS OF PIPELINED MODEL TRANSMISSION">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="FIGURE 7" target="THE TOTAL TIME MEASURED BY THE CLIENT FOR AN INFERENCE TASK TO PREEMPT A TRAINING TASK AND FINISH ITS INFERENCE">
  <data key="d0">SHOWS</data>
</edge>
<edge source="UNIFIED MEMORY MANAGEMENT TASK EXECUTION IN A GPU" target="GPU MEMORY">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="A GPU" target="ITS OWN MEMORY MANAGEMENT SYSTEM">
  <data key="d0">HAS</data>
</edge>
<edge source="A GPU" target="A MALLOC FUNCTION">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="MALLOC FUNCTION" target="CPUS FOR MEMORY ALLOCATION">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="MALLOC FUNCTION" target="CUDAMALLOC FOR NVIDIA GPUS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="EACH TASK" target="NATIVE CUDAMALLOCMANAGED FUNCTION FOR GPU MEMORY ALLOCATION">
  <data key="d0">USES</data>
</edge>
<edge source="EACH TASK" target="MODEL TRANSMISSION TO CUDA UNIFIED MEMORY">
  <data key="d0">DELEGATES</data>
</edge>
<edge source="EACH WORKER" target="CUDAMALLOC TO ALLOCATE GPU MEMORY">
  <data key="d0">USES</data>
</edge>
<edge source="EACH WORKER" target="THE MODEL TO GPU BY ITS OWN">
  <data key="d0">TRANSMITS</data>
</edge>
<edge source="EACH WORKER" target="GPU MEMORY WITH CUDAMALLOCMANAGED">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="EACH WORKER" target="A MEMORY POOL">
  <data key="d0">USES</data>
</edge>
<edge source="EACH WORKER" target="THE MEMORY TO STORE ITS MODEL AND INTERMEDIATE RESULTS">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="EACH WORKER" target="THE MEMORY TO THE POOL">
  <data key="d0">RECYCLES</data>
</edge>
<edge source="EACH WORKER" target="A SEPARATE PROCESS">
  <data key="d0">IS</data>
</edge>
<edge source="EACH WORKER" target="ITS OWN GPU ENVIRONMENT">
  <data key="d0">INITIALIZES</data>
</edge>
<edge source="CUDA" target="THE MODEL TO GPU WHEN NEEDED">
  <data key="d0">AUTOMATICALLY TRANSMITS</data>
</edge>
<edge source="CUDA" target="UNIFIED MEMORY">
  <data key="d0">USES</data>
</edge>
<edge source="THIS SOLUTION" target="HIGH OVERHEAD FOR DL APPLICATIONS">
  <data key="d0">INCURS</data>
</edge>
<edge source="THIS SOLUTION" target="THE LOWER BOUND">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="THIS SOLUTION" target="EXISTING SYSTEMS LIKE GANDIVA 24">
  <data key="d0">IS USED BY</data>
</edge>
<edge source="HIGH OVERHEAD" target="TWO REASONS">
  <data key="d0">IS BECAUSE OF</data>
</edge>
<edge source="NATIVE CUDAMALLOC FUNCTION" target="GENERAL-PURPOSE APPLICATIONS">
  <data key="d0">IS DESIGNED FOR</data>
</edge>
<edge source="CUDA UNIFIED MEMORY" target="GENERAL-PURPOSE APPLICATIONS">
  <data key="d0">IS DESIGNED FOR</data>
</edge>
<edge source="CUDA UNIFIED MEMORY" target="DL APPLICATIONS">
  <data key="d0">IS NOT OPTIMIZED FOR</data>
</edge>
<edge source="CUDA UNIFIED MEMORY" target="MORE THAN ONE HUNDRED MILLISECONDS OVERHEAD THAN PIPESWITCH">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="CUDA UNIFIED MEMORY" target="MEMORY SWAPPING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="NATIVE CUDAMALLOC FUNCTION AND CUDA UNIFIED MEMORY" target="UNNECESSARY OVERHEAD FOR DL APPLICATIONS">
  <data key="d0">MAY INCUR</data>
</edge>
<edge source="TWO CHARACTERISTICS OF DL APPLICATIONS" target="GPU MEMORY MANAGEMENT OVERHEAD">
  <data key="d0">MINIMIZE</data>
</edge>
<edge source="THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT" target="THESE CHARACTERISTICS">
  <data key="d0">DOES NOT CONSIDER</data>
</edge>
<edge source="THE GENERAL-PURPOSE GPU MEMORY MANAGEMENT" target="TOO HEAVY-WEIGHT FOR DL APPLICATIONS THAT REQUIRE FAST TASK SWITCHING">
  <data key="d0">IS</data>
</edge>
<edge source="AMOUNT OF MEMORY ALLOCATED TO THE DNN MODEL" target="FIXED">
  <data key="d0">IS</data>
</edge>
<edge source="AMOUNT OF MEMORY ALLOCATED TO THE DNN MODEL" target="TASK EXECUTION">
  <data key="d0">DOES NOT CHANGE DURING</data>
</edge>
<edge source="THE AMOUNT OF MEMORY NEEDED TO STORE THEM" target="THE SAME">
  <data key="d0">STAYS</data>
</edge>
<edge source="THE INTERMEDIATE RESULTS GENERATED IN THE FORWARD PASS" target="THE BACKWARD PASS">
  <data key="d0">ARE USED BY</data>
</edge>
<edge source="THE BACKWARD PASS" target="THE WEIGHTS">
  <data key="d0">UPDATES</data>
</edge>
<edge source="THE BACKWARD PASS" target="THE INTERMEDIATE RESULTS">
  <data key="d0">CONSUMES</data>
</edge>
<edge source="THE BACKWARD PASS" target="REVERSE ORDER">
  <data key="d0">CONSUMES IN ORDER</data>
</edge>
<edge source="THE FORWARD PASS" target="THE INTERMEDIATE RESULTS">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THE MEMORY ALLOCATION AND RELEASE" target="A SIMPLE STACK-LIKE MECHANISM">
  <data key="d0">CAN BE HANDLED BY</data>
</edge>
<edge source="A SIMPLE STACK-LIKE MECHANISM" target="MEMORY FRAGMENTATION">
  <data key="d0">DOES NOT CAUSE</data>
</edge>
<edge source="MEMORY ALLOCATION OVERHEAD" target="MINIMIZED">
  <data key="d0">SHOULD BE</data>
</edge>
<edge source="MINIMIZE MEMORY FOOTPRINT" target="AVOID EXTRA MEMORY COPIES">
  <data key="d0">AND</data>
</edge>
<edge source="THE MEMORY ALLOCATION OVERHEAD" target="THE MEMORY DAEMON">
  <data key="d0">IS ELIMINATED WITH</data>
</edge>
<edge source="PASSING MEMORY POINTERS TO THE WORKERS" target="LIGHT-WEIGHT">
  <data key="d0">IS</data>
</edge>
<edge source="ONE WORKER" target="THE GPU MEMORY">
  <data key="d0">OWNS</data>
</edge>
<edge source="THE MEMORY" target="THE POOL AFTER THE INTERMEDIATE RESULTS ARE NO LONGER NEEDED">
  <data key="d0">IS RECYCLED TO</data>
</edge>
<edge source="THE MEMORY MANAGEMENT OF PIPESWITCH" target="THAT OF PYTORCH">
  <data key="d0">EXTENDS</data>
</edge>
<edge source="MEMORY MANAGEMENT IN PYTORCH" target="MEMORY ALLOCATION FOR A TASK ITSELF">
  <data key="d0">HANDLES</data>
</edge>
<edge source="REPLICATING THE MODELS IN EACH WORKER" target="HIGH MEMORY FOOTPRINT">
  <data key="d0">INCURS</data>
</edge>
<edge source="REPLICATING THE MODELS IN EACH WORKER" target="THE NUMBER OF MODELS A SERVER CAN STORE">
  <data key="d0">REDUCES</data>
</edge>
<edge source="REPLICATING THE MODELS IN EACH WORKER" target="THE TYPES OF TASKS THE SERVER CAN EXECUTE">
  <data key="d0">REDUCES</data>
</edge>
<edge source="STORING THE MODELS IN A DEDICATE PROCESS" target="MINIMAL MEMORY FOOTPRINT">
  <data key="d0">HAS</data>
</edge>
<edge source="STORING THE MODELS IN A DEDICATE PROCESS" target="AN EXTRA MEMORY COPY FROM THIS PROCESS TO A WORKER TO START A TASK">
  <data key="d0">INCURS</data>
</edge>
<edge source="EACH MODEL" target="ONLY ONCE">
  <data key="d0">IS STORED</data>
</edge>
<edge source="AN EXTRA MEMORY COPY" target="THE TASK SWITCHING TIME">
  <data key="d0">HURTS</data>
</edge>
<edge source="IPC" target="OVERHEAD">
  <data key="d0">HAS</data>
</edge>
<edge source="IPC" target="NO OPTIMIZATION">
  <data key="d0">HAS</data>
</edge>
<edge source="IPC APIS" target="GPUS">
  <data key="d0">ARE PROVIDED BY</data>
</edge>
<edge source="CUDAIPCOPENMEMHANDLE" target="NVIDIA GPUS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THESE IPC APIS" target="HIGH OVERHEAD">
  <data key="d0">INCUR</data>
</edge>
<edge source="THE OVERHEAD" target="THE PIPELINE">
  <data key="d0">IS EXACERBATED BY</data>
</edge>
<edge source="THE PIPELINE" target="THE IPCS FREQUENTLY">
  <data key="d0">NEEDS TO INVOKE</data>
</edge>
<edge source="THE PIPELINE" target="FOR EVERY PIPELINE GROUP">
  <data key="d0">INVOKES THE IPCS</data>
</edge>
<edge source="THE PIPELINE" target="ONLY ONCE FOR THE ENTIRE MODEL TRANSMISSION">
  <data key="d0">INVOKES THE IPC</data>
</edge>
<edge source="THE IPCS" target="SYNCHRONIZE MODEL TRANSMISSION AND TASK EXECUTION">
  <data key="d0">ARE INVOKED TO</data>
</edge>
<edge source="MEMORY ALLOCATION PROCESS FOR A NEURAL NETWORK MODEL" target="DETERMINISTIC">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY DAEMON AND THE WORKER" target="THE SAME ORDER TO ALLOCATE MEMORY FOR THE MODEL PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="THE MEMORY POINTERS FOR THE PARAMETERS" target="THE SAME">
  <data key="d0">WOULD BE</data>
</edge>
<edge source="THE NEURAL NETWORK MODEL" target="KNOWN AND GIVEN">
  <data key="d0">IS</data>
</edge>
<edge source="LATENCY" target="NO UNIFIED MEMORY MANAGEMENT">
  <data key="d0">IS HIGHER THAN</data>
</edge>
<edge source="LATENCY" target="MILLISECONDS">
  <data key="d0">IS MEASURED IN</data>
</edge>
<edge source="LATENCY" target="5000 TO 10000 MS">
  <data key="d0">RANGES</data>
</edge>
<edge source="LATENCY" target="5000 TO 7500 MS">
  <data key="d0">RANGES FROM</data>
</edge>
<edge source="LATENCY" target="7500 TO 10000 MS">
  <data key="d0">RANGES FROM</data>
</edge>
<edge source="LATENCY" target="DIFFERENT SCHEDULING CYCLES">
  <data key="d0">IS MEASURED UNDER</data>
</edge>
<edge source="CHEAP CPU IPCS" target="THE WORKER">
  <data key="d0">NOTIFY</data>
</edge>
<edge source="THE WORKER" target="WHICH PIPELINE GROUP">
  <data key="d0">HAS BEEN TRANSMITTED</data>
</edge>
<edge source="THE WORKER" target="THE MODEL STRUCTURES">
  <data key="d0">LOADS</data>
</edge>
<edge source="THE WORKER" target="THE MODEL PARAMETERS">
  <data key="d0">DOES NOT LOAD</data>
</edge>
<edge source="THE WORKER" target="THE SCHEDULER TO TRANSFER REQUIRED PARAMETERS FOR DNN MODELS">
  <data key="d0">WAITS FOR</data>
</edge>
<edge source="THE WORKER" target="INFERENCE OR TRAINING">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="NO PIN" target="MEMORY">
  <data key="d0">HAS</data>
</edge>
<edge source="THE OS" target="A MEMORY PAGE TO DISK">
  <data key="d0">WOULD SWAP</data>
</edge>
<edge source="THE PAGE" target="INACTIVE FOR A CERTAIN AMOUNT OF TIME">
  <data key="d0">IS</data>
</edge>
<edge source="A PAGE IN THE HOST MEMORY" target="IN ORDER TO TRANSMIT THE DATA IN THE PAGE TO THE GPU MEMORY">
  <data key="d0">IS PINNED (OR PAGE-LOCKED)</data>
</edge>
<edge source="A TEMPORARY PINNED PAGE" target="THE TRANSMISSION">
  <data key="d0">IS CREATED FOR</data>
</edge>
<edge source="SEPARATE PROCESSES" target="PROCESS-LEVEL ISOLATION">
  <data key="d0">ACHIEVE</data>
</edge>
<edge source="SEPARATE PROCESSES" target="THE NAIVE SOLUTION">
  <data key="d0">ARE SIMILAR TO</data>
</edge>
<edge source="A NAIVE SOLUTION" target="SEPARATE PROCESSES">
  <data key="d0">IS TO USE</data>
</edge>
<edge source="A NAIVE SOLUTION" target="THE NEW TASK AFTER THE CURRENT TASK IS STOPPED">
  <data key="d0">IS TO START</data>
</edge>
<edge source="SEQUENTIAL EXECUTION" target="LONG DELAY">
  <data key="d0">INCURS</data>
</edge>
<edge source="LONG DELAY" target="OLD TASK CLEANING">
  <data key="d0">IS DUE TO</data>
</edge>
<edge source="LONG DELAY" target="NEW TASK INITIALIZATION">
  <data key="d0">IS DUE TO</data>
</edge>
<edge source="ANOTHER POSSIBLE SOLUTION" target="TO LET THE CURRENT AND NEW TASKS SHARE THE SAME PROCESS WITH A WARM CUDA CONTEXT">
  <data key="d0">IS</data>
</edge>
<edge source="THE PROCESS OF THE OLD TASK" target="THE GPU ENVIRONMENT">
  <data key="d0">CLEANS</data>
</edge>
<edge source="ANOTHER PROCESS" target="THE NEW TASK">
  <data key="d0">IS INITIALIZED FOR</data>
</edge>
<edge source="THE PROCESS" target="THE ENVIRONMENT FOR THE NEW TASK">
  <data key="d0">REUSES</data>
</edge>
<edge source="GPU ENVIRONMENT" target="CUDA CONTEXT">
  <data key="d0">IS</data>
</edge>
<edge source="GPU ENVIRONMENT" target="WHEN IT IS RST CREATED">
  <data key="d0">IS INITIALIZED</data>
</edge>
<edge source="CUDA CONTEXT" target="A FEW HUNDRED MB GPU MEMORY">
  <data key="d0">CONSUMES</data>
</edge>
<edge source="A MAJOR JOB" target="ASYNCHRONOUS CUDA FUNCTIONS QUEUED ON THE GPU">
  <data key="d0">IS TO CLEAR</data>
</edge>
<edge source="A CURRENT TASK" target="STOPPED">
  <data key="d0">IS</data>
</edge>
<edge source="NUMBER OF QUEUED FUNCTIONS" target="LIMITED">
  <data key="d0">ARE</data>
</edge>
<edge source="NUMBER OF QUEUED FUNCTIONS" target="QUICKLY CLEARED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="SYNCHRONIZATION POINTS" target="INFERENCE TASKS">
  <data key="d0">ARE NOT NEEDED FOR</data>
</edge>
<edge source="ANOTHER JOB" target="ITS GPU MEMORY">
  <data key="d0">IS TO FREE</data>
</edge>
<edge source="AN IMPORTANT PROPERTY OF THE CLEANING PROCEDURE" target="THAT IT DOES NOT MODIFY THE CONTENT OF THE MEMORY">
  <data key="d0">IS</data>
</edge>
<edge source="AN IMPORTANT PROPERTY OF THE CLEANING PROCEDURE" target="THE METADATA">
  <data key="d0">ONLY CLEANS</data>
</edge>
<edge source="THE METADATA" target="GPU MEMORY POINTERS">
  <data key="d0">IS</data>
</edge>
<edge source="CLEANING PROCEDURE" target="POINTERS POINTING TO THE TENSOR DATA">
  <data key="d0">DELETES</data>
</edge>
<edge source="CLEANING PROCEDURE" target="ACTUAL DATA">
  <data key="d0">DOES NOT FREE</data>
</edge>
<edge source="PARALLELIZING THE TASK CLEANING AND THE PIPELINED MODEL TRANSMISSION" target="THE TASK CLEANING OVERHEAD">
  <data key="d0">HIDES</data>
</edge>
<edge source="THIS CHOICE" target="PERFORMANCE">
  <data key="d0">IS OPTIMIZED FOR</data>
</edge>
<edge source="THIS CHOICE" target="A PROBLEM FOR A TRUSTED ENVIRONMENT">
  <data key="d0">IS NOT</data>
</edge>
<edge source="A LATTER PROCESS" target="THE MEMORY DATA OF A PREVIOUS PROCESS">
  <data key="d0">CAN READ</data>
</edge>
<edge source="AN ADDITIONAL ZERO-OUT OPERATION" target="ADDED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="HIGH MEMORY BANDWIDTH" target="900GBS FOR V100">
  <data key="d0">E.G.</data>
</edge>
<edge source="SUB-MILLISECOND OVERHEAD" target="ZEROING-OUT MOST MODELS LIKE RESNET-152">
  <data key="d0">IS FOR</data>
</edge>
<edge source="RESNET-152" target="AROUND 240MB">
  <data key="d0">HAS SIZE</data>
</edge>
<edge source="NEW PROCESS" target="ENTIRE GPU MEMORY">
  <data key="d0">DOES NOT REQUIRE</data>
</edge>
<edge source="ONLY ONE ACTIVE WORKER" target="EXCLUSIVE OCCUPATION OF THE GPU">
  <data key="d0">GUARANTEES</data>
</edge>
<edge source="TRADE-OFF" target="NUMBER OF STANDBY WORKERS AND THEIR GPU MEMORY CONSUMPTION">
  <data key="d0">EXISTS BETWEEN</data>
</edge>
<edge source="EVERY STANDBY WORKER" target="ITS OWN CUDA CONTEXT">
  <data key="d0">NEEDS TO MAINTAIN</data>
</edge>
<edge source="TWO STANDBY WORKERS" target="AT LEAST ONE IDLE WORKER">
  <data key="d0">ARE SUFFICIENT TO ENSURE</data>
</edge>
<edge source="TWO STANDBY WORKERS" target="MODERATE GPU MEMORY CONSUMPTION">
  <data key="d0">HAS</data>
</edge>
<edge source="AT LEAST ONE IDLE WORKER" target="THE WAITING TIME">
  <data key="d0">ELIMINATES</data>
</edge>
<edge source="A TRANSACTION" target="A MODEL IS SWITCHED IN OR OUT ON ALL OF ITS GPUS">
  <data key="d0">MEANS</data>
</edge>
<edge source="A TRANSACTION" target="INFERENCE ON THIS MODEL">
  <data key="d0">ENABLES OR DISABLES</data>
</edge>
<edge source="TASKS IN THIS TRACE" target="111,883">
  <data key="d0">NUMBER</data>
</edge>
<edge source="SINGLE-GPU TRAINING TASKS" target="96,662">
  <data key="d0">NUMBER</data>
</edge>
<edge source="SINGLE-GPU TRAINING TASKS" target="86%">
  <data key="d0">PERCENTAGE OF ALL TASKS</data>
</edge>
<edge source="THESE JOBS" target="18 OF TOTAL GPU HOURS">
  <data key="d0">ACCOUNT FOR</data>
</edge>
<edge source="CURRENT TRAINING FRAMEWORKS" target="MATURE SUPPORT OF ELASTIC TRAINING">
  <data key="d0">DO NOT HAVE</data>
</edge>
<edge source="THESE SCHEDULING SOLUTIONS" target="ORTHOGONAL AND COMPLEMENTARY TO PIPESWITCH">
  <data key="d0">ARE</data>
</edge>
<edge source="HTTPS" target="PYTORCH.ORG">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="DEVBLOGS.NVIDIA.COM/UNIFIED-MEMORY-CUDA-BEGINNERS">
  <data key="d0">IS PART OF</data>
</edge>
<edge source="HTTPS" target="AWS.AMAZON.COM">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="AZURE.MICROSOFT.COM">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="CLOUD.GOOGLE.COM">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="DEVELOPER.NVIDIA.COMDEEP-LEARNING-PERFORMANCE-TRAINING-INFERENCE">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="WWW.TENSORFLOW.ORG">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="DEVELOPER.NVIDIA.COM/NCCL">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="KUBERNETES.IO">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="GITHUB.COM/NVIDIA/NVIDIA-DOCKER">
  <data key="d0">IS</data>
</edge>
<edge source="SHARED GPU MEMORY" target="PYTORCH GPU MEMORY POOL">
  <data key="d0">CAN BE INSERTED INTO</data>
</edge>
<edge source="SHARED GPU MEMORY" target="DIFFERENT CUDA STREAMS">
  <data key="d0">CAN BE INSERTED MULTIPLE TIMES FOR</data>
</edge>
<edge source="THE CONTROLLER PROCESS" target="A TCP THREAD">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="THE CONTROLLER PROCESS" target="A SCHEDULER THREAD">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="THE SCHEDULER AND THE MEMORY DAEMON" target="FOR BETTER PERFORMANCE">
  <data key="d0">ARE IMPLEMENTED TOGETHER</data>
</edge>
<edge source="THE TCP THREAD" target="TASK THROUGH TCP FROM CLIENTS">
  <data key="d0">ACCEPTS</data>
</edge>
<edge source="THE TCP THREAD" target="THE TASK TO THE SCHEDULER THREAD">
  <data key="d0">SENDS</data>
</edge>
<edge source="THE SCHEDULER THREAD" target="THE GPU MEMORY WITH WORKERS">
  <data key="d0">ALLOCATES AND SHARES</data>
</edge>
<edge source="THE SCHEDULER THREAD" target="WORKERS">
  <data key="d0">ACTIVATES OR DEACTIVATES</data>
</edge>
<edge source="THE SCHEDULER THREAD" target="THE TASK TO A WORKER">
  <data key="d0">SENDS</data>
</edge>
<edge source="THE SCHEDULER THREAD" target="PARAMETERS FOR THE CORRESPONDING MODEL TO THE GPU MEMORY">
  <data key="d0">TRANSFERS</data>
</edge>
<edge source="THE USER" target="THE MODEL IN THE SCHEDULER">
  <data key="d0">SHOULD REGISTER</data>
</edge>
<edge source="THE SCHEDULER" target="THE CONTROLLER">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="PARAMETERS" target="THE GPU MEMORY">
  <data key="d0">ARE TRANSMITTED TO</data>
</edge>
<edge source="PARAMETERS" target="GROUPS">
  <data key="d0">ARE TRANSMITTED IN</data>
</edge>
<edge source="PARAMETERS" target="A PIPELINE">
  <data key="d0">ARE TRANSMITTED IN</data>
</edge>
<edge source="THE WORKER PROCESS" target="TWO THREADS">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="THE TERMINATION THREAD" target="THE TERMINATION SIGNAL FROM THE CONTROLLER">
  <data key="d0">WAITS FOR</data>
</edge>
<edge source="THE TERMINATION THREAD" target="THE MAIN THREAD">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="THE MAIN THREAD" target="THE DNN MODELS">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE MAIN THREAD" target="THE COMPUTATION FOR INFERENCE OR TRAINING">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="THE MODEL STRUCTURES" target="SMALL">
  <data key="d0">IS</data>
</edge>
<edge source="THE PARAMETERS" target="ONCE IN THE MEMORY DAEMON">
  <data key="d0">ARE STORED</data>
</edge>
<edge source="THE PARAMETERS" target="MINIMAL MEMORY FOOTPRINT">
  <data key="d0">ARE STORED FOR</data>
</edge>
<edge source="THEIR PARAMETERS" target="LOCATIONS IN THE SHARED GPU MEMORY">
  <data key="d0">ARE ASSIGNED TO</data>
</edge>
<edge source="DIFFERENT MODELS" target="THE SAME GPU MEMORY LOCATION">
  <data key="d0">MIGHT USE</data>
</edge>
<edge source="THE VALUE" target="THE CONTROLLER TRANSFERS THE CORRESPONDING PARAMETERS TO THESE LOCATIONS">
  <data key="d0">IS NOT VALID UNTIL</data>
</edge>
<edge source="ALL EXPERIMENTS" target="AWS">
  <data key="d0">ARE CONDUCTED ON</data>
</edge>
<edge source="THE SOFTWARE ENVIRONMENT" target="PYTORCH-1.3.0">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE SOFTWARE ENVIRONMENT" target="TORCHVISION-0.4.2">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE SOFTWARE ENVIRONMENT" target="SCIPY-1.3.2">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE SOFTWARE ENVIRONMENT" target="CUDA-10.1">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PYTORCH WITH OUR PLUGINS" target="BETTER RESULTS FOR STOP-AND-START">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="BETTER RESULTS FOR STOP-AND-START" target="NATIVE PYTORCH FROM PYTHON-PYPI USED IN TABLE 1">
  <data key="d0">ARE BETTER THAN</data>
</edge>
<edge source="INCEPTIONV3 22" target="STANDARD BENCHMARKS FOR EVALUATING DL SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="BERTBASE 23" target="STANDARD BENCHMARKS FOR EVALUATING DL SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="THE EXPERIMENTS" target="TRAINING AND INFERENCE">
  <data key="d0">COVER</data>
</edge>
<edge source="SINGLE-GPU INFERENCE AND TRAINING TASKS" target="4.5">
  <data key="d0">ARE DISCUSSED IN</data>
</edge>
<edge source="THE CHECKPOINTING FREQUENCY OF TRAINING TASKS" target="THE SCHEDULING CYCLE">
  <data key="d0">IS SET ACCORDING TO</data>
</edge>
<edge source="THE CHECKPOINTING FREQUENCY OF TRAINING TASKS" target="MINIMIZE CHECKPOINTING OVERHEAD">
  <data key="d0">IS SET TO</data>
</edge>
<edge source="DEFAULT BATCH SIZE FOR TRAINING" target="32">
  <data key="d0">IS</data>
</edge>
<edge source="DEFAULT BATCH SIZE FOR INFERENCE" target="8">
  <data key="d0">IS</data>
</edge>
<edge source="FIGURE 5" target="TOTAL LATENCY EXPERIENCED BY THE CLIENT FOR DIFFERENT MECHANISMS">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="FIGURE 5" target="THE LATENCY EXPERIENCED BY THE CLIENT">
  <data key="d0">SHOWS</data>
</edge>
<edge source="TABLE 3" target="THE TOTAL OVERHEAD">
  <data key="d0">SHOWS</data>
</edge>
<edge source="THE TOTAL OVERHEAD" target="THE DIFFERENCE BETWEEN THE LATENCY OF A MECHANISM AND THAT OF THE READY MODEL">
  <data key="d0">IS</data>
</edge>
<edge source="EACH NUMBER" target="THE AVERAGE OF 100 RUNS">
  <data key="d0">IS REPORTED WITH</data>
</edge>
<edge source="FIGURE 6(B)" target="MINIMUM AND MAXIMUM LATENCIES USING THE ERROR BAR">
  <data key="d0">REPORTS</data>
</edge>
<edge source="FIGURE 6(B)" target="THE AVERAGE LATENCY OF THE INFERENCE TASKS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="LATENCY OF THE RST BATCH AND THOSE OF LATER BATCHES IN ONE SCHEDULING CYCLE" target="SIGNIFICANTLY">
  <data key="d0">CAN DIFFER</data>
</edge>
<edge source="LATENCY DIFFERENCE" target="SWITCHING OVERHEAD">
  <data key="d0">IS DUE TO</data>
</edge>
<edge source="6.1 END-TO-END EXPERIMENTS" target="END-TO-END OVERHEAD">
  <data key="d0">MINIMIZE</data>
</edge>
<edge source="A CLIENT" target="AN INFERENCE TASK TO A GPU SERVER">
  <data key="d0">SENDS</data>
</edge>
<edge source="THE GPU SERVER" target="THE TRAINING TASK">
  <data key="d0">PREEMPTS</data>
</edge>
<edge source="THE GPU SERVER" target="THE INFERENCE TASK">
  <data key="d0">EXECUTES</data>
</edge>
<edge source="THE GPU SERVER" target="A REPLY BACK TO THE CLIENT">
  <data key="d0">SENDS</data>
</edge>
<edge source="THE PROCESS WITH THE REQUIRED MODEL" target="THE GPU">
  <data key="d0">IS LOADED IN</data>
</edge>
<edge source="GANDIVA 24" target="TASK SWITCHING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="MPS" target="POOR THROUGHPUT AROUND 100 BATCHES PER SECOND">
  <data key="d0">KEEPS</data>
</edge>
<edge source="MPS" target="ABOUT 80 MS AVERAGE LATENCY">
  <data key="d0">HAS</data>
</edge>
<edge source="MPS" target="SEVERAL HUNDRED MILLISECONDS LATENCY FOR THE RST BATCH">
  <data key="d0">HAS</data>
</edge>
<edge source="THE PROPERTIES" target="4">
  <data key="d0">ARE DESCRIBED IN</data>
</edge>
<edge source="STOP-AND-START" target="THE WORST">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="STOP-AND-START" target="SEVERAL SECONDS">
  <data key="d0">TAKES</data>
</edge>
<edge source="RESNET152" target="0 20 40 60 80 100 MS">
  <data key="d0">HAS LATENCY</data>
</edge>
<edge source="RESNET152" target="TEXT">
  <data key="d0">IS MENTIONED IN</data>
</edge>
<edge source="RESNET152" target="0 100 200 300 400 MS">
  <data key="d0">HAS LATENCY</data>
</edge>
<edge source="RESNET152" target="3.62 MS">
  <data key="d0">HAS STARTUP OVERHEAD</data>
</edge>
<edge source="RESNET152" target="TRAINING">
  <data key="d0">USED FOR</data>
</edge>
<edge source="RESNET152" target="INFERENCE">
  <data key="d0">USED FOR</data>
</edge>
<edge source="RESNET152" target="HUNDREDS OF LAYERS">
  <data key="d0">HAS</data>
</edge>
<edge source="INCEPTIONV3" target="0 20 40 60 80 100 MS">
  <data key="d0">HAS LATENCY</data>
</edge>
<edge source="INCEPTIONV3" target="TEXT">
  <data key="d0">IS MENTIONED IN</data>
</edge>
<edge source="INCEPTIONV3" target="0 100 200 300 400 MS">
  <data key="d0">HAS LATENCY</data>
</edge>
<edge source="INCEPTIONV3" target="4.82 MS">
  <data key="d0">HAS STARTUP OVERHEAD</data>
</edge>
<edge source="BERTBASE" target="0 20 40 60 80 100 MS">
  <data key="d0">HAS LATENCY</data>
</edge>
<edge source="BERTBASE" target="TEXT">
  <data key="d0">IS MENTIONED IN</data>
</edge>
<edge source="BERTBASE" target="0 100 200 300 400 MS">
  <data key="d0">HAS LATENCY</data>
</edge>
<edge source="BERTBASE" target="3.62 MS">
  <data key="d0">HAS STARTUP OVERHEAD</data>
</edge>
<edge source="PER-LAYER" target="PIPELINE">
  <data key="d0">IS</data>
</edge>
<edge source="OPTIMIZATION" target="NO OPTIMIZATION">
  <data key="d0">STATUS</data>
</edge>
<edge source="NO OPTIMIZATION" target="CONDITION">
  <data key="d0">IS A</data>
</edge>
<edge source="NO OPTIMIZATION" target="THE WORST IN MOST CASES">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="PER-LAYER PIPELINE" target="TECHNIQUE">
  <data key="d0">IS A</data>
</edge>
<edge source="PER-LAYER PIPELINE" target="TRANSMISSION AND COMPUTATION AT THE GRANULARITY OF LAYER">
  <data key="d0">OVERLAPS</data>
</edge>
<edge source="ONE PROCESS" target="THE CUDA ENVIRONMENT">
  <data key="d0">REUSES</data>
</edge>
<edge source="ONE PROCESS" target="THE OVERHEAD TO CLEAN THE ENVIRONMENT">
  <data key="d0">PAYS</data>
</edge>
<edge source="TWO PROCESSES" target="THE WORST">
  <data key="d0">PERFORM</data>
</edge>
<edge source="INSTANCE" target="G4DN.2XLARGE">
  <data key="d0">IS</data>
</edge>
<edge source="INSTANCE" target="NVIDIA T4">
  <data key="d0">HAS GPU</data>
</edge>
<edge source="ITS PERFORMANCE" target="THE READY MODEL WHEN THE MODEL IS PRELOADED">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="ITS PERFORMANCE" target="NVIDIA MPS WHEN THE MODEL IS IN THE HOST MEMORY">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="THE WORST" target="THE TRAINING TASK">
  <data key="d0">STOPS</data>
</edge>
<edge source="THE WORST" target="A NEW PROCESS FOR THE NEW TASK">
  <data key="d0">INITIALIZES</data>
</edge>
<edge source="THE MAIN SOURCE OF THE OVERHEAD" target="CUDA CONTEXT INITIALIZATION AND RST-TIME LIBRARY LOADING OPERATIONS IN PYTORCH">
  <data key="d0">IS</data>
</edge>
<edge source="ANOTHER SOURCE" target="GPU MEMORY SWAPPING">
  <data key="d0">IS</data>
</edge>
<edge source="THE OVERHEAD OF PIPESWITCH FOR MOST CONFIGURATIONS" target="UP TO 10MS">
  <data key="d0">IS</data>
</edge>
<edge source="BERT ON T4" target="THE LARGE MODEL SIZE">
  <data key="d0">HAS OVERHEAD DUE TO</data>
</edge>
<edge source="BERT ON T4" target="THE SMALLER PCIE BANDWIDTH ON T4 THAN THAT ON V100">
  <data key="d0">HAS OVERHEAD DUE TO</data>
</edge>
<edge source="TIME TO COMPUTE BERT ON T4" target="120MS">
  <data key="d0">IS</data>
</edge>
<edge source="THE RELATIVE OVERHEAD" target="ACCEPTABLE">
  <data key="d0">IS</data>
</edge>
<edge source="READY MODEL" target="COMPUTING">
  <data key="d0">TO START</data>
</edge>
<edge source="THE STARTUP OVERHEAD OF PIPESWITCH" target="ONLY A FEW MILLISECONDS">
  <data key="d0">IS</data>
</edge>
<edge source="SWITCHING" target="EACH SCHEDULING CYCLE">
  <data key="d0">OCCURS AFTER</data>
</edge>
<edge source="FIGURE 6(A)" target="INFERENCE THROUGHPUT">
  <data key="d0">SHOWS</data>
</edge>
<edge source="THE DASHED LINE" target="THE UPPER BOUND">
  <data key="d0">IS</data>
</edge>
<edge source="THE DASHED LINE" target="THE LOWER BOUND">
  <data key="d0">IS</data>
</edge>
<edge source="THE UPPER BOUND" target="THE THROUGHPUT OF THE READY MODEL ASSUMING NO TASK SWITCHING">
  <data key="d0">IS</data>
</edge>
<edge source="THE AVERAGE LATENCY OF THE READY MODEL" target="NO TASK SWITCHING">
  <data key="d0">ASSUMES</data>
</edge>
<edge source="THROUGHPUT OF STOP-AND-START" target="NEARLY ZERO FOR SCHEDULING CYCLES SMALLER THAN 10 S">
  <data key="d0">IS</data>
</edge>
<edge source="THE ERROR BAR" target="THE MINIMUM AND MAXIMUM LATENCY">
  <data key="d0">INDICATES</data>
</edge>
<edge source="STOP- AND-START" target="POOR LATENCY">
  <data key="d0">HAS</data>
</edge>
<edge source="STOP- AND-START" target="RST BATCH HAS SEVERAL SECONDS OVERHEAD">
  <data key="d0">HAS POOR LATENCY BECAUSE</data>
</edge>
<edge source="RST BATCH" target="SEVERAL SECONDS OVERHEAD">
  <data key="d0">HAS</data>
</edge>
<edge source="PIPESWITCH MPS" target="B">
  <data key="d0">HAS LATENCY LOWER BOUND</data>
</edge>
<edge source="THROUGHPUT AND LATENCY" target="RESNET">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="RESNET" target="P3.2XLARGE">
  <data key="d0">RUNS ON</data>
</edge>
<edge source="COMPUTATION" target="ONCE PARAMETERS ARE TRANSMITTED">
  <data key="d0">STARTS</data>
</edge>
<edge source="FIGURE 8" target="THE TOTAL TIME MEASURED BY THE CLIENT">
  <data key="d0">SHOWS</data>
</edge>
<edge source="FIGURE 8" target="EFFECTIVENESS OF UNIFIED MEMORY MANAGEMENT">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="MODELS WITH MANY LAYERS BUT RELATIVELY LIGHT COMPUTATION SUCH AS RESNET152 AND INCEPTION" target="WORSE THAN GROUPED TRANSMISSION">
  <data key="d0">CAN PERFORM</data>
</edge>
<edge source="MODELS WITH MANY LAYERS BUT RELATIVELY LIGHT COMPUTATION SUCH AS RESNET152 AND INCEPTION" target="SOMETIMES EVEN WORSE THAN NO PIPELINE">
  <data key="d0">CAN PERFORM</data>
</edge>
<edge source="THIS REDUCTION" target="SIGNIFICANT">
  <data key="d0">IS</data>
</edge>
<edge source="THIS REDUCTION" target="THE OPTIMIZATIONS ON MEMORY MANAGEMENT AND WORKER SWITCHING HAVE ALREADY BEEN APPLIED">
  <data key="d0">IS EVALUATED WHEN</data>
</edge>
<edge source="ALL OVERHEADS FOR TASK SWITCHING" target="NOT ONLY THE MOST SIGNIFICANT ONE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="NUMBER OF LAYERS" target="BOTH WEIGHTED AND UNWEIGHTED LAYERS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="BOTH WEIGHTED AND UNWEIGHTED LAYERS" target="COMPUTATION TIME">
  <data key="d0">CONTRIBUTE TO</data>
</edge>
<edge source="THE PARAMETER SIZE" target="EACH LAYER">
  <data key="d0">IS MEASURED FOR</data>
</edge>
<edge source="RUNNING TIME" target="EACH LAYER">
  <data key="d0">IS MEASURED FOR</data>
</edge>
<edge source="NO PRUNING" target="FOR ALL THREE MODELS AFTER RUNNING FOR 24 HOURS">
  <data key="d0">DOES NOT FINISH</data>
</edge>
<edge source="UNIED MEMORY MANAGEMENT" target="THE EFFECTIVENESS OF UNIED MEMORY MANAGEMENT">
  <data key="d0">IS USED TO EVALUATE</data>
</edge>
<edge source="NO UNIED MEMORY MANAGEMENT" target="STATED">
  <data key="d0">IS</data>
</edge>
<edge source="THE PAGES OF THE MEMORY DAEMON" target="THE MAIN MEMORY">
  <data key="d0">ARE NOT PINNED TO</data>
</edge>
<edge source="THIS EXPERIMENT" target="ALL THE OPTIMIZATIONS ON MEMORY MANAGEMENT ARE EFFECTIVE">
  <data key="d0">DEMONSTRATES</data>
</edge>
<edge source="UNIED MEMORY MANAGEMENT MECHANISM" target="PIPESWITCH">
  <data key="d0">IS USED BY</data>
</edge>
<edge source="IPC OPTIMIZATION" target="IMPORTANT">
  <data key="d0">IS</data>
</edge>
<edge source="IPC OPTIMIZATION" target="LATENCY BY 1648 MS">
  <data key="d0">REDUCES</data>
</edge>
<edge source="PINNING THE PAGES TO THE HOST MEMORY" target="THE LATENCY WITH A FEW MILLISECONDS">
  <data key="d0">CAN REDUCE</data>
</edge>
<edge source="FIGURE 9" target="EFFECTIVENESS OF ACTIVE-STANDBY SWITCHING">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="FIGURE 9" target="THE RESULTS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="THE NEW PROCESS" target="A NEW CUDA ENVIRONMENT">
  <data key="d0">NEEDS TO CREATE</data>
</edge>
<edge source="A NEW CUDA ENVIRONMENT" target="THE TOTAL TIME">
  <data key="d0">DOMINATES</data>
</edge>
<edge source="MANY FRAMEWORKS" target="DEEP LEARNING">
  <data key="d0">HAVE BEEN DEVELOPED FOR</data>
</edge>
<edge source="FRAMEWORKS" target="TENSORFLOW">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FRAMEWORKS" target="PYTORCH">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FRAMEWORKS" target="MXNET">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MXNET" target="A FLEXIBLE AND EFFICIENT MACHINE LEARNING LIBRARY">
  <data key="d0">IS</data>
</edge>
<edge source="MXNET" target="HETEROGENEOUS DISTRIBUTED SYSTEMS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="MXNET" target="ARXIV PREPRINT ARXIV:1512.01274">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="SEVERAL ALGORITHMS AND SYSTEMS" target="EXECUTING AND SCHEDULING DEEP LEARNING TASKS ON CLUSTERS">
  <data key="d0">HAVE BEEN DESIGNED FOR</data>
</edge>
<edge source="SEVERAL ALGORITHMS AND SYSTEMS" target="BOTH TRAINING AND INFERENCE TASKS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MANY TECHNIQUES AND SYSTEMS" target="OPTIMIZE COMMUNICATION">
  <data key="d0">HAVE BEEN PROPOSED TO</data>
</edge>
<edge source="MANY TECHNIQUES AND SYSTEMS" target="IMPROVE DISTRIBUTED TRAINING">
  <data key="d0">HAVE BEEN PROPOSED TO</data>
</edge>
<edge source="THE MOST RELEVANT ONES" target="PIPEDREAM 8">
  <data key="d0">ARE</data>
</edge>
<edge source="THE MOST RELEVANT ONES" target="BYTESCHEDULER 9">
  <data key="d0">ARE</data>
</edge>
<edge source="THE MOST RELEVANT ONES" target="POSEIDON 40">
  <data key="d0">ARE</data>
</edge>
<edge source="OTHER WORKS LIKE VDNN 43 AND SWAPADVISOR 44" target="GPU MEMORY MANAGEMENT MODULE">
  <data key="d0">HAVE</data>
</edge>
<edge source="CLUSTER MANAGERS 4548" target="GPUS TO VMS OR CONTAINERS AT DEVICE GRANULARITY">
  <data key="d0">ALLOCATE</data>
</edge>
<edge source="SEVERAL SOLUTIONS" target="SHARE A GPU AT APPLICATION GRANULARITY">
  <data key="d0">HAVE BEEN PROPOSED TO</data>
</edge>
<edge source="TECHNIQUES" target="LIBRARY INTERCEPTION 6,4953">
  <data key="d0">LIKE</data>
</edge>
<edge source="DEEP LEARNING APPLICATIONS" target="HUNDREDS OF KERNELS">
  <data key="d0">TYPICALLY REQUIRE</data>
</edge>
<edge source="EFFORTS" target="GPU OPTIMIZATION">
  <data key="d0">ARE ON</data>
</edge>
<edge source="EFFORTS" target="TENSOR FUSION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="EFFORTS" target="KERNEL-LEVEL CONCURRENCY">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="EFFORTS" target="SCHEDULING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="GPU OPTIMIZATION" target="PERFORMANCE OF RUNNING A SINGLE TASK">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="THE ANONYMOUS REVIEWERS" target="VALUABLE FEEDBACK">
  <data key="d0">HAVE</data>
</edge>
<edge source="ZHIHAO BAI" target="AN AWS MACHINE LEARNING RESEARCH AWARD">
  <data key="d0">WAS SUPPORTED BY</data>
</edge>
<edge source="ZHEN ZHANG" target="AN AWS MACHINE LEARNING RESEARCH AWARD">
  <data key="d0">WAS SUPPORTED BY</data>
</edge>
<edge source="XIN JIN" target="AN AWS MACHINE LEARNING RESEARCH AWARD">
  <data key="d0">WAS SUPPORTED BY</data>
</edge>
<edge source="A. VERMA, L. PEDROSA, M. KORUPOLU, D. OPPENHEIMER, E. TUNE, AND J. WILKES" target="LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG" target="EUROSYS">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="LARGE-SCALE CLUSTER MANAGEMENT AT GOOGLE WITH BORG" target="2015">
  <data key="d0">PUBLISHED_IN_YEAR</data>
</edge>
<edge source="DEAN AND L. A. BARROSO" target="THE TAIL AT SCALE">
  <data key="d0">WROTE</data>
</edge>
<edge source="THE TAIL AT SCALE" target="COMMUNICATIONS OF THE ACM">
  <data key="d0">APPEARED IN</data>
</edge>
<edge source="COMMUNICATIONS OF THE ACM" target="VOL.">
  <data key="d0">HAS VOLUME</data>
</edge>
<edge source="NEXUS" target="A GPU CLUSTER ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="NEXUS" target="ACCELERATING DNN-BASED VIDEO ANALYSIS">
  <data key="d0">PURPOSE</data>
</edge>
<edge source="NEXUS" target="ACM SOSP">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="NEXUS" target="2019">
  <data key="d0">PUBLISHED_YEAR</data>
</edge>
<edge source="H. SHEN" target="NEXUS">
  <data key="d0">IS_AUTHOR_OF</data>
</edge>
<edge source="L. CHEN" target="NEXUS">
  <data key="d0">IS_AUTHOR_OF</data>
</edge>
<edge source="Y. JIN" target="NEXUS">
  <data key="d0">IS_AUTHOR_OF</data>
</edge>
<edge source="L. ZHAO" target="NEXUS">
  <data key="d0">IS_AUTHOR_OF</data>
</edge>
<edge source="B. KONG" target="NEXUS">
  <data key="d0">IS_AUTHOR_OF</data>
</edge>
<edge source="M. PHILIPOSE" target="NEXUS">
  <data key="d0">IS_AUTHOR_OF</data>
</edge>
<edge source="A. KRISHNAMURTHY" target="NEXUS">
  <data key="d0">IS_AUTHOR_OF</data>
</edge>
<edge source="R. SUNDARAM" target="NEXUS">
  <data key="d0">IS_AUTHOR_OF</data>
</edge>
<edge source="FRIED, J. BEHRENS, A. BELAY, AND H. BAL-AKRISHNAN" target="SHENANGO: ACHIEVING HIGH CPU EFFICIENCY FOR LATENCY-SENSITIVE DATACENTER WORKLOADS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="SHENANGO" target="USENIX NSDI">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="SHENANGO" target="2019">
  <data key="d0">PUBLISHED_YEAR</data>
</edge>
<edge source="USENIX NSDI" target="2011">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="CUDA MULTI-PROCESS SERVICE" target="6">
  <data key="d0">IS</data>
</edge>
<edge source="CUDAMULTIPROCESSSERVICEOVERVIEW.PDF" target="HTTPS://DOCS.NVIDIA.COM/DEPLOY.PDF">
  <data key="d0">IS LOCATED AT</data>
</edge>
<edge source="7 P. YU AND M. CHOWDHURY" target="SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS" target="CONFERENCE ON MACHINE LEARNING AND SYSTEMS">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="SALUS: FINE-GRAINED GPU SHARING PRIMITIVES FOR DEEP LEARNING APPLICATIONS" target="2020">
  <data key="d0">PUBLISHED_YEAR</data>
</edge>
<edge source="PIPEDREAM" target="GENERALIZED PIPELINE PARALLELISM FOR DNN TRAINING">
  <data key="d0">IS</data>
</edge>
<edge source="PIPEDREAM" target="ACM SOSP">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="PIPEDREAM" target="2019">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="D. NARAYANAN" target="PIPEDREAM">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="A. HARLAP" target="PIPEDREAM">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="A. PHANISHAYEE" target="PIPEDREAM">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="V. SESHADRI" target="PIPEDREAM">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="N. R. DEVANUR" target="PIPEDREAM">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="G. R. GANGER" target="PIPEDREAM">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="P. B. GIBBONS" target="PIPEDREAM">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="M. ZAHARIA" target="PIPEDREAM">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="9 Y. PENG, Y. ZHU, Y. CHEN, Y. BAO, B. YI, C. LAN, C. WU, AND C. GUO" target="A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION" target="ACM SOSP">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="A GENERIC COMMUNICATION SCHEDULER FOR DISTRIBUTED DNN TRAINING ACCELERATION" target="2019">
  <data key="d0">PUBLISHED_IN_YEAR</data>
</edge>
<edge source="TIRESIAS" target="A GPU CLUSTER MANAGER FOR DISTRIBUTED DEEP LEARNING">
  <data key="d0">IS</data>
</edge>
<edge source="TIRESIAS" target="USENIX NSDI">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="TIRESIAS" target="2019">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="J. GU, M. CHOWDHURY, K. G. SHIN, Y. ZHU, M. JEON, J. QIAN, H. LIU, AND C. GUO" target="TIRESIAS">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="POSEIDON" target="AN EFFICIENT COMMUNICATION ARCHITECTURE">
  <data key="d0">IS</data>
</edge>
<edge source="POSEIDON" target="DISTRIBUTED DEEP LEARNING ON GPU CLUSTERS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="POSEIDON" target="USENIX ATC">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="POSEIDON" target="2017">
  <data key="d0">WAS PRESENTED IN YEAR</data>
</edge>
<edge source="H. ZHANG, Z. ZHENG, S. XU, W. DAI, Q. HO, X. LIANG, Z. HU, J. WEI, P. XIE, AND E. P. XING" target="POSEIDON">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="AMAZON WEB SERVICES" target="12">
  <data key="d0">IS</data>
</edge>
<edge source="MICROSOFT" target="AZURE">
  <data key="d0">OFFERS</data>
</edge>
<edge source="GOOGLE CLOUD PLATFORM" target="14">
  <data key="d0">IS</data>
</edge>
<edge source="HOROVOD" target="FAST AND EASY DISTRIBUTED DEEP LEARNING FRAMEWORK">
  <data key="d0">IS</data>
</edge>
<edge source="HOROVOD" target="TENSORFLOW">
  <data key="d0">IS USED IN</data>
</edge>
<edge source="15 A. SERGEEV AND M. DEL BALSO" target="HOROVOD">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="HOROVOD PAPER" target="ARXIV PREPRINT ARXIV:1802.05799">
  <data key="d0">IS PUBLISHED IN</data>
</edge>
<edge source="HOROVOD PAPER" target="2018">
  <data key="d0">IS PUBLISHED IN YEAR</data>
</edge>
<edge source="SU" target="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER">
  <data key="d0">PUBLISHED</data>
</edge>
<edge source="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER" target="USENIX OSDI">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="SCALING DISTRIBUTED MACHINE LEARNING WITH THE PARAMETER SERVER" target="2014">
  <data key="d0">PUBLISHED_YEAR</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION" target="SUN">
  <data key="d0">AUTHORED BY</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION" target="IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION" target="2016">
  <data key="d0">PUBLISHED IN YEAR</data>
</edge>
<edge source="NVIDIA DATA CENTER DEEP LEARNING PRODUCT" target="PERFORMANCE">
  <data key="d0">HAS</data>
</edge>
<edge source="PHILLY" target="20 TRACES">
  <data key="d0">HAS</data>
</edge>
<edge source="HTTPS:GITHUB.COM/MSR-FIDDLE" target="PHILLY-TRACES">
  <data key="d0">RELATED TO</data>
</edge>
<edge source="21" target="PYTORCH">
  <data key="d0">IS</data>
</edge>
<edge source="22 C. SZEGEDY, V. VANHOUCKE, S. IOFFE, J. SHLENS, AND Z. WOJNA" target="RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION" target="IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="RETHINKING THE INCEPTION ARCHITECTURE FOR COMPUTER VISION" target="2016">
  <data key="d0">PUBLISHED_YEAR</data>
</edge>
<edge source="J. DEVLIN, M.-W. CHANG, K. LEE, AND K. TOUTANOVA" target="BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="BERT: PRE-TRAINING OF DEEP BIDIRECTIONAL TRANSFORMERS FOR LANGUAGE UNDERSTANDING" target="PROCEEDINGS OF THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, VOLUME 1 (LONG AND SHORT PAPERS)">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="PROCEEDINGS OF THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, VOLUME 1 (LONG AND SHORT PAPERS)" target="2019">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="GANDIVA" target="INTROSPECTIVE CLUSTER SCHEDULING FOR DEEP LEARNING">
  <data key="d0">IS</data>
</edge>
<edge source="GANDIVA" target="USENIX OSDI">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="GANDIVA" target="2018">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="24 W. XIAO, R. BHARDWAJ, R. RAMJEE, M. SIVATHANU, N. KWATRA, Z. HAN, P. PATEL, X. PENG, H. ZHAO, Q. ZHANG, ET AL." target="GANDIVA">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="25" target="TENSORFLOW">
  <data key="d0">IS</data>
</edge>
<edge source="TENSORFLOW XLA" target="54">
  <data key="d0">HAS NUMBER</data>
</edge>
<edge source="HTTPS://WWW.TENSORFLOW.ORG" target="XLA">
  <data key="d0">HAS</data>
</edge>
<edge source="26" target="MXNET">
  <data key="d0">IS</data>
</edge>
<edge source="MXNET.APACHE.ORG" target="HTTPS">
  <data key="d0">USES</data>
</edge>
<edge source="SLAQ" target="QUALITY-DRIVEN SCHEDULING FOR DISTRIBUTED MACHINE LEARNING">
  <data key="d0">IS</data>
</edge>
<edge source="SLAQ" target="M. J. FREEDMAN">
  <data key="d0">AUTHORED BY</data>
</edge>
<edge source="SLAQ" target="ACM SYMPOSIUM ON CLOUD COMPUTING">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="SLAQ" target="2017">
  <data key="d0">PUBLISHED IN YEAR</data>
</edge>
<edge source="OPTIMUS" target="AN EFFICIENT DYNAMIC RESOURCE SCHEDULER FOR DEEP LEARNING CLUSTERS">
  <data key="d0">IS</data>
</edge>
<edge source="OPTIMUS" target="EUROSYS">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="OPTIMUS" target="2018">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="Y. PENG, Y. BAO, Y. CHEN, C. WU, AND C. GUO" target="OPTIMUS">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="THEMIS" target="FAIR AND EFFICIENT GPU CLUSTER SCHEDULING">
  <data key="d0">IS</data>
</edge>
<edge source="THEMIS" target="USENIX NSDI">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="THEMIS" target="2020">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="K. MAHAJAN, A. BALASUBRAMANIAN, A. SINGHVI, S. VENKATARAMAN, A. AKELLA, A. PHANISHAYEE, AND S. CHAWLA" target="THEMIS">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="HYPERSCHED" target="DYNAMIC RESOURCE REALLOCATION FOR MODEL DEVELOPMENT ON A DEADLINE">
  <data key="d0">IS</data>
</edge>
<edge source="HYPERSCHED" target="ACM SYMPOSIUM ON CLOUD COMPUTING">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="HYPERSCHED" target="2019">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="R. LIAW" target="HYPERSCHED">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="R. BHARDWAJ" target="HYPERSCHED">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="L. DUNLAP" target="HYPERSCHED">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="Y. ZOU" target="HYPERSCHED">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="J. E. GONZALEZ" target="HYPERSCHED">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="I. STOICA" target="HYPERSCHED">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="A. TUMANOV" target="HYPERSCHED">
  <data key="d0">IS AN AUTHOR OF</data>
</edge>
<edge source="CHET" target="AN OPTIMIZING COMPILER FOR FULLY-HOMOMORPHIC NEURAL-NETWORK INFERENCING">
  <data key="d0">IS</data>
</edge>
<edge source="CHET" target="ACM CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="CHET" target="2019">
  <data key="d0">WAS PRESENTED IN YEAR</data>
</edge>
<edge source="R. DATHATHRI" target="CHET">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="O. SAARIKIVI" target="CHET">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="H. CHEN" target="CHET">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="K. LAINE" target="CHET">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="K. LAUTER" target="CHET">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="S. MALEKI" target="CHET">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="M. MUSUVATHI" target="CHET">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="T. MYTKOWICZ" target="CHET">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="TVM" target="AN AUTOMATED END-TO-END OPTIMIZING COMPILER FOR DEEP LEARNING">
  <data key="d0">IS</data>
</edge>
<edge source="TVM" target="USENIX OSDI">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="TVM" target="2018">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="T. CHEN, T. MOREAU, Z. JIANG, L. ZHENG, E. YAN, H. SHEN, M. COWAN, L. WANG, Y. HU, L. CEZE, ET AL." target="TVM">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="GPIPE" target="EFFICIENT TRAINING OF GIANT NEURAL NETWORKS USING PIPELINE PARALLELISM">
  <data key="d0">IS</data>
</edge>
<edge source="GPIPE" target="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="GPIPE" target="2019">
  <data key="d0">PUBLISHED_YEAR</data>
</edge>
<edge source="Y. HUANG ET AL." target="GPIPE">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="BLINK" target="FAST AND GENERIC COLLECTIVES FOR DISTRIBUTED ML">
  <data key="d0">IS</data>
</edge>
<edge source="BLINK" target="CONFERENCE ON MACHINE LEARNING AND SYSTEMS">
  <data key="d0">IS PRESENTED IN</data>
</edge>
<edge source="BLINK" target="2020">
  <data key="d0">IS PUBLISHED IN</data>
</edge>
<edge source="G. WANG, S. VENKATARAMAN, A. PHANISHAYEE, J. THELIN, N. DEVANUR, AND I. STOICA" target="BLINK">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="NVIDIA COLLECTIVE COMMUNICATIONS LIBRARY" target="NCCL">
  <data key="d0">ABBREVIATED AS</data>
</edge>
<edge source="36 J. LIU, J. WU, AND D. K. PANDA" target="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION OVER INNIBAND">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="HIGH PERFORMANCE RDMA-BASED MPI IMPLEMENTATION" target="OVER INNIBAND">
  <data key="d0">IS</data>
</edge>
<edge source="37 Q. HO, J. CIPAR, H. CUI, S. LEE, J. K. KIM, P. B. GIBBONS, G. A. GIBSON, G. GANGER, AND E. P. XING" target="MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER" target="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="MORE EFFECTIVE DISTRIBUTED ML VIA A STALE SYNCHRONOUS PARALLEL PARAMETER SERVER" target="2013">
  <data key="d0">PUBLISHED_YEAR</data>
</edge>
<edge source="A. AWAN, C.-H. CHU, H. SUBRAMONI, AND D. K. PANDA" target="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="OPTIMIZED BROADCAST FOR DEEP LEARNING WORKLOADS ON DENSE-GPU INNIBAND CLUSTERS: MPI OR NCCL?" target="PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="PROCEEDINGS OF THE 25TH EUROPEAN MPI USERS GROUP MEETING" target="2018">
  <data key="d0">YEAR</data>
</edge>
<edge source="DAILY, A. VISHNU, C. SIEGEL, T. WARFEL, AND V. AMATYA" target="GOSSIPGRAD: SCALABLE DEEP LEARNING USING GOSSIP COMMUNICATION BASED ASYNCHRONOUS GRADIENT DESCENT">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="GOSSIPGRAD" target="SCALABLE DEEP LEARNING USING GOSSIP COMMUNICATION BASED ASYNCHRONOUS GRADIENT DESCENT">
  <data key="d0">IS</data>
</edge>
<edge source="GOSSIPGRAD" target="CORR, VOL.">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="41 Z. ZHANG, C. CHANG, H. LIN, Y. WANG, R. ARORA, AND X. JIN" target="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?" target="ACM SIGCOMM WORKSHOP ON NETWORK MEETS AI ML (NETAI)">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="IS NETWORK THE BOTTLENECK OF DISTRIBUTED TRAINING?" target="AUGUST 2020">
  <data key="d0">PUBLISHED_DATE</data>
</edge>
<edge source="42 Y. CHEN, Z. LIU, B. REN, AND X. JIN" target="ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS">
  <data key="d0">WROTE</data>
</edge>
<edge source="ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS" target="INTERNATIONAL CONFERENCE ON MACHINE LEARNING (ICML)">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="ON EFFICIENT CONSTRUCTIONS OF CHECKPOINTS" target="JULY 2020">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="VDNN" target="VIRTUALIZED DEEP NEURAL NETWORKS FOR SCALABLE, MEMORY-EFFICIENT NEURAL NETWORK DESIGN">
  <data key="d0">IS</data>
</edge>
<edge source="VDNN" target="2016 49TH ANNUAL IEEEACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO)">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="PAPER" target="43 M. RHU, N. GIMELSHEIN, J. CLEMONS, A. ZULQAR, AND S. W. KECKLER">
  <data key="d0">WAS WRITTEN BY</data>
</edge>
<edge source="SYMPOSIUM" target="2016">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="SWAPADVISOR" target="44 C.-C. HUANG, G. JIN, AND J. LI">
  <data key="d0">AUTHORED_BY</data>
</edge>
<edge source="SWAPADVISOR" target="ACM ASPLOS">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="SWAPADVISOR" target="2020">
  <data key="d0">PUBLISHED_YEAR</data>
</edge>
<edge source="SWAPADVISOR" target="PUSHING DEEP LEARNING BEYOND THE GPU MEMORY LIMIT VIA SMART SWAPPING">
  <data key="d0">DESCRIPTION</data>
</edge>
<edge source="NVIDIA CONTAINER RUNTIME" target="DOCKER">
  <data key="d0">IS FOR</data>
</edge>
<edge source="MESOS" target="A PLATFORM FOR NE-GRAINED RESOURCE SHARING IN THE DATA CENTER">
  <data key="d0">IS</data>
</edge>
<edge source="MESOS" target="USENIX NSDI">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="47 B. HINDMAN, A. KONWINSKI, M. ZAHARIA, A. GHODSI, A. D. JOSEPH, R. H. KATZ, S. SHENKER, AND I. STOICA" target="MESOS: A PLATFORM FOR NE-GRAINED RESOURCE SHARING IN THE DATA CENTER">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="APACHE HADOOP YARN" target="YET ANOTHER RESOURCE NEGOTIATOR">
  <data key="d0">IS</data>
</edge>
<edge source="V. K. VAVILAPALLI, A. C. MURTHY, C. DOUGLAS, S. AGARWAL, M. KONAR, R. EVANS, T. GRAVES, J. LOWE, H. SHAH, S. SETH, ET AL." target="APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR" target="ACM SYMPOSIUM ON CLOUD COMPUTING">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="APACHE HADOOP YARN: YET ANOTHER RESOURCE NEGOTIATOR" target="2013">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="G. GIUNTA, R. MONTELLA, G. AGRILLO, AND G. COVIELLO" target="A GPGPU TRANSPARENT VIRTUALIZATION COMPONENT FOR HIGH PERFORMANCE COMPUTING CLOUDS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="A GPGPU TRANSPARENT VIRTUALIZATION COMPONENT FOR HIGH PERFORMANCE COMPUTING CLOUDS" target="EUROPEAN CONFERENCE ON PARALLEL PROCESSING">
  <data key="d0">PRESENTED IN</data>
</edge>
<edge source="EUROPEAN CONFERENCE ON PARALLEL PROCESSING" target="2010">
  <data key="d0">YEAR</data>
</edge>
<edge source="V. T. RAVI, M. BECCHI, G. AGRAWAL, AND S. CHAKRADHAR" target="SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="SUPPORTING GPU SHARING IN CLOUD ENVIRONMENTS WITH A TRANSPARENT RUNTIME CONSOLIDATION FRAMEWORK" target="PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING" target="2011">
  <data key="d0">YEAR</data>
</edge>
<edge source="GVIM" target="GPU-ACCELERATED VIRTUAL MACHINES">
  <data key="d0">IS</data>
</edge>
<edge source="GVIM" target="PROCEEDINGS OF THE 3RD ACM WORKSHOP ON SYSTEM-LEVEL VIRTUALIZATION FOR HIGH PERFORMANCE COMPUTING">
  <data key="d0">IS PRESENTED IN</data>
</edge>
<edge source="PROCEEDINGS OF THE 3RD ACM WORKSHOP ON SYSTEM-LEVEL VIRTUALIZATION FOR HIGH PERFORMANCE COMPUTING" target="2009">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="50 V. GUPTA, A. GAVRILOVSKA, K. SCHWAN, H. KHARCHE, N. TOLIA, V. TALWAR, AND P. RANGANATHAN" target="GVIM">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="RCUDA" target="THE NUMBER OF GPU-BASED ACCELERATORS IN HIGH PERFORMANCE CLUSTERS">
  <data key="d0">REDUCES</data>
</edge>
<edge source="RCUDA" target="2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION">
  <data key="d0">WAS PRESENTED IN</data>
</edge>
<edge source="51 J. DUATO, A. J. PENA, F. SILLA, R. MAYO, AND E. S. QUINTANA-ORT" target="RCUDA">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="2010 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING SIMULATION" target="2010">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="VCUDA" target="GPU-ACCELERATED HIGH-PERFORMANCE COMPUTING IN VIRTUAL MACHINES">
  <data key="d0">IS</data>
</edge>
<edge source="VCUDA" target="IEEE TRANSACTIONS ON COMPUTERS">
  <data key="d0">IS PUBLISHED IN</data>
</edge>
<edge source="SUN AND K. LI" target="VCUDA">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="55 T. CHEN, M. LI, Y. LI, M. LIN, N. WANG, M. WANG, T. XIAO, B. XU, C. ZHANG, AND Z. ZHANG" target="MXNET">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="ARXIV PREPRINT ARXIV:1512.01274" target="2015">
  <data key="d0">WAS PUBLISHED IN YEAR</data>
</edge>
<edge source="56 C. GREGG, J. DORN, K. HAZELWOOD, AND K. SKADRON" target="FINE-GRAINED RESOURCE SHARING FOR CONCURRENT GPGPU KERNELS">
  <data key="d0">PRESENTED</data>
</edge>
<edge source="FINE-GRAINED RESOURCE SHARING FOR CONCURRENT GPGPU KERNELS" target="4TH USENIX WORKSHOP ON HOT TOPICS IN PARALLELISM">
  <data key="d0">PRESENTED AS PART OF</data>
</edge>
<edge source="4TH USENIX WORKSHOP ON HOT TOPICS IN PARALLELISM" target="2012">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="57 S. PAI, M. J. THAZHUTHAVEETIL, AND R. GOVINDARAJAN" target="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="IMPROVING GPGPU CONCURRENCY WITH ELASTIC KERNELS" target="ACM SIGARCH COMPUTER ARCHITECTURE NEWS">
  <data key="d0">IS PUBLISHED IN</data>
</edge>
<edge source="TASO" target="DEEP LEARNING COMPUTATION">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="TASO" target="AUTOMATIC GENERATION OF GRAPH SUBSTITUTIONS">
  <data key="d0">USES</data>
</edge>
<edge source="TASO" target="ACM SOSP">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="TASO" target="2019">
  <data key="d0">PUBLISHED IN YEAR</data>
</edge>
<edge source="Z. JIA, O. PADON, J. THOMAS, T. WARSZAWSKI, M. ZAHARIA, AND A. AIKEN" target="TASO">
  <data key="d0">AUTHORED</data>
</edge>
</graph></graphml>