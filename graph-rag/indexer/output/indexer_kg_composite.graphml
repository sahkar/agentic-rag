<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d0" for="edge" attr.name="relationship" attr.type="string"/>
<graph edgedefault="directed"><node id="EXISTING SYSTEMS"/>
<node id="KEY-VALUE CACHE (KV CACHE) MEMORY FOR EACH REQUEST IS HUGE AND GROWS AND SHRINKS DYNAMICALLY"/>
<node id="THE KV CACHE SIZE"/>
<node id="THE NUMBER OF REQUESTS"/>
<node id="THIS MEMORY"/>
<node id="FRAGMENTATION AND REDUNDANT DUPLICATION"/>
<node id="THE BATCH SIZE"/>
<node id="INEFFICIENT MEMORY MANAGEMENT"/>
<node id="BATCH SIZE"/>
<node id="WE"/>
<node id="PAGEDATTENTION"/>
<node id="AN ATTENTION ALGORITHM"/>
<node id="THE CLASSICAL VIRTUAL MEMORY AND PAGING TECHNIQUES IN OPERATING SYSTEMS"/>
<node id="PAGEDAT-TENTION"/>
<node id="OPERATING SYSTEMS (OS) SOLUTION TO MEMORY FRAGMENTATION AND SHARING"/>
<node id="VIRTUAL MEMORY WITH PAGING"/>
<node id="KV CACHE STORED IN NON-CONTIGUOUS PAGED MEMORY"/>
<node id="VIRTUAL MEMORY AND PAGING IN OS"/>
<node id="PAGE-DATTENTION"/>
<node id="THE CLASSIC IDEA OF PAGING IN OPERATING SYSTEMS"/>
<node id="STORING CONTINUOUS KEYS AND VALUES IN NON-CONTIGUOUS MEMORY SPACE"/>
<node id="TRADITIONAL ATTENTION ALGORITHMS"/>
<node id="ATTENTION KEY AND VALUES VECTORS"/>
<node id="NON-CONTIGUOUS BLOCKS IN THE MEMORY"/>
<node id="THIS PAPER"/>
<node id="A NEW ATTENTION ALGORITHM"/>
<node id="ATTENTION KEYS AND VALUES TO BE STORED IN NON-CONTIGUOUS PAGED MEMORY"/>
<node id="VLLM"/>
<node id="A HIGH-THROUGHPUT LLM SERVING SYSTEM"/>
<node id="EFFICIENT MEMORY MANAGEMENT ENABLED BY PAGEDATTENTION"/>
<node id="AN LLM SERVING SYSTEM"/>
<node id="NEAR-ZERO WASTE IN KV CACHE MEMORY"/>
<node id="FLEXIBLE SHARING OF KV CACHE WITHIN AND ACROSS REQUESTS"/>
<node id="FLEXIBLE SHARING OF KV CACHE"/>
<node id="MEMORY USAGE"/>
<node id="EXISTING LLM SERVING SYSTEMS 31, 60"/>
<node id="MANAGING THE KV CACHE MEMORY EFFICIENTLY"/>
<node id="THE KV CACHE IN A MORE FLEXIBLE WAY"/>
<node id="BLOCKS"/>
<node id="PAGES"/>
<node id="TOKENS"/>
<node id="BYTES"/>
<node id="REQUESTS"/>
<node id="PROCESSES"/>
<node id="A HIGH-THROUGHPUT DISTRIBUTED LLM SERVING ENGINE"/>
<node id="KV CACHE MEMORY"/>
<node id="ESTABLISHED TECHNIQUES"/>
<node id="OPERATING SYSTEMS"/>
<node id="VIRTUAL MEMORY"/>
<node id="COPY-ON-WRITE"/>
<node id="EFFICIENTLY MANAGE KV CACHE"/>
<node id="HANDLE VARIOUS DECODING ALGORITHMS IN LLM SERVING"/>
<node id="THROUGHPUT OF POPULAR LLMS BY 2-4"/>
<node id="THE SAME LEVEL OF LATENCY"/>
<node id="STATE-OF-THE-ART SYSTEMS"/>
<node id="FASTERTRANSFORMER AND ORCA"/>
<node id="THE PERFORMANCE OF VLLM UNDER A VARIETY OF WORKLOADS"/>
<node id="2-4 THROUGHPUT IMPROVEMENTS OVER THE STATE-OF-THE-ART SYSTEMS"/>
<node id="THE IMPROVEMENT"/>
<node id="LONGER SEQUENCES"/>
<node id="LARGER MODELS"/>
<node id="MORE COMPLEX DECODING ALGORITHMS"/>
<node id="IMPROVEMENTS"/>
<node id="VLLMS SOURCE CODE"/>
<node id="PUBLICLY AVAILABLE"/>
<node id="HTTPS:GITHUB.COMVLLM-PROJECTVLLM"/>
<node id="LARGE LANGUAGE MODELS (LLMS) LIKE GPT 5, 37 AND PALM 9"/>
<node id="NEW APPLICATIONS SUCH AS PROGRAMMING ASSISTANTS 6, 18 AND UNIVERSAL CHATBOTS 19, 35"/>
<node id="OUR WORK AND DAILY ROUTINES"/>
<node id="MANY CLOUD COMPANIES 34, 44"/>
<node id="THESE APPLICATIONS AS HOSTED SERVICES"/>
<node id="RUNNING THESE APPLICATIONS"/>
<node id="VERY EXPENSIVE"/>
<node id="A LARGE NUMBER OF HARDWARE ACCELERATORS SUCH AS GPUS"/>
<node id="PROCESSING AN LLM REQUEST"/>
<node id="10 MORE EXPENSIVE THAN A TRADITIONAL KEYWORD QUERY"/>
<node id="THIS WORK"/>
<node id="A CREATIVE COMMONS ATTRIBUTION INTERNATIONAL 4.0 LICENSE"/>
<node id="SOSP 23"/>
<node id="OCTOBER 23 26 2023"/>
<node id="KOBLENZ GERMANY"/>
<node id="COPYRIGHT"/>
<node id="2023"/>
<node id="OWNERAUTHOR(S)"/>
<node id="ACM"/>
<node id="ISBN 979-8-4007-0229-72310"/>
<node id="NVIDIA A100"/>
<node id="40GB PARAMETERS"/>
<node id="KV CACHE"/>
<node id="26GB"/>
<node id="65"/>
<node id="OTHERS"/>
<node id="20"/>
<node id="30"/>
<node id="40"/>
<node id="GB"/>
<node id="PARAMETER SIZE"/>
<node id="EXISTING SYSTEMS VLLM"/>
<node id="THROUGHPUT"/>
<node id="MEMORY LAYOUT"/>
<node id="LEFT WHEN SERVING AN LLM WITH 13B PARAMETERS ON NVIDIA A100"/>
<node id="MEMORY DISTRIBUTION"/>
<node id="1 (LEFT)"/>
<node id="13B-PARAMETER LLM"/>
<node id="NVIDIA A100 GPU"/>
<node id="40GB RAM"/>
<node id="THE PARAMETERS (GRAY)"/>
<node id="GPU MEMORY"/>
<node id="SERVING"/>
<node id="THE MEMORY FOR THE KV CACHE (RED)"/>
<node id="PER SERVING REQUEST"/>
<node id="A CONTIGUOUS CHUNK OF MEMORY"/>
<node id="THE REQUESTS MAXIMUM LENGTH"/>
<node id="2048 TOKENS"/>
<node id="ALL AVAILABLE MEMORY"/>
<node id="OUTPUT LENGTH OF A REQUEST"/>
<node id="DECODING"/>
<node id="MEMORY REQUIRED FOR ITS KV CACHE"/>
<node id="AS THE OUTPUT LENGTH OF A REQUEST GROWS AT DECODING"/>
<node id="AVAILABLE MEMORY FOR INCOMING REQUESTS OR ONGOING GENERATION FOR EXISTING PROMPTS"/>
<node id="A SMALL AMOUNT OF MEMORY (YELLOW)"/>
<node id="EPHEMERALLY FOR ACTIVATION"/>
<node id="THE RAPID GROWTH CURVE OF KV CACHE MEMORY SEEN IN EXISTING SYSTEMS 31, 60"/>
<node id="A NOTABLE BOOST IN SERVING THROUGHPUT"/>
<node id="KEY IDEA BEHIND VLLMS MEMORY MANAGER"/>
<node id="VIRTUAL MEMORY 25 IN OPERATING SYSTEMS"/>
<node id="THE IDEAS BEHIND VIRTUAL MEMORY"/>
<node id="THE KV CACHE IN AN LLM SERVICE"/>
<node id="COST PER REQUEST OF LLM SERVING SYSTEMS"/>
<node id="MORE IMPORTANT"/>
<node id="LLMS"/>
<node id="AN AUTOREGRESSIVE TRANSFORMER MODEL 53"/>
<node id="THIS MODEL"/>
<node id="WORDS (TOKENS)"/>
<node id="ONE AT A TIME"/>
<node id="BASED ON THE INPUT (PROMPT)"/>
<node id="BASED ON THE PREVIOUS SEQUENCE OF THE OUTPUTS TOKENS IT HAS GENERATED SO FAR"/>
<node id="THIS EXPENSIVE PROCESS"/>
<node id="FOR EACH REQUEST"/>
<node id="THE MODEL OUTPUTS A TERMINATION TOKEN"/>
<node id="SEQUENTIAL GENERATION PROCESS"/>
<node id="WORKLOAD MEMORY-BOUND"/>
<node id="COMPUTATION POWER OF GPUS"/>
<node id="SERVING THROUGHPUT"/>
<node id="IMPROVING THE THROUGHPUT"/>
<node id="BATCHING MULTIPLE REQUESTS TOGETHER"/>
<node id="MEMORY SPACE FOR EACH REQUEST"/>
<node id="EFFICIENTLY MANAGED"/>
<node id="APPROXIMATELY 65 OF THE MEMORY"/>
<node id="THE MODEL WEIGHTS"/>
<node id="STATIC DURING SERVING"/>
<node id="CLOSE TO 30 OF THE MEMORY"/>
<node id="THE DYNAMIC STATES OF THE REQUESTS"/>
<node id="STATES"/>
<node id="KEY AND VALUE TENSORS"/>
<node id="ATTENTION MECHANISM"/>
<node id="KV CACHE 41"/>
<node id="CONTEXT FROM EARLIER TOKENS"/>
<node id="NEW OUTPUT TOKENS IN SEQUENCE"/>
<node id="THE REMAINING SMALL EQUAL CONTRIBUTION"/>
<node id="UNKNOWN"/>
<node id="ORCA (MAX)"/>
<node id="20.4"/>
<node id="ORCA (POW2)"/>
<node id="13.3"/>
<node id="ORCA (ORACLE)"/>
<node id="57.3"/>
<node id="8.9"/>
<node id="26.8"/>
<node id="17.9"/>
<node id="13.6"/>
<node id="41.6"/>
<node id="38.2"/>
<node id="25.2"/>
<node id="36.6"/>
<node id="96.3"/>
<node id="AVERAGE PERCENTAGE OF MEMORY"/>
<node id="DIFFERENT LLM SERVING SYSTEMS"/>
<node id="EXPERIMENT"/>
<node id="6.2"/>
<node id="CONTRIBUTIONS"/>
<node id="CHALLENGES IN MEMORY ALLOCATION IN SERVING LLMS"/>
<node id="IMPACT ON SERVING PERFORMANCE"/>
<node id="PERCENTAGE OF MEMORY"/>
<node id="OTHER DATA"/>
<node id="ACTIVATIONS"/>
<node id="EPHEMERAL TENSORS"/>
<node id="EVALUATING THE LLM"/>
<node id="MODEL WEIGHTS"/>
<node id="CONSTANT"/>
<node id="A SMALL FRACTION OF THE GPU MEMORY"/>
<node id="THE WAY THE KV CACHE IS MANAGED"/>
<node id="CRITICAL IN DETERMINING THE MAXIMUM BATCH SIZE"/>
<node id="THROUGHPUT OF THE LLM"/>
<node id="LIMITATION OF BATCH SIZE AND THROUGHPUT"/>
<node id="INEFFICIENT MANAGEMENT OF KV CACHE MEMORY"/>
<node id="FINE-GRAINED BATCHING"/>
<node id="THE WASTE OF COMPUTING"/>
<node id="REQUESTS TO BE BATCHED IN A MORE FLEXIBLE WAY"/>
<node id="THE NUMBER OF REQUESTS THAT CAN BE BATCHED TOGETHER"/>
<node id="GPU MEMORY CAPACITY"/>
<node id="THE SPACE TO STORE THE KV CACHE"/>
<node id="THE IDEA OF VIRTUAL MEMORY AND PAGING"/>
<node id="MANAGING THE KV CACHE IN LLM SERVING"/>
<node id="THE WORKLOAD"/>
<node id="DYNAMIC MEMORY ALLOCATION"/>
<node id="THE OUTPUT LENGTH"/>
<node id="NOT KNOWN A PRIORI"/>
<node id="ITS PERFORMANCE"/>
<node id="THE GPU MEMORY CAPACITY"/>
<node id="THE KV CACHE OF A REQUEST"/>
<node id="CONTIGUOUS MEMORY SPACE"/>
<node id="MOST DEEP LEARNING FRAMEWORKS 33, 39"/>
<node id="TENSORS TO BE STORED IN CONTIGUOUS MEMORY"/>
<node id="OPERATORS IN CURRENT DEEP LEARNING FRAMEWORKS"/>
<node id="PREVIOUS LLM SERVING SYSTEMS"/>
<node id="THE KV CACHE OF ONE REQUEST AS A CONTIGUOUS TENSOR ACROSS THE DIFFERENT POSITIONS"/>
<node id="UNIQUE CHARACTERISTICS"/>
<node id="OVER TIME"/>
<node id="MODEL"/>
<node id="NEW TOKENS"/>
<node id="LIFETIME AND LENGTH OF KV CACHE"/>
<node id="THE EXISTING SYSTEMS"/>
<node id="INTERNAL AND EXTERNAL MEMORY FRAGMENTATION"/>
<node id="THE REQUESTS ACTUAL LENGTH"/>
<node id="MUCH SHORTER THAN ITS MAXIMUM LENGTH"/>
<node id="PRE-ALLOCATION"/>
<node id="INEFFICIENT"/>
<node id="ENTIRE CHUNK"/>
<node id="REQUESTS LIFETIME"/>
<node id="OTHER SHORTER REQUESTS"/>
<node id="ANY PART OF THE CHUNK THAT IS CURRENTLY UNUSED"/>
<node id="ONLY 20.4 - 38.2 OF THE KV CACHE MEMORY"/>
<node id="THE ACTUAL TOKEN STATES"/>
<node id="KV CACHE OF ONE TOKEN"/>
<node id="ALL ITS PREVIOUS TOKENS"/>
<node id="THE KV CACHE OF THE SAME TOKEN"/>
<node id="DIFFERENT"/>
<node id="THE SAME TOKEN"/>
<node id="DIFFERENT POSITIONS IN A SEQUENCE"/>
<node id="THE TOKEN IN EACH MEMORY SLOT"/>
<node id="ITS KV CACHE"/>
<node id="THE SAME TOKENS"/>
<node id="DIFFERENT KV CACHE"/>
<node id="AT DIFFERENT POSITIONS"/>
<node id="THE OPPORTUNITIES FOR MEMORY SHARING"/>
<node id="LLM SERVICES"/>
<node id="ADVANCED DECODING ALGORITHMS"/>
<node id="PARALLEL SAMPLING"/>
<node id="BEAM SEARCH"/>
<node id="MULTIPLE OUTPUTS PER REQUEST"/>
<node id="A RANGE OF DECODING ALGORITHMS"/>
<node id="DECODING ALGORITHMS"/>
<node id="USERS TO SELECT FROM"/>
<node id="VARYING IMPLICATIONS FOR MEMORY MANAGEMENT COMPLEXITY"/>
<node id="LLM SERVICE"/>
<node id="MORE COMPLEX DECODING SCENARIOS"/>
<node id="COMPLEX ACCESSING PATTERNS"/>
<node id="MORE OPPORTUNITIES FOR MEMORY SHARING"/>
<node id="THE REQUEST"/>
<node id="MULTIPLE SEQUENCES"/>
<node id="THEIR KV CACHE"/>
<node id="OF TWO REQUESTS"/>
<node id="TWO REQUESTS"/>
<node id="AT THE SAME TIME"/>
<node id="IN VLLM"/>
<node id="REQUEST"/>
<node id="ITS GENERATION"/>
<node id="ITS KV BLOCKS"/>
<node id="THE KV CACHE OF OTHER REQUESTS"/>
<node id="MEMORY SHARING"/>
<node id="THE KV CACHE OF THE SEQUENCES"/>
<node id="SEPARATE CONTIGUOUS SPACES"/>
<node id="THE REQUESTS KV CACHE INTO BLOCKS"/>
<node id="EACH BLOCK"/>
<node id="THE ATTENTION KEYS AND VALUES OF A FIXED NUMBER OF TOKENS"/>
<node id="PAGEDATTENTION KERNEL"/>
<node id="DIFFERENT KV BLOCKS"/>
<node id="BLOCKS FOR THE KV CACHE"/>
<node id="CONTIGUOUS SPACE"/>
<node id="THE KV CACHE MANAGER"/>
<node id="THE KV CACHE"/>
<node id="A PAGED FASHION"/>
<node id="THE DESIGN OF THE KV CACHE MANAGER IN 4.2"/>
<node id="THE DESIGN OF THE KV CACHE MANAGER"/>
<node id="PAGEDATTENTION IN 4.3"/>
<node id="THE KV CACHE OF EACH SEQUENCE INTO KV BLOCKS"/>
<node id="THE KV CACHE AS FIXED-SIZE KV BLOCKS"/>
<node id="KV BLOCKS"/>
<node id="PAGES IN VIRTUAL MEMORY"/>
<node id="WE TO ORGANIZE THE KV CACHE"/>
<node id="THIS DESIGN"/>
<node id="INTERNAL FRAGMENTATION"/>
<node id="RELATIVELY SMALL BLOCKS"/>
<node id="THEM ON DEMAND"/>
<node id="ALL BLOCKS"/>
<node id="THE SAME SIZE"/>
<node id="THE DIFFERENT SEQUENCES ASSOCIATED WITH THE SAME REQUEST"/>
<node id="THE DIFFERENT REQUESTS"/>
<node id="BLOCK-LEVEL MEMORY MANAGEMENT"/>
<node id="PREEMPTIVE REQUEST SCHEDULING"/>
<node id="PAGEDATTENTION ALGORITHM"/>
<node id="KV BLOCKS TO BE STORED IN NON-CONTIGUOUS PHYSICAL MEMORY"/>
<node id="NON-CONTIGUOUS PHYSICAL MEMORY"/>
<node id="MORE FLEXIBLE PAGED MEMORY MANAGEMENT IN VLLM"/>
<node id="THE MEMORY DURING THE DECODING PROCESS OF A SINGLE INPUT SEQUENCE"/>
<node id="RESERVING THE MEMORY FOR THE MAXIMUM POSSIBLE GENERATED SEQUENCE LENGTH INITIALLY"/>
<node id="OSS VIRTUAL MEMORY"/>
<node id="PREVIOUS KV CACHE"/>
<node id="LOGICAL KV BLOCKS"/>
<node id="NEWLY GENERATED KV CACHE"/>
<node id="PHYSICAL KV BLOCKS"/>
<node id="THIS SHARING EASILY"/>
<node id="MEMORY"/>
<node id="PAGEDATTENTION AND PAGED MEMORY MANAGEMENT"/>
<node id="POPULAR LLMS SUCH AS GPT 5, OPT 62, AND LLAMA 52"/>
<node id="POPULAR LLMS"/>
<node id="GPT 5"/>
<node id="OPT 62"/>
<node id="LLAMA 52"/>
<node id="VARYING SIZES"/>
<node id="ONES EXCEEDING THE MEMORY CAPACITY OF A SINGLE GPU"/>
<node id="A DISTRIBUTED LLM SERVING ENGINE"/>
<node id="VLLM ON VARIOUS SCENARIOS"/>
<node id="PREVIOUS STATE-OF-THE-ART SOLUTIONS"/>
<node id="FASTERTRANSFORMER 31"/>
<node id="ORCA 60"/>
<node id="UP TO 22 HIGHER REQUEST RATES"/>
<node id="FASTERTRANSFORMER"/>
<node id="A FINE-GRAINED SCHEDULING MECHANISM"/>
<node id="THE MEMORY LIKE ORCA (MAX)"/>
<node id="MEMORY FRAGMENTATION"/>
<node id="SHARING"/>
<node id="MORE REQUESTS IN A BATCH IN PARALLEL"/>
<node id="2-4 SPEEDUP"/>
<node id="ORCA"/>
<node id="THIS SECTION"/>
<node id="THE GENERATION AND SERVING PROCEDURES OF TYPICAL LLMS"/>
<node id="THE ITERATION-LEVEL SCHEDULING USED IN LLM SERVING"/>
<node id="THE TASK OF LANGUAGE MODELING"/>
<node id="THE PROBABILITY OF A LIST OF TOKENS"/>
<node id="LANGUAGE"/>
<node id="A NATURAL SEQUENTIAL ORDERING"/>
<node id="TRANSFORMERS 53"/>
<node id="THE DE FACTO STANDARD ARCHITECTURE FOR MODELING THE PROBABILITY ABOVE AT A LARGE SCALE"/>
<node id="THE MOST IMPORTANT COMPONENT OF A TRANSFORMER-BASED LANGUAGE MODEL"/>
<node id="ITS SELF-ATTENTION LAYERS"/>
<node id="A SELF-ATTENTION LAYER"/>
<node id="LINEAR TRANSFORMATIONS ON EACH POSITION"/>
<node id="THE QUERY VECTOR"/>
<node id="THE KEY VECTOR"/>
<node id="THE VALUE VECTOR"/>
<node id="THE SELF-ATTENTION LAYER"/>
<node id="THE ATTENTION SCORE"/>
<node id="MULTIPLYING THE QUERY VECTOR AT ONE POSITION WITH ALL THE KEY VECTORS BEFORE IT"/>
<node id="THE OUTPUT"/>
<node id="THE WEIGHTED AVERAGE OVER THE VALUE VECTORS"/>
<node id="ALL OTHER COMPONENTS"/>
<node id="THE TRANSFORMER MODEL"/>
<node id="THE EMBEDDING LAYER"/>
<node id="FEED-FORWARD LAYER"/>
<node id="LAYER NORMALIZATION 2"/>
<node id="RESIDUAL CONNECTION 22"/>
<node id="OUTPUT LOGIT COMPUTATION"/>
<node id="THE QUERY TRANSFORMATION"/>
<node id="THE KEY TRANSFORMATION"/>
<node id="THE VALUE TRANSFORMATION"/>
<node id="CONDITIONAL GENERATION SERVICE"/>
<node id="COMPLETION API 34"/>
<node id="CHATBOT 19, 35"/>
<node id="A REQUEST TO AN LLM SERVICE"/>
<node id="A LIST OF INPUT PROMPT TOKENS"/>
<node id="THE CONCATENATION OF THE PROMPT AND OUTPUT LISTS AS SEQUENCE"/>
<node id="DECOMPOSITION"/>
<node id="IN EQ"/>
<node id="THE LLM"/>
<node id="NEW TOKENS ONE BY ONE"/>
<node id="THE GENERATION PROCESS OF EACH NEW TOKEN"/>
<node id="ALL THE PREVIOUS TOKENS IN THAT SEQUENCE"/>
<node id="KEY AND VALUE VECTORS"/>
<node id="KEY AND VALUE VECTORS OF EXISTING TOKENS"/>
<node id="GENERATING FUTURE TOKENS"/>
<node id="A REQUESTS KV CACHE"/>
<node id="A SERIES OF LOGICAL KV BLOCKS"/>
<node id="LEFT TO RIGHT"/>
<node id="NEW TOKENS AND THEIR KV CACHE ARE GENERATED"/>
<node id="GENERATION COMPUTATION IN THE LLM SERVICE"/>
<node id="TWO PHASES"/>
<node id="THE PROMPT PHASE"/>
<node id="THE WHOLE USER PROMPT"/>
<node id="REQUESTS AND THE LATEST TOKENS FOR GENERATION PHASE REQUESTS"/>
<node id="THE LLM AS ONE SEQUENCE"/>
<node id="THIS PROCESS"/>
<node id="THE KEY VECTORS 1"/>
<node id="THE COMPUTATION OF THE PROMPT PHASE"/>
<node id="MATRIX-MATRIX MULTIPLICATION OPERATIONS"/>
<node id="THIS PHASE"/>
<node id="PARALLELISM INHERENT IN GPUS"/>
<node id="THE AUTOREGRESSIVE GENERATION PHASE"/>
<node id="THE REMAINING NEW TOKENS SEQUENTIALLY"/>
<node id="THE MODEL"/>
<node id="ONE TOKEN AS INPUT"/>
<node id="KEY AND VALUE VECTORS AT POSITIONS 1 TO 1"/>
<node id="CACHED AT PREVIOUS ITERATIONS"/>
<node id="NEW KEY AND VALUE VECTOR"/>
<node id="COMPUTED AT THIS ITERATION"/>
<node id="WHEN THE SEQUENCE REACHES A MAXIMUM LENGTH"/>
<node id="MAXIMUM LENGTH"/>
<node id="USERS"/>
<node id="WHEN AN END-OF-SEQUENCE (EOS) TOKEN IS EMITTED"/>
<node id="THE COMPUTATION AT DIFFERENT ITERATIONS"/>
<node id="DUE TO THE DATA DEPENDENCY"/>
<node id="MATRIX-VECTOR MULTIPLICATION"/>
<node id="LESS EFFICIENT"/>
<node id="GPU COMPUTATION"/>
<node id="MEMORY-BOUND"/>
<node id="MOST PORTION OF THE LATENCY OF A SINGLE REQUEST"/>
<node id="COMPUTE UTILIZATION IN SERVING LLMS"/>
<node id="BATCHING MULTIPLE REQUESTS"/>
<node id="BATCHING THE REQUESTS TO AN LLM SERVICE"/>
<node id="NON-TRIVIAL"/>
<node id="OVERHEAD OF MOVING WEIGHTS"/>
<node id="REQUESTS IN A BATCH"/>
<node id="COMPUTATIONAL OVERHEAD"/>
<node id="SUFFICIENTLY LARGE"/>
<node id="DIFFERENT TIMES"/>
<node id="A NAIVE BATCHING STRATEGY"/>
<node id="EARLIER REQUESTS WAIT FOR LATER ONES"/>
<node id="THE INCOMING REQUESTS UNTIL EARLIER ONES FINISH"/>
<node id="SIGNIFICANT QUEUEING DELAYS"/>
<node id="VASTLY DIFFERENT INPUT AND OUTPUT LENGTHS"/>
<node id="A STRAIGHTFORWARD BATCHING TECHNIQUE"/>
<node id="THE INPUTS AND OUTPUTS OF THE REQUESTS"/>
<node id="EQUALIZE THEIR LENGTHS"/>
<node id="GPU COMPUTATION AND MEMORY"/>
<node id="FINE-GRAINED BATCHING MECHANISMS"/>
<node id="CELLULAR BATCHING 16"/>
<node id="ITERATION-LEVEL SCHEDULING 60"/>
<node id="PROPOSED"/>
<node id="THESE TECHNIQUES"/>
<node id="THE ITERATION LEVEL"/>
<node id="TRADITIONAL METHODS"/>
<node id="THE REQUEST LEVEL"/>
<node id="COMPLETED REQUESTS"/>
<node id="THE BATCH"/>
<node id="A NEW REQUEST"/>
<node id="WAITING FOR A SINGLE ITERATION"/>
<node id="WAITING FOR THE ENTIRE BATCH TO COMPLETE"/>
<node id="SPECIAL GPU KERNELS"/>
<node id="THE NEED TO PAD THE INPUTS AND OUTPUTS"/>
<node id="QUEUEING DELAY"/>
<node id="INEFFICIENCIES FROM PADDING"/>
<node id="THROUGHPUT OF LLM SERVING"/>
<node id="THREE TYPES OF MEMORY WASTES"/>
<node id="RESERVED, INTERNAL FRAGMENTATION, AND EXTERNAL FRAGMENTATION"/>
<node id="THAT PREVENT OTHER REQUESTS FROM FITTING INTO THE MEMORY"/>
<node id="THE SERVING SYSTEMS THROUGHPUT"/>
<node id="THE PERFORMANCE OF THE SYSTEMS"/>
<node id="COMPUTE-BOUND RATHER THAN MEMORY-BOUND"/>
<node id="OVERCOMING THIS MEMORY-BOUND"/>
<node id="ADDRESSING THE FOLLOWING CHALLENGES IN THE MEMORY MANAGEMENT"/>
<node id="CHALLENGES"/>
<node id="LARGE KV CACHE"/>
<node id="OPT"/>
<node id="SEQUENCES UP TO 2048 TOKENS"/>
<node id="MEMORY REQUIRED TO STORE THE KV CACHE OF ONE REQUEST"/>
<node id="1.6 GB"/>
<node id="CONCURRENT GPUS"/>
<node id="MEMORY CAPACITIES IN THE TENS OF GBS"/>
<node id="GPU'S COMPUTATION SPEED"/>
<node id="MEMORY CAPACITY"/>
<node id="FLOPS"/>
<node id="NVIDIA A100 TO H100"/>
<node id="MORE THAN 2X"/>
<node id="80GB MAXIMUM"/>
<node id="AN INCREASINGLY SIGNIFICANT BOTTLENECK"/>
<node id="COMPLEX DECODING ALGORITHMS"/>
<node id="ALGORITHMS"/>
<node id="MULTIPLE RANDOM SAMPLES FROM A SINGLE INPUT PROMPT"/>
<node id="KV CACHE OF THE PROMPT PART"/>
<node id="12 OF THE TOTAL KV CACHE MEMORY IN OUR EXPERIMENT"/>
<node id="MINIMIZE MEMORY USAGE"/>
<node id="UNSHARED DURING THE AUTOREGRESSIVE GENERATION PHASE"/>
<node id="DIFFERENT SAMPLE RESULTS"/>
<node id="CONTEXT AND POSITION"/>
<node id="THE EXTENT OF KV CACHE SHARING"/>
<node id="THE SPECIFIC DECODING ALGORITHM EMPLOYED"/>
<node id="DIFFERENT REQUEST BEAMS"/>
<node id="LARGER PORTIONS OF THEIR KV CACHE"/>
<node id="LARGER PORTIONS"/>
<node id="55 MEMORY SAVING"/>
<node id="SHARING PATTERN"/>
<node id="THE DECODING PROCESS ADVANCES"/>
<node id="BEAM SEARCH 49"/>
<node id="MORE SOPHISTICATED ALGORITHMS"/>
<node id="SCHEDULING"/>
<node id="UNKNOWN INPUT OUTPUT LENGTHS"/>
<node id="THE REQUESTS TO AN LLM SERVICE"/>
<node id="VARIABILITY IN THEIR INPUT AND OUTPUT LENGTHS"/>
<node id="A UNIQUE CHALLENGE"/>
<node id="INPUT PROMPTS FOR AN LLM"/>
<node id="SIGNIFICANTLY IN LENGTH"/>
<node id="RESULTING OUTPUT LENGTHS"/>
<node id="A PRIORI"/>
<node id="BOTH THE INPUT PROMPT AND THE MODEL"/>
<node id="THE MEMORY MANAGEMENT SYSTEM"/>
<node id="A WIDE RANGE OF PROMPT LENGTHS"/>
<node id="THE SYSTEM"/>
<node id="SCHEDULING DECISIONS"/>
<node id="DELETING OR SWAPPING OUT THE KV CACHE OF SOME REQUESTS FROM GPU MEMORY"/>
<node id="THE ALLOCATION"/>
<node id="THE ACTUAL INPUT OR EVENTUAL OUTPUT LENGTH OF THE REQUEST"/>
<node id="REQUEST A"/>
<node id="2048"/>
<node id="REQUEST B"/>
<node id="512"/>
<node id="THE EXTERNAL FRAGMENTATION"/>
<node id="GENERATED TOKENS"/>
<node id="BEFORE SERVING A REQUEST"/>
<node id="UNUSED"/>
<node id="RESERVING THIS SPACE"/>
<node id="THE SPACE THAT COULD OTHERWISE BE USED TO PROCESS OTHER REQUESTS"/>
<node id="THE RESERVED MEMORY"/>
<node id="EVENTUALLY"/>
<node id="THE ENTIRE REQUESTS DURATION"/>
<node id="THE RESERVED SPACE"/>
<node id="LARGE"/>
<node id="THE AVERAGE PERCENTAGE OF MEMORY WASTES IN OUR EXPERIMENTS IN FIG."/>
<node id="THE ACTUAL EFFECTIVE MEMORY IN PREVIOUS SYSTEMS"/>
<node id="THE ARCHITECTURE OF VLLM"/>
<node id="FIG."/>
<node id="SYSTEM DESIGN OF VLLM"/>
<node id="DISTRIBUTED SETTING"/>
<node id="THE GENERAL APPLICABILITY OF VLLM ON THEM"/>
<node id="COMPACTION 54"/>
<node id="A POTENTIAL SOLUTION TO FRAGMENTATION"/>
<node id="PERFORMING COMPACTION IN A PERFORMANCE-SENSITIVE LLM SERVING SYSTEM"/>
<node id="IMPRATICAL DUE TO THE MASSIVE KV CACHE"/>
<node id="THE PRE-ALLOCATED CHUNK SPACE FOR EACH REQUEST"/>
<node id="MEMORY SHARING SPECIFIC TO DECODING ALGORITHMS IN EXISTING MEMORY MANAGEMENT SYSTEMS"/>
<node id="A NEW ATTENTION ALGORITHM PAGE-DATTENTION"/>
<node id="AN LLM SERVING ENGINE VLLM"/>
<node id="THE CHALLENGES OUTLINED IN 3"/>
<node id="A CENTRALIZED SCHEDULER"/>
<node id="THE EXECUTION OF DISTRIBUTED GPU WORKERS"/>
<node id="THE PHYSICAL KV CACHE MEMORY ON THE GPU WORKERS"/>
<node id="THE INSTRUCTIONS SENT BY THE CENTRALIZED SCHEDULER"/>
<node id="EACH GPU WORKER"/>
<node id="THE SAME PHYSICAL BLOCK IDS"/>
<node id="A WORKER"/>
<node id="A PORTION OF THE KV CACHE FOR ITS CORRESPONDING ATTENTION HEADS"/>
<node id="GPU WORKERS"/>
<node id="BLOCK TABLE"/>
<node id="CONTROL MESSAGE"/>
<node id="THE PAGEDATTENTION ALGORITHM"/>
<node id="4.1"/>
<node id="AN EXAMPLE OF PAGEDATTENTION IN FIG."/>
<node id="EFFECTIVE MEMORY MANAGEMENT FOR VARIOUS DECODING METHODS"/>
<node id="VARIABLE LENGTH INPUT AND OUTPUT SEQUENCES"/>
<node id="THE KEY AND VALUE VECTORS FOR A FIXED NUMBER OF TOKENS"/>
<node id="THE FIXED NUMBER OF TOKENS"/>
<node id="KV 1 IN TRANSFORMER"/>
<node id="EACH TOKEN"/>
<node id="A SET OF KEY AND VALUE VECTORS ACROSS LAYERS AND ATTENTION HEADS WITHIN A LAYER"/>
<node id="ALL THE KEY AND VALUE VECTORS"/>
<node id="TOGETHER WITHIN A SINGLE KV BLOCK"/>
<node id="THE KEY AND VALUE VECTORS AT DIFFERENT HEADS AND LAYERS"/>
<node id="A SEPARATE BLOCK"/>
<node id="IN SEPARATE BLOCK TABLES"/>
<node id="THE TWO DESIGNS"/>
<node id="NO PERFORMANCE DIFFERENCE"/>
<node id="THE SECOND ONE"/>
<node id="EASY IMPLEMENTATION"/>
<node id="OUR FATHERS"/>
<node id="FOUR SCORE AND SEVEN KEY AND VALUE VECTORS"/>
<node id="THE EFFECT OF BLOCK SIZE IN 7.2"/>
<node id="KEY BLOCK"/>
<node id="((1)1"/>
<node id="THE ATTENTION COMPUTATION"/>
<node id="EQ."/>
<node id="4"/>
<node id="THE FOLLOWING BLOCK-WISE COMPUTATION"/>
<node id="ROW VECTOR"/>
<node id="ATTENTION SCORE ON -TH KV BLOCK"/>
<node id="THE KEY AND VALUE VECTORS"/>
<node id="THREE BLOCKS"/>
<node id="THE THREE BLOCKS"/>
<node id="THE PHYSICAL MEMORY"/>
<node id="THE KERNEL"/>
<node id="THE QUERY VECTOR OF THE QUERY TOKEN (FORTH) AND THE KEY VECTORS IN A BLOCK"/>
<node id="THE VALUE VECTORS IN A BLOCK"/>
<node id="THE FINAL ATTENTION OUTPUT"/>
<node id="OS"/>
<node id="MEMORY INTO FIXED-SIZED PAGES"/>
<node id="USER PROGRAMS LOGICAL PAGES TO PHYSICAL PAGES"/>
<node id="CONTIGUOUS LOGICAL PAGES"/>
<node id="NON-CONTIGUOUS PHYSICAL MEMORY PAGES"/>
<node id="USER PROGRAMS"/>
<node id="MEMORY AS THOUGH IT WERE CONTIGUOUS"/>
<node id="PHYSICAL MEMORY SPACE"/>
<node id="FULLY RESERVED IN ADVANCE"/>
<node id="PHYSICAL PAGES DYNAMICALLY"/>
<node id="THE LAST KV BLOCKS UNFILLED POSITIONS"/>
<node id="FUTURE GENERATIONS"/>
<node id="THE KV BLOCK MANAGER"/>
<node id="BLOCK TABLES"/>
<node id="THE MAPPING BETWEEN LOGICAL AND PHYSICAL KV BLOCKS OF EACH REQUEST"/>
<node id="EACH BLOCK TABLE ENTRY"/>
<node id="THE CORRESPONDING PHYSICAL BLOCKS OF A LOGICAL BLOCK"/>
<node id="THE NUMBER OF FILLED POSITIONS"/>
<node id="SEPARATING LOGICAL AND PHYSICAL KV BLOCKS"/>
<node id="VLLM TO DYNAMICALLY GROW THE KV CACHE MEMORY WITHOUT RESERVING IT FOR ALL POSITIONS IN ADVANCE"/>
<node id="MOST MEMORY WASTE IN EXISTING SYSTEMS"/>
<node id="ALL THE BLOCKS"/>
<node id="A NEW PHYSICAL BLOCK"/>
<node id="ALL PREVIOUS BLOCKS ARE FULL"/>
<node id="ALL THE MEMORY WASTES FOR A REQUEST WITHIN ONE BLOCK"/>
<node id="ALL THE MEMORY"/>
<node id="THE SHARING OF MOST OF THE SPACE USED TO STORE THE PROMPTS KV CACHE ACROSS MULTIPLE OUTPUT SAMPLES"/>
<node id="THE FINAL LOGICAL BLOCK"/>
<node id="A COPY-ON-WRITE MECHANISM"/>
<node id="FREQUENT MEMORY COPY OVERHEAD"/>
<node id="VLLMS PHYSICAL BLOCK SHARING"/>
<node id="THE COMPLEX MEMORY SHARING BETWEEN DIFFERENT SEQUENCES"/>
<node id="A COMMON MAPPING LAYER"/>
<node id="LOGICAL BLOCKS TO PHYSICAL BLOCKS"/>
<node id="INTRODUCING THE VLLMS TECHNIQUES"/>
<node id="THE PERFORMANCE"/>
<node id="THE DEGRADATION"/>
<node id="THE EXTRA OVERHEAD OF MEMORY INDIRECTION AND NON-CONTIGUOUS BLOCK MEMORY"/>
<node id="PAGEDATTENTION AND VLLM"/>
<node id="EXAMPLE"/>
<node id="IN FIG"/>
<node id="THE NEW TOKEN"/>
<node id="PHYSICAL BLOCKS 7 AND 1"/>
<node id="THE FIRST AUTOREGRESSIVE DECODING STEP"/>
<node id="VLLM GENERATING THE NEW TOKEN"/>
<node id="4.3"/>
<node id="HOW PAGEDATTENTION AND VLLM HANDLE BASIC DECODING ALGORITHMS"/>
<node id="BASIC DECODING ALGORITHMS"/>
<node id="GREEDY DECODING AND SAMPLING"/>
<node id="ONE USER PROMPT AS INPUT"/>
<node id="A SINGLE OUTPUT SEQUENCE"/>
<node id="THE NECESSARY KV BLOCKS"/>
<node id="THE KV CACHE GENERATED DURING PROMPT COMPUTATION"/>
<node id="THE PROMPT"/>
<node id="7 TOKENS"/>
<node id="THE FIRST 2 LOGICAL KV BLOCKS (0 AND 1) TO 2 PHYSICAL KV BLOCKS (7 AND 1, RESPECTIVELY)"/>
<node id="KV CACHE OF THE PROMPTS"/>
<node id="FIRST OUTPUT TOKEN"/>
<node id="CONVENTIONAL SELF-ATTENTION ALGORITHM"/>
<node id="THE KV CACHE OF THE FIRST 4 TOKENS IN LOGICAL BLOCK 0"/>
<node id="THE FOLLOWING 3 TOKENS IN LOGICAL BLOCK 1"/>
<node id="NEW PHYSICAL BLOCKS TO LOGICAL BLOCKS"/>
<node id="TOKENS AND THEIR KV CACHE"/>
<node id="MORE"/>
<node id="FREE PHYSICAL BLOCKS FOR NEW TOKENS"/>
<node id="A SET OF SEQUENCES TO EVICT"/>
<node id="THEIR KV CACHE TO THE CPU"/>
<node id="THE REMAINING SLOT"/>
<node id="THE SUBSEQUENT AUTOREGRESSIVE GENERATION PHASE"/>
<node id="ONE SLOT"/>
<node id="AVAILABLE IN THE LAST LOGICAL BLOCK"/>
<node id="THE NEWLY GENERATED KV CACHE"/>
<node id="THERE"/>
<node id="A SET OF CANDIDATE SEQUENCES FOR BATCHING"/>
<node id="THE PHYSICAL BLOCKS FOR THE NEWLY REQUIRED LOGICAL BLOCKS"/>
<node id="STORING MULTIPLE TOKENS WITHIN A KV BLOCK (BLOCK SIZE 1)"/>
<node id="THE PAGEDATTENTION KERNEL TO PROCESS THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL"/>
<node id="PROCESSING THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL"/>
<node id="THE HARDWARE UTILIZATION"/>
<node id="LATENCY"/>
<node id="A LARGER BLOCK SIZE"/>
<node id="THE MEMORY FOR TWO SEQUENCES"/>
<node id="THE LOGICAL BLOCKS OF THE TWO SEQUENCES"/>
<node id="DIFFERENT PHYSICAL BLOCKS"/>
<node id="THE SPACE RESERVED BY THE BLOCK ENGINE IN GPU WORKERS"/>
<node id="THE NEIGHBORING LOGICAL BLOCKS OF BOTH SEQUENCES"/>
<node id="CONTIGUOUS IN PHYSICAL GPU MEMORY"/>
<node id="THE SPACE OF PHYSICAL BLOCKS"/>
<node id="BOTH SEQUENCES"/>
<node id="AN LLM"/>
<node id="MULTIPLE SAMPLED OUTPUTS FOR A SINGLE INPUT PROMPT"/>
<node id="A FAVORITE OUTPUT FROM VARIOUS CANDIDATES"/>
<node id="GENERATES"/>
<node id="A SINGLE SEQUENCE"/>
<node id="THE MORE GENERAL CASE"/>
<node id="A REQUEST"/>
<node id="ONE REQUEST"/>
<node id="MULTIPLE SAMPLES"/>
<node id="THE SAME INPUT PROMPT"/>
<node id="THE KV CACHE OF THE PROMPT"/>
<node id="SHARED"/>
<node id="ALL PARALLEL SEQUENCES IN A REQUEST"/>
<node id="THE KV CACHE FOR THE PROMPT"/>
<node id="8"/>
<node id="AN EXAMPLE OF PARALLEL DECODING FOR TWO OUTPUTS"/>
<node id="OUTPUTS"/>
<node id="THE SAME PROMPT"/>
<node id="ONE COPY OF THE PROMPTS STATE AT THE PROMPT PHASE"/>
<node id="THE LOGICAL BLOCKS FOR THE PROMPTS OF BOTH SEQUENCES"/>
<node id="THE SAME PHYSICAL BLOCKS"/>
<node id="THE LOGICAL BLOCK 0 AND 1 OF BOTH SEQUENCES"/>
<node id="A SINGLE PHYSICAL BLOCK"/>
<node id="MULTIPLE LOGICAL BLOCKS"/>
<node id="A REFERENCE COUNT FOR EACH PHYSICAL BLOCK"/>
<node id="REFERENCE COUNTS FOR PHYSICAL BLOCK 7"/>
<node id="2"/>
<node id="THE TWO OUTPUTS"/>
<node id="DIFFERENT OUTPUT TOKENS"/>
<node id="SEPARATE STORAGE FOR KV CACHE"/>
<node id="AT THE GENERATION PHASE"/>
<node id="THE TWO OUTPUTS SAMPLE DIFFERENT OUTPUT TOKENS"/>
<node id="THE TWO OUTPUTS NEED SEPARATE STORAGE FOR KV CACHE"/>
<node id="SAMPLE A2"/>
<node id="PHYSICAL BLOCK 1"/>
<node id="REFERENCE COUNT"/>
<node id="1"/>
<node id="A2"/>
<node id="NEWLY GENERATED KV CACHE TO PHYSICAL BLOCK 1"/>
<node id="SHARING PHYSICAL BLOCKS ACROSS MULTIPLE SAMPLES"/>
<node id="GREATLY REDUCED"/>
<node id="ESPECIALLY FOR LONG INPUT PROMPTS"/>
<node id="TOP-MOST APPROPRIATE TRANSLATIONS OUTPUT BY THE LLM"/>
<node id="LLM TASKS LIKE MACHINE TRANSLATION 59"/>
<node id="MACHINE TRANSLATION 59"/>
<node id="EACH CANDIDATE SEQUENCE IN THE BEAM"/>
<node id="ALL POSSIBLE TOKENS"/>
<node id="THEIR RESPECTIVE PROBABILITIES USING THE LLM"/>
<node id="THE TOP-MOST PROBABLE SEQUENCES"/>
<node id="THE ALGORITHM"/>
<node id="THE BEAM WIDTH PARAMETER"/>
<node id="THE NUMBER OF TOP CANDIDATES RETAINED AT EVERY STEP"/>
<node id="SHARING NOT ONLY THE INITIAL PROMPT BLOCKS BUT ALSO OTHER BLOCKS ACROSS DIFFERENT CANDIDATES"/>
<node id="SHARING PATTERNS"/>
<node id="AS THE DECODING PROCESS ADVANCES"/>
<node id="THE PROCESS TREE IN THE OS CREATED BY COMPOUND FORKS"/>
<node id="THE KV BLOCKS"/>
<node id="A BEAM SEARCH EXAMPLE"/>
<node id="EACH CANDIDATE SEQUENCE"/>
<node id="4 FULL LOGICAL BLOCKS"/>
<node id="THE ITERATION ILLUSTRATED AS THE DOTTED LINE"/>
<node id="EACH CANDIDATE SEQUENCE USING 4 FULL LOGICAL BLOCKS"/>
<node id="ALL BEAM CANDIDATES"/>
<node id="THE FIRST BLOCK 0 (I.E., PROMPT)"/>
<node id="CANDIDATE 3"/>
<node id="OTHERS FROM THE SECOND BLOCK"/>
<node id="CANDIDATES 0-2"/>
<node id="THE FIRST 3 BLOCKS"/>
<node id="AT THE FOURTH BLOCK"/>
<node id="ALL CANDIDATES"/>
<node id="BLOCKS 0, 1, 3"/>
<node id="CANDIDATES 0 AND 1"/>
<node id="BLOCK 6"/>
<node id="ORIGINAL CANDIDATES 0 AND 3"/>
<node id="TOP CANDIDATES"/>
<node id="THEIR LOGICAL BLOCKS"/>
<node id="FREED"/>
<node id="REFERENCE COUNTS OF CORRESPONDING PHYSICAL BLOCKS"/>
<node id="REDUCED"/>
<node id="ALL PHYSICAL BLOCKS WHOSE REFERENCE COUNTS REACH 0"/>
<node id="2, 4, 5, 8"/>
<node id="NEW PHYSICAL BLOCKS (BLOCKS 9-12)"/>
<node id="THE NEW KV CACHE"/>
<node id="THE NEW CANDIDATES"/>
<node id="FREQUENT MEMORY COPIES OF THE KV CACHE ACROSS THE BEAM CANDIDATES"/>
<node id="A LARGE PORTION OF CANDIDATE 2S KV CACHE"/>
<node id="A LARGE PORTION OF CANDIDATE 2S KV CACHE TO CONTINUE GENERATION"/>
<node id="MOST BLOCKS OF DIFFERENT BEAM CANDIDATES"/>
<node id="MOST BLOCKS"/>
<node id="DIFFERENT BEAM CANDIDATES"/>
<node id="THE SAME STRATEGY"/>
<node id="PREFIX SHARING"/>
<node id="THE COPY-ON-WRITE MECHANISM"/>
<node id="THE NEWLY GENERATED TOKENS ARE WITHIN AN OLD SHARED BLOCK"/>
<node id="PARALLEL DECODING"/>
<node id="LLM USER"/>
<node id="DESCRIPTION OF THE TASK"/>
<node id="INSTRUCTIONS"/>
<node id="EXAMPLE INPUTS AND OUTPUTS"/>
<node id="SYSTEM PROMPT 36"/>
<node id="THE DESCRIPTION"/>
<node id="THE ACTUAL TASK INPUT"/>
<node id="THE DESCRIPTION AND THE ACTUAL TASK INPUT"/>
<node id="THE PROMPT OF THE REQUEST"/>
<node id="SHARED PROMPT EXAMPLE"/>
<node id="MACHINE TRANSLATION"/>
<node id="THE EXAMPLES"/>
<node id="5"/>
<node id="10"/>
<node id="AN EXAMPLE"/>
<node id="SHARED PREFIX"/>
<node id="PROMPT ENGINEERING"/>
<node id="TUNING SHARED PREFIX"/>
<node id="ACCURACY OF DOWNSTREAM TASKS"/>
<node id="MANY USER PROMPTS"/>
<node id="A PREFIX"/>
<node id="THE LLM SERVICE PROVIDER"/>
<node id="THE KV CACHE OF THE PREFIX"/>
<node id="STORING THE KV CACHE OF THE PREFIX"/>
<node id="THE REDUNDANT COMPUTATION SPENT ON THE PREFIX"/>
<node id="RESERVING A SET OF PHYSICAL BLOCKS FOR A SET OF PREDEFINED SHARED PREFIXES BY THE LLM SERVICE PROVIDER"/>
<node id="SHARED LIBRARY ACROSS PROCESSES"/>
<node id="A USER INPUT PROMPT WITH THE SHARED PREFIX"/>
<node id="ITS LOGICAL BLOCKS TO THE CACHED PHYSICAL BLOCKS"/>
<node id="THE LAST BLOCK"/>
<node id="THE PROMPT PHASE COMPUTATION"/>
<node id="THE USERS TASK INPUT"/>
<node id="THE DECODING METHODS DISCUSSED EARLIER"/>
<node id="DIVERSE MEMORY SHARING AND ACCESSING PATTERNS"/>
<node id="SIMULTANEOUS PROCESSING OF REQUESTS WITH DIFFERENT DECODING PREFERENCES"/>
<node id="SIMULTANEOUS PROCESSING OF REQUESTS WITH DIFFERENT DECODING PREFERENCES EFFICIENTLY"/>
<node id="THE LLM AND ITS EXECUTION KERNEL"/>
<node id="A LIST OF PHYSICAL BLOCK IDS FOR EACH SEQUENCE"/>
<node id="SHARING PATTERNS ACROSS SEQUENCES"/>
<node id="THIS APPROACH"/>
<node id="THE BATCHING OPPORTUNITIES FOR REQUESTS WITH DIFFERENT SAMPLING REQUIREMENTS"/>
<node id="THE SYSTEMS OVERALL THROUGHPUT"/>
<node id="A SUBSET OF REQUESTS"/>
<node id="REQUEST TRAFFIC"/>
<node id="THE SYSTEMS CAPACITY"/>
<node id="EARLIEST ARRIVED REQUESTS ARE SERVED FIRST"/>
<node id="LATEST REQUESTS ARE PREEMPTED FIRST"/>
<node id="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY"/>
<node id="ALL REQUESTS"/>
<node id="FAIRNESS"/>
<node id="STARVATION"/>
<node id="THE GPUS PHYSICAL BLOCKS TO STORE THE NEWLY GENERATED KV CACHE"/>
<node id="BLOCK SIZE"/>
<node id="TOO SMALL"/>
<node id="GPUS PARALLELISM FOR READING AND PROCESSING KV CACHE"/>
<node id="TWO CLASSIC QUESTIONS"/>
<node id="WHICH BLOCKS SHOULD IT EVICT"/>
<node id="EVICTED BLOCKS"/>
<node id="IF NEEDED AGAIN"/>
<node id="TWO TECHNIQUES"/>
<node id="SWAPPING"/>
<node id="EVICTION POLICIES"/>
<node id="HEURISTICS"/>
<node id="WHICH BLOCK WILL BE ACCESSED FURTHEST IN THE FUTURE"/>
<node id="THAT BLOCK"/>
<node id="ALL BLOCKS OF A SEQUENCE"/>
<node id="TOGETHER"/>
<node id="AN ALL-OR-NOTHING EVICTION POLICY"/>
<node id="ALL-OR-NOTHING EVICTION POLICY"/>
<node id="EITHER EVICT ALL OR NONE OF THE BLOCKS OF A SEQUENCE"/>
<node id="MULTIPLE SEQUENCES WITHIN ONE REQUEST"/>
<node id="A SEQUENCE GROUP"/>
<node id="THE SEQUENCES WITHIN ONE SEQUENCE GROUP"/>
<node id="TRUE"/>
<node id="DUE TO POTENTIAL MEMORY SHARING ACROSS THOSE SEQUENCES"/>
<node id="CLASSIC TECHNIQUE"/>
<node id="MOST VIRTUAL MEMORY IMPLEMENTATIONS"/>
<node id="EVICTED PAGES TO A SWAP SPACE ON THE DISK"/>
<node id="EVICTED BLOCKS TO THE CPU MEMORY"/>
<node id="GPU BLOCK ALLOCATOR"/>
<node id="CPU BLOCK ALLOCATOR"/>
<node id="PHYSICAL BLOCKS SWAPPED TO CPU RAM"/>
<node id="A SEQUENCE"/>
<node id="ITS BLOCKS"/>
<node id="NEW REQUESTS"/>
<node id="ALL PREEMPTED SEQUENCES ARE COMPLETED"/>
<node id="ONCE"/>
<node id="THE BLOCKS OF A PREEMPTED SEQUENCE"/>
<node id="CONTINUE THE PROCESSING OF THAT SEQUENCE"/>
<node id="NUMBER OF BLOCKS SWAPPED TO THE CPU RAM"/>
<node id="NUMBER OF TOTAL PHYSICAL BLOCKS IN THE GPU RAM"/>
<node id="SWAP SPACE ON THE CPU RAM"/>
<node id="GPU MEMORY ALLOCATED FOR THE KV CACHE"/>
<node id="WHEN THE PREEMPTED SEQUENCES ARE RESCHEDULED"/>
<node id="RECOMPUTATION LATENCY"/>
<node id="SIGNIFICANTLY LOWER THAN THE ORIGINAL LATENCY"/>
<node id="TOKENS GENERATED AT DECODING"/>
<node id="THE ORIGINAL USER PROMPT AS A NEW PROMPT"/>
<node id="THEIR KV CACHE AT ALL POSITIONS"/>
<node id="ONE PROMPT PHASE ITERATION"/>
<node id="PERFORMANCES OF SWAPPING AND RECOMPUTATION"/>
<node id="BANDWIDTH BETWEEN CPU RAM AND GPU MEMORY"/>
<node id="COMPUTATION POWER OF THE GPU"/>
<node id="THE SPEEDS OF SWAPPING AND RECOMPUTATION IN 7.3"/>
<node id="RECOMPUTATION"/>
<node id="RECOMPUTATION AND SWAPPING"/>
<node id="RECOVERY MECHANISMS"/>
<node id="MANY LLMS"/>
<node id="PARAMETER SIZES EXCEEDING THE CAPACITY OF A SINGLE GPU"/>
<node id="DISTRIBUTED SETTINGS"/>
<node id="MEGATRON-LM STYLE TENSOR MODEL PARALLELISM STRATEGY"/>
<node id="TRANSFORMERS 47"/>
<node id="THIS STRATEGY"/>
<node id="AN SPMD (SINGLE PROGRAM MULTIPLE DATA) EXECUTION SCHEDULE"/>
<node id="THE LINEAR LAYERS"/>
<node id="618 TABLE 1"/>
<node id="MODEL SIZES"/>
<node id="SERVER CONFIGURATIONS"/>
<node id="THE DETAILED MODEL SIZES AND SERVER CONFIGURATIONS"/>
<node id="TABLE 1"/>
<node id="KV CACHE SLOTS"/>
<node id="15.7K 9.7K 60.1K"/>
<node id="GPUS"/>
<node id="INTERMEDIATE RESULTS"/>
<node id="AN ALL-REDUCE OPERATION"/>
<node id="BLOCK-WISE MATRIX MULTIPLICATION"/>
<node id="ATTENTION OPERATOR"/>
<node id="ATTENTION HEAD DIMENSION"/>
<node id="EACH SPMD PROCESS"/>
<node id="A SUBSET OF ATTENTION HEADS IN MULTI-HEAD ATTENTION"/>
<node id="MODEL SHARD"/>
<node id="THE SAME SET OF INPUT TOKENS"/>
<node id="THE KV CACHE FOR THE SAME POSITIONS"/>
<node id="MODEL PARALLEL EXECUTION"/>
<node id="EACH MODEL SHARD PROCESSING THE SAME SET OF INPUT TOKENS"/>
<node id="A SINGLE KV CACHE MANAGER"/>
<node id="KV CACHE MANAGER"/>
<node id="THE CENTRALIZED SCHEDULER"/>
<node id="DIFFERENT GPU WORKERS"/>
<node id="THE MANAGER"/>
<node id="THE MAPPING FROM LOGICAL BLOCKS TO PHYSICAL BLOCKS"/>
<node id="THIS COMMON MAPPING"/>
<node id="GPU WORKERS TO EXECUTE THE MODEL WITH THE PHYSICAL BLOCKS PROVIDED BY THE SCHEDULER FOR EACH INPUT REQUEST"/>
<node id="THE SCHEDULER"/>
<node id="THE MESSAGE WITH INPUT TOKEN IDS FOR EACH REQUEST IN THE BATCH"/>
<node id="THE BLOCK TABLE FOR EACH REQUEST"/>
<node id="THIS CONTROL MESSAGE TO THE GPU WORKERS"/>
<node id="SAMPLED TOKENS OF THIS ITERATION BACK TO THE SCHEDULER"/>
<node id="THE INPUT TOKEN IDS"/>
<node id="ALL-REDUCE COMMUNICATION PRIMITIVE"/>
<node id="SYNCHRONIZATION"/>
<node id="COORDINATION OF THE SCHEDULER"/>
<node id="MEMORY MANAGEMENT"/>
<node id="ALL THE MEMORY MANAGEMENT INFORMATION AT THE BEGINNING OF EACH DECODING ITERATION"/>
<node id="STEP INPUTS"/>
<node id="AN END-TO-END SERVING SYSTEM"/>
<node id="A FASTAPI 15 FRONTEND"/>
<node id="A GPU-BASED INFERENCE ENGINE"/>
<node id="THE FRONTEND"/>
<node id="THE OPENAI API 34 INTERFACE"/>
<node id="USERS TO CUSTOMIZE SAMPLING PARAMETERS FOR EACH REQUEST"/>
<node id="SAMPLING PARAMETERS"/>
<node id="THE MAXIMUM SEQUENCE LENGTH"/>
<node id="THE BEAM WIDTH"/>
<node id="THE VLLM ENGINE"/>
<node id="8.5K LINES OF PYTHON"/>
<node id="2K LINES OF CCUDA CODE"/>
<node id="CONTROL-RELATED COMPONENTS"/>
<node id="THE BLOCK MANAGER"/>
<node id="CUSTOM CUDA KERNELS"/>
<node id="KEY OPERATIONS"/>
<node id="INPUT AND OUTPUT LENGTH DISTRIBUTIONS"/>
<node id="THE (A) SHAREGPT DATASET"/>
<node id="THE (B) ALPACA DATASET"/>
<node id="THE SHAREGPT DATASET"/>
<node id="THE ALPACA DATASET"/>
<node id="PYTORCH"/>
<node id="39"/>
<node id="TRANSFORMERS"/>
<node id="58"/>
<node id="NCCL 32 FOR TENSOR COMMUNICATION ACROSS THE DISTRIBUTED GPU WORKERS"/>
<node id="MEMORY ACCESS PATTERNS"/>
<node id="SEVERAL GPU KERNELS"/>
<node id="OPTIMIZING PAGEDATTENTION"/>
<node id="THE DYNAMIC BLOCK MAPPING IN PAGEDATTENTION"/>
<node id="THE PERFORMANCE OF THE GPU OPERATIONS INVOLVING THE STORED KV CACHE"/>
<node id="THE GPU OPERATIONS"/>
<node id="THE STORED KV CACHE"/>
<node id="BLOCK READWRITES AND ATTENTION"/>
<node id="FUSED RE-SHAPE"/>
<node id="BLOCK WRITE"/>
<node id="FUSED BLOCK COPY"/>
<node id="3"/>
<node id="NEW KV CACHE"/>
<node id="A MEMORY LAYOUT OPTIMIZED FOR BLOCK READ"/>
<node id="POSITIONS SPECIFIED BY THE BLOCK TABLE"/>
<node id="THEM INTO A SINGLE KERNEL"/>
<node id="FUSING THEM INTO A SINGLE KERNEL"/>
<node id="KERNEL LAUNCH OVERHEADS"/>
<node id="A KERNEL"/>
<node id="THE COPY OPERATIONS FOR DIFFERENT BLOCKS"/>
<node id="A SINGLE KERNEL LAUNCH"/>
<node id="THE ATTENTION KERNEL IN FASTERTRANSFORMER 31"/>
<node id="KV CACHE ACCORDING TO THE BLOCK TABLE"/>
<node id="ATTENTION OPERATIONS ON THE FLY"/>
<node id="A GPU WARP TO READ EACH BLOCK"/>
<node id="SUPPORT FOR VARIABLE SEQUENCE LENGTHS WITHIN A REQUEST BATCH"/>
<node id="BLOCK COPY OPERATIONS"/>
<node id="DISCONTINUOUS BLOCKS"/>
<node id="THE CUDAMEMCPYASYNC API"/>
<node id="THE FORK METHOD"/>
<node id="A NEW SEQUENCE FROM AN EXISTING ONE"/>
<node id="THE APPEND METHOD"/>
<node id="A NEW TOKEN TO THE SEQUENCE"/>
<node id="THE FREE METHOD"/>
<node id="THE SEQUENCE"/>
<node id="MULTIPLE OUTPUT SEQUENCES"/>
<node id="THE SINGLE INPUT SEQUENCE"/>
<node id="FUTURE DECODING ALGORITHMS"/>
<node id="COMBINING THESE METHODS"/>
<node id="PARALLEL GENERATION"/>
<node id="MAX"/>
<node id="POW2"/>
<node id="ORACLE"/>
<node id="NORMALIZED LATENCY"/>
<node id="STOKEN"/>
<node id="REQUEST RATE"/>
<node id="REQS"/>
<node id="FIGURE"/>
<node id="14"/>
<node id="FIGURE 17"/>
<node id="REQUEST RATE (REQS)"/>
<node id="NORMALIZED LATENCY (STOKEN)"/>
<node id="SINGLE SEQUENCE GENERATION"/>
<node id="OPT MODELS"/>
<node id="SHAREGPT DATASET"/>
<node id="ALPACA DATASET"/>
<node id="OPT MODEL"/>
<node id="BATCHED REQUESTS"/>
<node id="0 5 10 15 20 25 30 35 FOR SHAREGPT"/>
<node id="0 25 50 75 100 125 150 FOR ALPACA"/>
<node id="7.00 FOR SHAREGPT"/>
<node id="9.81 FOR SHAREGPT"/>
<node id="13.62 FOR SHAREGPT"/>
<node id="30.42 FOR SHAREGPT"/>
<node id="43.24 FOR ALPACA"/>
<node id="72.75 FOR ALPACA"/>
<node id="132.44 FOR ALPACA"/>
<node id="AVERAGE NUMBER OF BATCHED REQUESTS"/>
<node id="WHEN SERVING OPT-13B FOR THE SHAREGPT (2 REQSS) AND ALPACA (30 REQSS) TRACES"/>
<node id="6.1"/>
<node id="EXPERIMENTAL SETUP MODEL AND SERVER CONFIGURATIONS"/>
<node id="OPT 62 MODELS WITH 13B PARAMETERS"/>
<node id="OPT 62 MODELS WITH 66B PARAMETERS"/>
<node id="OPT 62 MODELS WITH 175B PARAMETERS"/>
<node id="LLAMA 52 WITH 13B PARAMETERS"/>
<node id="OPT 62 MODELS"/>
<node id="13B PARAMETERS"/>
<node id="66B PARAMETERS"/>
<node id="175B PARAMETERS"/>
<node id="13B"/>
<node id="POPULAR SIZES FOR LLMS"/>
<node id="66B"/>
<node id="13B AND 66B"/>
<node id="LLM LEADERBOARD 38"/>
<node id="175B"/>
<node id="THE SIZE OF THE FAMOUS GPT-3 5 MODEL"/>
<node id="A2 INSTANCES WITH NVIDIA A100 GPUS ON GOOGLE CLOUD PLATFORM"/>
<node id="WORKLOADS BASED ON SHAREGPT 51 AND ALPACA 50 DATASETS"/>
<node id="SHAREGPT 51 AND ALPACA 50 DATASETS"/>
<node id="INPUT AND OUTPUT TEXTS OF REAL LLM SERVICES"/>
<node id="A COLLECTION OF USER-SHARED CONVERSATIONS WITH CHATGPT 35"/>
<node id="THE CHATTING HISTORY AND USER QUERY USING THE SHAREGPT DATASET"/>
<node id="THE DATASETS"/>
<node id="THEIR INPUT AND OUTPUT LENGTHS"/>
<node id="CLIENT REQUESTS"/>
<node id="REQUEST ARRIVAL TIMES"/>
<node id="POISSON DISTRIBUTION"/>
<node id="DIFFERENT REQUEST RATES"/>
<node id="THESE DATASETS"/>
<node id="TIMESTAMPS"/>
<node id="BASELINE 1"/>
<node id="A DISTRIBUTED INFERENCE ENGINE"/>
<node id="HIGHLY OPTIMIZED FOR LATENCY"/>
<node id="ITS OWN SCHEDULER"/>
<node id="A CUSTOM SCHEDULER"/>
<node id="CUSTOM SCHEDULER"/>
<node id="A DYNAMIC BATCHING MECHANISM"/>
<node id="DYNAMIC BATCHING MECHANISM"/>
<node id="EXISTING SERVING SYSTEMS SUCH AS TRITON 30"/>
<node id="A MAXIMUM BATCH SIZE AS LARGE AS POSSIBLE FOR EACH EXPERIMENT"/>
<node id="MAXIMUM BATCH SIZE"/>
<node id="EARLIEST ARRIVED REQUESTS"/>
<node id="THE BATCH TO FASTERTRANS-FORMER FOR PROCESSING"/>
<node id="BASELINE 2"/>
<node id="THE THREE ORCA BASELINES"/>
<node id="SIMILARLY"/>
<node id="STATE-OF-THE-ART LLM SERVING SYSTEM"/>
<node id="PUBLICLY AVAILABLE FOR USE"/>
<node id="OUR OWN VERSION OF ORCA"/>
<node id="BUDDY ALLOCATION ALGORITHM"/>
<node id="MEMORY ADDRESS TO STORE KV CACHE"/>
<node id="THE SYSTEM HAS THE KNOWLEDGE OF THE LENGTHS OF THE OUTPUTS THAT WILL BE ACTUALLY GENERATED FOR THE REQUESTS"/>
<node id="THE UPPER-BOUND PERFORMANCE OF ORCA"/>
<node id="INFEASIBLE TO ACHIEVE IN PRACTICE"/>
<node id="THE SPACE FOR OUTPUTS"/>
<node id="AT MOST 2"/>
<node id="TRUE OUTPUT LENGTH"/>
<node id="25"/>
<node id="NORMALIZED LATENCY OF THE SYSTEMS"/>
<node id="THE MEAN OF EVERY REQUESTS END-TO-END LATENCY DIVIDED BY ITS OUTPUT LENGTH"/>
<node id="USING THE WORKLOADS WITH DIFFERENT REQUEST RATES"/>
<node id="SPECIFICALLY"/>
<node id="A HIGH-THROUGHPUT SERVING SYSTEM"/>
<node id="LOW NORMALIZED LATENCY"/>
<node id="HIGH REQUEST RATES"/>
<node id="THE SYSTEMS WITH 1-HOUR TRACES"/>
<node id="15-MINUTE TRACES FOR THE OPT-175B MODEL"/>
<node id="15-MINUTE TRACES"/>
<node id="AS AN EXCEPTION"/>
<node id="DUE TO THE COST LIMIT"/>
<node id="PARALLEL GENERATION AND BEAM SEARCH"/>
<node id="OPT-13B"/>
<node id="THE PERFORMANCE OF VLLM WITH BASIC SAMPLING"/>
<node id="BASIC SAMPLING"/>
<node id="ONE SAMPLE PER REQUEST"/>
<node id="THREE MODELS AND TWO DATASETS"/>
<node id="12"/>
<node id="THE RESULTS ON THE SHAREGPT DATASET"/>
<node id="THE RESULTS ON THE ALPACA DATASET"/>
<node id="A SIMILAR TREND TO THE SHAREGPT DATASET"/>
<node id="THE CURVES"/>
<node id="AS THE REQUEST RATE INCREASES, THE LATENCY INITIALLY INCREASES AT A GRADUAL PACE BUT THEN SUDDENLY EXPLODES"/>
<node id="CAPACITY OF THE SERVING SYSTEM"/>
<node id="QUEUE LENGTH"/>
<node id="INFINITELY"/>
<node id="LATENCY OF THE REQUESTS"/>
<node id="SO"/>
<node id="1.72.7 HIGHER REQUEST RATES COMPARED TO ORCA (ORACLE)"/>
<node id="2.78 HIGHER REQUEST RATES COMPARED TO ORCA (MAX)"/>
<node id="SIMILAR LATENCIES"/>
<node id="13A, FOR OPT-13B VLLM"/>
<node id="2.2 MORE REQUESTS AT THE SAME TIME THAN ORCA (ORACLE)"/>
<node id="4.3 MORE REQUESTS THAN ORCA (MAX)"/>
<node id="3.58 HIGHER THROUGHPUT THAN ORCA (ORACLE)"/>
<node id="VLLMS PAGEDATTENTION"/>
<node id="MEMORY USAGE EFFICIENTLY"/>
<node id="BATCHING MORE REQUESTS THAN ORCA"/>
<node id="ONE EXCEPTION"/>
<node id="VLLMS"/>
<node id="ADVANTAGE OF VLLMS OVER ORCA (ORACLE) AND ORCA (POW2)"/>
<node id="LESS PRONOUNCED"/>
<node id="MODEL AND SERVER CONFIGURATION FOR OPT-175B"/>
<node id="LARGE GPU MEMORY SPACE AVAILABLE TO STORE KV CACHE"/>
<node id="SHORT SEQUENCES"/>
<node id="ORCA (ORACLE) AND ORCA (POW2)"/>
<node id="A LARGE NUMBER OF REQUESTS"/>
<node id="REQUESTS DESPITE THE INEFFICIENCIES IN THEIR MEMORY MANAGEMENT"/>
<node id="OUTPUT SEQUENCES"/>
<node id="2 4 6"/>
<node id="0 4 8 12"/>
<node id="MEMORY SAVING"/>
<node id="6.09"/>
<node id="8.53"/>
<node id="9.79"/>
<node id="BEAM WIDTH"/>
<node id="0 20 40 60"/>
<node id="37.56"/>
<node id="53.13"/>
<node id="55.16"/>
<node id="FIGURE 15"/>
<node id="AVERAGE AMOUNT OF MEMORY SAVING"/>
<node id="SHARING KV BLOCKS"/>
<node id="SERVING OPT-13B FOR THE ALPACA TRACE"/>
<node id="THE EFFECTIVENESS OF MEMORY SHARING IN PAGE-DATTENTION"/>
<node id="PARALLEL SAMPLING AND BEAM SEARCH"/>
<node id="TWO POPULAR SAMPLING METHODS"/>
<node id="6.1 - 9.8 MEMORY SAVING ON PARALLEL SAMPLING"/>
<node id="37.6 - 55.2 MEMORY SAVING ON BEAM SEARCH"/>
<node id="16.2 - 30.5"/>
<node id="44.3 - 66.3"/>
<node id="MORE IMPROVEMENT OVER THE ORCA BASELINES"/>
<node id="2 HIGHER REQUEST RATES"/>
<node id="RESULTS FOR BEAM SEARCH WITH DIFFERENT BEAM WIDTHS"/>
<node id="15"/>
<node id="THE AMOUNT OF MEMORY SAVING"/>
<node id="THE NUMBER OF BLOCKS WE SAVED BY SHARING DIVIDED BY THE NUMBER OF TOTAL BLOCKS WITHOUT SHARING"/>
<node id="INPUT PROMPTS"/>
<node id="A COMMON PREFIX"/>
<node id="THE PREFIX"/>
<node id="(A) 1 EXAMPLE WITH 80 TOKENS"/>
<node id="(B) 5 EXAMPLES WITH 341 TOKENS"/>
<node id="PERFORMANCE"/>
<node id="CHATBOT WORKLOAD"/>
<node id="LLAMA-13B 52"/>
<node id="MULTILINGUAL"/>
<node id="WMT16 4 ENGLISH-TO-GERMAN TRANSLATION DATASET"/>
<node id="TWO PREFIXES"/>
<node id="AN INSTRUCTION AND A FEW TRANSLATION EXAMPLES"/>
<node id="THE FIRST PREFIX"/>
<node id="A SINGLE EXAMPLE (I.E., ONE-SHOT)"/>
<node id="THE OTHER PREFIX"/>
<node id="5 EXAMPLES (I.E., FEW-SHOT)"/>
<node id="CHATBOT"/>
<node id="ONE OF THE MOST IMPORTANT APPLICATIONS OF LLMS"/>
<node id="THE MODEL GENERATE A RESPONSE"/>
<node id="A RESPONSE"/>
<node id="CONCATENATING THE CHATTING HISTORY AND THE LAST USER QUERY INTO A PROMPT"/>
<node id="THE CONTEXT LENGTH OF THE OPT-13B MODEL"/>
<node id="LIMITED"/>
<node id="THE PROMPT TO THE LAST 1024 TOKENS"/>
<node id="THE MODEL GENERATE AT MOST 1024 TOKENS"/>
<node id="THE KV CACHE BETWEEN DIFFERENT CONVERSATION ROUNDS"/>
<node id="DOING THIS"/>
<node id="THE SPACE FOR OTHER REQUESTS BETWEEN THE CONVERSATION ROUNDS"/>
<node id="MANY LONG CONVERSATIONS"/>
<node id="INPUT PROMPTS FOR MOST REQUESTS"/>
<node id="1024 TOKENS"/>
<node id="ORCA BASELINES"/>
<node id="SPACE FOR 1024 TOKENS FOR THE REQUEST OUTPUTS"/>
<node id="ORCA BASELINES TO RESERVE THE SPACE FOR 1024 TOKENS FOR THE REQUEST OUTPUTS"/>
<node id="REGARDLESS OF HOW THEY PREDICT THE OUTPUT LENGTHS"/>
<node id="64 128 256 CONTEXT LENGTH"/>
<node id="1 2 4 8 16 32 64 128 256"/>
<node id="0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5"/>
<node id="SHAREGPT ALPACA (B)"/>
<node id="END-TO-END LATENCY"/>
<node id="DIFFERENT BLOCK SIZES"/>
<node id="VARIOUS ASPECTS OF VLLM"/>
<node id="THE DESIGN CHOICES"/>
<node id="ABLATION EXPERIMENTS"/>
<node id="THE PROBLEM OF MEMORY FRAGMENTATION AND RESERVATION"/>
<node id="OUR GPU KERNELS (5)"/>
<node id="EXTRA OVERHEADS OF ACCESSING THE BLOCK TABLE"/>
<node id="EXECUTING EXTRA BRANCHES"/>
<node id="HANDLING VARIABLE SEQUENCE LENGTHS"/>
<node id="18A"/>
<node id="2026 HIGHER ATTENTION KERNEL LATENCY"/>
<node id="HIGHLY-OPTIMIZED FASTERTRANSFORMER IMPLEMENTATION"/>
<node id="THE OVERHEAD"/>
<node id="SMALL"/>
<node id="THE ATTENTION OPERATOR"/>
<node id="THE OTHER OPERATORS IN THE MODEL"/>
<node id="LINEAR"/>
<node id="VLLM SIGNIFICANTLY OUTPERFORM FASTERTRANSFORMER IN END-TO-END PERFORMANCE"/>
<node id="CHOICE OF BLOCK SIZE"/>
<node id="SUBSTANTIAL IMPACT ON THE PERFORMANCE OF VLLM"/>
<node id="THE PERFORMANCE OF VLLM WITH DIFFERENT BLOCK SIZES"/>
<node id="THE SHAREGPT AND ALPACA TRACES WITH BASIC SAMPLING"/>
<node id="THE PERFORMANCE OF VLLM"/>
<node id="FIXED REQUEST RATES"/>
<node id="BLOCK SIZES FROM 16 TO 128"/>
<node id="THE BEST PERFORMANCE"/>
<node id="BLOCK SIZE 16 AND 32"/>
<node id="WELL IN THE ALPACA TRACE"/>
<node id="LARGER BLOCK SIZES"/>
<node id="SEQUENCES"/>
<node id="SHORTER THAN THE BLOCK SIZES"/>
<node id="BLOCK SIZE 16"/>
<node id="EFFICIENTLY UTILIZE THE GPU"/>
<node id="AVOID SIGNIFICANT INTERNAL FRAGMENTATION IN MOST WORKLOADS"/>
<node id="DEFAULT BLOCK SIZE AS 16"/>
<node id="TIME"/>
<node id="MS"/>
<node id="MICROBENCHMARK"/>
<node id="RECOMPUTE SWAP IN SWAP OUT SWAP IN OUT"/>
<node id="19"/>
<node id="END-TO-END PERFORMANCE"/>
<node id="(B)"/>
<node id="(A)"/>
<node id="OVERHEAD"/>
<node id="THE OVERHEAD OF RECOMPUTATION"/>
<node id="CONSTANT ACROSS DIFFERENT BLOCK SIZES"/>
<node id="BLOCK SIZE IS SMALL"/>
<node id="BLOCK SIZE IS LARGE"/>
<node id="RECOMPUTATION OVERHEAD"/>
<node id="20 OF SWAPPINGS LATENCY"/>
<node id="OPT-13B WITH THE SHAREGPT TRACES AT THE SAME REQUEST RATE"/>
<node id="THEIR END-TO-END PERFORMANCE"/>
<node id="THEIR OVERHEADS"/>
<node id="EXCESSIVE OVERHEAD WITH SMALL BLOCK SIZES"/>
<node id="SMALL BLOCK SIZES"/>
<node id="NUMEROUS SMALL DATA TRANSFERS BETWEEN CPU AND GPU"/>
<node id="EFFECTIVE PCIE BANDWIDTH"/>
<node id="THE TWO METHODS"/>
<node id="COMPARABLE END-TO-END PERFORMANCE FOR MEDIUM BLOCK SIZES FROM 16 TO 64"/>
<node id="VIRTUAL MEMORY AND PAGING TECHNIQUE"/>
<node id="OTHER GPU WORKLOADS"/>
<node id="THE OVERHEAD OF MEMORY INDIRECTION IN PAGING"/>
<node id="THE GPU KERNELS FOR MEMORY ACCESS OPERATIONS WITH THOSE FOR OTHER OPERATIONS SUCH AS ATTENTION"/>
<node id="TENSOR SHAPES"/>
<node id="TYPICALLY STATIC"/>
<node id="MEMORY ALLOCATION"/>
<node id="OPTIMIZED AHEAD OF TIME"/>
<node id="AN INCREASE IN MEMORY EFFICIENCY"/>
<node id="ANY PERFORMANCE IMPROVEMENT"/>
<node id="PRIMARILY COMPUTE-BOUND"/>
<node id="VLLMS TECHNIQUES BEING APPLIED TO OTHER WORKLOADS WITH SIMILAR PROPERTIES TO LLM SERVING"/>
<node id="LLM-SPECIFIC OPTIMIZATIONS"/>
<node id="VIRTUAL MEMORY AND PAGING"/>
<node id="THE APPLICATION-SPECIFIC SEMANTICS"/>
<node id="VLLMS ALL-OR-NOTHING SWAP-OUT POLICY"/>
<node id="ONE EXAMPLE"/>
<node id="THE FACT THAT PROCESSING A REQUEST REQUIRES ALL OF ITS CORRESPONDING TOKEN STATES TO BE STORED IN GPU MEMORY"/>
<node id="RECOMPUTATION METHOD"/>
<node id="ANOTHER EXAMPLE"/>
<node id="FEASIBLE IN OS"/>
<node id="RELATED WORK"/>
<node id="GENERAL MODEL SERVING SYSTEMS"/>
<node id="MODEL SERVING"/>
<node id="AN ACTIVE AREA OF RESEARCH"/>
<node id="NUMEROUS SYSTEMS"/>
<node id="DIVERSE ASPECTS OF DEEP LEARNING MODEL DEPLOYMENT"/>
<node id="CLIPPER 11"/>
<node id="EARLIER GENERAL MODEL SERVING SYSTEMS"/>
<node id="TENSORFLOW SERVING 33"/>
<node id="NEXUS 45"/>
<node id="INFERLINE 10"/>
<node id="CLOCKWORK 20"/>
<node id="BATCH-ING"/>
<node id="SERVING SINGLE OR MULTIPLE MODELS"/>
<node id="CACHING"/>
<node id="PLACEMENT"/>
<node id="DVABATCH 12"/>
<node id="MULTI-ENTRY MULTI-EXIT BATCHING"/>
<node id="REEF 21"/>
<node id="PREEMPTION FOR SERVING"/>
<node id="SHEP-HERD 61"/>
<node id="ALPASERVE 28"/>
<node id="MODEL PARALLELISM"/>
<node id="STATISTICAL MULTIPLEXING"/>
<node id="ALPASERVE"/>
<node id="STATISTICAL MULTIPLEXING WITH MODEL PARALLELISM FOR DEEP LEARNING SERVING"/>
<node id="GENERAL SYSTEMS"/>
<node id="AUTO-REGRESSIVE PROPERTY AND TOKEN STATE OF LLM INFERENCE"/>
<node id="SPECIALIZED SERVING SYSTEMS"/>
<node id="NUMEROUS SPECIALIZED SERVING SYSTEMS"/>
<node id="THE TRANSFORMER ARCHITECTURE"/>
<node id="THESE SYSTEMS"/>
<node id="GPU KERNEL OPTIMIZATIONS"/>
<node id="ADVANCED BATCHING MECHANISMS"/>
<node id="PARAMETER SHARING"/>
<node id="EFFICIENT SERVING"/>
<node id="MOST RELEVANT TO OUR APPROACH"/>
<node id="FINE-GRAINED SCHEDULING AND INTERLEAVING OF THE REQUESTS LIKE IN ORCA"/>
<node id="MEMORY MANAGEMENT MORE CHALLENGING"/>
<node id="TECHNIQUES PROPOSED IN VLLM"/>
<node id="MORE CRUCIAL"/>
<node id="THE WIDENING GAP BETWEEN THE COMPUTE CAPABILITY AND MEMORY CAPACITY OF ACCELERATORS"/>
<node id="MEMORY TO BECOME A BOTTLENECK FOR BOTH TRAINING AND INFERENCE"/>
<node id="SWAPPING 23, 42, 55, RECOMPUTATION 7, 24 AND THEIR COMBINATION 40"/>
<node id="REDUCE THE PEAK MEMORY OF TRAINING"/>
<node id="FLEXGEN 46 STUDIES"/>
<node id="HOW TO SWAP WEIGHTS AND TOKEN STATES FOR LLM INFERENCE WITH 623 LIMITED GPU MEMORY"/>
<node id="OLLA 48"/>
<node id="THE LIFETIME AND LOCATION OF TENSORS"/>
<node id="FRAGMENTATION"/>
<node id="FINE-GRAINED BLOCK-LEVEL MANAGEMENT"/>
<node id="ONLINE SERVING"/>
<node id="FLASHAT-TENTION 13"/>
<node id="TILING AND KERNEL OPTIMIZATIONS"/>
<node id="THE PEAK MEMORY OF ATTENTION COMPUTATION"/>
<node id="IO COSTS"/>
<node id="A NEW IDEA OF BLOCK-LEVEL MEMORY MANAGEMENT"/>
<node id="XIAOXUAN LIU"/>
<node id="ZHIFENG CHEN"/>
<node id="YAN-PING HUANG"/>
<node id="ANONYMOUS SOSP REVIEWERS"/>
<node id="OUR SHEPHERD"/>
<node id="LIDONG ZHOU"/>
<node id="XIAOXUAN LIU, ZHIFENG CHEN, YAN-PING HUANG, ANONYMOUS SOSP REVIEWERS, AND LIDONG ZHOU"/>
<node id="INSIGHTFUL FEEDBACK"/>
<node id="THIS RESEARCH"/>
<node id="GIFTS FROM ANDREESSEN HOROWITZ"/>
<node id="GIFTS FROM ANYSCALE"/>
<node id="GIFTS FROM ASTRONOMER"/>
<node id="GIFTS FROM GOOGLE"/>
<node id="GIFTS FROM IBM"/>
<node id="GIFTS FROM INTEL"/>
<node id="GIFTS FROM LACEWORK"/>
<node id="GIFTS FROM MICROSOFT"/>
<node id="GIFTS FROM MOHAMED BIN ZAYED UNIVERSITY OF ARTIFICIAL INTELLIGENCE"/>
<node id="GIFTS FROM SAMSUNG SDS"/>
<node id="GIFTS FROM UBER"/>
<node id="GIFTS FROM VMWARE"/>
<node id="REFERENCES"/>
<node id="REZA YAZDANI AMINABADI"/>
<node id="SAMYAM RAJBHANDARI"/>
<node id="MINJIA ZHANG"/>
<node id="AMMAR AHMAD AWAN"/>
<node id="CHENG LI"/>
<node id="DU LI"/>
<node id="ELTON ZHENG"/>
<node id="JEFF RASLEY"/>
<node id="SHADEN SMITH"/>
<node id="OLATUNJI RUWASE"/>
<node id="DEEPSPEED INFERENCE"/>
<node id="EFFICIENT INFERENCE OF TRANSFORMER MODELS AT UNPRECEDENTED SCALE"/>
<node id="ARXIV PREPRINT"/>
<node id="ARXIV:2207.00032"/>
<node id="2022"/>
<node id="ARXIV:1607.06450"/>
<node id="2016"/>
<node id="ARXIV:2107.03374"/>
<node id="2021"/>
<node id="ARXIV:1604.06174"/>
<node id="ARXIV:2204.02311"/>
<node id="ARXIV:2104.08691"/>
<node id="ARXIV:2101.00190"/>
<node id="ARXIV:2302.11665"/>
<node id="ARXIV:1712.06139"/>
<node id="2017"/>
<node id="ARXIV:2211.05102"/>
<node id="ARXIV:2303.06865"/>
<node id="ARXIV:1909.08053"/>
<node id="2019"/>
<node id="ARXIV:2302.13971"/>
<node id="ARXIV:2212.10560"/>
<node id="ARXIV:1609.08144"/>
<node id="ARXIV:2205.01068"/>
<node id="JIMMY LEI BA"/>
<node id="AUTHOR"/>
<node id="JAMIE RYAN KIROS"/>
<node id="GEOFFREY E HINTON"/>
<node id="LAYER NORMALIZATION"/>
<node id="A TECHNIQUE"/>
<node id="YOSHUA BENGIO"/>
<node id="RJEAN DUCHARME"/>
<node id="PASCAL VINCENT"/>
<node id="NEURAL PROBABILISTIC LANGUAGE MODEL"/>
<node id="A MODEL"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS"/>
<node id="13"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13"/>
<node id="2000"/>
<node id="4 OND REJ BOJAR"/>
<node id="PERSON"/>
<node id="RAJEN CHATTERJEE"/>
<node id="CHRISTIAN FEDERMANN"/>
<node id="YVETTE GRAHAM"/>
<node id="BARRY HADDOW"/>
<node id="MATTHIAS HUCK"/>
<node id="ANTONIO JIMENO YEPES"/>
<node id="PHILIPP KOEHN"/>
<node id="VARVARA LOGACHEVA"/>
<node id="CHRISTOF MONZ"/>
<node id="MATTEO NEGRI"/>
<node id="AURELIE NEVEOL"/>
<node id="MARIANA NEVES"/>
<node id="MARTIN POPEL"/>
<node id="MATT POST"/>
<node id="RAPHAEL RUBINO"/>
<node id="CAROLINA SCARTON"/>
<node id="LUCIA SPECIA"/>
<node id="MARCO TURCHI"/>
<node id="KARIN VERSPOOR"/>
<node id="MARCOS ZAMPIERI"/>
<node id="2016 CONFERENCE"/>
<node id="PROCEEDINGS"/>
<node id="THE FIRST CONFERENCE ON MACHINE TRANSLATION"/>
<node id="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS"/>
<node id="BERLIN, GERMANY"/>
<node id="131198"/>
<node id="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301"/>
<node id="TOM BROWN"/>
<node id="BENJAMIN MANN"/>
<node id="NICK RYDER"/>
<node id="MELANIE SUBBIAH"/>
<node id="JARED D KAPLAN"/>
<node id="PRAFULLA DHARIWAL"/>
<node id="ARVIND NEELAKANTAN"/>
<node id="PRANAV SHYAM"/>
<node id="GIRISH SASTRY"/>
<node id="AMANDA ASKELL"/>
<node id="LANGUAGE MODELS"/>
<node id="FEW-SHOT LEARNERS"/>
<node id="33"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 33"/>
<node id="2020"/>
<node id="1877-1901"/>
<node id="35"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35"/>
<node id="16344-16359"/>
<node id="27TH EDITION"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27"/>
<node id="2014"/>
<node id="30TH EDITION"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30"/>
<node id="6 MARK CHEN"/>
<node id="JERRY TWOREK"/>
<node id="HEEWOO JUN"/>
<node id="QIMING YUAN"/>
<node id="HENRIQUE PONDE DE OLIVEIRA PINTO"/>
<node id="JARED KAPLAN"/>
<node id="HARRI EDWARDS"/>
<node id="YURI BURDA"/>
<node id="NICHOLAS JOSEPH"/>
<node id="GREG BROCKMAN"/>
<node id="LARGE LANGUAGE MODELS"/>
<node id="CODE"/>
<node id="7 TIANQI CHEN"/>
<node id="BING XU"/>
<node id="CHIYUAN ZHANG"/>
<node id="CARLOS GUESTRIN"/>
<node id="14 JIARUI FANG"/>
<node id="YANG YU"/>
<node id="CHENGDUO ZHAO"/>
<node id="JIE ZHOU"/>
<node id="21 MINGCONG HAN"/>
<node id="HANZE ZHANG"/>
<node id="RONG CHEN"/>
<node id="HAIBO CHEN"/>
<node id="22"/>
<node id="KAIMING HE"/>
<node id="XIANGYU ZHANG"/>
<node id="SHAOQING REN"/>
<node id="JIAN SUN"/>
<node id="23"/>
<node id="CHIEN-CHIN HUANG, GU JIN, AND JINYANG LI"/>
<node id="54 JING WANG"/>
<node id="YOUYOU LU"/>
<node id="QING WANG"/>
<node id="MINHUI XIE"/>
<node id="KEJI HUANG"/>
<node id="JIWU SHU"/>
<node id="56"/>
<node id="XIAOHUI WANG"/>
<node id="YING XIONG"/>
<node id="YANG WEI"/>
<node id="MINGXUAN WANG"/>
<node id="LEI LI"/>
<node id="64"/>
<node id="ZHE ZHOU"/>
<node id="XUECHAO WEI"/>
<node id="JIEJING ZHANG"/>
<node id="GUANGYU SUN"/>
<node id="TRAINING DEEP NETS"/>
<node id="SUBLINEAR MEMORY COST"/>
<node id="8 WEI-LIN CHIANG"/>
<node id="TEXT"/>
<node id="ZHUOHAN LI"/>
<node id="ZI LIN"/>
<node id="YING SHENG"/>
<node id="ZHANGHAO WU"/>
<node id="HAO ZHANG"/>
<node id="LIANMIN ZHENG"/>
<node id="SIYUAN ZHUANG"/>
<node id="YONGHAO ZHUANG"/>
<node id="JOSEPH E. GONZALEZ"/>
<node id="ION STOICA"/>
<node id="ERIC P. XING"/>
<node id="12 WEIHAO CUI"/>
<node id="HAN ZHAO"/>
<node id="QUAN CHEN"/>
<node id="HAO WEI"/>
<node id="ZIRUI LI"/>
<node id="DEZE ZENG"/>
<node id="CHAO LI"/>
<node id="MINYI GUO"/>
<node id="28"/>
<node id="YINMIN ZHONG"/>
<node id="VINCENT LIU"/>
<node id="XIN JIN"/>
<node id="YANPING HUANG"/>
<node id="JOSEPH E GONZALEZ"/>
<node id="ET AL"/>
<node id="59"/>
<node id="NUMBER"/>
<node id="YONGHUI WU"/>
<node id="MIKE SCHUSTER"/>
<node id="QUOC V LE"/>
<node id="MOHAMMAD NOROUZI"/>
<node id="WOLFGANG MACHEREY"/>
<node id="MAXIM KRIKUN"/>
<node id="YUAN CAO"/>
<node id="QIN GAO"/>
<node id="KLAUS MACHEREY"/>
<node id="63 LIANMIN ZHENG, ZHUOHAN LI, HAO ZHANG, YONGHAO ZHUANG, ZHIFENG CHEN, YANPING HUANG, YIDA WANG, YUANZHONG XU, DANYANG ZHUO, ERIC P XING"/>
<node id="ET AL."/>
<node id="VICUNA"/>
<node id="AN OPEN-SOURCE CHATBOT"/>
<node id="GPT-4"/>
<node id="90 CHATGPT QUALITY"/>
<node id="ORGBLOG2023-03-30-VICUNA 9"/>
<node id="AAKANKSHA CHOWDHERY, SHARAN NARANG, JACOB DEVLIN, MAARTEN BOSMA, GAURAV MISHRA, ADAM ROBERTS, PAUL BARHAM, HYUNG WON CHUNG, CHARLES SUTTON, SEBASTIAN GEHRMANN, ET AL."/>
<node id="PALM"/>
<node id="SCALING LANGUAGE MODELING WITH PATHWAYS"/>
<node id="DANIEL CRANKSHAW"/>
<node id="GUR-EYAL SELA"/>
<node id="XIANGXI MO"/>
<node id="COREY ZUMAR"/>
<node id="JOSEPH GONZALEZ"/>
<node id="ALEXEY TUMANOV"/>
<node id="XIN WANG"/>
<node id="GUILIO ZHOU"/>
<node id="MICHAEL J FRANKLIN"/>
<node id="INFERLINE"/>
<node id="LATENCY-AWARE PROVISIONING AND SCALING FOR PREDICTION SERVING PIPELINES"/>
<node id="11TH ACM SYMPOSIUM"/>
<node id="CLOUD COMPUTING"/>
<node id="CLIPPER"/>
<node id="A LOW-LATENCY ONLINE PREDICTION SERVING SYSTEM"/>
<node id="14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="NSDI 17"/>
<node id="20TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="NSDI 23"/>
<node id="DVABATCH"/>
<node id="DIVERSITY-AWARE MULTI-ENTRY MULTI-EXIT BATCHING"/>
<node id="EFFICIENT PROCESSING OF DNN SERVICES ON GPUS"/>
<node id="USENIX ANNUAL TECHNICAL CONFERENCE"/>
<node id="USENIX ATC 22"/>
<node id="13 TRI DAO, DAN FU, STEFANO ERMON, ATRI RUDRA, AND CHRISTOPHER R."/>
<node id="IN 2022"/>
<node id="FLASHATTENTION"/>
<node id="FAST AND MEMORY-EFFICIENT EXACT ATTENTION WITH IO-AWARENESS"/>
<node id="TURBOTRANSFORMERS"/>
<node id="AN EFFICIENT GPU SERVING SYSTEM"/>
<node id="TRANSFORMER MODELS"/>
<node id="26TH ACM SIGPLAN SYMPOSIUM"/>
<node id="PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING"/>
<node id="THE 23RD ACM SIGPLAN SYMPOSIUM"/>
<node id="FASTAPI"/>
<node id="A WEB FRAMEWORK"/>
<node id="HTTPS:GITHUB.COM"/>
<node id="TIANGOLOFASTAPI"/>
<node id="16"/>
<node id="PIN GAO"/>
<node id="LINGFAN YU"/>
<node id="YONGWEI WU"/>
<node id="JINYANG LI"/>
<node id="LOW LATENCY RNN INFERENCE"/>
<node id="CELLULAR BATCHING"/>
<node id="THIRTEENTH EUROSYS CONFERENCE"/>
<node id="17 AMIR GHOLAMI"/>
<node id="ZHEWEI YAO"/>
<node id="SEHOON KIM"/>
<node id="MICHAEL W MAHONEY"/>
<node id="KURT KEUTZER"/>
<node id="AI"/>
<node id="MEMORY WALL"/>
<node id="RISELAB MEDIUM POST 1"/>
<node id="6"/>
<node id="GITHUB"/>
<node id="18"/>
<node id="COPILOT"/>
<node id="GOOGLE"/>
<node id="HTTPS:BARD.GOOGLE.COM"/>
<node id="ARPAN GUJARATI"/>
<node id="REZA KARIMI"/>
<node id="SAFYA ALZAYAT"/>
<node id="WEI HAO"/>
<node id="ANTOINE KAUFMANN"/>
<node id="YMIR VIGFUSSON"/>
<node id="JONATHAN MACE"/>
<node id="SERVING DNNS"/>
<node id="LIKE CLOCKWORK"/>
<node id="PERFORMANCE PREDICTABILITY"/>
<node id="FROM THE BOTTOM UP"/>
<node id="OSDI 20"/>
<node id="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="14TH USENIX CONFERENCE"/>
<node id="OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="OSDI 22"/>
<node id="16TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION"/>
<node id="MICROSECOND-SCALE PREEMPTION"/>
<node id="CONCURRENT GPU-ACCELERATED DNN INFERENCES"/>
<node id="DEEP RESIDUAL LEARNING"/>
<node id="IMAGE RECOGNITION"/>
<node id="THE IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION"/>
<node id="SWAPADVISOR"/>
<node id="DEEP LEARNING BEYOND THE GPU MEMORY LIMIT VIA SMART SWAPPING"/>
<node id="THE TWENTY-FIFTH INTERNATIONAL CONFERENCE"/>
<node id="ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS"/>
<node id="24 PARAS JAIN"/>
<node id="AJAY JAIN"/>
<node id="ANIRUDDHA NRUSIMHA"/>
<node id="AMIR GHOLAMI"/>
<node id="PIETER ABBEEL"/>
<node id="40 SHISHIR G PATIL"/>
<node id="PARAS JAIN"/>
<node id="PRABAL DUTTA"/>
<node id="CHECK-MATE"/>
<node id="BREAKING THE MEMORY WALL WITH OPTIMAL TENSOR REMATERIALIZATION"/>
<node id="PROCEEDINGS OF MACHINE LEARNING AND SYSTEMS"/>
<node id="497-511"/>
<node id="TOM KILBURN"/>
<node id="DAVID BG EDWARDS"/>
<node id="MICHAEL J LANIGAN"/>
<node id="FRANK H SUMNER"/>
<node id="1962"/>
<node id="YEAR"/>
<node id="ONE-LEVEL STORAGE SYSTEM"/>
<node id="SYSTEM"/>
<node id="IRE TRANSACTIONS ON ELECTRONIC COMPUTERS"/>
<node id="223-235"/>
<node id="26"/>
<node id="BRIAN LESTER"/>
<node id="RAMI AL-RFOU"/>
<node id="NOAH CONSTANT"/>
<node id="POWER OF SCALE"/>
<node id="PARAMETER-EFFICIENT PROMPT TUNING"/>
<node id="XIANG LISA LI"/>
<node id="PERCY LIANG"/>
<node id="PREFIX-TUNING"/>
<node id="OPTIMIZING CONTINUOUS PROMPTS FOR GENERATION"/>
<node id="29 LINGXIAO MA"/>
<node id="ZHIQIANG XIE"/>
<node id="ZHI YANG"/>
<node id="JILONG XUE"/>
<node id="YOUSHAN MIAO"/>
<node id="WEI CUI"/>
<node id="WENXIANG HU"/>
<node id="FAN YANG"/>
<node id="LINTAO ZHANG"/>
<node id="RAMMER"/>
<node id="HOLISTIC DEEP LEARNING COMPILER OPTIMIZATIONS WITH RTASKS"/>
<node id="NVIDIA"/>
<node id="31"/>
<node id="32"/>
<node id="N. D."/>
<node id="TRITON INFERENCE SERVER"/>
<node id="HTTPS://DEVELOPER.NVIDIA.COM"/>
<node id="NVIDIA-TRITON-INFERENCE-SERVER"/>
<node id="HTTPS"/>
<node id="DEVELOPER.NVIDIA.COMNCCL"/>
<node id="NVIDIA FASTERTRANSFORMER"/>
<node id="NCCL"/>
<node id="THE NVIDIA COLLECTIVE COMMUNICATION LIBRARY"/>
<node id="CHRISTOPHER OLSTON"/>
<node id="NOAH FIEDEL"/>
<node id="KIRIL GOROVOY"/>
<node id="JEREMIAH HARMSEN"/>
<node id="LI LAO"/>
<node id="FANGWEI LI"/>
<node id="VINU RAJASHEKHAR"/>
<node id="SUKRITI RAMESH"/>
<node id="JORDAN SOYKE"/>
<node id="TENSORFLOW-SERVING"/>
<node id="FLEXIBLE ML SERVING"/>
<node id="HIGH-PERFORMANCE ML SERVING"/>
<node id="OPENAI"/>
<node id="34"/>
<node id="HTTPS://OPENAI.COM/BLOG/OPENAI-API"/>
<node id="HTTPS://OPENAI.COM/BLOG/CHATGPT"/>
<node id="OPENAI.COM/BLOG/CUSTOM-INSTRUCTIONS-FOR-CHATGPT"/>
<node id="BLOG CUSTOM INSTRUCTIONS FOR CHATGPT"/>
<node id="TECHNICAL REPORT"/>
<node id="CHATBOT ARENA LEADERBOARD"/>
<node id="WEEK 8"/>
<node id="MT-BENCH"/>
<node id="VICUNA-33B"/>
<node id="HTTPS:LMSYS.ORG"/>
<node id="BLOG POST DATE 2023-06-22"/>
<node id="LEADERBOARD"/>
<node id="AN IMPERATIVE STYLE HIGH-PERFORMANCE DEEP LEARNING LIBRARY"/>
<node id="32ND EDITION"/>
<node id="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32"/>
<node id="POET"/>
<node id="NEURAL NETWORKS"/>
<node id="TINY DEVICES"/>
<node id="INTEGRATED REMATERIALIZATION AND PAGING"/>
<node id="INTERNATIONAL CONFERENCE ON MACHINE LEARNING"/>
<node id="AN EVENT"/>
<node id="PMLR"/>
<node id="1757317583"/>
<node id="41"/>
<node id="REINER POPE"/>
<node id="SHOLTO DOUGLAS"/>
<node id="AAKANKSHA CHOWDHERY"/>
<node id="JACOB DEVLIN"/>
<node id="JAMES BRADBURY"/>
<node id="ANSELM LEVSKAYA"/>
<node id="JONATHAN HEEK"/>
<node id="KEFAN XIAO"/>
<node id="SHIVANI AGRAWAL"/>
<node id="JEFF DEAN"/>
<node id="ZERO-OFFLOAD"/>
<node id="DEMOCRATIZING BILLION-SCALE MODEL TRAINING"/>
<node id="AMAZON WEB SERVICES"/>
<node id="HTTPS://WWW.REUTERS.COM/TECHNOLOGY/TECH-GIANTS-AI-LIKE-BING-BARD-POSES-BILLION-DOLLAR-SEARCH-PROBLEM-2023-02-22"/>
<node id="HTTPS:AWS.AMAZON.COMBEDROCK"/>
<node id="HAICHEN SHEN"/>
<node id="LEQUN CHEN"/>
<node id="YUCHEN JIN"/>
<node id="LIANGYU ZHAO"/>
<node id="BINGYU KONG"/>
<node id="MATTHAI PHILIPOSE"/>
<node id="ARVIND KRISHNAMURTHY"/>
<node id="RAVI SUNDARAM"/>
<node id="NEXUS"/>
<node id="A GPU CLUSTER ENGINE"/>
<node id="ACCELERATING DNN-BASED VIDEO ANALYSIS"/>
<node id="27TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES"/>
<node id="HIGH-THROUGHPUT GENERATIVE INFERENCE"/>
<node id="A SINGLE GPU"/>
<node id="47"/>
<node id="MOHAMMAD SHOEYBI"/>
<node id="MOSTOFA PATWARY"/>
<node id="RAUL PURI"/>
<node id="PATRICK LEGRESLEY"/>
<node id="JARED CASPER"/>
<node id="BRYAN CATANZARO"/>
<node id="MEGATRON-LM"/>
<node id="TRAINING MULTI-BILLION PARAMETER LANGUAGE MODELS"/>
<node id="48"/>
<node id="BENOIT STEINER"/>
<node id="MOSTAFA ELHOUSHI"/>
<node id="JACOB KAHN"/>
<node id="JAMES HEGARTY"/>
<node id="OLLA"/>
<node id="THE LIFETIME OF ARRAYS"/>
<node id="THE LOCATION OF ARRAYS"/>
<node id="THE MEMORY USAGE OF NEURAL NETWORKS"/>
<node id="DOI.ORG/10.48550/ARXIV.2210.12924"/>
<node id="ILYA SUTSKEVER"/>
<node id="ORIOL VINYALS"/>
<node id="SEQUENCE TO SEQUENCE LEARNING"/>
<node id="50"/>
<node id="ROHAN TAORI"/>
<node id="ISHAAN GULRAJANI"/>
<node id="TIANYI ZHANG"/>
<node id="YANN DUBOIS"/>
<node id="XUECHEN LI"/>
<node id="TATSUNORI B. HASHIMOTO"/>
<node id="55"/>
<node id="LINNAN WANG"/>
<node id="JINMIAN YE"/>
<node id="YIYANG ZHAO"/>
<node id="WEI WU"/>
<node id="ANG LI"/>
<node id="SHUAI-WEN LEON SONG"/>
<node id="ZENGLIN XU"/>
<node id="TIM KRASKA"/>
<node id="57 YIZHONG WANG"/>
<node id="YEGANEH KORDI"/>
<node id="SWAROOP MISHRA"/>
<node id="ALISA LIU"/>
<node id="NOAH A SMITH"/>
<node id="DANIEL KHASHABI"/>
<node id="HANNANEH HAJISHIRZI"/>
<node id="STANFORD ALPACA"/>
<node id="AN INSTRUCTION-FOLLOWING LLAMA MODEL"/>
<node id="GITHUB.COM/TATSU-LABS"/>
<node id="SHAREGPT TEAM"/>
<node id="51"/>
<node id="HTTPS:SHAREGPT.COM"/>
<node id="HUGO TOUVRON, THIBAUT LAVRIL, GAUTIER IZACARD, XAVIER MARTINET, MARIE-ANNE LACHAUX, TIMOTHE LACROIX, BAPTISTE ROZIRE, NAMAN GOYAL, ERIC HAMBRO, FAISAL AZHAR, ET AL."/>
<node id="LLAMA"/>
<node id="OPEN AND EFFICIENT FOUNDATION LANGUAGE MODELS"/>
<node id="53"/>
<node id="ASHISH VASWANI"/>
<node id="NOAM SHAZEER"/>
<node id="NIKI PARMAR"/>
<node id="JAKOB USZKOREIT"/>
<node id="LLION JONES"/>
<node id="AIDAN N GOMEZ"/>
<node id="UKASZ KAISER"/>
<node id="ILLIA POLOSUKHIN"/>
<node id="ATTENTION"/>
<node id="ALL YOU NEED"/>
<node id="PACMAN"/>
<node id="AN EFFICIENT COMPACTION APPROACH"/>
<node id="LOG-STRUCTURED KEY-VALUE STORE"/>
<node id="PERSISTENT MEMORY"/>
<node id="SUPERNEURONS"/>
<node id="DYNAMIC GPU MEMORY MANAGEMENT FOR TRAINING DEEP NEURAL NETWORKS"/>
<node id="LIGHTSEQ"/>
<node id="A HIGH PERFORMANCE INFERENCE LIBRARY FOR TRANSFORMERS"/>
<node id="2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS"/>
<node id="2021 CONFERENCE"/>
<node id="NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS"/>
<node id="NORTH AMERICAN CHAPTER"/>
<node id="HUMAN LANGUAGE TECHNOLOGIES: INDUSTRY PAPERS"/>
<node id="SELF-INSTRUCT"/>
<node id="ALIGNING LANGUAGE MODEL WITH SELF GENERATED INSTRUCTIONS"/>
<node id="STATE-OF-THE-ART NATURAL LANGUAGE PROCESSING"/>
<node id="THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS"/>
<node id="GOOGLE'S NEURAL MACHINE TRANSLATION SYSTEM"/>
<node id="THE GAP BETWEEN HUMAN AND MACHINE TRANSLATION"/>
<node id="60 GYEONG-IN YU"/>
<node id="JOO SEONG JEONG"/>
<node id="GEON-WOO KIM"/>
<node id="SOOJEONG KIM"/>
<node id="BYUNG-GON CHUN"/>
<node id="A DISTRIBUTED SERVING SYSTEM"/>
<node id="TRANSFORMER-BASED GENERATIVE MODELS"/>
<node id="61"/>
<node id="HONG ZHANG"/>
<node id="YUPENG TANG"/>
<node id="ANURAG KHANDELWAL"/>
<node id="SHEPHERD"/>
<node id="DNNS IN THE WILD"/>
<node id="USENIX ASSOCIATION"/>
<node id="BOSTON, MA"/>
<node id="787808"/>
<node id="SUSAN ZHANG"/>
<node id="PRESENTER AT NSDI23"/>
<node id="STEPHEN ROLLER"/>
<node id="NAMAN GOYAL"/>
<node id="MIKEL ARTETXE"/>
<node id="MOYA CHEN"/>
<node id="SHUOHUI CHEN"/>
<node id="CHRISTOPHER DEWAN"/>
<node id="MONA DIAB"/>
<node id="XIAN LI"/>
<node id="XI VICTORIA LIN"/>
<node id="NSDI23"/>
<node id="CONFERENCE"/>
<node id="HTTPS://WWW.USENIX.ORG/CONFERENCE/NSDI23/PRESENTATION/ZHANG-HONG"/>
<node id="URL"/>
<node id="OPEN PRE-TRAINED TRANSFORMER LANGUAGE MODELS"/>
<node id="ALPA"/>
<node id="AUTOMATING INTER-AND INTRA-OPERATOR PARALLELISM FOR DISTRIBUTED DEEP LEARNING"/>
<node id="PETS"/>
<node id="A UNIFIED FRAMEWORK FOR PARAMETER-EFFICIENT TRANSFORMERS SERVING"/>
<node id="1032 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS"/>
<node id="A JOURNAL VOLUME"/>
<node id="1034 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS"/>
<node id="VOL."/>
<node id="IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS"/>
<node id="1038 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS"/>
<node id="1040 IEEE TRANSACTIONS"/>
<node id="VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS"/>
<node id="1042 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS"/>
<node id="1044 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS"/>
<node id="A JOURNAL"/>
<node id="PROCESSORS"/>
<node id="OPEN RISC-V INSTRUCTION SET ARCHITECTURE (ISA)"/>
<node id="FINDING INCREASING ADOPTION"/>
<node id="EMBEDDED WORLD"/>
<node id="A CLIC FOR THE CV32E40P"/>
<node id="CV32E40P"/>
<node id="AN INDUSTRIALLY SUPPORTED OPEN-SOURCE 32-BIT MICROCONTROLLER UNIT (MCU)-CLASS RISC-V CORE"/>
<node id="IT WITH FASTIRQ"/>
<node id="FASTIRQ"/>
<node id="A CUSTOM EXTENSION"/>
<node id="INTERRUPT LATENCY AS LOW AS SIX CYCLES"/>
<node id="CV32RT"/>
<node id="A 32-BIT RISC-V CORE"/>
<node id="THE INTERRUPT HANDLING CAPABILITIES OF CV32E40P"/>
<node id="AN INDUSTRIALLY SUPPORTED OPEN-SOURCE CORE"/>
<node id="BEST-IN-CLASS INTERRUPT LATENCY"/>
<node id="FAST CONTEXT SWITCHING"/>
<node id="THE ROAD FOR RISC-V ARCHITECTURES IN TIME-CRITICAL SYSTEMS"/>
<node id="FULL-SYSTEM PLATFORM"/>
<node id="PROPOSED INTERRUPT EXTENSION"/>
<node id="SECTION II-A"/>
<node id="RELEVANT TARGET METRICS"/>
<node id="SECTION II-B3"/>
<node id="CURRENT STATUS OF INTERRUPT HANDLING IN RISC-V"/>
<node id="SECTION II-C"/>
<node id="CV32E40P CORE"/>
<node id="CV32"/>
<node id="AN OPEN-SOURCE, INDUSTRY-GRADE, 32-BIT, IN-ORDER, FOUR-STAGE RISC-V CORE"/>
<node id="IMPLEMENTING OUR EXTENSIONS"/>
<node id="CV32 CORE AS THE BASELINE"/>
<node id="CV32 CORE"/>
<node id="CLINT INTERRUPT CONTROLLER"/>
<node id="CLINT INTERRUPT CONTROLLER WITH CLIC"/>
<node id="CLIC"/>
<node id="CV32RTCLIC"/>
<node id="FASTIRQ INTO THE CORE MICROARCHITECTURE"/>
<node id="THE EXTENSION PROPOSED IN THIS WORK"/>
<node id="CV32RTFASTIRQ"/>
<node id="CLIC FAST INTERRUPT CONTROLLER"/>
<node id="CLIC INTERRUPT CONTROLLER IN CV32RTCLIC 15"/>
<node id="CLIC INTERRUPT CONTROLLER"/>
<node id="DRAFT SPECIFICATION"/>
<node id="RISC-V PRIVILEGED SPECIFICATION 14"/>
<node id="FASTIRQ EXTENSION"/>
<node id="WEAKNESSES OF RISC-V CORE-SPECIFIC INTERRUPT CONTROLLERS"/>
<node id="CLIC BASE CAPABILITIES"/>
<node id="INTERRUPT LATENCY"/>
<node id="HW VECTORED INTERRUPTS"/>
<node id="SKIPPING OF REDUNDANT CONTEXT RESTORE OPERATIONS"/>
<node id="A FAST INTERRUPT EXTENSION"/>
<node id="RISC-V EMBEDDED SYSTEMS"/>
<node id="THE EXTENSION"/>
<node id="A 32-BIT, IN-ORDER, SINGLE-ISSUE CORE"/>
<node id="THE RISC-V CLIC FAST INTERRUPT CONTROLLER"/>
<node id="27 B. MAO, N. TAN, T. CHONG, AND L. LI"/>
<node id="A CLIC EXTENSION BASED FAST INTERRUPT SYSTEM FOR EMBEDDED RISC-V PROCESSORS"/>
<node id="A CLIC EXTENSION BASED FAST INTERRUPT SYSTEM"/>
<node id="EMBEDDED RISC-V PROCESSORS"/>
<node id="PROC."/>
<node id="MANY EMBEDDED USE CASES"/>
<node id="REAL-TIME CONSTRAINTS"/>
<node id="FLEXIBLE, PREDICTABLE, AND FAST REACTIVE HANDLING OF INCOMING EVENTS"/>
<node id="RISC-V PROCESSORS"/>
<node id="THIS AREA"/>
<node id="MORE MATURE PROPRIETARY ARCHITECTURES"/>
<node id="ARM CORTEX-M"/>
<node id="TRICORE"/>
<node id="ARM CORTEX-M AND TRICORE"/>
<node id="TUNED FOR YEARS"/>
<node id="THE DEFAULT INTERRUPT CONTROLLER"/>
<node id="RISC-V"/>
<node id="THE CORE LOCAL INTERRUPTOR (CLINT)"/>
<node id="CONFIGURABILITY IN PRIORITIZATION AND PREEMPTION OF INTERRUPTS"/>
<node id="THE RISC-V CORE LOCAL INTERRUPT CONTROLLER (CLIC) SPECIFICATION"/>
<node id="THIS CONCERN"/>
<node id="PREEMPTIBLE, LOW-LATENCY VECTORED INTERRUPTS"/>
<node id="OPTIONAL EXTENSIONS TO IMPROVE INTERRUPT LATENCY"/>
<node id="THE RISC-V COMMUNITY"/>
<node id="AN EXTENSION TO THE PRIVILEGED SPECIFICATIONS 14"/>
<node id="THE RISC-V CORE LOCAL INTERRUPT CONTROLLER (CLIC) 15"/>
<node id="CURRENTLY UNDER RATIFICATION BY THE COMMUNITY"/>
<node id="REAL-TIME SCENARIOS"/>
<node id="CLICS"/>
<node id="LOCAL TO EACH HARDWARE THREAD (HART)"/>
<node id="PLATFORM LEVEL INTERRUPT CONTROLLERS (PLICS)"/>
<node id="CENTRALIZED INTERRUPT CONTROLLERS"/>
<node id="MANAGING MULTIPLE HARTS"/>
<node id="PLATFORM-LEVEL INTERRUPT CONTROLLERS"/>
<node id="THIS CATEGORY"/>
<node id="RISC-V PLIC"/>
<node id="PLATFORM-LEVEL INTERRUPT CONTROLLER"/>
<node id="ADVANCED PLIC (APLIC)"/>
<node id="RISC-V INCOMING MESSAGE SIGNALED INTERRUPT CONTROLLER (IMSIC)"/>
<node id="RISC-V PLIC AND ADVANCED PLIC (APLIC)"/>
<node id="WIRE-BASED INTERRUPT COMMUNICATION"/>
<node id="MESSAGE-SIGNALED INTERRUPT COMMUNICATION"/>
<node id="DISTRIBUTE TIME-CRITICAL INTERRUPTS TO THE RUNNING HARTS"/>
<node id="B. CORE-LOCAL INTERRUPT CONTROLLERS CLICS"/>
<node id="PROVIDING FAST INTERRUPT-HANDLING CAPABILITIES"/>
<node id="REAL-TIME EMBEDDED APPLICATION DOMAINS"/>
<node id="RISC-V SMCLIC CORE-LOCAL INTERRUPT CONTROLLER (CLIC)"/>
<node id="RISC-V PRIVILEGED ARCHITECTURE EXTENSION"/>
<node id="DOCUMENT"/>
<node id="HTTP://WWW2.EECS.BERKELEY.EDU/PUB/STECHRPTS/2016/EECS-2016-129.HTML"/>
<node id="CV32RT OUR ENHANCED CORE"/>
<node id="THE FIRST FULLY OPEN-SOURCE RV32 CORE"/>
<node id="COMPETITIVE INTERRUPT-HANDLING FEATURES"/>
<node id="ARM CORTEX-M SERIES"/>
<node id="CV32RT WITHIN AN OPEN-SOURCE SYSTEM ON CHIP (SOC)"/>
<node id="CV32RT INTERRUPT HANDLING CAPABILITIES"/>
<node id="NEGLIGIBLE AREA OVERHEAD IN A MODERN TECHNOLOGY NODE"/>
<node id="PERFORMANCE BENEFITS"/>
<node id="VARIOUS CV32RT VERSIONS"/>
<node id="INTERRUPT LATENCY AND CONTEXT SWITCH TIMES"/>
<node id="HOW THE VARIOUS CV32RT VERSIONS PERFORM IN TERMS OF INTERRUPT LATENCY AND CONTEXT SWITCH TIMES"/>
<node id="THESE ADDITIONS"/>
<node id="OVERHEAD IN TERMS OF AREA AND TIMING"/>
<node id="THE OVERHEAD THESE ADDITIONS INCUR IN TERMS OF AREA AND TIMING"/>
<node id="THE PROPOSED EXTENSIONS"/>
<node id="TASK CONTEXT SWITCHING IN REAL-TIME OPERATING SYSTEMS (RTOSS)"/>
<node id="INDEX TERMS"/>
<node id="CONTEXT SWITCHING"/>
<node id="EMBEDDED"/>
<node id="MICROCONTROLLER UNIT (MCU)"/>
<node id="REAL-TIME"/>
<node id="SEVERAL MARKETS"/>
<node id="REAL-TIME AN SW-BASED SOLUTION"/>
<node id="THE AUTOMOTIVE INDUSTRY"/>
<node id="HUNDREDS OF ELECTRONIC CONTROL UNITS (ECUS)"/>
<node id="ELECTRONIC CONTROL UNITS (ECUS)"/>
<node id="REAL-TIME APPLICATIONS"/>
<node id="ELECTRONIC ENGINE CONTROL"/>
<node id="GEARBOX CONTROL"/>
<node id="CRUISE CONTROL"/>
<node id="ANTI-LOCK BRAKE SYSTEMS"/>
<node id="MANY OTHER TASKS"/>
<node id="GENERAL-PURPOSE OPERATING SYSTEMS (GPOSS)"/>
<node id="AVERAGE THROUGHPUT"/>
<node id="THE HORIZON KEY DIGITAL TECHNOLOGIES JOINT UNDERTAKING (KDT JU) PROGRAMME"/>
<node id="THE TRISTAN PROJECT"/>
<node id="101095947"/>
<node id="ROBERT BALAS"/>
<node id="CORRESPONDING AUTHOR"/>
<node id="INTEGRATED SYSTEMS LABORATORY (IIS)"/>
<node id="ALESSANDRO OTTAVIANO"/>
<node id="ETH ZURICH"/>
<node id="8092 ZURICH"/>
<node id="SWITZERLAND"/>
<node id="LUCA BENINI"/>
<node id="THE INTEGRATED SYSTEMS LABORATORY (IIS)"/>
<node id="ZURICH, SWITZERLAND"/>
<node id="THE DEPARTMENT OF ELECTRICAL, ELECTRONIC AND INFORMATION ENGINEERING (DEI)"/>
<node id="UNIVERSITY OF BOLOGNA"/>
<node id="40126 BOLOGNA, ITALY"/>
<node id="COLOR VERSIONS OF ONE OR MORE FIGURES IN THIS ARTICLE"/>
<node id="HTTPS:DOI.ORG10.1109TVLSI.2024.3377130"/>
<node id="DIGITAL OBJECT IDENTIFIER"/>
<node id="10.1109TVLSI.2024.3377130"/>
<node id="THE DEVELOPMENT OF LINUX"/>
<node id="AVERAGE PERFORMANCE"/>
<node id="LINUX"/>
<node id="POPULAR OPEN-SOURCE GPOS KERNEL"/>
<node id="EXTENSIONS AND MODIFICATIONS"/>
<node id="IMPROVING DETERMINISM AND LATENCIES OF CRITICAL OPERATIONS IN LINUX"/>
<node id="PROPOSED AND IMPLEMENTED"/>
<node id="STRICT BOUNDS ON MAXIMUM LATENCIES OF OPERATIONS"/>
<node id="INDUSTRY-GRADE MATURITY"/>
<node id="EMPLOYED IN HARD REAL-TIME SCENARIOS"/>
<node id="REAL-TIME OPERATING SYSTEMS (RTOSS) KERNELS"/>
<node id="SPECIAL-PURPOSE OPERATING SYSTEMS (OSS)"/>
<node id="REAL-TIME GUARANTEES"/>
<node id="TASK SCHEDULING ACCORDING TO A GIVEN EXPECTED COMPLETION DEADLINE"/>
<node id="DETERMINISTIC LATENCIES OF VARIOUS OPERATIONS"/>
<node id="A. STANKOVIC"/>
<node id="SCHEDULING ALGORITHMS AND OPERATING SYSTEMS SUPPORT FOR REAL-TIME SYSTEMS"/>
<node id="THE RTOS SCHEDULER"/>
<node id="A SIGNIFICANT OVERHEAD"/>
<node id="THE SIGNIFICANT OVERHEAD"/>
<node id="THE COMBINED EFFECT OF BOTH THE CONTEXT SWITCHES REQUIRED TO HANDLE THE TRANSITION FROM A FOREGROUND TO A BACKGROUND TASK AND THE AMOUNT OF TIME ELAPSED FROM THE SOURCE EVENT THAT CAUSES THE PREEMPTION AND THE FIRST INSTRUCTION OF THE AWAKENED TASK"/>
<node id="THE FIRST INSTRUCTION OF THE AWAKENED TASK"/>
<node id="INTERRUPT LATENCY 7"/>
<node id="THE WORST CASE EXECUTION TIME (WCET) 2, 8, 9"/>
<node id="THE COST OF SAVING AND RESTORING THE TASK STATE DURING A CONTEXT SWITCH"/>
<node id="A SIGNIFICANT CONCERN"/>
<node id="RELATIVELY HIGH"/>
<node id="LONG CONTEXT SWITCH TIMES"/>
<node id="AVAILABLE TASK UTILIZATION"/>
<node id="THE MINIMUM VIABLE SWITCHING GRANULARITY"/>
<node id="PROCESS STATE"/>
<node id="CONTEXT SWITCH"/>
<node id="PROGRAM COUNTER"/>
<node id="REGISTER FILES (RFS)"/>
<node id="STATUS REGISTERS"/>
<node id="ADDRESS SPACE MAPPING"/>
<node id="MEMORY ACCESS OPERATIONS"/>
<node id="STORE THE STATE OF THE PREEMPTED TASK"/>
<node id="RESTORE THE STATE OF THE NEW TASK TO BE EXECUTED"/>
<node id="A SWITCH INTO AN INTERRUPT CONTEXT"/>
<node id="NORMAL PROGRAM EXECUTION"/>
<node id="AN ASYNCHRONOUS EVENT IS TRIGGERED"/>
<node id="AN ASYNCHRONOUS EVENT"/>
<node id="AN IO PERIPHERAL DEVICE"/>
<node id="HW-INDUCED INTERRUPT LATENCY"/>
<node id="ONLY ONE PART OF THE PROBLEM"/>
<node id="SW-INDUCED INTERRUPT LATENCY"/>
<node id="PART OF THE PROBLEM"/>
<node id="SW-DEPENDENT CONTRIBUTIONS"/>
<node id="HW-DEPENDENT CONTRIBUTIONS"/>
<node id="PERSONAL USE"/>
<node id="PERMITTED"/>
<node id="REPUBLICATIONREDISTRIBUTION"/>
<node id="IEEE PERMISSION"/>
<node id="HTTPS://WWW.IEEE.ORG/PUBLICATIONS/RIGHTS/INDEX.HTML"/>
<node id="MORE INFORMATION"/>
<node id="IEEE"/>
<node id="39TH INT."/>
<node id="AUTHORIZED LICENSED USE"/>
<node id="CALIFORNIA POLYTECHNIC STATE UNIVERSITY SAN LUIS OBISPO"/>
<node id="THE CLIC DESIGN"/>
<node id="AUTHORIZED LICENSED USE LIMITED TO CALIFORNIA POLYTECHNIC STATE UNIVERSITY SAN LUIS OBISPO"/>
<node id="INTERRUPT STATE"/>
<node id="PUSHED"/>
<node id="THE LATTER"/>
<node id="THE EXECUTION TIME OF THE AUTHORIZED LICENSED USE LIMITED TO CALIFORNIA POLYTECHNIC STATE UNIVERSITY SAN LUIS OBISPO"/>
<node id="DATA"/>
<node id="MAY 28, 2025 AT 17:50:39 UTC"/>
<node id="IEEE XPLORE"/>
<node id="MAY 28, 2025"/>
<node id="17:50:39 UTC"/>
<node id="FAST INTERRUPT AND CONTEXT SWITCHING"/>
<node id="GPOSRTOS SCHEDULER AND THE USER CODE"/>
<node id="CAPABILITY OF THE SYSTEM TO PROVIDE TIMELY RESPONSES TO ASYNCHRONOUS EVENTS"/>
<node id="TABLE I"/>
<node id="NESTED INTERRUPT PREEMPTION SCHEME"/>
<node id="RISC-V CLIC"/>
<node id="THE NON-NESTED INTERRUPT CASE"/>
<node id="BANK SWITCHING AND THE NESTED INTERRUPT CASE"/>
<node id="AN AUTOMATIC CONTEXT-SAVING MECHANISM IN THE BACKGROUND"/>
<node id="C. CV32RTFASTIRQ"/>
<node id="FAST INTERRUPT EXTENSION"/>
<node id="A BLOCK DIAGRAM OF CV32RTFASTIRQ"/>
<node id="SW INTERRUPT"/>
<node id="WRITING TO CLICS MEMORY MAP"/>
<node id="POINTER"/>
<node id="APPROPRIATELY BEFORE TRIGGERING AN SW INTERRUPT"/>
<node id="FAST INTERRUPT AND CONTEXT SWITCHING 1043 INSTRUCTION IN MACHINE MODE"/>
<node id="EMRET"/>
<node id="REDUNDANT CONTEXT SAVING AND RESTORING SEQUENCES"/>
<node id="DIRECTLY JUMPING TO THE NEXT AVAILABLE INTERRUPT HANDLER"/>
<node id="LOW INTERRUPT LATENCY"/>
<node id="CRUCIAL METRICS"/>
<node id="CONTEXT SWITCH TIME"/>
<node id="A WIDE RANGE OF PLATFORMS"/>
<node id="PLATFORMS"/>
<node id="COMMODITY MCU-CLASS EMBEDDED SYSTEMS"/>
<node id="MORE ADVANCED AND COMPLEX APPLICATION-CLASS MIXED CRITICALITY SYSTEMS (MCSS)"/>
<node id="TIMESAFETY-CRITICAL AND NON-CRITICAL APPLICATIONS"/>
<node id="DIFFERENT ISOLATED PARTITIONS"/>
<node id="THE SAME HW PLATFORM 12"/>
<node id="RESPONSE AND CONTEXT SWITCH TIME MINIMIZATION"/>
<node id="A CHALLENGE TO BE TACKLED AT THE HWSW INTERFACE"/>
<node id="SW PROGRAMMING TECHNIQUES AND HW INTERRUPT CONTROLLER ARCHITECTURES"/>
<node id="TO ENSURE MINIMAL RESPONSE TIME"/>
<node id="COMMERCIAL VENDORS AND IP PROVIDERS"/>
<node id="SUCH FEATURES AS IN-HOUSE SOLUTIONS"/>
<node id="OFTEN PROPRIETARY"/>
<node id="TIGHTLY COUPLED WITH THE VENDORS INSTRUCTION SET ARCHITECTURE (ISA)"/>
<node id="TIGHTLY COUPLED WITH THE VENDORS TARGET HW FAMILY"/>
<node id="TIGHTLY COUPLED WITH THE VENDORS ASSOCIATED SW STACK"/>
<node id="RISC-V ECOSYSTEM 13"/>
<node id="A MODULAR, FREE, AND OPEN-SOURCE ISA"/>
<node id="THE DE FACTO LINGUA FRANCA OF COMPUTING"/>
<node id="RISC-V SUPPORT FOR FAST INTERRUPT AND CONTEXT SWITCH HANDLING"/>
<node id="INCUMBENT PROPRIETARY ARCHITECTURES"/>
<node id="FLEXIBLE INTERRUPT PRIORITIZATION"/>
<node id="PREEMPTION MECHANISMS"/>
<node id="MODULAR RISC-V ISA"/>
<node id="DEVELOPING ORTHOGONAL CUSTOM EXTENSIONS"/>
<node id="FEW PUBLISHED WORKS"/>
<node id="THE PROBLEM OF MINIMIZING INTERRUPT LATENCY AND CONTEXT SWITCH TIME FROM A HOLISTIC (HW AND SW) VIEWPOINT FOR RISC-V"/>
<node id="THE GAP WITH MORE ESTABLISHED PROPRIETARY SOLUTIONS"/>
<node id="CLOSING THE GAP WITH MORE ESTABLISHED PROPRIETARY SOLUTIONS"/>
<node id="RISC-V AS A VALUABLE CANDIDATE FOR TIME- AND SAFETY-CRITICAL APPLICATION DOMAINS SUCH AS AUTOMOTIVE AND AEROSPACE"/>
<node id="AN OPEN-SOURCE SOLUTION TO BE SHARED WITH THE COMMUNITY"/>
<node id="VARIOUS FLAVORS OF RISC-V CORE-SPECIFIC INTERRUPT CONTROLLERS"/>
<node id="INTERRUPT HANDLERS"/>
<node id="NESTING"/>
<node id="CALLING OF C-FUNCTIONS WITHIN IT"/>
<node id="CALLING OF C-FUNCTIONS"/>
<node id="SAVING AND RESTORING STATE FOLLOWING THE C-ABI"/>
<node id="OUR DESIGN"/>
<node id="INTERRUPT LATENCIES OF SIX CLOCK CYCLES"/>
<node id="EFFICIENT BACK-TO-BACK INTERRUPT HANDLING IN 12 CYCLES"/>
<node id="INTERRUPT LATENCIES OF SIX CLOCK CYCLES AND EFFICIENT BACK-TO-BACK INTERRUPT HANDLING IN 12 CYCLES"/>
<node id="THE FASTEST AVAILABLE APPROACHES CURRENTLY IMPLEMENTED IN THE RISC-V LANDSCAPE"/>
<node id="THE FASTEST AVAILABLE APPROACHES"/>
<node id="FULLY OPEN-SOURCE"/>
<node id="COMPETITIVE AGAINST CLOSED-SOURCE AND PROPRIETARY COMMERCIAL SOLUTIONS"/>
<node id="THE IMPLEMENTATION AVAILABLE UNDER A PERMISSIVE OPEN-SOURCE LICENSE"/>
<node id="A FAST INTERRUPT EXTENSION (FASTIRQ)"/>
<node id="FAST INTERRUPT EXTENSION (FASTIRQ)"/>
<node id="BOTH NESTED INTERRUPT CASE SCENARIOS"/>
<node id="NON-NESTED INTERRUPT CASE SCENARIOS"/>
<node id="HIDING THE LATENCY THROUGH MEMORY BANKS"/>
<node id="A BACKGROUND-SAVING MECHANISM"/>
<node id="CLINT AND CLIC"/>
<node id="ABOUT 33 CYCLES INTERRUPT LATENCY"/>
<node id="INTERRUPT LATENCY TO SIX CYCLES"/>
<node id="THE SAME MECHANISM"/>
<node id="ONE TO ACCELERATE CONTEXT SWITCHING THROUGH HWSW COOPERATION"/>
<node id="THE PROPOSED SOLUTION"/>
<node id="RISC-V AS A COMPETITIVE CANDIDATE FOR BUILDING THE NEXT GENERATION OF TIME-CRITICAL SYSTEMS"/>
<node id="THIS CORE"/>
<node id="CONTROLPULP 19"/>
<node id="A SOC"/>
<node id="RUNNING REAL-TIME WORKLOADS"/>
<node id="A CV32 MANAGER CORE"/>
<node id="A PROGRAMMABLE ACCELERATOR SUBSYSTEM"/>
<node id="EIGHT CV32 CORES"/>
<node id="A SET OF STANDARD PERIPHERALS"/>
<node id="QUAD SERIAL PERIPHERAL INTERFACE (QSPI)"/>
<node id="INTER-INTEGRATED CIRCUIT (I2C)"/>
<node id="UNIVERSAL ASYNCHRONOUS RECEIVER-TRANSMITTER (UART)"/>
<node id="THE MANAGER CORE"/>
<node id="SCHEDULING TASKS"/>
<node id="COMMUNICATING WITH THE PERIPHERALS"/>
<node id="OFFLOADING TASKS TO THE ACCELERATOR SUBSYSTEM"/>
<node id="ASYNCHRONOUS EXTERNAL EVENTS"/>
<node id="INTERRUPTS"/>
<node id="CONTROLPULP"/>
<node id="A SET OF SCRATCHPAD MEMORIES (SPMS)"/>
<node id="SCRATCHPAD MEMORIES (SPMS)"/>
<node id="SINGLE-CYCLE ACCESS TIME"/>
<node id="THE CV32 MANAGER CORE"/>
<node id="THIS DESIGN CHOICE"/>
<node id="DETERMINISTIC MEMORY ACCESS LATENCY FOR BOTH DATA LOAD, STORE, AND INSTRUCTION FETCH"/>
<node id="THE WORST CASE LATENCY WHEN HANDLING UNPREDICTABLE EVENTS"/>
<node id="APPLICATIONS"/>
<node id="FREERTOS 20"/>
<node id="AN OPEN-SOURCE PRIORITY-BASED PREEMPTIVE RTOS"/>
<node id="TASKS"/>
<node id="DATE"/>
<node id="6 JUNE 2024"/>
<node id="INTERRUPT SOURCES"/>
<node id="LEVEL CHANGE OF THE INTERRUPT LINE"/>
<node id="EDGE-TRIGGERED INTERRUPT"/>
<node id="LOGIC LEVEL ITSELF"/>
<node id="LEVEL-TRIGGERED INTERRUPT"/>
<node id="NESTED INTERRUPTS"/>
<node id="PREEMPTION OF A LOW-LEVEL INTERRUPT BY A HIGH-LEVEL INTERRUPT"/>
<node id="TRANSITION"/>
<node id="DIFFERENT INTERRUPT LEVELS"/>
<node id="THE RECEIVING SIDE OF THE INTERRUPT TO CLEAR THE SOURCE OFTEN THROUGH ACCESSING APPROPRIATE HW REGISTERS"/>
<node id="THE FORMER"/>
<node id="UNIDIRECTIONAL NOTIFICATION WITHOUT CONFIRMATION"/>
<node id="ASYNCHRONOUS EVENTS"/>
<node id="THE NORMAL PROGRAM ORDER EXECUTION"/>
<node id="A SWITCH TO A DIFFERENT CONTEXT TO HANDLE THE EVENT"/>
<node id="A PROCESSOR"/>
<node id="VECTORED INTERRUPTS"/>
<node id="EACH INTERRUPT"/>
<node id="A SPECIFIC INTERRUPT SERVICE ROUTINE (ISR)"/>
<node id="AN INTERRUPT VECTOR TABLE"/>
<node id="FAST INTERRUPT RESPONSE"/>
<node id="INCREASED CODE SIZE"/>
<node id="INTERRUPT VECTORING"/>
<node id="IMPROVE INTERRUPT LATENCIES"/>
<node id="NON-VECTORED OR DIRECT INTERRUPTS"/>
<node id="A SHARED ISR"/>
<node id="THE LATTER APPROACH"/>
<node id="CODE SIZE OFF FOR A SLOWER INTERRUPT RESPONSE"/>
<node id="THE OVERHEAD OF RESOLVING THE INTERRUPTION CAUSE AND JUMPING TO THE CORRECT ISR"/>
<node id="EXPLICIT INSTRUCTIONS"/>
<node id="THE INTERRUPT TABLE"/>
<node id="MUCH MORE COMPACT"/>
<node id="MULTIPLE SOURCES"/>
<node id="INTERRUPT LATENCY IN A SYSTEM"/>
<node id="UNDERLYING HW"/>
<node id="SCHEDULER OR OS"/>
<node id="APPLICATION RUNNING ON TOP"/>
<node id="MINIMIZING THE LATENCY IMPOSED BY THE HW"/>
<node id="ITS HW-CONTRIBUTED PART"/>
<node id="RESTORING THE INTERRUPT CONTEXT AND THE REGULAR INTERRUPT LATENCY"/>
<node id="THIS BREAKDOWN"/>
<node id="THE TIME IT TAKES FROM AN INTERRUPT EDGE ARRIVING AT THE HW TO THE EXECUTION OF THE FIRST INSTRUCTION OF THE CORRESPONDING INTERRUPT HANDLER ROUTINE"/>
<node id="INTERRUPT EDGE"/>
<node id="THE HW"/>
<node id="HW"/>
<node id="THE INTERRUPT CONTROLLER"/>
<node id="NUMBER OF CYCLES IT TAKES FOR AN INTERRUPT TO ARRIVE AT THE INTERRUPT CONTROLLER INPUT TO THE FIRST INSTRUCTION OF AN INTERRUPT HANDLER THAT ALLOWS THE CALLING OF A C-FUNCTION"/>
<node id="TO MAKE A FAIR COMPARISON BETWEEN SW-BASED AND MORE HW-ORIENTED INTERRUPT SOLUTIONS"/>
<node id="WHAT EXACTLY CONSTITUTES THE FIRST INSTRUCTION OF THE INTERRUPT HANDLER"/>
<node id="EACH HW CONFIGURATION"/>
<node id="A HANDWRITTEN OPTIMIZED INTERRUPT HANDLER"/>
<node id="HANDWRITTEN OPTIMIZED INTERRUPT HANDLER"/>
<node id="ALL REQUIRED GENERAL-PURPOSE AND MACHINE-SPECIFIC REGISTERS FOR NESTING INTERRUPTS"/>
<node id="THE FIRST INSTRUCTION"/>
<node id="THE ONE AFTER ALL NECESSARY INTERRUPT CONTEXT HAS BEEN SAVED ON THE STACK TO BE ABLE TO CALL A FUNCTION"/>
<node id="WHAT EXACTLY ENTAILS A FUNCTION CALL"/>
<node id="THE USED APPLICATION BINARY INTERFACE (ABI)"/>
<node id="ITS CALLING CONVENTION"/>
<node id="INTERRUPT HANDLER AND INTERRUPT CONTEXT SAVING CODE"/>
<node id="SOME OF THE CONTEXT SAVING CODE"/>
<node id="REDUNDANT"/>
<node id="THE ACTIVE INTERRUPT HANDLERS CONTEXT"/>
<node id="RESTORED ON INTERRUPT RETURN"/>
<node id="IMMEDIATELY SAVED AGAIN"/>
<node id="THE NEXT PENDING INTERRUPT FIRING"/>
<node id="RESTORING THE PRE-INTERRUPT CONTEXT"/>
<node id="SW FOR THE NESTED INTERRUPT CASE"/>
<node id="CONTEXT SWITCHING TIME"/>
<node id="THE RESPONSIVENESS OF THE ARCHITECTURE IN SWAPPING FROM ONE EXECUTION CONTEXT TO ANOTHER"/>
<node id="THE EXECUTION CONTEXT"/>
<node id="THE OS BEING USED"/>
<node id="THE STATE OF THE ARCHITECTURAL REGISTERS OF THE ISA"/>
<node id="THE CHOSEN ABI"/>
<node id="PREEMPTION"/>
<node id="AN INTERRUPT REQUEST"/>
<node id="A CURRENT TASK"/>
<node id="PURPOSE OF PREEMPTION"/>
<node id="RESUMING ITS EXECUTION LATER"/>
<node id="THE SIMPLEST CASE FOR PREEMPTION"/>
<node id="NON-NESTED INTERRUPT HANDLERS"/>
<node id="GLOBALLY DURING THE EXECUTION OF AN ISR"/>
<node id="A MORE COMPLEX CASE FOR PREEMPTION"/>
<node id="NESTED INTERRUPT HANDLERS"/>
<node id="THE CASE OF MULTIPLE INTERRUPTS AT A TIME"/>
<node id="THIS SITUATION"/>
<node id="PENDING INTERRUPTS TO BE SERVICED IN SEQUENCE ACCORDING TO INCREASING PRIORITY"/>
<node id="LEVELPRIORITY ARBITRATION"/>
<node id="SW-DRIVEN"/>
<node id="PRIORITY SIMPLESTANDARD INTERRUPT HANDLERS"/>
<node id="HIGHEST PRIORITY INTERRUPT IDENTIFICATION CODE"/>
<node id="NOT EXECUTED"/>
<node id="LEVELPRIORITY ARBITRATION LOGIC"/>
<node id="INTERRUPT CONTROLLER"/>
<node id="HIGHEST LEVELPRIORITY INTERRUPT"/>
<node id="PENDING BUT DISABLED"/>
<node id="CORE"/>
<node id="LEVELPRIORITY INTERRUPT SCHEME"/>
<node id="ADDITIONAL MASKING OF INCOMING INTERRUPTS OF EQUAL OR LOWER LEVELPRIORITY THAN THE EXECUTING ISR"/>
<node id="ADDITIONAL MASKING OF INCOMING INTERRUPTS SOMETIMES LARGER THAN A CONFIGURABLE LEVELPRIORITY THRESHOLD"/>
<node id="THIS SCENARIO"/>
<node id="REAL-TIME AND COMPLEX EMBEDDED SYSTEMS"/>
<node id="SEQUENTIALLY"/>
<node id="ATTEMPTS TO HANDLE A MORE COMPLEX INTERRUPT SCHEME"/>
<node id="SW EMULATION"/>
<node id="UNTENABLE INTERRUPT LATENCIES"/>
<node id="A HIGH-PRIORITY INTERRUPT"/>
<node id="A LOWER-PRIORITY INTERRUPT TO FINISH"/>
<node id="GLOBALLY ENABLED"/>
<node id="THE SCOPE OF AN EXECUTING ISR"/>
<node id="GLOBAL INTERRUPTS"/>
<node id="DISABLED WHENEVER AN INTERRUPT HANDLER IS ENTERED"/>
<node id="THE ISR"/>
<node id="CARE DESIGNED TO ENSURE THEY ARE REENTRANT"/>
<node id="THE NESTING"/>
<node id="HIGHER PRIORITY INTERRUPTS TO PREEMPT A CURRENT LOWER PRIORITY ISR EXECUTING"/>
<node id="REDUNDANT INTERRUPT CONTEXT"/>
<node id="BACK-TO-BACK INTERRUPTS"/>
<node id="INTERRUPTS THAT NEED TO BE SERVED SEQUENTIALLY"/>
<node id="WHENEVER THERE ARE MULTIPLE INTERRUPTS PENDING"/>
<node id="THE TRANSITION FROM ONE INTERRUPT TO THE NEXT ONE"/>
<node id="A REDUNDANT SEQUENCE OF CONTEXT RESTORES AND CONTEXT SAVES"/>
<node id="REDUNDANT CONTEXT RESTORE SEQUENCES"/>
<node id="HIGHER INTERRUPT LOADS"/>
<node id="REDUNDANT CONTEXT RESTORE"/>
<node id="UNWANTED ADDITIONAL INTERRUPT LATENCY"/>
<node id="NON-NESTED OR NESTED HORIZONTAL INTERRUPTS"/>
<node id="TWO NON-PREEMPTIVE INTERRUPTS"/>
<node id="TWO INTERRUPTS WITH SAME LEVEL BUT DIFFERENT PRIORITIES"/>
<node id="TAIL-CHAINING"/>
<node id="OPTIMIZE IT"/>
<node id="INTERRUPT CONTEXT RESTORE AND STORE SEQUENCE BETWEEN BACK-TO-BACK INTERRUPTS"/>
<node id="THE REDUNDANT CONTEXT RESTORING SEQUENCES"/>
<node id="THE FULL INTERRUPT EXIT CODE SEQUENCE"/>
<node id="REDUNDANT CONTEXT RESTORE WITH NON-NESTED INTERRUPTS"/>
<node id="CHAINING TWO BACK-TO-BACK INTERRUPTS"/>
<node id="BYPASSING THE SUPERFLUOUS RESTORESAVE OPERATION"/>
<node id="SECTION III-D"/>
<node id="THE OPTIMIZATIONS IMPLEMENTED IN THIS WORK TO ADDRESS THIS SCENARIO"/>
<node id="THE PRIVILEGED SPECIFICATION 14"/>
<node id="A SIMPLE INTERRUPT SCHEME"/>
<node id="A SET OF TIMER AND INTER-PROCESSOR INTERRUPTS"/>
<node id="THE RISC-V ECOSYSTEM"/>
<node id="PENDING AND ENABLED INTERRUPTS"/>
<node id="A THRESHOLD VALUE REPRESENTING AN INTERRUPT LEVEL"/>
<node id="THRESHOLD VALUE"/>
<node id="A CSR"/>
<node id="COMPARED TO STANDARD RISC-V PRIVILEGED SPECIFICATIONS"/>
<node id="PLATFORM-LEVEL INTERRUPT CONTROLLER SPECIFICATION"/>
<node id="A FIXED PRIORITY INTERRUPT SCHEME"/>
<node id="16 PREDEFINED OR RESERVED INTERRUPTS"/>
<node id="16 IMPLEMENTATION-DEFINED INTERRUPTS"/>
<node id="OPTIONALLY VECTORED"/>
<node id="THE CLINT"/>
<node id="PRIORITIZATION OF INTERRUPTS BASED ON PRIVILEGE MODE"/>
<node id="THE INFLEXIBLE INTERRUPT SCHEME OF THE CLINT-MODE"/>
<node id="MUCH MORE WORK TO BE DONE IN MANAGING INTERRUPT MASK (SOMEIRQMASK) AND OTHER MACHINE STATE"/>
<node id="INTERRUPTS WITH LOWER PRIORITY THAN THE CURRENT INTERRUPT RUNNING"/>
<node id="IN CLINT-MODE WHEN GLOBAL INTERRUPTS ARE RE-ENABLED"/>
<node id="FINE-GRAINED CONTROL OVER INTERRUPT PRIORITIZATION"/>
<node id="PLIC 22"/>
<node id="CLINT"/>
<node id="NUMBER OF CUSTOM INTERRUPTS"/>
<node id="ACTIVE INTERRUPT HANDLER"/>
<node id="IRQ2"/>
<node id="IRQ1"/>
<node id="PENDING"/>
<node id="PRIVILEGE MODE (VERTICAL INTERRUPTS)"/>
<node id="INTERRUPT LEVEL WHEN THE PRIVILEGE MODE IS THE SAME (HORIZONTAL INTERRUPTS)"/>
<node id="NUMBER OF INTERRUPTS"/>
<node id="FLEXIBLE (AT DESIGN TIME)"/>
<node id="ONE OR MORE TARGETS"/>
<node id="INTERRUPTS THAT ARE ASSIGNED A HIGHER LEVEL"/>
<node id="LOWER-LEVEL INTERRUPTS"/>
<node id="A PRIORITY"/>
<node id="EACH TARGET"/>
<node id="A THRESHOLD"/>
<node id="BELOW THE THRESHOLD"/>
<node id="INTERRUPT SELECTION"/>
<node id="THE CLIC IN HW"/>
<node id="THE HIGHEST LEVEL, HIGHEST PRIORITY PENDING INTERRUPT TO THE CORES INTERFACE"/>
<node id="INTERRUPT PRIORITY"/>
<node id="TIE-BREAKER"/>
<node id="THE CASE OF MULTIPLE INTERRUPTS PENDING WITH THE SAME LEVEL"/>
<node id="SELECTING THE INTERRUPT WITH MAXIMUM LEVEL AND PRIORITY"/>
<node id="THREE BINARY TREES"/>
<node id="ENABLED INTERRUPTS AND THEIR LEVEL AND PRIORITY INFORMATION"/>
<node id="PRIORITIZATION LOGIC"/>
<node id="BINARY ARBITRATION TREE"/>
<node id="HIGHEST-LEVEL INTERRUPT"/>
<node id="A LEVEL"/>
<node id="THE PRIORITIES"/>
<node id="CONCURRENT PENDING INTERRUPTS TO BE TAKEN IN THE ORDER PREFERRED BY THE PROGRAMMER"/>
<node id="THE LEVEL INFORMATION"/>
<node id="PRE-EMPTION OF SAME-PRIVILEGE LEVEL INTERRUPTS"/>
<node id="SAME-PRIVILEGE LEVEL INTERRUPTS"/>
<node id="HORIZONTAL INTERRUPTS"/>
<node id="THIS SCHEME"/>
<node id="INTERRUPTS TO BE DIVIDED ACCORDING TO THEIR PRIORITIES ON THE PLIC-LEVEL"/>
<node id="SOME FLEXIBILITY IN TERMS OF PRIORITIZATION"/>
<node id="THE FLEXIBILITY PROBLEM ON THE CORE LOCAL-LEVEL"/>
<node id="CLIC 15"/>
<node id="THESE LIMITATIONS"/>
<node id="INTERRUPTS TO BE PRIORITIZED"/>
<node id="SO-CALLED LEVELS AND PRIORITIES"/>
<node id="MULTIPLE HORIZONTAL INTERRUPTS"/>
<node id="EQUAL LEVELS AND PRIORITIES"/>
<node id="THE CASE WHERE MULTIPLE HORIZONTAL INTERRUPTS HAVE EQUAL LEVELS AND PRIORITIES"/>
<node id="THE CLIC SELECTING THE HIGHEST NUMBERED INTERRUPT 15"/>
<node id="INTERRUPT 15"/>
<node id="AN ARBITRARY ASSIGNMENT"/>
<node id="DECIDED AT DESIGN TIME"/>
<node id="THIS FEATURE"/>
<node id="RTOSS THAT ONLY WANT TO DISABLE A SUBSET OF ALL INTERRUPTS DURING CRITICAL SECTIONS"/>
<node id="ONLY A SUBSET OF INTERRUPTS"/>
<node id="DISABLED"/>
<node id="INTERRUPTS THAT DO NOT INTERFERE WITH THE DATA ACCESSED IN SUCH A CRITICAL SECTION"/>
<node id="STILL FIRE"/>
<node id="PRIV1"/>
<node id="L1"/>
<node id="P1"/>
<node id="PRIV2"/>
<node id="L2"/>
<node id="P2"/>
<node id="IRQ1 LEVEL"/>
<node id="INTERRUPT THRESHOLD"/>
<node id="IRQ2 LEVEL"/>
<node id="INTERRUPTS FIRED FROM DIFFERENT PRIVILEGE MODES"/>
<node id="VERTICAL INTERRUPTS"/>
<node id="INTERRUPTS FIRED FROM THE SAME PRIVILEGE MODE"/>
<node id="PREEMPTION CONDITIONS OF TWO NESTED INTERRUPTS IRQ2 AND IRQ1"/>
<node id="PREEMPTION CONDITIONS"/>
<node id="CLIC SPECIFICATION"/>
<node id="THE CLIC SPECIFICATION"/>
<node id="THE CASE OF REDUNDANT CONTEXT RESTORE"/>
<node id="XNXTI"/>
<node id="A CSR SHORT FOR NEXT INTERRUPT HANDLER ADDRESS"/>
<node id="AN INTERRUPT-ENABLE CSR"/>
<node id="NON-VECTORED INTERRUPTS"/>
<node id="READING FROM THIS CSR"/>
<node id="TO FAST-TRACK INTERRUPTS THAT ARRIVE LATE"/>
<node id="TO AVOID REDUNDANT CONTEXT SAVERESTORE"/>
<node id="REDUNDANT CONTEXT SAVERESTORE"/>
<node id="RUNNING THROUGH PENDING INTERRUPTS BACK-TO-BACK"/>
<node id="AN ACTIVE HANDLER"/>
<node id="ARCHITECTURE"/>
<node id="HW TO STORE ENOUGH INFORMATION"/>
<node id="ENOUGH INFORMATION"/>
<node id="RESUME OPERATION CORRECTLY AFTER RETURNING FROM THE AFOREMENTIONED CONTEXT"/>
<node id="CONTROLLING THE AMOUNT OF STATE THAT NEEDS TO BE PRESERVED TO ENTER AND LEAVE AN INTERRUPT CONTEXT"/>
<node id="INCREASING THE BANDWIDTH AND DECREASING THE LATENCY TO MEMORY"/>
<node id="RELYING ON LATENCY-HIDING TECHNIQUES THAT DEFER THE EFFECTIVE SAVING OF THE STATE TO A LATER POINT IN TIME"/>
<node id="ARCHITECTURAL FEATURES AND HWSW CODESIGN OF CV32RT"/>
<node id="TYPICAL CASE SCENARIOS"/>
<node id="THE BACKGROUND-SAVING MECHANISM"/>
<node id="THE STACK POINTER"/>
<node id="THE BANK-SWITCHED CONTENTS IN MEMORY"/>
<node id="THE EXECUTION OF THE CORE"/>
<node id="IN PARALLEL"/>
<node id="AN OVERVIEW OF THE DESIGN"/>
<node id="INCOMING INTERRUPTS"/>
<node id="A GATEWAY MODULE"/>
<node id="THERE IS A PENDING AND ENABLED REQUEST FOR EACH INTERRUPT SOURCE I (IRQ I)"/>
<node id="THE GATEWAY"/>
<node id="PROGRAMMABLE CONFIGURATION INFORMATION"/>
<node id="EACH INTERRUPT LINE"/>
<node id="LEVEL"/>
<node id="PRIORITY"/>
<node id="ENABLE STATUS"/>
<node id="SENSITIVITY (LEVELEDGE)"/>
<node id="THE INTERRUPT PRIORITIZATION MODULE"/>
<node id="THE TREE FROM LEAVES TO THE ROOT"/>
<node id="THE SOUGHT-AFTER MAXIMUM LEVEL AND PRIORITY INTERRUPT"/>
<node id="EACH TREE"/>
<node id="LOW OVERHEAD IN TERMS OF AREA AND DELAY"/>
<node id="O(N) AND O(LOG(N)) RESPECTIVELY"/>
<node id="THE INTERRUPT"/>
<node id="THE CORE"/>
<node id="A HANDSHAKE-BASED INTERFACE"/>
<node id="THE ADDITIONAL KILL SIGNAL"/>
<node id="A HANDSHAKE TO RESTART"/>
<node id="A POTENTIALLY MORE IMPORTANT INTERRUPT CAN BE PRESENTED TO THE CORE"/>
<node id="N 4096 LOCAL INTERRUPT SOURCES"/>
<node id="ADDITIONAL PIPELINE STAGES"/>
<node id="THE ARBITRATION TREE"/>
<node id="RELAX TIMING"/>
<node id="OUR VERSION OF THE CLIC"/>
<node id="SHV"/>
<node id="THE XNXTI CSR"/>
<node id="THE CLIC"/>
<node id="THIS WITH THE LEVELPRIORITY SCHEME"/>
<node id="BY MOVING THE INTERRUPT STATE SAVING LOGIC IN HW"/>
<node id="EMRET TO HANDLE REDUNDANT INTERRUPT CONTEXT SEQUENCES"/>
<node id="A WRAPPER AROUND THE CORES RF"/>
<node id="THE RF BY AN ADDITIONAL READ PORT FOR THE BACKGROUND-SAVING MECHANISM"/>
<node id="REGISTERS FOR LATCHING THE ADDITIONAL PROCESSOR STATE"/>
<node id="THE ADDITIONAL PROCESSOR STATE"/>
<node id="PROPER INTERRUPT NESTING"/>
<node id="A NEW INTERRUPT AT THE CLIC"/>
<node id="THE INTERRUPT LEVEL EXCEEDS THE CONFIGURED THRESHOLD"/>
<node id="THE SAVING LOGIC FINITE STATE MACHINE (FSM)"/>
<node id="THE EXTENDED RF"/>
<node id="THE CORES STATE MACHINE"/>
<node id="THE PIPELINE"/>
<node id="THE PROGRAM COUNTER"/>
<node id="THE VECTOR TABLE ENTRY"/>
<node id="SAVING LOGIC"/>
<node id="BANK SWITCH"/>
<node id="INTERRUPT CONTEXT TO HAVE A FRESH SET OF REGISTERS"/>
<node id="OTHER BANK CONTENTS"/>
<node id="SEPARATE PORT TO THE MAIN MEMORY"/>
<node id="RF BANKS"/>
<node id="ON AN INTERRUPT"/>
<node id="STACK POINTER"/>
<node id="DURING A BANK SWITCH"/>
<node id="A DEDICATED ADDER BETWEEN THE TWO RFS"/>
<node id="THE RISC-V EMBEDDED AND INTEGER ABI"/>
<node id="THAT THE STACK POINTER POINTS BELOW THE LAST SAVED REGISTER ON THE STACK"/>
<node id="APPROPRIATELY TO MAINTAIN ABI INVARIANTS"/>
<node id="THE PROGRAM CODE RUNNING IN THE INTERRUPT HANDLER"/>
<node id="THE VALUES ON THE STACK"/>
<node id="THIS MECHANISM FOR LEAF-TYPE INTERRUPTS"/>
<node id="INTERACTIONS"/>
<node id="BACKGROUND-SAVING MECHANISM AND REGULAR LOADSTORE INSTRUCTIONS OF THE CORE"/>
<node id="INCORRECT EXECUTION"/>
<node id="LOAD INSTRUCTION"/>
<node id="ARCHITECTURAL REGISTER"/>
<node id="BACKGROUND-SAVING MECHANISM"/>
<node id="SAME REGISTER"/>
<node id="NO CONFLICT"/>
<node id="INTERRUPTS ARE INJECTED INTO THE PIPELINE"/>
<node id="WRITE-BACK STAGE OF THE CORE"/>
<node id="UPDATES TO THE RF"/>
<node id="RESOLVED"/>
<node id="BANK SWITCHING OPERATION"/>
<node id="AT THIS POINT"/>
<node id="CORRECTNESS OF THE EXECUTION"/>
<node id="INTERRUPT HANDLER"/>
<node id="ALREADY EXECUTING"/>
<node id="LOADS OR STORES"/>
<node id="STACK MEMORY REGIONS"/>
<node id="LOADS OR STORES ACCESSING STACK MEMORY REGIONS WHERE THE BACKGROUND-SAVING MECHANISM IS WRITING TO"/>
<node id="PROPERLY RESOLVED"/>
<node id="EXECUTION OF THE HANDLER"/>
<node id="IF THIS HAPPENS WHILE THE BACKGROUND-SAVING MECHANISM IS STILL AT WORK"/>
<node id="STALE DATA"/>
<node id="DATA THAT IS IMMEDIATELY OVERWRITTEN"/>
<node id="A STRAIGHTFORWARD SOLUTION"/>
<node id="TO STALL THE CORES PIPELINE WHILE THE BACKGROUND-SAVING MECHANISM IS AT WORK"/>
<node id="A LOAD SOON IN THE INTERRUPT HANDLER"/>
<node id="SPACE AVAILABLE"/>
<node id="INTERRUPT STATE WORD BY WORD"/>
<node id="ADDRESS OFFSET OF THE LAST WORD PUSHED OUT BY THE BACKGROUND-SAVING MECHANISM"/>
<node id="ANY INCOMING LOAD AND STORES"/>
<node id="ADDRESS OFFSET OF THE LAST WORD"/>
<node id="LOAD-STORE UNIT"/>
<node id="LOAD AND STORES THAT TRY TO ACCESS DATA THAT IS NOT YET PUSHED TO MEMORY"/>
<node id="THE CORES PIPELINE TO STALL"/>
<node id="THIS MECHANISM"/>
<node id="THE CORRECTNESS OF LOADS AND STORES ISSUED BY THE CORE"/>
<node id="A SYSTEM CALL HANDLER"/>
<node id="THE ECALL INSTRUCTION IN RISC-V"/>
<node id="USER-PROVIDED ARGUMENTS"/>
<node id="MOST"/>
<node id="GENERAL-PURPOSE REGISTERS"/>
<node id="SOME"/>
<node id="THE STACK"/>
<node id="SHORT INTERRUPT HANDLERS"/>
<node id="THE FULL INTERRUPT STATE HAS BEEN SAVED"/>
<node id="EACH OF THESE CASES"/>
<node id="THE STALLING LOGIC OUTLINED IN SECTION III-C3"/>
<node id="HIGHER INTERRUPT LATENCIES"/>
<node id="SOME REGISTER VALUES"/>
<node id="BECAUSE THEY MIGHT NOT HAVE REACHED THE LOAD-STORE UNIT YET"/>
<node id="THE APPROACH"/>
<node id="HW COMPLEXITY"/>
<node id="A DYNAMIC ADDRESS LOOKUP INTO A QUEUE-LIKE BUFFER"/>
<node id="AN SW-BASED SOLUTION"/>
<node id="ANY KIND OF STALLING"/>
<node id="THE PIPELINE STALLING LOGIC"/>
<node id="ORDERING THE LOADS TO ACCESS THE ALREADY STORED INTERRUPT STATE FIRST"/>
<node id="AN ANALYSIS OF THE FUNCTIONAL IMPROVEMENTS"/>
<node id="THE CV32S RF"/>
<node id="ADDITIONAL LOGIC FOR THE BACKGROUND-SAVING MECHANISM"/>
<node id="PARTS OF THE OLD MEMORY BANK (THE INTERRUPT CONTEXT)"/>
<node id="THE CORES STACK LOCATION"/>
<node id="EXECUTION"/>
<node id="THE NEW BANK"/>
<node id="A DEDICATED MEMORY PORT FOR THE BACKGROUND-SAVING MECHANISM"/>
<node id="THE MEMORY PORT"/>
<node id="THE PORT FROM THE LOAD-STORE UNIT"/>
<node id="THE GENERAL-PURPOSE REGISTERS X1, X2"/>
<node id="USE THE SAME ORDER TO LOAD WORDS BACK IN THE INTERRUPT HANDLER"/>
<node id="THE PROGRAMMER"/>
<node id="THAT TO ACHIEVE THE BEST POSSIBLE LATENCY"/>
<node id="PROGRAMMER"/>
<node id="COMPILER-SPECIFIC ATTRIBUTES TO WRITE INTERRUPT HANDLERS"/>
<node id="ATTRIBUTE((INTERRUPT))"/>
<node id="GCC"/>
<node id="COMPILER"/>
<node id="THE FACT THAT PROGRAMMER USES COMPILER-SPECIFIC ATTRIBUTES"/>
<node id="INTERRUPT HANDLER ROUTINES"/>
<node id="IN FIG."/>
<node id="AN OVERVIEW OF HOW NESTED INTERRUPT HANDLING CODE WORKS FOR THE BASIC CLINT-MODE"/>
<node id="AN OVERVIEW OF HOW NESTED INTERRUPT HANDLING CODE WORKS FOR THE BASELINE CLIC"/>
<node id="AN OVERVIEW OF HOW NESTED INTERRUPT HANDLING CODE WORKS FOR OUR FASTIRQ EXTENSION"/>
<node id="ROUTINES"/>
<node id="SAVING STATE FOR VECTORED NESTING INTERRUPTS"/>
<node id="(A) CLINT"/>
<node id="(B) CLIC"/>
<node id="(C) PROPOSED FASTIRQ EXTENSION"/>
<node id="RV32E AS A VARIANT OF RV32I"/>
<node id="RV32E"/>
<node id="16 REGISTERS"/>
<node id="RV32I"/>
<node id="32 REGISTERS"/>
<node id="REDUCING THE RF SIZE"/>
<node id="CONTEXT SWITCH TIMES"/>
<node id="THE SET OF CALLER-SAVE REGISTERS"/>
<node id="THE SAME"/>
<node id="WHEN USING THE EMBEDDED-APPLICATION BINARY INTERFACE (EABI)"/>
<node id="OUR IMPLEMENTATION"/>
<node id="THE CORE TO DYNAMICALLY SWITCH BETWEEN RV32I AND RV32E WITH FASTIRQ DEPENDING ON THE WORKLOAD"/>
<node id="INCREASED PRESSURE ON THE RF"/>
<node id="NOT ACCEPTABLE"/>
<node id="ONE"/>
<node id="ADDITIONAL REGISTERS FOR THE SEVEN CALLER-SAVE REGISTERS TO SAVE AREA"/>
<node id="DOUBLING THE RF SIZE FOR THE BANKING LOGIC"/>
<node id="ALTERNATIVE TO ADDING ADDITIONAL REGISTERS"/>
<node id="THE INTERRUPT STATE"/>
<node id="SIMPLY SWITCHING REGISTER BANKS"/>
<node id="TO DIFFERENTIATE BETWEEN A REGULAR RETURN FROM AN INTERRUPT HANDLER USING MRET"/>
<node id="MRET"/>
<node id="THE INTERRUPT STATE HAS BEEN RESTORED BY SW"/>
<node id="THIS INSTRUCTION"/>
<node id="THE SAME FUNCTION AS MRET"/>
<node id="SWITCHING REGISTER BANKS"/>
<node id="DIRECTLY CHECKING FOR OTHER INTERRUPTS PENDING ON THE SAME LEVEL BEFORE RESTORING THE EXECUTIONS INTERRUPT CONTEXT"/>
<node id="AN HW-ASSISTED SOLUTION"/>
<node id="SUCH A SCENARIO"/>
<node id="THE HW-ASSISTED SOLUTION"/>
<node id="THE XNXTI CSRS"/>
<node id="NUCLEIS ENHANCED CLIC (ECLIC) 18"/>
<node id="THE JUMP TO THE QUEUING INTERRUPT HANDLER IN THE XNXTI HW (JALXNXTI)"/>
<node id="NUCLEI SYSTEM TECHNOLOGY ECLIC 39"/>
<node id="TRADITIONAL XNXTI"/>
<node id="A NOVEL CSR FOR MACHINE PRIVILEGE MODE"/>
<node id="JALMNXTI 18"/>
<node id="THIS WORK IN SECTION IV"/>
<node id="THE CONTROL FLOW TO THE PENDING INTERRUPTS HANDLER"/>
<node id="SOME HW"/>
<node id="THIS CONCEPT OF REMOVING REDUNDANT CONTEXT RESTORES AS TAIL-CHAINING 23"/>
<node id="INTERRUPT RESPONSE TIME"/>
<node id="MINIMIZED"/>
<node id="RE-ENABLING GLOBAL INTERRUPTS"/>
<node id="A HIGH-LEVEL INTERRUPT"/>
<node id="THE CURRENT RUNNING INTERRUPT HANDLER"/>
<node id="THIS COMBINATION OF LATENCY HIDING AND BACKGROUND SAVING"/>
<node id="THE CORE TO QUICKLY ENTER A FIRST-LEVEL INTERRUPT HANDLER"/>
<node id="MIE"/>
<node id="TO PREVENT THAT"/>
<node id="LOWER-PRIORITY INTERRUPTS"/>
<node id="THIS RETURN PATH"/>
<node id="HW BY ADDING AN ADDITIONAL WRITE PORT TO THE CORES RF"/>
<node id="EXITING AN INTERRUPT HANDLER"/>
<node id="LESS TIME-CRITICAL"/>
<node id="HOW QUICKLY AN EXTERNAL EVENT IS ADDRESSED"/>
<node id="CONTEXT SWITCHES"/>
<node id="OS-SPECIFIC PARTS"/>
<node id="HW-SPECIFIC PARTS"/>
<node id="THE OS PART"/>
<node id="ALL CONTRIBUTIONS TO THE CONTEXT SWITCH TIME THAT IS SPECIFIC TO THE OS ITSELF"/>
<node id="COMPUTING THE NEXT TASK TO BE SCHEDULED"/>
<node id="BOOKKEEPING OPERATIONS"/>
<node id="THE REMAINDER"/>
<node id="THE HW-DEPENDENT SAVING AND RESTORING OF THE STATE BELONGING TO THE NEW CONTEXT"/>
<node id="THE IDEA"/>
<node id="THE STATE SAVING AND RESTORING PART OF CONTEXT FIG"/>
<node id="SWITCHES"/>
<node id="INTERLEAVING THE LOADING OF A NEW STATE WITH THE AUTOMATIC SAVING HW"/>
<node id="THE PREVIOUS REGISTER STATE TO MEMORY IN THE BACKGROUND"/>
<node id="THE HW MECHANISM"/>
<node id="THE REGISTERS"/>
<node id="THEM IN THE BACKGROUND"/>
<node id="THE REST OF THE CONTEXT SWITCH ROUTINE"/>
<node id="THE INITIAL PART OF THE CONTEXT SWITCH ROUTINE"/>
<node id="FOR THAT"/>
<node id="ANY"/>
<node id="THE CURRENT RUNNING TASKS STATE TO MEMORY"/>
<node id="ADDITIONAL RISC-V EXTENSIONS"/>
<node id="MORE CONTEXT SWITCHING STATE"/>
<node id="CONTRARY TO THE GOALS OF FASTIRQ REGARDING LATENCIES"/>
<node id="ADDING MORE STATE TO FASTIRQ"/>
<node id="NO TECHNICAL LIMITATIONS"/>
<node id="THE RESULTING DESIGN"/>
<node id="A SIGNIFICANT INCREASE IN AREA AND POWER"/>
<node id="A DIRTY BIT"/>
<node id="KEEP THE FAST PATH COMPETITIVE"/>
<node id="CONTEXT LAZY SWITCHABLE"/>
<node id="A SPECIFIC EXTENSION"/>
<node id="REQUIRED"/>
<node id="A FUNCTIONAL AND QUANTITATIVE EVALUATION"/>
<node id="THE VARIOUS FLAVORS OF THE CV32RT"/>
<node id="INTERRUPT LINES"/>
<node id="A HARDWIRED PRIORITIZATION SCHEME"/>
<node id="THE BASELINE CLIC"/>
<node id="THESE WEAKNESSES"/>
<node id="A LEVEL THRESHOLD REGISTER PER PRIVILEGE LEVEL (XINTTHRESH)"/>
<node id="THE SET OF ALLOWED HORIZONTAL INTERRUPTS"/>
<node id="THE LEVEL THRESHOLD REGISTER PER PRIVILEGE LEVEL (XINTTHRESH)"/>
<node id="THE SET OF ALLOWED HORIZONTAL INTERRUPTS TO THOSE WHOSE LEVEL EXCEEDS THE GIVEN VALUE IN THE REGISTER"/>
<node id="VECTORING"/>
<node id="SELECTIVELY ENABLED OR DISABLED PER INTERRUPT LINE"/>
<node id="THIS SOLUTION"/>
<node id="BOTH VECTORED AND NON-VECTORED INTERRUPTS"/>
<node id="THE REGULAR CLINT"/>
<node id="STORING AND RESTORING THE INTERRUPT CONTEXT"/>
<node id="SW"/>
<node id="THE OPTIONAL XNXTI EXTENSION"/>
<node id="MULTIPLE HORIZONTAL INTERRUPTS TO BE SERVICED IN SEQUENCE WITHOUT REDUNDANT CONTEXT-RESTORING OPERATIONS IN BETWEEN"/>
<node id="THE FIRST INTERRUPT"/>
<node id="THE FULL LATENCY COST"/>
<node id="THE INTERRUPT CONTEXT STORING PART ITSELF"/>
<node id="READING THE XNXTI CSR"/>
<node id="A POINTER TO THE VECTOR TABLE ENTRY FOR THE NEXT PENDING AND QUALIFYING INTERRUPT"/>
<node id="THE POINTER"/>
<node id="A DIRECT JUMP THERE"/>
<node id="THE LATENCY ADVANTAGE OF HW VECTORING"/>
<node id="AN SW EMULATION THEREOF"/>
<node id="THE DISCUSSED DIFFERENCES"/>
<node id="TABLE II"/>
<node id="MEASUREMENTS"/>
<node id="RUNNING RTL SIMULATIONS"/>
<node id="RTL SIMULATIONS"/>
<node id="DIFFERENT VERSIONS OF CV32RT"/>
<node id="THE MEMORY BANK WE ARE USING"/>
<node id="OTHER BUS MASTERS"/>
<node id="ADDITIONAL LATENCIES"/>
<node id="ON INTERRUPT LINES BETWEEN INTERRUPT SOURCES AND THE CLIC"/>
<node id="HW CONTRIBUTED INTERRUPT LATENCY"/>
<node id="HW CONTRIBUTED INTERRUPT LATENCY OF OUR FASTIRQ EXTENSION"/>
<node id="HW CONTRIBUTED INTERRUPT LATENCY OF OUR FASTIRQ EXTENSION TO THE CV32 AND CV32RT VARIATIONS"/>
<node id="CV32 AND CV32RT VARIATIONS"/>
<node id="STANDARD CLIC, XNXTI, JALXNXTI"/>
<node id="9"/>
<node id="THE AREA BREAKDOWN COMPARISON OF CV32RT WITH BASELINE CLIC (INCLUDING MNXTI AND JALMNXTI CSRS) AND FASTIRQ EXTENSIONS WITH 256 INPUT INTERRUPTS"/>
<node id="ALL CALLER-SAVE REGISTERS"/>
<node id="SAVED"/>
<node id="ALL CALLER-SAVE REGISTERS NEED TO BE SAVED"/>
<node id="THE GENERAL CASE"/>
<node id="THE SAVED GENERAL-PURPOSE REGISTERS"/>
<node id="THE RESPECTIVE ABI"/>
<node id="INTERRUPT HANDLER ROUTINES THAT SAVE THE INTERRUPT CONTEXT IN SW"/>
<node id="THE MINIMUM STATE"/>
<node id="THE COMPILER"/>
<node id="FULLY INLINE THE HANDLERS FUNCTION BODY"/>
<node id="SW-BASED MECHANISMS TO SAVE AND RESTORE INTERRUPT STATE"/>
<node id="THE HANDLER CODE"/>
<node id="FULLY INLINE THE HANDLER CODE"/>
<node id="SOME CALLER-SAVE REGISTERS"/>
<node id="THE INTERRUPT LATENCY IN THE OPTIMAL CASE"/>
<node id="ONLY ONE CALLER-SAVE REGISTER"/>
<node id="SAVING FOR SW-BASED INTERRUPT HANDLERS"/>
<node id="THE DESIGN AT THE INTERRUPT CONTROLLER INPUTS"/>
<node id="BOTH THE EABI AND REGULAR INTEGER ABI OF RISC-V"/>
<node id="THIS AMOUNT OF STATE"/>
<node id="FOR OUR FASTIRQ EXTENSION"/>
<node id="A PRIORI WHICH CALLER-SAVE REGISTERS NEED TO BE SAVED"/>
<node id="TABLE III"/>
<node id="THE MAIN TECHNIQUES FOR OPTIMIZING INTERRUPT CONTEXT AND TASK CONTEXT SAVE-RESTORE WITH NESTED AND NON-NESTED INTERRUPTS"/>
<node id="THE MAIN TECHNIQUES"/>
<node id="INDUSTRY AND ACADEMIA"/>
<node id="EMBEDDED AND REAL-TIME APPLICATION DOMAINS"/>
<node id="THE RESULTS"/>
<node id="SEVERAL DESIGNS"/>
<node id="THE INTERRUPT CONTEXT"/>
<node id="THE INTERRUPT CONTEXT DIRECTLY IN HW"/>
<node id="AUTOMATIC INTERRUPT CONTEXT SAVERESTORE"/>
<node id="SW HOUSEKEEPING OVERHEAD BEFORE AND AFTER HANDLING THE INTERRUPT ROUTINE"/>
<node id="ACCELERATION OF THE COMPLETE TASK CONTEXT SWITCH"/>
<node id="PRESENTED SOLUTIONS"/>
<node id="OPTIMIZING CONTEXT"/>
<node id="SAVERESTORE WITH HW AND SW COOPERATION"/>
<node id="A COHESIVE APPROACH TO ADDRESS BOTH INTERRUPT CONTEXT AND TASK CONTEXT SWITCH ACCELERATION"/>
<node id="EXISTING RISC-V-BASED APPROACHES"/>
<node id="THE GAP WITH WELL-ESTABLISHED INDUSTRY VENDORS"/>
<node id="INTERRUPT CONTEXT SAVERESTORE"/>
<node id="XNXTI AND JALXNXTI"/>
<node id="EVEN WORSE"/>
<node id="42 AND 35 CYCLES RESPECTIVELY"/>
<node id="ADDITIONAL INSTRUCTIONS IN THE CODE PATH BETWEEN THE HANDLER AND INTERRUPT EVENT"/>
<node id="PENDING INTERRUPTS"/>
<node id="THE RESPECTIVE HANDLERS"/>
<node id="A POINTER TO THE ADDRESS OF THE NEXT HANDLER"/>
<node id="A SMALL CODE SEQUENCE"/>
<node id="THE SMALL CODE SEQUENCE"/>
<node id="LOAD, JUMP, AND RETRY LOOP"/>
<node id="JALXNXTI"/>
<node id="THESE OPERATIONS INTO ONE INSTRUCTION"/>
<node id="FUSING THESE OPERATIONS INTO ONE INSTRUCTION"/>
<node id="SAVING NINE CYCLES"/>
<node id="ARM CORTEX-M4"/>
<node id="INTERRUPT LATENCY OF 12 CYCLES"/>
<node id="A SINGLE-CYCLE MEMORY 24"/>
<node id="THE ARM CORTEX-M4"/>
<node id="THE SAME TASK IN SIX CYCLES"/>
<node id="SINGLE-CYCLE MEMORY 24"/>
<node id="THE COST IN CLOCK CYCLES OF SUCH SEQUENCES"/>
<node id="BASELINE CLIC"/>
<node id="68 CYCLES WHEN USING INTEGER ABI"/>
<node id="50 CYCLES WHEN USING EMBEDDED ABI"/>
<node id="THE EMRET MECHANISM OF FASTIRQ"/>
<node id="EIGHT CLOCK CYCLES"/>
<node id="THE CONTEXT SWITCH TIME IN NUMBER OF CLOCK CYCLES BETWEEN TWO FREERTOS DUMMY TASKS"/>
<node id="THE CONTEXT SWITCH TIME"/>
<node id="THE BASELINE CV32RT AGAINST CV32RTFASTIRQ"/>
<node id="AVERAGE CONTEXT SWITCH TIME"/>
<node id="FREERTOS"/>
<node id="TWO TASKS"/>
<node id="VARIOUS FLAVORS OF CV32RT"/>
<node id="ALL COMPILE TIME OPTIONS SUCH AS TRACING, STACK OVERFLOW SIGNALING, AND THE MORE GENERIC TASK SELECTION MECHANISM"/>
<node id="TURNED OFF"/>
<node id="MINIMIZE THE CONTEXT SWITCH CODE"/>
<node id="CONFIGURATIONS THAT DO NOT HAVE FIG"/>
<node id="GENERIC CORTEX-M4 CORE"/>
<node id="SINGLE-CYCLE ACCESS TO MEMORY"/>
<node id="16 CORE REGISTERS"/>
<node id="US TO SKIP AHEAD THE SAVING OF THE GENERAL-PURPOSE REGISTERS"/>
<node id="AN SW INTERRUPT"/>
<node id="THE SAVE SEQUENCE"/>
<node id="THE SW INTERRUPT"/>
<node id="THE FASTIRQ MECHANISM"/>
<node id="SAVING THE GENERAL-PURPOSE REGISTERS TO MEMORY"/>
<node id="USING THE I-EXTENSION"/>
<node id="31 CYCLES FOR A CONTEXT SWITCH"/>
<node id="USING THE E-EXTENSION"/>
<node id="16 CYCLES FOR A CONTEXT SWITCH"/>
<node id="NOT TO USE REGISTERS THAT ARE STILL BEING SAVED BY THE BACKGROUND-SAVING MECHANISM"/>
<node id="SAVING CYCLES FOR A CONTEXT SWITCH"/>
<node id="SOME SOCS SUCH AS THE STM32L476RG"/>
<node id="HIGHER LATENCIES"/>
<node id="MEMORY ACCESS STALLS AND OTHER IMPLEMENTATION CHOICES IN THE MEMORY SUBSYSTEM"/>
<node id="THE RISC-V E-EXTENSION"/>
<node id="THE AVAILABLE GENERAL-PURPOSE REGISTERS FROM 32 TO 16"/>
<node id="THE CONTEXT SWITCH STATE THAT NEEDS TO BE SAVED AND RESTORED"/>
<node id="RISC-V CLIC AREA"/>
<node id="VARYING NUMBERS OF INTERRUPT SOURCES"/>
<node id="REPORTS"/>
<node id="THE CLIC IMPLEMENTED"/>
<node id="THE PROPOSED"/>
<node id="DIFFERENT INTERRUPT SOURCES"/>
<node id="AREA OVERHEAD"/>
<node id="AREA OVERHEAD OF CV32RT"/>
<node id="THE TWO MAIN CONFIGURATIONS"/>
<node id="THE OVERHEAD OF FASTIRQ IN CV32RTFASTIRQ CORE"/>
<node id="A MINIMAL 10 AREA INCREASE"/>
<node id="THE ID STAGE"/>
<node id="THE DESIGN"/>
<node id="GF12LP TECHNOLOGY"/>
<node id="500 MHZ"/>
<node id="TT CORNER"/>
<node id="25 C"/>
<node id="0.8 V"/>
<node id="SUPER LOW VT STANDARD CELLS"/>
<node id="CV32E40PRT ID STAGE"/>
<node id="AREA BREAKDOWN WITH THE PROPOSED HW EXTENSIONS"/>
<node id="MORE THAN HALF OF THE RESOURCES"/>
<node id="THE CONFIGURATION REGISTERS REQUIRED TO CONTROL THE CLIC"/>
<node id="THE SIZE"/>
<node id="LINEARLY WITH THE NUMBER OF INPUT INTERRUPTS"/>
<node id="THE REMAINING AREA"/>
<node id="THE GATEWAY AND BINARY TREE ARBITRATION LOGIC AT THE CORE OF THE CLIC WORKING PRINCIPLE"/>
<node id="THE GATEWAY AND BINARY TREE ARBITRATION LOGIC"/>
<node id="SECTION III-B"/>
<node id="ADDITIONAL HOUSEKEEPING CONTROL LOGIC"/>
<node id="LINEARLY WITH THE NUMBER OF INTERRUPT SOURCES"/>
<node id="THE FRACTION OF THE DESIGN OCCUPIED BY THE ARBITRATION TREE"/>
<node id="WHEN INCREASING THE NUMBER OF SOURCES"/>
<node id="A LARGER AREA OVERHEAD COMPARED TO TRADITIONAL RISC-V CLINT 14"/>
<node id="THE GAIN IN FLEXIBILITY"/>
<node id="A BROADER APPLICATION SCOPE WITH TIME-CRITICAL SYSTEMS"/>
<node id="INSTRUCTION DECODE (ID) STAGE"/>
<node id="AN AREA OVERHEAD OF 21 COMPARED TO CV32RTCLIC"/>
<node id="OTHER HW BLOCKS OF THE CORE"/>
<node id="PRIMARILY UNAFFECTED"/>
<node id="THE HW BLOCK WHERE THE ADDITIONAL REGISTERS AND THE AUTOMATIC STACKING-UNSTACKING LOGIC ARE LOCALIZED"/>
<node id="A BREAKDOWN OF THE ID STAGE"/>
<node id="ADDITIONAL STORAGE SPACE FOR AUTOMATIC CONTEXT SAVE AND RESTORE IN HW"/>
<node id="AREA OF THE RF BY ABOUT 36 IN THE PROPOSED IMPLEMENTATION"/>
<node id="THE LOGIC FOR MANAGING THE SHADOW REGISTERS"/>
<node id="AN OVERHEAD OF 40 ON THE BASELINE ID STAGE CONTROLLER"/>
<node id="HW OVERHEAD"/>
<node id="NEGLIGIBLE"/>
<node id="ADDITIONAL EMRET INSTRUCTION"/>
<node id="SECTION IV"/>
<node id="INCREASED SIZE OF THE ID STAGE"/>
<node id="BENEFITS OF A SIMPLIFIED PROGRAMMING MODEL THAT MOVES SEVERAL SW OPERATIONS IN HW"/>
<node id="BENEFITS OF SIGNIFICANTLY LOWERED INTERRUPT LATENCY THAN STANDARD RISC-V"/>
<node id="CRITICAL PATH OF THE BASE CORE DESIGN"/>
<node id="TIME-CRITICAL SYSTEMS"/>
<node id="AREA EFFICIENCY"/>
<node id="SAFETY, SECURITY, AND RELIABILITY"/>
<node id="THE LEADING SOLUTIONS TO OPTIMIZE HANDLING ASYNCHRONOUS EVENTS IN STATE-OF-THE-ART EMBEDDED AND REAL-TIME MCUS"/>
<node id="EXISTING PLICS AND CLICS AS INTRODUCED IN SECTION II-C"/>
<node id="SOLUTIONS ACROSS VARIOUS PLATFORMS IN INDUSTRY AND ACADEMIA"/>
<node id="SOLUTIONS"/>
<node id="INTERRUPT CONTEXT SAVERESTORE TECHNIQUES"/>
<node id="CONTEXT SWITCH TECHNIQUES"/>
<node id="DEDICATED STRATEGIES TO OPTIMIZE REDUNDANT CONTEXT RESTORE WITH BACK-TO-BACK INTERRUPTS"/>
<node id="RETURN AUTHORIZED LICENSED USE"/>
<node id="THE OVERVIEW"/>
<node id="DEFINITION PRESENTED IN SECTION II-B"/>
<node id="DIFFERENT VARIANTS ADOPTED BY SOTA"/>
<node id="ARMS GENERIC INTERRUPT CONTROLLER (GIC)"/>
<node id="INCOMING ASYNCHRONOUS EVENTS"/>
<node id="NON-CRITICAL (IRQ) OR CRITICAL INTERRUPTS (FAST IRQ, OR FIQ)"/>
<node id="DEDICATED REGISTER BANK"/>
<node id="UP TO EIGHT REGISTERS"/>
<node id="MINIMIZE CONTEXT SWITCHING"/>
<node id="A REFERENCE EXAMPLE IN THE FIELD"/>
<node id="NESTED VECTORED INTERRUPT CONTROLLER (NVIC)"/>
<node id="STATE MACHINE 32"/>
<node id="CALLER-SAVE REGISTER STACKING IN THE BACKGROUND"/>
<node id="THE LINK REGISTER A VALUE (EXCRETURN)"/>
<node id="THE VALUE (EXCRETURN)"/>
<node id="THE CORE TO START UNWINDING THE STACK TO RETURN TO NORMAL PROGRAM EXECUTION"/>
<node id="INTERRUPT CONTROL UNIT (ICU) 33, 34, 35"/>
<node id="INFINEON AURIX MCU-CLASS TRICORE FAMILYS"/>
<node id="THE CONTEXT OF THE CALLING ROUTINE"/>
<node id="MEMORY AUTONOMOUSLY"/>
<node id="RESTORING THE CONTEXT"/>
<node id="THE RET INSTRUCTION"/>
<node id="THE RETURN JUMP 36"/>
<node id="27"/>
<node id="EXTENSIONS FOR THE RISC-V CLIC"/>
<node id="INTERRUPT HANDLING"/>
<node id="AUTOMATIC STACKING IN HW"/>
<node id="CORES HARVARD ARCHITECTURE"/>
<node id="SIMULTANEOUS DATA AND INSTRUCTION MEMORY ACCESS"/>
<node id="REGISTER BANKING"/>
<node id="SEVERAL ARCHITECTURES"/>
<node id="TASKS CONTEXT"/>
<node id="REGISTER VALUES TO THE STACK"/>
<node id="ADDITIONAL AREA OVERHEAD IN THE DESIGN"/>
<node id="A TASKS CONTEXT SWITCH"/>
<node id="QUICKLY TRANSFERRING THE SUSPENDED CONTEXT TO THE DEDICATED REGISTER BANK"/>
<node id="ALREADY RESTORING THE NEXT TASK TO BE EXECUTED"/>
<node id="A SIMILAR APPROACH"/>
<node id="THE RENESAS M32C80 SERIES 30"/>
<node id="A DUAL REGISTER BANK"/>
<node id="QUICKLY SWAPPING THE CONTEXT WITHOUT SAVING OR RESTORING TO OR FROM THE STACK"/>
<node id="THE SECOND REGISTER BANK"/>
<node id="HIGH-SPEED INTERRUPTS"/>
<node id="THE AURIX FAMILY"/>
<node id="AN SW MANAGED SOLUTION"/>
<node id="THE SW MANAGED SOLUTION"/>
<node id="A SPECIFIC ORGANIZATION OF THE CONTEXT LAYOUT IN THE SYSTEM MEMORY"/>
<node id="THE CONTEXT LAYOUT IN THE SYSTEM MEMORY"/>
<node id="CONTEXT SAVE AREA (CSA) CHAINED IN A LINKED LIST FASHION"/>
<node id="A MORE COMPLEX APPROACH"/>
<node id="THE INTUITION THAT OFTEN THE CONTENT OF SOME REGISTER REMAINS UNTOUCHED AFTER A CONTEXT SWITCH"/>
<node id="HUANG ET AL."/>
<node id="A VALID-BASED MECHANISM IN HW TO BLOCK CONTEXT SWITCH ON SELECTED REGISTERS"/>
<node id="VALID-BASED MECHANISM IN HW"/>
<node id="CONTEXT SWITCH ON SELECTED REGISTERS"/>
<node id="REGISTER MOVEMENT BY ALMOST 50"/>
<node id="SEMI-SHADOWING"/>
<node id="THE TOP (FLIP) AND BOTTOM (FLOP) HALVES OF THE RF AS RF COPIES"/>
<node id="LEVERAGING RISC-VS RV32E BASE INSTRUCTION SET"/>
<node id="THE LOWER 16 ARCHITECTURAL REGISTERS OF THE RF"/>
<node id="THE LOWER 16 ARCHITECTURAL REGISTERS"/>
<node id="THE RF"/>
<node id="THE EVALUATION IN 28"/>
<node id="HW IMPLEMENTATION"/>
<node id="AREA OVERHEAD ASSESSMENT"/>
<node id="CONTEXT SWITCHING OVERHEAD"/>
<node id="24"/>
<node id="THE DSPSTONE BENCHMARK"/>
<node id="THESE APPROACHES"/>
<node id="HW- AND SW-INDUCED LATENCIES WHEN HANDLING ASYNCHRONOUS EVENTS"/>
<node id="A FULL HW SOLUTION BASED ON A HARDWARE SCHEDULING ENGINE (HSE)"/>
<node id="HARDWARE SCHEDULING ENGINE (HSE)"/>
<node id="INTERRUPTS TO RUNNING TASKS"/>
<node id="WITHOUT THE NEED FOR A SPECIALIZED INTERRUPT CONTROLLER"/>
<node id="LOWERING INTERRUPT LATENCY AND TASK CONTEXT SWITCHES DRAMATICALLY"/>
<node id="FLEXIBILITY"/>
<node id="ITS AREA OVERHEAD"/>
<node id="A HIGH NUMBER OF TASKS"/>
<node id="REPLICATING HARDWARE RESOURCES PER TASK"/>
<node id="ARCHITECTURES SUCH AS SUPERSCALAR CENTRAL PROCESSING UNITS (CPUS)"/>
<node id="HIDE LATENCY IN GRAPHIC PROCESSING UNITS (GPUS)"/>
<node id="SUCH WORKS"/>
<node id="RF CACHING"/>
<node id="REGISTER SHADOWING OR BANKING"/>
<node id="PERFORMANCE REASONS"/>
<node id="LOWER ACCESS LATENCY TO THE RF"/>
<node id="HIGHER THREAD-LEVEL PARALLELISM (TLP)"/>
<node id="SUCH TECHNIQUES"/>
<node id="THE SYSTEMS PRE-DICTABILITY"/>
<node id="RISC-V AIA WITHOUT APLIC"/>
<node id="MNXTI WITH THE XTOPI CSR"/>
<node id="XTOPI CSR"/>
<node id="THE HIGHEST-PRIORITY, PENDING, AND ENABLED INTERRUPT"/>
<node id="A SPECIFIC PRIVILEGE MODE"/>
<node id="BOTH LATE ARRIVAL AND REDUNDANT CONTEXT RESTORE MECHANISMS"/>
<node id="A COMBINATION OF THE BACKGROUND-SAVING WITH A REGISTER BANKING APPROACH"/>
<node id="TASK CONTEXT SWITCH TIMES IN FREERTOS TO 104 CLOCK CYCLES USING FASTIRQ"/>
<node id="20 FASTER THAN AN SW-ONLY APPROACH"/>
<node id="THE AUTHORS"/>
<node id="SOME RESEARCH DIRECTIONS FOR FUTURE WORK"/>
<node id="SOME RESEARCH DIRECTIONS"/>
<node id="ANALYZING FASTIRQS IMPACT ON TIMING CHANNELS"/>
<node id="ITS INTEGRATION WITH DIFFERENT RISC-V EXTENSIONS"/>
<node id="C. ROCHANGE, S. UHRIG, AND P. SAINRAT"/>
<node id="TIME-PREDICTABLE ARCHITECTURES (FOCUS COMPUTER ENGINEERING SERIES)"/>
<node id="WILEY"/>
<node id="IN 2014"/>
<node id="HOBOKEN, NJ, USA"/>
<node id="L. M. PINHO ET AL."/>
<node id="HIGH-PERFORMANCE AND TIME-PREDICTABLE EMBEDDED COMPUTING"/>
<node id="HTTP:EU.WILEY.COMWILEYCDA WILEYTITLEPRODUCTCD-1848215932.HTML"/>
<node id="RIVER"/>
<node id="WHARTON, TX, USA"/>
<node id="2018"/>
<node id="3 F. REGHENZANI, G. MASSARI, AND W. FORNACIARI"/>
<node id="THE REAL-TIME LINUX KERNEL: A SURVEY ON PREEMPT"/>
<node id="136"/>
<node id="FEB. 2019"/>
<node id="DOI"/>
<node id="10.11453297714"/>
<node id="10.11453419973"/>
<node id="136, JAN. 2021"/>
<node id="M. LIU, D. LIU, Y. WANG, M. WANG, AND Z. SHAO"/>
<node id="ON IMPROVING REAL-TIME INTERRUPT LATENCIES OF HYBRID OPERATING SYSTEMS WITH TWO-LEVEL HARDWARE INTERRUPTS"/>
<node id="IEEE TRANS"/>
<node id="P. MANTEGAZZA, E. L. DOZIO, AND S. PAPACHARALAMBOUS"/>
<node id="RTAI: REAL TIME APPLICATION INTERFACE"/>
<node id="6 K. RAMAMRITHAM AND J."/>
<node id="J. VALVANO"/>
<node id="INTRODUCTION TO EMBEDDED SYSTEMS"/>
<node id="PDF"/>
<node id="HTTP:WEB.ENGR.OREGONSTATE.EDU/TRAYLORECE473 PDFSMINIMIZEINTERRUPTRESPONSETIME.PDF"/>
<node id="CREATESPACE"/>
<node id="SCOTTS VALLEY, CA, USA"/>
<node id="AUG. 2016"/>
<node id="9 Y. HUANG, L. SHI, J. LI, Q. LI, AND C. J. XUE"/>
<node id="WCET-AWARE RE-SCHEDULING REGISTER ALLOCATION FOR REAL-TIME EMBEDDED SYSTEMS WITH CLUSTERED VLIW ARCHITECTURE"/>
<node id="IEEE TRANS."/>
<node id="VERY LARGE SCALE INTEGR"/>
<node id="X. ZHOU AND P. PETROV"/>
<node id="RAPID AND LOW-COST CONTEXT-SWITCH THROUGH EMBEDDED PROCESSOR CUSTOMIZATION FOR REAL-TIME AND CONTROL APPLICATIONS"/>
<node id="ASSOCIATION FOR COMPUTING MACHINERY"/>
<node id="NEW YORK, NY, USA"/>
<node id="2006"/>
<node id="352"/>
<node id="10.11451146909.1147001"/>
<node id="I. BEHNKE, L. PIRL, L. THAMSEN, R. DANICKI, A. POLZE, AND O. KAO"/>
<node id="INTERRUPTING REAL-TIME IOT TASKS: HOW BAD CAN IT BE TO CONNECT YOUR CRITICAL EMBEDDED SYSTEM TO THE INTERNET?"/>
<node id="12 F. REHM ET AL."/>
<node id="THE ROAD TOWARDS PREDICTABLE AUTOMOTIVE HIGH PERFORMANCE PLATFORMS"/>
<node id="TEST"/>
<node id="EUR"/>
<node id="13 K. ASANOVIC AND D. A. PATTERSON"/>
<node id="INSTRUCTION SETS SHOULD BE FREE: THE CASE FOR RISC-V, DEPT."/>
<node id="A. WATERMAN, Y. LEE, R. AVIZIENIS, D. A. PATTERSON, AND K. ASANOVIC"/>
<node id="THE RISC-V INSTRUCTION SET MANUAL VOLUME II: PRIVILEGED ARCHITECTURE VERSION 1.9"/>
<node id="DEPT."/>
<node id="EECS"/>
<node id="UNIV."/>
<node id="CALIFORNIA"/>
<node id="BERKELEY"/>
<node id="CA"/>
<node id="USA"/>
<node id="TECH"/>
<node id="ACCESSED"/>
<node id="JUN."/>
<node id="RISCV-FAST-INTERRUPT-BLOB"/>
<node id="HTTPS://GITHUB.COM/RISCV/RISCV-FAST-INTERRUPT-BLOB/MASTER/CLIC"/>
<node id="ADOC 16 M. GAUTSCHI ET AL."/>
<node id="NEAR-THRESHOLD RISC-V CORE WITH DSP EXTENSIONS FOR SCALABLE IOT ENDPOINT DEVICES"/>
<node id="17"/>
<node id="D. SCHIAVONE"/>
<node id="OPENHW GROUP"/>
<node id="CV32E40P USER MANUAL"/>
<node id="ORGANIZATION"/>
<node id="AVAILABLE AT CV32E40P"/>
<node id="READTHEDOCS.IOEN"/>
<node id="18 NUCLEI SYSTEM TECHNOLOGY"/>
<node id="39 NUCLEI SYSTEM TECHNOLOGY CO. LTD."/>
<node id="ECLIC UNIT"/>
<node id="INTRODUCTION"/>
<node id="NUCLEI"/>
<node id="SPEC"/>
<node id="CONTROL PULP"/>
<node id="A RISC-V ON-CHIP PARALLEL POWER CONTROLLER"/>
<node id="MANY-CORE HPC PROCESSORS"/>
<node id="FPGA-BASED HARDWARE-IN-THE-LOOP POWER AND THERMAL EMULATION"/>
<node id="HTTPS:DOC.NUCLEISYS.COMNUCLEISPECISAINTRODUCTION.HTML"/>
<node id="PARALLEL PROGRAM."/>
<node id="FEB. 2024"/>
<node id="10.1007S10766-024-00761-4"/>
<node id="R. BARRY"/>
<node id="REAL-TIME OPERATING SYSTEM FOR MICROCONTROLLERS"/>
<node id="REAL TIME ENGINEERS LTD."/>
<node id="ONLINE"/>
<node id="HTTPS://WWW.FREERTOS.ORG/INDEX.HTML"/>
<node id="AVAILABLE"/>
<node id="C.-M. LIN"/>
<node id="NESTED INTERRUPT ANALYSIS OF LOW COST AND HIGH PERFORMANCE EMBEDDED SYSTEMS USING GSPN FRAMEWORK"/>
<node id="IEICE TRANS."/>
<node id="HTTPS:GITHUB.COMRISCVRISCV-PLIC-SPECBLOBMASTERRISCV-PLIC-1.0.0.PDF"/>
<node id="J. YIU"/>
<node id="THE DEFINITIVE GUIDE TO ARM CORTEX-M3 CORTEX-M4 PROCESSORS"/>
<node id="3RD ED."/>
<node id="NEWNES"/>
<node id="BOSTON, MA, USA"/>
<node id="2013"/>
<node id="ARM"/>
<node id="CORTEX-M4"/>
<node id="DOCUMENTATION"/>
<node id="HTTPS://DEVELOPER.ARM.COM/DOCUMENTATION/1001660001"/>
<node id="ARM CORTEX-M PROCESSORS"/>
<node id="CIRCUITS MICROSYSTEMS (ICICM)"/>
<node id="OCT. 2021"/>
<node id="PP."/>
<node id="LI AND J. K. LEE"/>
<node id="PAGED REGISTER FILES"/>
<node id="EMBEDDED PROCESSORS"/>
<node id="29 R. BALAS AND L. BENINI"/>
<node id="RISC-V FOR REAL-TIME MCU SOFTWARE OPTIMIZATION AND MICROARCHITECTURAL GAP ANALYSIS"/>
<node id="RENESAS"/>
<node id="HARDWARE MANUAL"/>
<node id="RENESAS MCU M16C"/>
<node id="HTTPS://WWW.RENESAS.COM/US/ENDOCUMENT/MAHM32C87-GROUP-M32C87-M32C87A-M32C87B-HARDWARE-MANUAL"/>
<node id="SIFIVE INC."/>
<node id="SIFIVE E21 CORE COMPLEX"/>
<node id="MANUAL"/>
<node id="HTTPS:SIFIVE.CDN.PRISMIC.IOSIFIVE7C22C2EC-8AF4-4B6C-A5FE-9327D91E7808E21CORECOMPLEXMANUAL21G1.PDF"/>
<node id="STMICROELECTRONICS"/>
<node id="MENTIONED"/>
<node id="STM32L5-SYSTEM-NESTEDVECTORED INTERRUPTCONTROLNVIC.PDF"/>
<node id="HTTPS://WWW.ST.COM/CONTENT/CCC/RESOURCE/TRAINING/TECHNICAL/PRODUCTTRAINING/GROUP16135D207346F4E83/STM32L5-SYSTEM-NESTEDVECTOREDINTERRUPTCONTROLNVICFILES/STM32L5-SYSTEM-NESTEDVECTOREDINTERRUPTCONTROLNVIC.PDF"/>
<node id="INFINEON TECHNOLOGIES AG"/>
<node id="POWERTRAIN MICROCONTROLLER"/>
<node id="FAST"/>
<node id="HC163TUE6HC16SESS7PRES1BW.PDF"/>
<node id="HTTPS://OLD.HOTCHIPS.ORG/WP-CONTENT/UPLOADS/HCARCHIVES/HC163TUE6HC16SESS7PRES1BW.PDF"/>
<node id="FILE"/>
<node id="HTTPS:HITEX.CO.UKFILEADMINUK-FILESDOWNLOADSSHIELDBUDDYTC27XDUMV2.2.PDF"/>
<node id="TC27X D-STEP"/>
<node id="32-BIT SINGLE-CHIP MICROCONTROLLER"/>
<node id="TRICORE V1.6"/>
<node id="CORE ARCHITECTURE"/>
<node id="U.S. PATENT 7 434 222 B2"/>
<node id="TASK CONTEXT SWITCHING RTOS"/>
<node id="OCT. 2008"/>
<node id="H. ZENG AND K. GHOSE"/>
<node id="REGISTER FILE CACHING FOR ENERGY EFFICIENCY"/>
<node id="ISLPED PROC"/>
<node id="LOW POWER"/>
<node id="ELECTRON"/>
<node id="DESIGN"/>
<node id="OCT. 2006"/>
<node id="38 M. SADROSADATI ET AL."/>
<node id="HIGHLY CONCURRENT LATENCY-TOLERANT REGISTER FILES FOR GPUS"/>
<node id="ACM TRANS."/>
<node id="AUG. 4, 2023"/>
<node id="GRADUATE STUDENT MEMBER OF IEEE"/>
<node id="B.SC."/>
<node id="DEGREE"/>
<node id="THE DIGITAL CIRCUITS AND SYSTEMS GROUP OF PROF. BENINI"/>
<node id="HIS RESEARCH INTERESTS"/>
<node id="REAL-TIME COMPUTING"/>
<node id="COMPILERS"/>
<node id="POWER MANAGEMENT OF HPC PROCESSORS"/>
<node id="ENERGY-EFFICIENT PROCESSOR ARCHITECTURE"/>
<node id="THE B.SC."/>
<node id="FELLOW OF IEEE"/>
<node id="PH.D."/>
<node id="PHYSICAL ENGINEERING"/>
<node id="POLITECNICO DI TURINO"/>
<node id="TURIN"/>
<node id="ITALY"/>
<node id="ELECTRICAL ENGINEERING"/>
<node id="GRENOBLE INP-PHELMA"/>
<node id="GRENOBLE, FRANCE"/>
<node id="EPFL LAUSANNE"/>
<node id="LAUSANNE, SWITZERLAND"/>
<node id="STANFORD UNIVERSITY"/>
<node id="STANFORD, CA, USA"/>
<node id="1997"/>
<node id="DR. BENINI"/>
<node id="THE ACM"/>
<node id="THE ACADEMIA EUROPAEA"/>
<node id="MCCLUSKEY AWARD"/>
<node id="AWARD"/>
<edge source="EXISTING SYSTEMS" target="KEY-VALUE CACHE (KV CACHE) MEMORY FOR EACH REQUEST IS HUGE AND GROWS AND SHRINKS DYNAMICALLY">
  <data key="d0">STRUGGLE BECAUSE</data>
</edge>
<edge source="EXISTING SYSTEMS" target="SIMULTANEOUS PROCESSING OF REQUESTS WITH DIFFERENT DECODING PREFERENCES EFFICIENTLY">
  <data key="d0">CANNOT DO</data>
</edge>
<edge source="THE KV CACHE SIZE" target="THE NUMBER OF REQUESTS">
  <data key="d0">GROWS QUICKLY WITH</data>
</edge>
<edge source="THIS MEMORY" target="FRAGMENTATION AND REDUNDANT DUPLICATION">
  <data key="d0">CAN BE WASTED BY</data>
</edge>
<edge source="FRAGMENTATION AND REDUNDANT DUPLICATION" target="THE BATCH SIZE">
  <data key="d0">LIMIT</data>
</edge>
<edge source="INEFFICIENT MEMORY MANAGEMENT" target="BATCH SIZE">
  <data key="d0">CAN DECREASE</data>
</edge>
<edge source="BATCH SIZE" target="REQUESTS">
  <data key="d0">IS MEASURED IN</data>
</edge>
<edge source="BATCH SIZE" target="SUFFICIENTLY LARGE">
  <data key="d0">IS</data>
</edge>
<edge source="WE" target="PAGEDATTENTION">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="WE" target="VLLM">
  <data key="d0">DESIGN AND IMPLEMENT</data>
</edge>
<edge source="WE" target="THE KV CACHE IN A MORE FLEXIBLE WAY">
  <data key="d0">CAN MANAGE</data>
</edge>
<edge source="WE" target="THE PERFORMANCE OF VLLM UNDER A VARIETY OF WORKLOADS">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="CONTRIBUTIONS">
  <data key="d0">MAKE</data>
</edge>
<edge source="WE" target="CHALLENGES IN MEMORY ALLOCATION IN SERVING LLMS">
  <data key="d0">IDENTIFY</data>
</edge>
<edge source="WE" target="IMPACT ON SERVING PERFORMANCE">
  <data key="d0">QUANTIFY</data>
</edge>
<edge source="WE" target="THE DESIGN OF THE KV CACHE MANAGER IN 4.2">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="THE KV CACHE AS FIXED-SIZE KV BLOCKS">
  <data key="d0">ORGANIZE</data>
</edge>
<edge source="WE" target="VLLM ON VARIOUS SCENARIOS">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THE CONCATENATION OF THE PROMPT AND OUTPUT LISTS AS SEQUENCE">
  <data key="d0">REFER TO</data>
</edge>
<edge source="WE" target="THE AVERAGE PERCENTAGE OF MEMORY WASTES IN OUR EXPERIMENTS IN FIG.">
  <data key="d0">VISUALIZE</data>
</edge>
<edge source="WE" target="THE GENERAL APPLICABILITY OF VLLM ON THEM">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="A NEW ATTENTION ALGORITHM PAGE-DATTENTION">
  <data key="d0">DEVELOP</data>
</edge>
<edge source="WE" target="AN LLM SERVING ENGINE VLLM">
  <data key="d0">BUILD</data>
</edge>
<edge source="WE" target="THE PAGEDATTENTION ALGORITHM">
  <data key="d0">DESCRIBE</data>
</edge>
<edge source="WE" target="AN EXAMPLE OF PAGEDATTENTION IN FIG.">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="THE SECOND ONE">
  <data key="d0">CHOOSE</data>
</edge>
<edge source="WE" target="THE EFFECT OF BLOCK SIZE IN 7.2">
  <data key="d0">STUDY</data>
</edge>
<edge source="WE" target="THE MORE GENERAL CASE">
  <data key="d0">ASSUME</data>
</edge>
<edge source="WE" target="ONE COPY OF THE PROMPTS STATE AT THE PROMPT PHASE">
  <data key="d0">RESERVE SPACE FOR</data>
</edge>
<edge source="WE" target="A REFERENCE COUNT FOR EACH PHYSICAL BLOCK">
  <data key="d0">INTRODUCE</data>
</edge>
<edge source="WE" target="TWO TECHNIQUES">
  <data key="d0">CONSIDER</data>
</edge>
<edge source="WE" target="AN ALL-OR-NOTHING EVICTION POLICY">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="EVICTED BLOCKS TO THE CPU MEMORY">
  <data key="d0">COPY</data>
</edge>
<edge source="WE" target="THE KV CACHE">
  <data key="d0">RECOMPUTE</data>
</edge>
<edge source="WE" target="THE SPEEDS OF SWAPPING AND RECOMPUTATION IN 7.3">
  <data key="d0">EXAMINE</data>
</edge>
<edge source="WE" target="CONTROL-RELATED COMPONENTS">
  <data key="d0">DEVELOP</data>
</edge>
<edge source="WE" target="CUSTOM CUDA KERNELS">
  <data key="d0">DEVELOP</data>
</edge>
<edge source="WE" target="NCCL 32 FOR TENSOR COMMUNICATION ACROSS THE DISTRIBUTED GPU WORKERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="SEVERAL GPU KERNELS">
  <data key="d0">DEVELOP</data>
</edge>
<edge source="WE" target="THEM INTO A SINGLE KERNEL">
  <data key="d0">FUSE</data>
</edge>
<edge source="WE" target="A KERNEL">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="THE ATTENTION KERNEL IN FASTERTRANSFORMER 31">
  <data key="d0">ADAPT</data>
</edge>
<edge source="WE" target="KV CACHE ACCORDING TO THE BLOCK TABLE">
  <data key="d0">READ</data>
</edge>
<edge source="WE" target="ATTENTION OPERATIONS ON THE FLY">
  <data key="d0">PERFORM</data>
</edge>
<edge source="WE" target="A GPU WARP TO READ EACH BLOCK">
  <data key="d0">ASSIGN</data>
</edge>
<edge source="WE" target="SUPPORT FOR VARIABLE SEQUENCE LENGTHS WITHIN A REQUEST BATCH">
  <data key="d0">ADD</data>
</edge>
<edge source="WE" target="THE CUDAMEMCPYASYNC API">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="OPT 62 MODELS WITH 13B PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="OPT 62 MODELS WITH 66B PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="OPT 62 MODELS WITH 175B PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="LLAMA 52 WITH 13B PARAMETERS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="OPT 62 MODELS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="A2 INSTANCES WITH NVIDIA A100 GPUS ON GOOGLE CLOUD PLATFORM">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="WORKLOADS BASED ON SHAREGPT 51 AND ALPACA 50 DATASETS">
  <data key="d0">SYNTHESIZE</data>
</edge>
<edge source="WE" target="THE CHATTING HISTORY AND USER QUERY USING THE SHAREGPT DATASET">
  <data key="d0">SYNTHESIZE</data>
</edge>
<edge source="WE" target="THE DATASETS">
  <data key="d0">TOKENIZE</data>
</edge>
<edge source="WE" target="THEIR INPUT AND OUTPUT LENGTHS">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="REQUEST ARRIVAL TIMES">
  <data key="d0">GENERATE</data>
</edge>
<edge source="WE" target="A CUSTOM SCHEDULER">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="A MAXIMUM BATCH SIZE AS LARGE AS POSSIBLE FOR EACH EXPERIMENT">
  <data key="d0">SET</data>
</edge>
<edge source="WE" target="OUR OWN VERSION OF ORCA">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="THE SYSTEM HAS THE KNOWLEDGE OF THE LENGTHS OF THE OUTPUTS THAT WILL BE ACTUALLY GENERATED FOR THE REQUESTS">
  <data key="d0">ASSUME</data>
</edge>
<edge source="WE" target="SERVING THROUGHPUT">
  <data key="d0">FOCUS ON</data>
</edge>
<edge source="WE" target="NORMALIZED LATENCY OF THE SYSTEMS">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="THE SYSTEMS WITH 1-HOUR TRACES">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="15-MINUTE TRACES FOR THE OPT-175B MODEL">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="THE PERFORMANCE OF VLLM WITH BASIC SAMPLING">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THREE MODELS AND TWO DATASETS">
  <data key="d0">EVALUATE ON</data>
</edge>
<edge source="WE" target="THE EFFECTIVENESS OF MEMORY SHARING IN PAGE-DATTENTION">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="6.1 - 9.8 MEMORY SAVING ON PARALLEL SAMPLING">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="37.6 - 55.2 MEMORY SAVING ON BEAM SEARCH">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="WMT16 4 ENGLISH-TO-GERMAN TRANSLATION DATASET">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="TWO PREFIXES">
  <data key="d0">SYNTHESIZE</data>
</edge>
<edge source="WE" target="THE MODEL GENERATE A RESPONSE">
  <data key="d0">LET</data>
</edge>
<edge source="WE" target="THE PROMPT TO THE LAST 1024 TOKENS">
  <data key="d0">CUT</data>
</edge>
<edge source="WE" target="THE MODEL GENERATE AT MOST 1024 TOKENS">
  <data key="d0">LET</data>
</edge>
<edge source="WE" target="THE KV CACHE BETWEEN DIFFERENT CONVERSATION ROUNDS">
  <data key="d0">DO NOT STORE</data>
</edge>
<edge source="WE" target="VARIOUS ASPECTS OF VLLM">
  <data key="d0">STUDY</data>
</edge>
<edge source="WE" target="THE DESIGN CHOICES">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THE PERFORMANCE OF VLLM WITH DIFFERENT BLOCK SIZES">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THE SHAREGPT AND ALPACA TRACES WITH BASIC SAMPLING">
  <data key="d0">USE</data>
</edge>
<edge source="WE" target="THEIR END-TO-END PERFORMANCE">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THEIR OVERHEADS">
  <data key="d0">MICROBENCHMARK</data>
</edge>
<edge source="WE" target="VLLMS TECHNIQUES BEING APPLIED TO OTHER WORKLOADS WITH SIMILAR PROPERTIES TO LLM SERVING">
  <data key="d0">WOULD BE EXCITED TO SEE</data>
</edge>
<edge source="WE" target="XIAOXUAN LIU">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="ZHIFENG CHEN">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="YAN-PING HUANG">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="ANONYMOUS SOSP REVIEWERS">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="OUR SHEPHERD">
  <data key="d0">LIKE TO THANK</data>
</edge>
<edge source="WE" target="A CLIC FOR THE CV32E40P">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="IT WITH FASTIRQ">
  <data key="d0">ENHANCE</data>
</edge>
<edge source="WE" target="CV32RT">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="WE" target="CLIC INTERRUPT CONTROLLER IN CV32RTCLIC 15">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="FASTIRQ">
  <data key="d0">PRESENT</data>
</edge>
<edge source="WE" target="THE EXTENSION">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="WE" target="CV32RT OUR ENHANCED CORE">
  <data key="d0">CALL</data>
</edge>
<edge source="WE" target="CV32RT WITHIN AN OPEN-SOURCE SYSTEM ON CHIP (SOC)">
  <data key="d0">INTEGRATE</data>
</edge>
<edge source="WE" target="CV32RT INTERRUPT HANDLING CAPABILITIES">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="HOW THE VARIOUS CV32RT VERSIONS PERFORM IN TERMS OF INTERRUPT LATENCY AND CONTEXT SWITCH TIMES">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="THE OVERHEAD THESE ADDITIONS INCUR IN TERMS OF AREA AND TIMING">
  <data key="d0">QUANTIFY</data>
</edge>
<edge source="WE" target="THE IMPLEMENTATION AVAILABLE UNDER A PERMISSIVE OPEN-SOURCE LICENSE">
  <data key="d0">MAKE</data>
</edge>
<edge source="WE" target="A FAST INTERRUPT EXTENSION (FASTIRQ)">
  <data key="d0">DESIGN</data>
</edge>
<edge source="WE" target="MINIMIZING THE LATENCY IMPOSED BY THE HW">
  <data key="d0">FOCUS ON</data>
</edge>
<edge source="WE" target="THE CORE LOCAL INTERRUPTOR (CLINT)">
  <data key="d0">REFER TO</data>
</edge>
<edge source="WE" target="INTERRUPT LATENCY AND CONTEXT SWITCH TIMES">
  <data key="d0">CAN IMPROVE</data>
</edge>
<edge source="WE" target="CONTROLLING THE AMOUNT OF STATE THAT NEEDS TO BE PRESERVED TO ENTER AND LEAVE AN INTERRUPT CONTEXT">
  <data key="d0">IMPROVE INTERRUPT LATENCY AND CONTEXT SWITCH TIMES BY</data>
</edge>
<edge source="WE" target="INCREASING THE BANDWIDTH AND DECREASING THE LATENCY TO MEMORY">
  <data key="d0">IMPROVE INTERRUPT LATENCY AND CONTEXT SWITCH TIMES BY</data>
</edge>
<edge source="WE" target="RELYING ON LATENCY-HIDING TECHNIQUES THAT DEFER THE EFFECTIVE SAVING OF THE STATE TO A LATER POINT IN TIME">
  <data key="d0">IMPROVE INTERRUPT LATENCY AND CONTEXT SWITCH TIMES BY</data>
</edge>
<edge source="WE" target="ARCHITECTURAL FEATURES AND HWSW CODESIGN OF CV32RT">
  <data key="d0">DETAIL</data>
</edge>
<edge source="WE" target="THE RF BY AN ADDITIONAL READ PORT FOR THE BACKGROUND-SAVING MECHANISM">
  <data key="d0">EXTEND</data>
</edge>
<edge source="WE" target="REGISTERS FOR LATCHING THE ADDITIONAL PROCESSOR STATE">
  <data key="d0">EXTEND</data>
</edge>
<edge source="WE" target="A DEDICATED ADDER BETWEEN THE TWO RFS">
  <data key="d0">HAVE</data>
</edge>
<edge source="WE" target="THIS MECHANISM FOR LEAF-TYPE INTERRUPTS">
  <data key="d0">COULD DO AWAY WITH</data>
</edge>
<edge source="WE" target="STALE DATA">
  <data key="d0">COULD READ</data>
</edge>
<edge source="WE" target="DATA THAT IS IMMEDIATELY OVERWRITTEN">
  <data key="d0">COULD WRITE</data>
</edge>
<edge source="WE" target="A LOAD SOON IN THE INTERRUPT HANDLER">
  <data key="d0">LIKELY WANT TO ISSUE</data>
</edge>
<edge source="WE" target="AN SW-BASED SOLUTION">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="WE" target="THE PIPELINE STALLING LOGIC">
  <data key="d0">DO NOT NEED TO ENGAGE</data>
</edge>
<edge source="WE" target="AN ANALYSIS OF THE FUNCTIONAL IMPROVEMENTS">
  <data key="d0">GIVE</data>
</edge>
<edge source="WE" target="THE GENERAL-PURPOSE REGISTERS X1, X2">
  <data key="d0">PUSH OUT</data>
</edge>
<edge source="WE" target="USE THE SAME ORDER TO LOAD WORDS BACK IN THE INTERRUPT HANDLER">
  <data key="d0">NEED TO ENSURE</data>
</edge>
<edge source="WE" target="AN OVERVIEW OF HOW NESTED INTERRUPT HANDLING CODE WORKS FOR THE BASIC CLINT-MODE">
  <data key="d0">GIVE</data>
</edge>
<edge source="WE" target="AN OVERVIEW OF HOW NESTED INTERRUPT HANDLING CODE WORKS FOR THE BASELINE CLIC">
  <data key="d0">GIVE</data>
</edge>
<edge source="WE" target="AN OVERVIEW OF HOW NESTED INTERRUPT HANDLING CODE WORKS FOR OUR FASTIRQ EXTENSION">
  <data key="d0">GIVE</data>
</edge>
<edge source="WE" target="THIS SCENARIO">
  <data key="d0">ADDRESS</data>
</edge>
<edge source="WE" target="INTERRUPT LATENCY">
  <data key="d0">MINIMIZE</data>
</edge>
<edge source="WE" target="LATENCY">
  <data key="d0">HIDE</data>
</edge>
<edge source="WE" target="HOW QUICKLY AN EXTERNAL EVENT IS ADDRESSED">
  <data key="d0">CARE ABOUT</data>
</edge>
<edge source="WE" target="THE REST OF THE CONTEXT SWITCH ROUTINE">
  <data key="d0">CAN PROCEED WITH</data>
</edge>
<edge source="WE" target="THE CURRENT RUNNING TASKS STATE TO MEMORY">
  <data key="d0">WANT TO SAVE</data>
</edge>
<edge source="WE" target="MEASUREMENTS">
  <data key="d0">TAKE</data>
</edge>
<edge source="WE" target="HW CONTRIBUTED INTERRUPT LATENCY">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="HW CONTRIBUTED INTERRUPT LATENCY OF OUR FASTIRQ EXTENSION">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="HW CONTRIBUTED INTERRUPT LATENCY OF OUR FASTIRQ EXTENSION TO THE CV32 AND CV32RT VARIATIONS">
  <data key="d0">COMPARE</data>
</edge>
<edge source="WE" target="THE INTERRUPT LATENCY IN THE OPTIMAL CASE">
  <data key="d0">MEASURE</data>
</edge>
<edge source="WE" target="BOTH THE EABI AND REGULAR INTEGER ABI OF RISC-V">
  <data key="d0">EVALUATE</data>
</edge>
<edge source="WE" target="THE COST IN CLOCK CYCLES OF SUCH SEQUENCES">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="THE CONTEXT SWITCH TIME IN NUMBER OF CLOCK CYCLES BETWEEN TWO FREERTOS DUMMY TASKS">
  <data key="d0">SHOW</data>
</edge>
<edge source="WE" target="CONFIGURATIONS THAT DO NOT HAVE FIG">
  <data key="d0">LEFT OUT</data>
</edge>
<edge source="WE" target="EXISTING PLICS AND CLICS AS INTRODUCED IN SECTION II-C">
  <data key="d0">DIFFERENTIATE BETWEEN</data>
</edge>
<edge source="WE" target="THE LATTER">
  <data key="d0">FOCUS ON</data>
</edge>
<edge source="WE" target="SOLUTIONS ACROSS VARIOUS PLATFORMS IN INDUSTRY AND ACADEMIA">
  <data key="d0">DISCUSS</data>
</edge>
<edge source="WE" target="DIFFERENT VARIANTS ADOPTED BY SOTA">
  <data key="d0">PROVIDE REFERENCES TO</data>
</edge>
<edge source="WE" target="TASK CONTEXT SWITCH TIMES IN FREERTOS TO 104 CLOCK CYCLES USING FASTIRQ">
  <data key="d0">IMPROVE</data>
</edge>
<edge source="PAGEDATTENTION" target="AN ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGEDATTENTION" target="THE CLASSICAL VIRTUAL MEMORY AND PAGING TECHNIQUES IN OPERATING SYSTEMS">
  <data key="d0">IS INSPIRED BY</data>
</edge>
<edge source="PAGEDATTENTION" target="KV CACHE STORED IN NON-CONTIGUOUS PAGED MEMORY">
  <data key="d0">OPERATES ON</data>
</edge>
<edge source="PAGEDATTENTION" target="VIRTUAL MEMORY AND PAGING IN OS">
  <data key="d0">IS INSPIRED BY</data>
</edge>
<edge source="PAGEDATTENTION" target="STORING CONTINUOUS KEYS AND VALUES IN NON-CONTIGUOUS MEMORY SPACE">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="PAGEDATTENTION" target="A NEW ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGEDATTENTION" target="ATTENTION KEYS AND VALUES TO BE STORED IN NON-CONTIGUOUS PAGED MEMORY">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="PAGEDATTENTION" target="THE REQUESTS KV CACHE INTO BLOCKS">
  <data key="d0">DIVIDES</data>
</edge>
<edge source="PAGEDATTENTION" target="THE KV CACHE OF EACH SEQUENCE INTO KV BLOCKS">
  <data key="d0">PARTITIONS</data>
</edge>
<edge source="PAGEDATTENTION" target="WE TO ORGANIZE THE KV CACHE">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PAGEDATTENTION" target="MEMORY ACCESS PATTERNS">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="PAGEDATTENTION" target="THE PROBLEM OF MEMORY FRAGMENTATION AND RESERVATION">
  <data key="d0">RESOLVES</data>
</edge>
<edge source="PAGEDATTENTION" target="VLLM SIGNIFICANTLY OUTPERFORM FASTERTRANSFORMER IN END-TO-END PERFORMANCE">
  <data key="d0">MAKES</data>
</edge>
<edge source="PAGEDAT-TENTION" target="AN ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGEDAT-TENTION" target="OPERATING SYSTEMS (OS) SOLUTION TO MEMORY FRAGMENTATION AND SHARING">
  <data key="d0">IS INSPIRED BY</data>
</edge>
<edge source="OPERATING SYSTEMS (OS) SOLUTION TO MEMORY FRAGMENTATION AND SHARING" target="VIRTUAL MEMORY WITH PAGING">
  <data key="d0">IS</data>
</edge>
<edge source="PAGE-DATTENTION" target="AN ATTENTION ALGORITHM">
  <data key="d0">IS</data>
</edge>
<edge source="PAGE-DATTENTION" target="THE CLASSIC IDEA OF PAGING IN OPERATING SYSTEMS">
  <data key="d0">IS INSPIRED BY</data>
</edge>
<edge source="TRADITIONAL ATTENTION ALGORITHMS" target="PAGEDATTENTION">
  <data key="d0">ARE UNLIKE</data>
</edge>
<edge source="ATTENTION KEY AND VALUES VECTORS" target="NON-CONTIGUOUS BLOCKS IN THE MEMORY">
  <data key="d0">ARE STORED AS</data>
</edge>
<edge source="THIS PAPER" target="PAGEDATTENTION">
  <data key="d0">PROPOSES</data>
</edge>
<edge source="THIS PAPER" target="VLLM">
  <data key="d0">PRESENTS</data>
</edge>
<edge source="THIS PAPER" target="A NEW IDEA OF BLOCK-LEVEL MEMORY MANAGEMENT">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="VLLM" target="A HIGH-THROUGHPUT LLM SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="EFFICIENT MEMORY MANAGEMENT ENABLED BY PAGEDATTENTION">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="AN LLM SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="NEAR-ZERO WASTE IN KV CACHE MEMORY">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="FLEXIBLE SHARING OF KV CACHE WITHIN AND ACROSS REQUESTS">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="A HIGH-THROUGHPUT DISTRIBUTED LLM SERVING ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="PAGEDATTENTION">
  <data key="d0">EXECUTES</data>
</edge>
<edge source="VLLM" target="THROUGHPUT OF POPULAR LLMS BY 2-4">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="VLLM" target="THE SAME LEVEL OF LATENCY">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="STATE-OF-THE-ART SYSTEMS">
  <data key="d0">IS COMPARED TO</data>
</edge>
<edge source="VLLM" target="2-4 THROUGHPUT IMPROVEMENTS OVER THE STATE-OF-THE-ART SYSTEMS">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="THE RAPID GROWTH CURVE OF KV CACHE MEMORY SEEN IN EXISTING SYSTEMS 31, 60">
  <data key="d0">SMOOTHS OUT</data>
</edge>
<edge source="VLLM" target="A NOTABLE BOOST IN SERVING THROUGHPUT">
  <data key="d0">LEADS TO</data>
</edge>
<edge source="VLLM" target="THE IDEAS BEHIND VIRTUAL MEMORY">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE KV CACHE IN AN LLM SERVICE">
  <data key="d0">MANAGES</data>
</edge>
<edge source="VLLM" target="8.9">
  <data key="d0">HAS KV CACHE USAGE</data>
</edge>
<edge source="VLLM" target="41.6">
  <data key="d0">HAS TOKEN STATES RESERVATION</data>
</edge>
<edge source="VLLM" target="96.3">
  <data key="d0">HAS INTERNAL FRAGMENTATION</data>
</edge>
<edge source="VLLM" target="BLOCK-LEVEL MEMORY MANAGEMENT">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="PREEMPTIVE REQUEST SCHEDULING">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE MEMORY DURING THE DECODING PROCESS OF A SINGLE INPUT SEQUENCE">
  <data key="d0">MANAGES</data>
</edge>
<edge source="VLLM" target="RESERVING THE MEMORY FOR THE MAXIMUM POSSIBLE GENERATED SEQUENCE LENGTH INITIALLY">
  <data key="d0">DOES NOT REQUIRE</data>
</edge>
<edge source="VLLM" target="OSS VIRTUAL MEMORY">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="VLLM" target="PAGEDATTENTION KERNEL">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="NEWLY GENERATED KV CACHE">
  <data key="d0">SAVES</data>
</edge>
<edge source="VLLM" target="THIS SHARING EASILY">
  <data key="d0">CAN REALIZE</data>
</edge>
<edge source="VLLM" target="MEMORY">
  <data key="d0">CAN SAVE</data>
</edge>
<edge source="VLLM" target="PAGEDATTENTION AND PAGED MEMORY MANAGEMENT">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="POPULAR LLMS SUCH AS GPT 5, OPT 62, AND LLAMA 52">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VLLM" target="A DISTRIBUTED LLM SERVING ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="PREVIOUS STATE-OF-THE-ART SOLUTIONS">
  <data key="d0">OUTPERFORMS</data>
</edge>
<edge source="VLLM" target="UP TO 22 HIGHER REQUEST RATES">
  <data key="d0">CAN SUSTAIN</data>
</edge>
<edge source="VLLM" target="MEMORY FRAGMENTATION">
  <data key="d0">REDUCES</data>
</edge>
<edge source="VLLM" target="SHARING">
  <data key="d0">ENABLES</data>
</edge>
<edge source="VLLM" target="MORE REQUESTS IN A BATCH IN PARALLEL">
  <data key="d0">RUNS</data>
</edge>
<edge source="VLLM" target="2-4 SPEEDUP">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="THE CHALLENGES OUTLINED IN 3">
  <data key="d0">TACKLE</data>
</edge>
<edge source="VLLM" target="A CENTRALIZED SCHEDULER">
  <data key="d0">ADOPTS</data>
</edge>
<edge source="VLLM" target="ALL THE MEMORY WASTES FOR A REQUEST WITHIN ONE BLOCK">
  <data key="d0">LIMITS</data>
</edge>
<edge source="VLLM" target="ALL THE MEMORY">
  <data key="d0">CAN EFFECTIVELY UTILIZE</data>
</edge>
<edge source="VLLM" target="THE SHARING OF MOST OF THE SPACE USED TO STORE THE PROMPTS KV CACHE ACROSS MULTIPLE OUTPUT SAMPLES">
  <data key="d0">ENABLES</data>
</edge>
<edge source="VLLM" target="THE COMPLEX MEMORY SHARING BETWEEN DIFFERENT SEQUENCES">
  <data key="d0">CONCEALS</data>
</edge>
<edge source="VLLM" target="A COMMON MAPPING LAYER">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE NEW TOKEN">
  <data key="d0">GENERATES</data>
</edge>
<edge source="VLLM" target="THE PAGEDATTENTION ALGORITHM">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE FIRST 2 LOGICAL KV BLOCKS (0 AND 1) TO 2 PHYSICAL KV BLOCKS (7 AND 1, RESPECTIVELY)">
  <data key="d0">MAPS</data>
</edge>
<edge source="VLLM" target="KV CACHE OF THE PROMPTS">
  <data key="d0">GENERATES</data>
</edge>
<edge source="VLLM" target="FIRST OUTPUT TOKEN">
  <data key="d0">GENERATES</data>
</edge>
<edge source="VLLM" target="CONVENTIONAL SELF-ATTENTION ALGORITHM">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="THE KV CACHE OF THE FIRST 4 TOKENS IN LOGICAL BLOCK 0">
  <data key="d0">STORES</data>
</edge>
<edge source="VLLM" target="THE FOLLOWING 3 TOKENS IN LOGICAL BLOCK 1">
  <data key="d0">STORES</data>
</edge>
<edge source="VLLM" target="NEW PHYSICAL BLOCKS TO LOGICAL BLOCKS">
  <data key="d0">DYNAMICALLY ASSIGNS</data>
</edge>
<edge source="VLLM" target="FREE PHYSICAL BLOCKS FOR NEW TOKENS">
  <data key="d0">EXHAUSTS</data>
</edge>
<edge source="VLLM" target="A SET OF SEQUENCES TO EVICT">
  <data key="d0">SELECTS</data>
</edge>
<edge source="VLLM" target="THEIR KV CACHE TO THE CPU">
  <data key="d0">TRANSFERS</data>
</edge>
<edge source="VLLM" target="A SET OF CANDIDATE SEQUENCES FOR BATCHING">
  <data key="d0">SELECTS</data>
</edge>
<edge source="VLLM" target="THE PHYSICAL BLOCKS FOR THE NEWLY REQUIRED LOGICAL BLOCKS">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="VLLM" target="THE MEMORY FOR TWO SEQUENCES">
  <data key="d0">MANAGES</data>
</edge>
<edge source="VLLM" target="THE KV BLOCKS">
  <data key="d0">MANAGES</data>
</edge>
<edge source="VLLM" target="ALL PHYSICAL BLOCKS WHOSE REFERENCE COUNTS REACH 0">
  <data key="d0">FREES</data>
</edge>
<edge source="VLLM" target="NEW PHYSICAL BLOCKS (BLOCKS 9-12)">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="VLLM" target="RESERVING A SET OF PHYSICAL BLOCKS FOR A SET OF PREDEFINED SHARED PREFIXES BY THE LLM SERVICE PROVIDER">
  <data key="d0">CAN ACHIEVE</data>
</edge>
<edge source="VLLM" target="SIMULTANEOUS PROCESSING OF REQUESTS WITH DIFFERENT DECODING PREFERENCES">
  <data key="d0">FACILITATES</data>
</edge>
<edge source="VLLM" target="A SUBSET OF REQUESTS">
  <data key="d0">MUST PRIORITIZE</data>
</edge>
<edge source="VLLM" target="REQUESTS">
  <data key="d0">NEEDS TO PREEMPT</data>
</edge>
<edge source="VLLM" target="EARLIEST ARRIVED REQUESTS ARE SERVED FIRST">
  <data key="d0">ENSURES</data>
</edge>
<edge source="VLLM" target="LATEST REQUESTS ARE PREEMPTED FIRST">
  <data key="d0">ENSURES</data>
</edge>
<edge source="VLLM" target="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY">
  <data key="d0">ADOPTS</data>
</edge>
<edge source="VLLM" target="THE GPUS PHYSICAL BLOCKS TO STORE THE NEWLY GENERATED KV CACHE">
  <data key="d0">CAN RUN OUT OF</data>
</edge>
<edge source="VLLM" target="GPUS PARALLELISM FOR READING AND PROCESSING KV CACHE">
  <data key="d0">MAY NOT FULLY UTILIZE</data>
</edge>
<edge source="VLLM" target="TWO CLASSIC QUESTIONS">
  <data key="d0">NEEDS TO ANSWER</data>
</edge>
<edge source="VLLM" target="GPU BLOCK ALLOCATOR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="VLLM" target="CPU BLOCK ALLOCATOR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="VLLM" target="A SEQUENCE">
  <data key="d0">PREEMPTS</data>
</edge>
<edge source="VLLM" target="ITS BLOCKS">
  <data key="d0">EVICTS</data>
</edge>
<edge source="VLLM" target="NEW REQUESTS">
  <data key="d0">STOPS ACCEPTING</data>
</edge>
<edge source="VLLM" target="ALL PREEMPTED SEQUENCES ARE COMPLETED">
  <data key="d0">STOPS ACCEPTING NEW REQUESTS UNTIL</data>
</edge>
<edge source="VLLM" target="RECOMPUTATION">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VLLM" target="SWAPPING">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VLLM" target="DISTRIBUTED SETTINGS">
  <data key="d0">IS EFFECTIVE IN</data>
</edge>
<edge source="VLLM" target="MEGATRON-LM STYLE TENSOR MODEL PARALLELISM STRATEGY">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VLLM" target="A SINGLE KV CACHE MANAGER">
  <data key="d0">FEATURES</data>
</edge>
<edge source="VLLM" target="AN END-TO-END SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="VLLM" target="A FASTAPI 15 FRONTEND">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="A GPU-BASED INFERENCE ENGINE">
  <data key="d0">HAS</data>
</edge>
<edge source="VLLM" target="MULTIPLE OUTPUT SEQUENCES">
  <data key="d0">CREATES</data>
</edge>
<edge source="VLLM" target="THE SINGLE INPUT SEQUENCE">
  <data key="d0">CREATES MULTIPLE OUTPUT SEQUENCES FROM</data>
</edge>
<edge source="VLLM" target="THE FORK METHOD">
  <data key="d0">USES</data>
</edge>
<edge source="VLLM" target="OPT MODEL">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="VLLM" target="30.42 FOR SHAREGPT">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="VLLM" target="1.72.7 HIGHER REQUEST RATES COMPARED TO ORCA (ORACLE)">
  <data key="d0">CAN SUSTAIN</data>
</edge>
<edge source="VLLM" target="2.78 HIGHER REQUEST RATES COMPARED TO ORCA (MAX)">
  <data key="d0">CAN SUSTAIN</data>
</edge>
<edge source="VLLM" target="SIMILAR LATENCIES">
  <data key="d0">MAINTAINS</data>
</edge>
<edge source="VLLM" target="3.58 HIGHER THROUGHPUT THAN ORCA (ORACLE)">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="VLLM" target="MORE IMPROVEMENT OVER THE ORCA BASELINES">
  <data key="d0">BRINGS</data>
</edge>
<edge source="VLLM" target="2 HIGHER REQUEST RATES">
  <data key="d0">CAN SUSTAIN</data>
</edge>
<edge source="VLLM" target="64 128 256 CONTEXT LENGTH">
  <data key="d0">CAN EFFECTIVELY HANDLE</data>
</edge>
<edge source="VLLM" target="DEFAULT BLOCK SIZE AS 16">
  <data key="d0">SETS</data>
</edge>
<edge source="VLLM" target="THE OVERHEAD OF MEMORY INDIRECTION IN PAGING">
  <data key="d0">MITIGATES</data>
</edge>
<edge source="VLLM" target="THE GPU KERNELS FOR MEMORY ACCESS OPERATIONS WITH THOSE FOR OTHER OPERATIONS SUCH AS ATTENTION">
  <data key="d0">FUSES</data>
</edge>
<edge source="VLLM" target="THE IDEA OF VIRTUAL MEMORY AND PAGING">
  <data key="d0">AUGMENTS</data>
</edge>
<edge source="VLLM" target="THE APPLICATION-SPECIFIC SEMANTICS">
  <data key="d0">LEVERAGES</data>
</edge>
<edge source="FLEXIBLE SHARING OF KV CACHE" target="MEMORY USAGE">
  <data key="d0">REDUCES</data>
</edge>
<edge source="MEMORY USAGE" target="GB">
  <data key="d0">IS MEASURED IN</data>
</edge>
<edge source="MEMORY USAGE" target="GREATLY REDUCED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="MEMORY USAGE" target="ESPECIALLY FOR LONG INPUT PROMPTS">
  <data key="d0">IS REDUCED</data>
</edge>
<edge source="EXISTING LLM SERVING SYSTEMS 31, 60" target="MANAGING THE KV CACHE MEMORY EFFICIENTLY">
  <data key="d0">FALL SHORT OF</data>
</edge>
<edge source="BLOCKS" target="PAGES">
  <data key="d0">ARE</data>
</edge>
<edge source="BLOCKS" target="2, 4, 5, 8">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="TOKENS" target="BYTES">
  <data key="d0">ARE</data>
</edge>
<edge source="REQUESTS" target="PROCESSES">
  <data key="d0">ARE</data>
</edge>
<edge source="REQUESTS" target="MODEL WEIGHTS">
  <data key="d0">SHARE</data>
</edge>
<edge source="REQUESTS" target="DIFFERENT TIMES">
  <data key="d0">MAY ARRIVE AT</data>
</edge>
<edge source="REQUESTS" target="VASTLY DIFFERENT INPUT AND OUTPUT LENGTHS">
  <data key="d0">MAY HAVE</data>
</edge>
<edge source="KV CACHE MEMORY" target="EXISTING SYSTEMS">
  <data key="d0">IS MANAGED IN</data>
</edge>
<edge source="KV CACHE MEMORY" target="BATCH SIZE">
  <data key="d0">CAN LIMIT</data>
</edge>
<edge source="KV CACHE MEMORY" target="THROUGHPUT OF THE LLM">
  <data key="d0">CAN LIMIT</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="OPERATING SYSTEMS">
  <data key="d0">ARE INSPIRED BY</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="VIRTUAL MEMORY">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="COPY-ON-WRITE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="EFFICIENTLY MANAGE KV CACHE">
  <data key="d0">CAN BE ADAPTED TO</data>
</edge>
<edge source="ESTABLISHED TECHNIQUES" target="HANDLE VARIOUS DECODING ALGORITHMS IN LLM SERVING">
  <data key="d0">CAN BE ADAPTED TO</data>
</edge>
<edge source="STATE-OF-THE-ART SYSTEMS" target="FASTERTRANSFORMER AND ORCA">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE IMPROVEMENT" target="LONGER SEQUENCES">
  <data key="d0">IS MORE PRONOUNCED WITH</data>
</edge>
<edge source="THE IMPROVEMENT" target="LARGER MODELS">
  <data key="d0">IS MORE PRONOUNCED WITH</data>
</edge>
<edge source="THE IMPROVEMENT" target="MORE COMPLEX DECODING ALGORITHMS">
  <data key="d0">IS MORE PRONOUNCED WITH</data>
</edge>
<edge source="IMPROVEMENTS" target="LONGER SEQUENCES">
  <data key="d0">ARE MORE PRONOUNCED WITH</data>
</edge>
<edge source="IMPROVEMENTS" target="LARGER MODELS">
  <data key="d0">ARE MORE PRONOUNCED WITH</data>
</edge>
<edge source="IMPROVEMENTS" target="MORE COMPLEX DECODING ALGORITHMS">
  <data key="d0">ARE MORE PRONOUNCED WITH</data>
</edge>
<edge source="VLLMS SOURCE CODE" target="PUBLICLY AVAILABLE">
  <data key="d0">IS</data>
</edge>
<edge source="VLLMS SOURCE CODE" target="HTTPS:GITHUB.COMVLLM-PROJECTVLLM">
  <data key="d0">IS AVAILABLE AT</data>
</edge>
<edge source="LARGE LANGUAGE MODELS (LLMS) LIKE GPT 5, 37 AND PALM 9" target="NEW APPLICATIONS SUCH AS PROGRAMMING ASSISTANTS 6, 18 AND UNIVERSAL CHATBOTS 19, 35">
  <data key="d0">HAVE ENABLED</data>
</edge>
<edge source="NEW APPLICATIONS SUCH AS PROGRAMMING ASSISTANTS 6, 18 AND UNIVERSAL CHATBOTS 19, 35" target="OUR WORK AND DAILY ROUTINES">
  <data key="d0">ARE STARTING TO IMPACT</data>
</edge>
<edge source="MANY CLOUD COMPANIES 34, 44" target="THESE APPLICATIONS AS HOSTED SERVICES">
  <data key="d0">ARE RACING TO PROVIDE</data>
</edge>
<edge source="RUNNING THESE APPLICATIONS" target="VERY EXPENSIVE">
  <data key="d0">IS</data>
</edge>
<edge source="RUNNING THESE APPLICATIONS" target="A LARGE NUMBER OF HARDWARE ACCELERATORS SUCH AS GPUS">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="PROCESSING AN LLM REQUEST" target="10 MORE EXPENSIVE THAN A TRADITIONAL KEYWORD QUERY">
  <data key="d0">CAN BE</data>
</edge>
<edge source="THIS WORK" target="A CREATIVE COMMONS ATTRIBUTION INTERNATIONAL 4.0 LICENSE">
  <data key="d0">IS LICENSED UNDER</data>
</edge>
<edge source="THIS WORK" target="THE HORIZON KEY DIGITAL TECHNOLOGIES JOINT UNDERTAKING (KDT JU) PROGRAMME">
  <data key="d0">WAS SUPPORTED IN PART BY</data>
</edge>
<edge source="THIS WORK" target="N 4096 LOCAL INTERRUPT SOURCES">
  <data key="d0">CAN SCALE UP TO</data>
</edge>
<edge source="THIS WORK" target="A COMBINATION OF THE BACKGROUND-SAVING WITH A REGISTER BANKING APPROACH">
  <data key="d0">PROPOSES</data>
</edge>
<edge source="SOSP 23" target="OCTOBER 23 26 2023">
  <data key="d0">DATE</data>
</edge>
<edge source="SOSP 23" target="KOBLENZ GERMANY">
  <data key="d0">LOCATION</data>
</edge>
<edge source="COPYRIGHT" target="2023">
  <data key="d0">YEAR</data>
</edge>
<edge source="COPYRIGHT" target="OWNERAUTHOR(S)">
  <data key="d0">HELD BY</data>
</edge>
<edge source="ACM" target="ISBN 979-8-4007-0229-72310">
  <data key="d0">HAS</data>
</edge>
<edge source="NVIDIA A100" target="40GB PARAMETERS">
  <data key="d0">HAS</data>
</edge>
<edge source="KV CACHE" target="26GB">
  <data key="d0">HAS SIZE</data>
</edge>
<edge source="KV CACHE" target="65">
  <data key="d0">HAS SIZE</data>
</edge>
<edge source="KV CACHE" target="UNIQUE CHARACTERISTICS">
  <data key="d0">HAS</data>
</edge>
<edge source="KV CACHE" target="OVER TIME">
  <data key="d0">DYNAMICALLY GROWS AND SHRINKS</data>
</edge>
<edge source="KV CACHE" target="OF TWO REQUESTS">
  <data key="d0">IS STORED</data>
</edge>
<edge source="KV CACHE" target="IN VLLM">
  <data key="d0">IS STORED</data>
</edge>
<edge source="KV CACHE" target="UNSHARED DURING THE AUTOREGRESSIVE GENERATION PHASE">
  <data key="d0">SHOULD REMAIN</data>
</edge>
<edge source="OTHERS" target="20">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="OTHERS" target="30">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="OTHERS" target="40">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="20" target="R. BARRY">
  <data key="d0">IS</data>
</edge>
<edge source="30" target="RENESAS">
  <data key="d0">IS</data>
</edge>
<edge source="PARAMETER SIZE" target="EXISTING SYSTEMS VLLM">
  <data key="d0">IS ATTRIBUTE OF</data>
</edge>
<edge source="THROUGHPUT" target="TOKENS">
  <data key="d0">IS MEASURED IN</data>
</edge>
<edge source="MEMORY LAYOUT" target="LEFT WHEN SERVING AN LLM WITH 13B PARAMETERS ON NVIDIA A100">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY DISTRIBUTION" target="1 (LEFT)">
  <data key="d0">IS ILLUSTRATED BY</data>
</edge>
<edge source="13B-PARAMETER LLM" target="NVIDIA A100 GPU">
  <data key="d0">RUNS ON</data>
</edge>
<edge source="NVIDIA A100 GPU" target="40GB RAM">
  <data key="d0">HAS</data>
</edge>
<edge source="THE PARAMETERS (GRAY)" target="GPU MEMORY">
  <data key="d0">PERSIST IN</data>
</edge>
<edge source="THE PARAMETERS (GRAY)" target="SERVING">
  <data key="d0">PERSIST THROUGHOUT</data>
</edge>
<edge source="GPU MEMORY" target="80GB MAXIMUM">
  <data key="d0">STAYS AT</data>
</edge>
<edge source="THE MEMORY FOR THE KV CACHE (RED)" target="PER SERVING REQUEST">
  <data key="d0">IS (DE)ALLOCATED</data>
</edge>
<edge source="A CONTIGUOUS CHUNK OF MEMORY" target="THE REQUESTS MAXIMUM LENGTH">
  <data key="d0">HAS</data>
</edge>
<edge source="THE REQUESTS MAXIMUM LENGTH" target="2048 TOKENS">
  <data key="d0">IS</data>
</edge>
<edge source="ALL AVAILABLE MEMORY" target="KV CACHE">
  <data key="d0">WAS ALLOCATED TO</data>
</edge>
<edge source="OUTPUT LENGTH OF A REQUEST" target="DECODING">
  <data key="d0">GROWS AT</data>
</edge>
<edge source="DECODING" target="PAGEDATTENTION AND VLLM">
  <data key="d0">USES</data>
</edge>
<edge source="MEMORY REQUIRED FOR ITS KV CACHE" target="AS THE OUTPUT LENGTH OF A REQUEST GROWS AT DECODING">
  <data key="d0">EXPANDS</data>
</edge>
<edge source="MEMORY REQUIRED FOR ITS KV CACHE" target="AVAILABLE MEMORY FOR INCOMING REQUESTS OR ONGOING GENERATION FOR EXISTING PROMPTS">
  <data key="d0">MAY EXHAUST</data>
</edge>
<edge source="A SMALL AMOUNT OF MEMORY (YELLOW)" target="EPHEMERALLY FOR ACTIVATION">
  <data key="d0">IS USED</data>
</edge>
<edge source="KEY IDEA BEHIND VLLMS MEMORY MANAGER" target="VIRTUAL MEMORY 25 IN OPERATING SYSTEMS">
  <data key="d0">IS ANALOGOUS TO</data>
</edge>
<edge source="COST PER REQUEST OF LLM SERVING SYSTEMS" target="MORE IMPORTANT">
  <data key="d0">IS BECOMING</data>
</edge>
<edge source="LLMS" target="AN AUTOREGRESSIVE TRANSFORMER MODEL 53">
  <data key="d0">LIES AT THE CORE OF</data>
</edge>
<edge source="LLMS" target="CONDITIONAL GENERATION SERVICE">
  <data key="d0">ARE DEPLOYED AS</data>
</edge>
<edge source="THIS MODEL" target="WORDS (TOKENS)">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THIS MODEL" target="ONE AT A TIME">
  <data key="d0">GENERATES WORDS</data>
</edge>
<edge source="THIS MODEL" target="BASED ON THE INPUT (PROMPT)">
  <data key="d0">GENERATES WORDS</data>
</edge>
<edge source="THIS MODEL" target="BASED ON THE PREVIOUS SEQUENCE OF THE OUTPUTS TOKENS IT HAS GENERATED SO FAR">
  <data key="d0">GENERATES WORDS</data>
</edge>
<edge source="THIS EXPENSIVE PROCESS" target="FOR EACH REQUEST">
  <data key="d0">IS REPEATED</data>
</edge>
<edge source="THIS EXPENSIVE PROCESS" target="THE MODEL OUTPUTS A TERMINATION TOKEN">
  <data key="d0">IS REPEATED UNTIL</data>
</edge>
<edge source="SEQUENTIAL GENERATION PROCESS" target="WORKLOAD MEMORY-BOUND">
  <data key="d0">MAKES</data>
</edge>
<edge source="SEQUENTIAL GENERATION PROCESS" target="COMPUTATION POWER OF GPUS">
  <data key="d0">UNDERUTILIZES</data>
</edge>
<edge source="SEQUENTIAL GENERATION PROCESS" target="SERVING THROUGHPUT">
  <data key="d0">LIMITS</data>
</edge>
<edge source="IMPROVING THE THROUGHPUT" target="BATCHING MULTIPLE REQUESTS TOGETHER">
  <data key="d0">IS POSSIBLE BY</data>
</edge>
<edge source="MEMORY SPACE FOR EACH REQUEST" target="EFFICIENTLY MANAGED">
  <data key="d0">SHOULD BE</data>
</edge>
<edge source="APPROXIMATELY 65 OF THE MEMORY" target="THE MODEL WEIGHTS">
  <data key="d0">IS ALLOCATED FOR</data>
</edge>
<edge source="THE MODEL WEIGHTS" target="STATIC DURING SERVING">
  <data key="d0">REMAIN</data>
</edge>
<edge source="CLOSE TO 30 OF THE MEMORY" target="THE DYNAMIC STATES OF THE REQUESTS">
  <data key="d0">IS USED TO STORE</data>
</edge>
<edge source="STATES" target="KEY AND VALUE TENSORS">
  <data key="d0">CONSIST OF</data>
</edge>
<edge source="KEY AND VALUE TENSORS" target="ATTENTION MECHANISM">
  <data key="d0">ARE ASSOCIATED WITH</data>
</edge>
<edge source="KEY AND VALUE TENSORS" target="KV CACHE 41">
  <data key="d0">ARE COMMONLY REFERRED TO AS</data>
</edge>
<edge source="KV CACHE 41" target="CONTEXT FROM EARLIER TOKENS">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="CONTEXT FROM EARLIER TOKENS" target="NEW OUTPUT TOKENS IN SEQUENCE">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THE REMAINING SMALL EQUAL CONTRIBUTION" target="UNKNOWN">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA (MAX)" target="20.4">
  <data key="d0">HAS KV CACHE USAGE</data>
</edge>
<edge source="ORCA (MAX)" target="26.8">
  <data key="d0">HAS TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (MAX)" target="38.2">
  <data key="d0">HAS INTERNAL FRAGMENTATION</data>
</edge>
<edge source="ORCA (MAX)" target="OPT MODEL">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="ORCA (MAX)" target="7.00 FOR SHAREGPT">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (MAX)" target="43.24 FOR ALPACA">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (POW2)" target="13.3">
  <data key="d0">HAS KV CACHE USAGE</data>
</edge>
<edge source="ORCA (POW2)" target="17.9">
  <data key="d0">HAS TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (POW2)" target="25.2">
  <data key="d0">HAS INTERNAL FRAGMENTATION</data>
</edge>
<edge source="ORCA (POW2)" target="OPT MODEL">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="ORCA (POW2)" target="9.81 FOR SHAREGPT">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (POW2)" target="72.75 FOR ALPACA">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (ORACLE)" target="57.3">
  <data key="d0">HAS KV CACHE USAGE</data>
</edge>
<edge source="ORCA (ORACLE)" target="13.6">
  <data key="d0">HAS TOKEN STATES RESERVATION</data>
</edge>
<edge source="ORCA (ORACLE)" target="36.6">
  <data key="d0">HAS INTERNAL FRAGMENTATION</data>
</edge>
<edge source="ORCA (ORACLE)" target="OPT MODEL">
  <data key="d0">IS A TYPE OF</data>
</edge>
<edge source="ORCA (ORACLE)" target="13.62 FOR SHAREGPT">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="ORCA (ORACLE)" target="132.44 FOR ALPACA">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="AVERAGE PERCENTAGE OF MEMORY" target="DIFFERENT LLM SERVING SYSTEMS">
  <data key="d0">WASTES IN</data>
</edge>
<edge source="EXPERIMENT" target="6.2">
  <data key="d0">IS IN</data>
</edge>
<edge source="PERCENTAGE OF MEMORY" target="OTHER DATA">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="OTHER DATA" target="ACTIVATIONS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ACTIVATIONS" target="EPHEMERAL TENSORS">
  <data key="d0">ARE</data>
</edge>
<edge source="ACTIVATIONS" target="A SMALL FRACTION OF THE GPU MEMORY">
  <data key="d0">OCCUPY</data>
</edge>
<edge source="EPHEMERAL TENSORS" target="EVALUATING THE LLM">
  <data key="d0">ARE CREATED WHEN</data>
</edge>
<edge source="MODEL WEIGHTS" target="CONSTANT">
  <data key="d0">ARE</data>
</edge>
<edge source="THE WAY THE KV CACHE IS MANAGED" target="CRITICAL IN DETERMINING THE MAXIMUM BATCH SIZE">
  <data key="d0">IS</data>
</edge>
<edge source="LIMITATION OF BATCH SIZE AND THROUGHPUT" target="INEFFICIENT MANAGEMENT OF KV CACHE MEMORY">
  <data key="d0">IS CAUSED BY</data>
</edge>
<edge source="FINE-GRAINED BATCHING" target="THE WASTE OF COMPUTING">
  <data key="d0">REDUCES</data>
</edge>
<edge source="FINE-GRAINED BATCHING" target="REQUESTS TO BE BATCHED IN A MORE FLEXIBLE WAY">
  <data key="d0">ENABLES</data>
</edge>
<edge source="THE NUMBER OF REQUESTS THAT CAN BE BATCHED TOGETHER" target="GPU MEMORY CAPACITY">
  <data key="d0">IS CONSTRAINED BY</data>
</edge>
<edge source="GPU MEMORY CAPACITY" target="THE SPACE TO STORE THE KV CACHE">
  <data key="d0">PARTICULARLY ALLOCATES</data>
</edge>
<edge source="THE IDEA OF VIRTUAL MEMORY AND PAGING" target="MANAGING THE KV CACHE IN LLM SERVING">
  <data key="d0">IS EFFECTIVE FOR</data>
</edge>
<edge source="THE WORKLOAD" target="DYNAMIC MEMORY ALLOCATION">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="THE OUTPUT LENGTH" target="NOT KNOWN A PRIORI">
  <data key="d0">IS</data>
</edge>
<edge source="ITS PERFORMANCE" target="THE GPU MEMORY CAPACITY">
  <data key="d0">IS BOUND BY</data>
</edge>
<edge source="THE KV CACHE OF A REQUEST" target="CONTIGUOUS MEMORY SPACE">
  <data key="d0">IS STORED IN</data>
</edge>
<edge source="MOST DEEP LEARNING FRAMEWORKS 33, 39" target="TENSORS TO BE STORED IN CONTIGUOUS MEMORY">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="OPERATORS IN CURRENT DEEP LEARNING FRAMEWORKS" target="TENSORS TO BE STORED IN CONTIGUOUS MEMORY">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="PREVIOUS LLM SERVING SYSTEMS" target="THE KV CACHE OF ONE REQUEST AS A CONTIGUOUS TENSOR ACROSS THE DIFFERENT POSITIONS">
  <data key="d0">STORE</data>
</edge>
<edge source="PREVIOUS LLM SERVING SYSTEMS" target="FREQUENT MEMORY COPIES OF THE KV CACHE ACROSS THE BEAM CANDIDATES">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="MODEL" target="NEW TOKENS">
  <data key="d0">GENERATES</data>
</edge>
<edge source="LIFETIME AND LENGTH OF KV CACHE" target="NOT KNOWN A PRIORI">
  <data key="d0">ARE</data>
</edge>
<edge source="THE EXISTING SYSTEMS" target="INTERNAL AND EXTERNAL MEMORY FRAGMENTATION">
  <data key="d0">SUFFER FROM</data>
</edge>
<edge source="THE EXISTING SYSTEMS" target="THE OPPORTUNITIES FOR MEMORY SHARING">
  <data key="d0">CANNOT EXPLOIT</data>
</edge>
<edge source="THE REQUESTS ACTUAL LENGTH" target="MUCH SHORTER THAN ITS MAXIMUM LENGTH">
  <data key="d0">CAN BE</data>
</edge>
<edge source="PRE-ALLOCATION" target="INEFFICIENT">
  <data key="d0">IS</data>
</edge>
<edge source="ENTIRE CHUNK" target="REQUESTS LIFETIME">
  <data key="d0">IS RESERVED DURING</data>
</edge>
<edge source="OTHER SHORTER REQUESTS" target="ANY PART OF THE CHUNK THAT IS CURRENTLY UNUSED">
  <data key="d0">CANNOT UTILIZE</data>
</edge>
<edge source="ONLY 20.4 - 38.2 OF THE KV CACHE MEMORY" target="THE ACTUAL TOKEN STATES">
  <data key="d0">IS USED TO STORE</data>
</edge>
<edge source="THE ACTUAL TOKEN STATES" target="THE EXISTING SYSTEMS">
  <data key="d0">ARE STORED IN</data>
</edge>
<edge source="KV CACHE OF ONE TOKEN" target="ALL ITS PREVIOUS TOKENS">
  <data key="d0">DEPENDS ON</data>
</edge>
<edge source="THE KV CACHE OF THE SAME TOKEN" target="DIFFERENT">
  <data key="d0">WILL BE</data>
</edge>
<edge source="THE SAME TOKEN" target="DIFFERENT POSITIONS IN A SEQUENCE">
  <data key="d0">APPEARS AT</data>
</edge>
<edge source="THE TOKEN IN EACH MEMORY SLOT" target="ITS KV CACHE">
  <data key="d0">REPRESENTS</data>
</edge>
<edge source="THE SAME TOKENS" target="DIFFERENT KV CACHE">
  <data key="d0">CAN HAVE</data>
</edge>
<edge source="THE SAME TOKENS" target="AT DIFFERENT POSITIONS">
  <data key="d0">ARE</data>
</edge>
<edge source="LLM SERVICES" target="ADVANCED DECODING ALGORITHMS">
  <data key="d0">USE</data>
</edge>
<edge source="LLM SERVICES" target="A RANGE OF DECODING ALGORITHMS">
  <data key="d0">OFFER</data>
</edge>
<edge source="LLM SERVICES" target="A UNIQUE CHALLENGE">
  <data key="d0">FACE</data>
</edge>
<edge source="ADVANCED DECODING ALGORITHMS" target="PARALLEL SAMPLING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ADVANCED DECODING ALGORITHMS" target="BEAM SEARCH">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ADVANCED DECODING ALGORITHMS" target="MULTIPLE OUTPUTS PER REQUEST">
  <data key="d0">GENERATE</data>
</edge>
<edge source="PARALLEL SAMPLING" target="EXAMPLE">
  <data key="d0">IS</data>
</edge>
<edge source="PARALLEL SAMPLING" target="OUTPUT SEQUENCES">
  <data key="d0">ASSOCIATED WITH</data>
</edge>
<edge source="BEAM SEARCH" target="EXAMPLE">
  <data key="d0">IS</data>
</edge>
<edge source="BEAM SEARCH" target="EACH CANDIDATE SEQUENCE IN THE BEAM">
  <data key="d0">EXPANDS</data>
</edge>
<edge source="BEAM SEARCH" target="ALL POSSIBLE TOKENS">
  <data key="d0">CONSIDERS</data>
</edge>
<edge source="BEAM SEARCH" target="THEIR RESPECTIVE PROBABILITIES USING THE LLM">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="BEAM SEARCH" target="THE TOP-MOST PROBABLE SEQUENCES">
  <data key="d0">RETAINS</data>
</edge>
<edge source="BEAM SEARCH" target="SHARING NOT ONLY THE INITIAL PROMPT BLOCKS BUT ALSO OTHER BLOCKS ACROSS DIFFERENT CANDIDATES">
  <data key="d0">FACILITIES</data>
</edge>
<edge source="BEAM SEARCH" target="VLLM">
  <data key="d0">IS APPLIED BY</data>
</edge>
<edge source="BEAM SEARCH" target="2">
  <data key="d0">HAS BEAM WIDTH</data>
</edge>
<edge source="BEAM SEARCH" target="FIGURE 15">
  <data key="d0">REFERENCED IN</data>
</edge>
<edge source="DECODING ALGORITHMS" target="USERS TO SELECT FROM">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="DECODING ALGORITHMS" target="VARYING IMPLICATIONS FOR MEMORY MANAGEMENT COMPLEXITY">
  <data key="d0">HAVE</data>
</edge>
<edge source="LLM SERVICE" target="MORE COMPLEX DECODING SCENARIOS">
  <data key="d0">MUST OFFER</data>
</edge>
<edge source="LLM SERVICE" target="MORE OPPORTUNITIES FOR MEMORY SHARING">
  <data key="d0">MUST OFFER</data>
</edge>
<edge source="MORE COMPLEX DECODING SCENARIOS" target="COMPLEX ACCESSING PATTERNS">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="THE REQUEST" target="MULTIPLE SEQUENCES">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="MULTIPLE SEQUENCES" target="THEIR KV CACHE">
  <data key="d0">CAN PARTIALLY SHARE</data>
</edge>
<edge source="TWO REQUESTS" target="AT THE SAME TIME">
  <data key="d0">ARE STORED</data>
</edge>
<edge source="REQUEST" target="ITS GENERATION">
  <data key="d0">FINISHES</data>
</edge>
<edge source="ITS KV BLOCKS" target="THE KV CACHE OF OTHER REQUESTS">
  <data key="d0">CAN BE FREED TO STORE</data>
</edge>
<edge source="MEMORY SHARING" target="THE EXISTING SYSTEMS">
  <data key="d0">IS NOT POSSIBLE IN</data>
</edge>
<edge source="MEMORY SHARING" target="THE DIFFERENT SEQUENCES ASSOCIATED WITH THE SAME REQUEST">
  <data key="d0">OCCURS ACROSS</data>
</edge>
<edge source="MEMORY SHARING" target="THE DIFFERENT REQUESTS">
  <data key="d0">OCCURS ACROSS</data>
</edge>
<edge source="MEMORY SHARING" target="PAGE-DATTENTION">
  <data key="d0">IS USED IN</data>
</edge>
<edge source="THE KV CACHE OF THE SEQUENCES" target="SEPARATE CONTIGUOUS SPACES">
  <data key="d0">IS STORED IN</data>
</edge>
<edge source="EACH BLOCK" target="THE ATTENTION KEYS AND VALUES OF A FIXED NUMBER OF TOKENS">
  <data key="d0">CAN CONTAIN</data>
</edge>
<edge source="EACH BLOCK" target="THE KEY AND VALUE VECTORS FOR A FIXED NUMBER OF TOKENS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="PAGEDATTENTION KERNEL" target="DIFFERENT KV BLOCKS">
  <data key="d0">FETCHES</data>
</edge>
<edge source="PAGEDATTENTION KERNEL" target="PREVIOUS KV CACHE">
  <data key="d0">ACCESSES</data>
</edge>
<edge source="BLOCKS FOR THE KV CACHE" target="CONTIGUOUS SPACE">
  <data key="d0">ARE NOT NECESSARILY STORED IN</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="THE KV CACHE">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="A PAGED FASHION">
  <data key="d0">MANAGES IN</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="THE PHYSICAL KV CACHE MEMORY ON THE GPU WORKERS">
  <data key="d0">MANAGES</data>
</edge>
<edge source="THE KV CACHE MANAGER" target="THE INSTRUCTIONS SENT BY THE CENTRALIZED SCHEDULER">
  <data key="d0">MANAGES THROUGH</data>
</edge>
<edge source="THE KV CACHE" target="PAGEDATTENTION">
  <data key="d0">IS ENABLED BY</data>
</edge>
<edge source="THE KV CACHE" target="WHEN THE PREEMPTED SEQUENCES ARE RESCHEDULED">
  <data key="d0">IS RECOMPUTED</data>
</edge>
<edge source="THE DESIGN OF THE KV CACHE MANAGER" target="PAGEDATTENTION IN 4.3">
  <data key="d0">FACILITATES</data>
</edge>
<edge source="KV BLOCKS" target="PAGES IN VIRTUAL MEMORY">
  <data key="d0">ARE LIKE</data>
</edge>
<edge source="THIS DESIGN" target="INTERNAL FRAGMENTATION">
  <data key="d0">ALLEVIATES</data>
</edge>
<edge source="THIS DESIGN" target="RELATIVELY SMALL BLOCKS">
  <data key="d0">USES</data>
</edge>
<edge source="THIS DESIGN" target="THEM ON DEMAND">
  <data key="d0">ALLOCATES</data>
</edge>
<edge source="THIS DESIGN" target="EFFECTIVE MEMORY MANAGEMENT FOR VARIOUS DECODING METHODS">
  <data key="d0">FACILITATES</data>
</edge>
<edge source="THIS DESIGN" target="VARIABLE LENGTH INPUT AND OUTPUT SEQUENCES">
  <data key="d0">HANDLES</data>
</edge>
<edge source="THIS DESIGN" target="A DEDICATED MEMORY PORT FOR THE BACKGROUND-SAVING MECHANISM">
  <data key="d0">HAS</data>
</edge>
<edge source="THIS DESIGN" target="A LARGER AREA OVERHEAD COMPARED TO TRADITIONAL RISC-V CLINT 14">
  <data key="d0">INCURS</data>
</edge>
<edge source="INTERNAL FRAGMENTATION" target="UNUSED">
  <data key="d0">REMAINS</data>
</edge>
<edge source="ALL BLOCKS" target="THE SAME SIZE">
  <data key="d0">HAVE</data>
</edge>
<edge source="BLOCK-LEVEL MEMORY MANAGEMENT" target="PAGEDATTENTION">
  <data key="d0">ARE CO-DESIGNED WITH</data>
</edge>
<edge source="BLOCK-LEVEL MEMORY MANAGEMENT" target="ONLINE SERVING">
  <data key="d0">IS IN THE CONTEXT OF</data>
</edge>
<edge source="PREEMPTIVE REQUEST SCHEDULING" target="PAGEDATTENTION">
  <data key="d0">ARE CO-DESIGNED WITH</data>
</edge>
<edge source="PAGEDATTENTION ALGORITHM" target="KV BLOCKS TO BE STORED IN NON-CONTIGUOUS PHYSICAL MEMORY">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="NON-CONTIGUOUS PHYSICAL MEMORY" target="MORE FLEXIBLE PAGED MEMORY MANAGEMENT IN VLLM">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PREVIOUS KV CACHE" target="LOGICAL KV BLOCKS">
  <data key="d0">IS STORED IN</data>
</edge>
<edge source="LOGICAL KV BLOCKS" target="LEFT TO RIGHT">
  <data key="d0">ARE FILLED FROM</data>
</edge>
<edge source="LOGICAL KV BLOCKS" target="NEW TOKENS AND THEIR KV CACHE ARE GENERATED">
  <data key="d0">ARE FILLED AS</data>
</edge>
<edge source="NEWLY GENERATED KV CACHE" target="PHYSICAL KV BLOCKS">
  <data key="d0">IS SAVED INTO</data>
</edge>
<edge source="MEMORY" target="AN INCREASINGLY SIGNIFICANT BOTTLENECK">
  <data key="d0">WILL BECOME</data>
</edge>
<edge source="POPULAR LLMS" target="GPT 5">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="POPULAR LLMS" target="OPT 62">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="POPULAR LLMS" target="LLAMA 52">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="POPULAR LLMS" target="VARYING SIZES">
  <data key="d0">HAVE</data>
</edge>
<edge source="LLAMA 52" target="13B PARAMETERS">
  <data key="d0">HAS</data>
</edge>
<edge source="VARYING SIZES" target="ONES EXCEEDING THE MEMORY CAPACITY OF A SINGLE GPU">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="PREVIOUS STATE-OF-THE-ART SOLUTIONS" target="FASTERTRANSFORMER 31">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="PREVIOUS STATE-OF-THE-ART SOLUTIONS" target="ORCA 60">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FASTERTRANSFORMER 31" target="A DISTRIBUTED INFERENCE ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="FASTERTRANSFORMER 31" target="HIGHLY OPTIMIZED FOR LATENCY">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA 60" target="STATE-OF-THE-ART LLM SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA 60" target="THROUGHPUT">
  <data key="d0">IS OPTIMIZED FOR</data>
</edge>
<edge source="ORCA 60" target="MOST RELEVANT TO OUR APPROACH">
  <data key="d0">IS</data>
</edge>
<edge source="FASTERTRANSFORMER" target="A FINE-GRAINED SCHEDULING MECHANISM">
  <data key="d0">DOES NOT UTILIZE</data>
</edge>
<edge source="FASTERTRANSFORMER" target="THE MEMORY LIKE ORCA (MAX)">
  <data key="d0">INEFFICIENTLY MANAGES</data>
</edge>
<edge source="FASTERTRANSFORMER" target="ITS OWN SCHEDULER">
  <data key="d0">DOES NOT HAVE</data>
</edge>
<edge source="SHARING" target="VLLM">
  <data key="d0">OCCURS IN</data>
</edge>
<edge source="2-4 SPEEDUP" target="ORCA">
  <data key="d0">IS COMPARED TO</data>
</edge>
<edge source="ORCA" target="MAX">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA" target="POW2">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA" target="ORACLE">
  <data key="d0">HAS VARIANTS</data>
</edge>
<edge source="ORCA" target="PUBLICLY AVAILABLE FOR USE">
  <data key="d0">IS NOT</data>
</edge>
<edge source="ORCA" target="BUDDY ALLOCATION ALGORITHM">
  <data key="d0">USES</data>
</edge>
<edge source="ORCA" target="A DISTRIBUTED SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="ORCA" target="TRANSFORMER-BASED GENERATIVE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THIS SECTION" target="THE GENERATION AND SERVING PROCEDURES OF TYPICAL LLMS">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="THIS SECTION" target="THE ITERATION-LEVEL SCHEDULING USED IN LLM SERVING">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="THIS SECTION" target="A FUNCTIONAL AND QUANTITATIVE EVALUATION">
  <data key="d0">GIVES</data>
</edge>
<edge source="THIS SECTION" target="THE LEADING SOLUTIONS TO OPTIMIZE HANDLING ASYNCHRONOUS EVENTS IN STATE-OF-THE-ART EMBEDDED AND REAL-TIME MCUS">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="THE TASK OF LANGUAGE MODELING" target="THE PROBABILITY OF A LIST OF TOKENS">
  <data key="d0">IS TO MODEL</data>
</edge>
<edge source="LANGUAGE" target="A NATURAL SEQUENTIAL ORDERING">
  <data key="d0">HAS</data>
</edge>
<edge source="TRANSFORMERS 53" target="THE DE FACTO STANDARD ARCHITECTURE FOR MODELING THE PROBABILITY ABOVE AT A LARGE SCALE">
  <data key="d0">HAVE BECOME</data>
</edge>
<edge source="THE MOST IMPORTANT COMPONENT OF A TRANSFORMER-BASED LANGUAGE MODEL" target="ITS SELF-ATTENTION LAYERS">
  <data key="d0">IS</data>
</edge>
<edge source="A SELF-ATTENTION LAYER" target="LINEAR TRANSFORMATIONS ON EACH POSITION">
  <data key="d0">APPLIES</data>
</edge>
<edge source="A SELF-ATTENTION LAYER" target="THE QUERY VECTOR">
  <data key="d0">GETS</data>
</edge>
<edge source="A SELF-ATTENTION LAYER" target="THE KEY VECTOR">
  <data key="d0">GETS</data>
</edge>
<edge source="A SELF-ATTENTION LAYER" target="THE VALUE VECTOR">
  <data key="d0">GETS</data>
</edge>
<edge source="THE SELF-ATTENTION LAYER" target="THE ATTENTION SCORE">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="THE SELF-ATTENTION LAYER" target="THE OUTPUT">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="THE ATTENTION SCORE" target="MULTIPLYING THE QUERY VECTOR AT ONE POSITION WITH ALL THE KEY VECTORS BEFORE IT">
  <data key="d0">IS COMPUTED BY</data>
</edge>
<edge source="THE OUTPUT" target="THE WEIGHTED AVERAGE OVER THE VALUE VECTORS">
  <data key="d0">IS</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE TRANSFORMER MODEL">
  <data key="d0">ARE IN</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE EMBEDDING LAYER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="FEED-FORWARD LAYER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="LAYER NORMALIZATION 2">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="RESIDUAL CONNECTION 22">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="OUTPUT LOGIT COMPUTATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE QUERY TRANSFORMATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE KEY TRANSFORMATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ALL OTHER COMPONENTS" target="THE VALUE TRANSFORMATION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CONDITIONAL GENERATION SERVICE" target="COMPLETION API 34">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="CONDITIONAL GENERATION SERVICE" target="CHATBOT 19, 35">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="A REQUEST TO AN LLM SERVICE" target="A LIST OF INPUT PROMPT TOKENS">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="DECOMPOSITION" target="IN EQ">
  <data key="d0">OCCURS</data>
</edge>
<edge source="THE LLM" target="NEW TOKENS ONE BY ONE">
  <data key="d0">CAN ONLY SAMPLE AND GENERATE</data>
</edge>
<edge source="THE GENERATION PROCESS OF EACH NEW TOKEN" target="ALL THE PREVIOUS TOKENS IN THAT SEQUENCE">
  <data key="d0">DEPENDS ON</data>
</edge>
<edge source="ALL THE PREVIOUS TOKENS IN THAT SEQUENCE" target="KEY AND VALUE VECTORS">
  <data key="d0">HAVE</data>
</edge>
<edge source="KEY AND VALUE VECTORS OF EXISTING TOKENS" target="GENERATING FUTURE TOKENS">
  <data key="d0">ARE OFTEN CACHED FOR</data>
</edge>
<edge source="GENERATING FUTURE TOKENS" target="KV CACHE">
  <data key="d0">IS KNOWN AS</data>
</edge>
<edge source="A REQUESTS KV CACHE" target="A SERIES OF LOGICAL KV BLOCKS">
  <data key="d0">IS REPRESENTED AS</data>
</edge>
<edge source="GENERATION COMPUTATION IN THE LLM SERVICE" target="TWO PHASES">
  <data key="d0">CAN BE DECOMPOSED INTO</data>
</edge>
<edge source="THE PROMPT PHASE" target="THE WHOLE USER PROMPT">
  <data key="d0">TAKES</data>
</edge>
<edge source="REQUESTS AND THE LATEST TOKENS FOR GENERATION PHASE REQUESTS" target="THE LLM AS ONE SEQUENCE">
  <data key="d0">ARE FED INTO</data>
</edge>
<edge source="THIS PROCESS" target="THE KEY VECTORS 1">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THE COMPUTATION OF THE PROMPT PHASE" target="MATRIX-MATRIX MULTIPLICATION OPERATIONS">
  <data key="d0">CAN BE PARALLELIZED USING</data>
</edge>
<edge source="THIS PHASE" target="PARALLELISM INHERENT IN GPUS">
  <data key="d0">CAN USE</data>
</edge>
<edge source="THIS PHASE" target="WHEN THE SEQUENCE REACHES A MAXIMUM LENGTH">
  <data key="d0">COMPLETES</data>
</edge>
<edge source="THIS PHASE" target="WHEN AN END-OF-SEQUENCE (EOS) TOKEN IS EMITTED">
  <data key="d0">COMPLETES</data>
</edge>
<edge source="THIS PHASE" target="GPU COMPUTATION">
  <data key="d0">UNDERUTILIZES</data>
</edge>
<edge source="THIS PHASE" target="MEMORY-BOUND">
  <data key="d0">BECOMES</data>
</edge>
<edge source="THIS PHASE" target="MOST PORTION OF THE LATENCY OF A SINGLE REQUEST">
  <data key="d0">IS RESPONSIBLE FOR</data>
</edge>
<edge source="THE AUTOREGRESSIVE GENERATION PHASE" target="THE REMAINING NEW TOKENS SEQUENTIALLY">
  <data key="d0">GENERATES</data>
</edge>
<edge source="THE MODEL" target="ONE TOKEN AS INPUT">
  <data key="d0">TAKES</data>
</edge>
<edge source="THE MODEL" target="A RESPONSE">
  <data key="d0">GENERATE</data>
</edge>
<edge source="KEY AND VALUE VECTORS AT POSITIONS 1 TO 1" target="CACHED AT PREVIOUS ITERATIONS">
  <data key="d0">ARE</data>
</edge>
<edge source="NEW KEY AND VALUE VECTOR" target="COMPUTED AT THIS ITERATION">
  <data key="d0">ARE</data>
</edge>
<edge source="MAXIMUM LENGTH" target="USERS">
  <data key="d0">IS SPECIFIED BY</data>
</edge>
<edge source="MAXIMUM LENGTH" target="LLMS">
  <data key="d0">IS LIMITED BY</data>
</edge>
<edge source="USERS" target="MULTIPLE RANDOM SAMPLES FROM A SINGLE INPUT PROMPT">
  <data key="d0">REQUEST</data>
</edge>
<edge source="USERS" target="A FAVORITE OUTPUT FROM VARIOUS CANDIDATES">
  <data key="d0">CAN CHOOSE</data>
</edge>
<edge source="USERS" target="TOP-MOST APPROPRIATE TRANSLATIONS OUTPUT BY THE LLM">
  <data key="d0">EXPECT</data>
</edge>
<edge source="THE COMPUTATION AT DIFFERENT ITERATIONS" target="DUE TO THE DATA DEPENDENCY">
  <data key="d0">CANNOT BE PARALLELIZED</data>
</edge>
<edge source="THE COMPUTATION AT DIFFERENT ITERATIONS" target="MATRIX-VECTOR MULTIPLICATION">
  <data key="d0">OFTEN USES</data>
</edge>
<edge source="MATRIX-VECTOR MULTIPLICATION" target="LESS EFFICIENT">
  <data key="d0">IS</data>
</edge>
<edge source="COMPUTE UTILIZATION IN SERVING LLMS" target="BATCHING MULTIPLE REQUESTS">
  <data key="d0">CAN BE IMPROVED BY</data>
</edge>
<edge source="BATCHING THE REQUESTS TO AN LLM SERVICE" target="NON-TRIVIAL">
  <data key="d0">IS</data>
</edge>
<edge source="OVERHEAD OF MOVING WEIGHTS" target="REQUESTS IN A BATCH">
  <data key="d0">IS AMORTIZED ACROSS</data>
</edge>
<edge source="OVERHEAD OF MOVING WEIGHTS" target="COMPUTATIONAL OVERHEAD">
  <data key="d0">CAN BE OVERWHELMED BY</data>
</edge>
<edge source="A NAIVE BATCHING STRATEGY" target="EARLIER REQUESTS WAIT FOR LATER ONES">
  <data key="d0">WOULD MAKE</data>
</edge>
<edge source="A NAIVE BATCHING STRATEGY" target="THE INCOMING REQUESTS UNTIL EARLIER ONES FINISH">
  <data key="d0">WOULD DELAY</data>
</edge>
<edge source="A NAIVE BATCHING STRATEGY" target="SIGNIFICANT QUEUEING DELAYS">
  <data key="d0">LEADS TO</data>
</edge>
<edge source="A STRAIGHTFORWARD BATCHING TECHNIQUE" target="THE INPUTS AND OUTPUTS OF THE REQUESTS">
  <data key="d0">WOULD PAD</data>
</edge>
<edge source="A STRAIGHTFORWARD BATCHING TECHNIQUE" target="EQUALIZE THEIR LENGTHS">
  <data key="d0">WOULD PAD TO</data>
</edge>
<edge source="A STRAIGHTFORWARD BATCHING TECHNIQUE" target="GPU COMPUTATION AND MEMORY">
  <data key="d0">WASTES</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="CELLULAR BATCHING 16">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="ITERATION-LEVEL SCHEDULING 60">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="PROPOSED">
  <data key="d0">HAVE BEEN</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="QUEUEING DELAY">
  <data key="d0">REDUCE</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="INEFFICIENCIES FROM PADDING">
  <data key="d0">REDUCE</data>
</edge>
<edge source="FINE-GRAINED BATCHING MECHANISMS" target="THROUGHPUT OF LLM SERVING">
  <data key="d0">INCREASE</data>
</edge>
<edge source="THESE TECHNIQUES" target="THE ITERATION LEVEL">
  <data key="d0">OPERATE AT</data>
</edge>
<edge source="TRADITIONAL METHODS" target="THE REQUEST LEVEL">
  <data key="d0">WORK AT</data>
</edge>
<edge source="COMPLETED REQUESTS" target="THE BATCH">
  <data key="d0">ARE REMOVED FROM</data>
</edge>
<edge source="A NEW REQUEST" target="WAITING FOR A SINGLE ITERATION">
  <data key="d0">CAN BE PROCESSED AFTER</data>
</edge>
<edge source="A NEW REQUEST" target="WAITING FOR THE ENTIRE BATCH TO COMPLETE">
  <data key="d0">CANNOT BE PROCESSED AFTER</data>
</edge>
<edge source="SPECIAL GPU KERNELS" target="THE NEED TO PAD THE INPUTS AND OUTPUTS">
  <data key="d0">ELIMINATE</data>
</edge>
<edge source="THREE TYPES OF MEMORY WASTES" target="RESERVED, INTERNAL FRAGMENTATION, AND EXTERNAL FRAGMENTATION">
  <data key="d0">ARE</data>
</edge>
<edge source="THREE TYPES OF MEMORY WASTES" target="THAT PREVENT OTHER REQUESTS FROM FITTING INTO THE MEMORY">
  <data key="d0">EXIST</data>
</edge>
<edge source="THE SERVING SYSTEMS THROUGHPUT" target="MEMORY-BOUND">
  <data key="d0">IS</data>
</edge>
<edge source="THE PERFORMANCE OF THE SYSTEMS" target="COMPUTE-BOUND RATHER THAN MEMORY-BOUND">
  <data key="d0">BECOMES</data>
</edge>
<edge source="OVERCOMING THIS MEMORY-BOUND" target="ADDRESSING THE FOLLOWING CHALLENGES IN THE MEMORY MANAGEMENT">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="CHALLENGES" target="LARGE KV CACHE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="OPT" target="SEQUENCES UP TO 2048 TOKENS">
  <data key="d0">CAN GENERATE</data>
</edge>
<edge source="OPT" target="OPEN PRE-TRAINED TRANSFORMER LANGUAGE MODELS">
  <data key="d0">IS</data>
</edge>
<edge source="MEMORY REQUIRED TO STORE THE KV CACHE OF ONE REQUEST" target="1.6 GB">
  <data key="d0">CAN BE AS MUCH AS</data>
</edge>
<edge source="CONCURRENT GPUS" target="MEMORY CAPACITIES IN THE TENS OF GBS">
  <data key="d0">HAVE</data>
</edge>
<edge source="GPU'S COMPUTATION SPEED" target="MEMORY CAPACITY">
  <data key="d0">GROWS FASTER THAN</data>
</edge>
<edge source="FLOPS" target="NVIDIA A100 TO H100">
  <data key="d0">INCREASES FROM</data>
</edge>
<edge source="FLOPS" target="MORE THAN 2X">
  <data key="d0">INCREASES BY</data>
</edge>
<edge source="COMPLEX DECODING ALGORITHMS" target="ALGORITHMS">
  <data key="d0">IS</data>
</edge>
<edge source="KV CACHE OF THE PROMPT PART" target="12 OF THE TOTAL KV CACHE MEMORY IN OUR EXPERIMENT">
  <data key="d0">ACCOUNTS FOR</data>
</edge>
<edge source="KV CACHE OF THE PROMPT PART" target="MINIMIZE MEMORY USAGE">
  <data key="d0">CAN BE SHARED TO</data>
</edge>
<edge source="DIFFERENT SAMPLE RESULTS" target="CONTEXT AND POSITION">
  <data key="d0">DEPEND ON</data>
</edge>
<edge source="THE EXTENT OF KV CACHE SHARING" target="THE SPECIFIC DECODING ALGORITHM EMPLOYED">
  <data key="d0">DEPENDS ON</data>
</edge>
<edge source="DIFFERENT REQUEST BEAMS" target="LARGER PORTIONS OF THEIR KV CACHE">
  <data key="d0">CAN SHARE</data>
</edge>
<edge source="LARGER PORTIONS" target="55 MEMORY SAVING">
  <data key="d0">CAN BE UP TO</data>
</edge>
<edge source="SHARING PATTERN" target="THE DECODING PROCESS ADVANCES">
  <data key="d0">EVOLVES AS</data>
</edge>
<edge source="BEAM SEARCH 49" target="MORE SOPHISTICATED ALGORITHMS">
  <data key="d0">IS AN EXAMPLE OF</data>
</edge>
<edge source="SCHEDULING" target="UNKNOWN INPUT OUTPUT LENGTHS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="SCHEDULING" target="SERVING SINGLE OR MULTIPLE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE REQUESTS TO AN LLM SERVICE" target="VARIABILITY IN THEIR INPUT AND OUTPUT LENGTHS">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="INPUT PROMPTS FOR AN LLM" target="SIGNIFICANTLY IN LENGTH">
  <data key="d0">CAN VARY</data>
</edge>
<edge source="RESULTING OUTPUT LENGTHS" target="A PRIORI">
  <data key="d0">ARE NOT KNOWN</data>
</edge>
<edge source="RESULTING OUTPUT LENGTHS" target="BOTH THE INPUT PROMPT AND THE MODEL">
  <data key="d0">ARE CONTINGENT ON</data>
</edge>
<edge source="THE MEMORY MANAGEMENT SYSTEM" target="A WIDE RANGE OF PROMPT LENGTHS">
  <data key="d0">REQUIRES TO ACCOMMODATE</data>
</edge>
<edge source="THE SYSTEM" target="SCHEDULING DECISIONS">
  <data key="d0">NEEDS TO MAKE</data>
</edge>
<edge source="THE SYSTEM" target="THE SPACE FOR OUTPUTS">
  <data key="d0">OVER-RESERVES</data>
</edge>
<edge source="THE SYSTEM" target="AT MOST 2">
  <data key="d0">OVER-RESERVES BY</data>
</edge>
<edge source="THE SYSTEM" target="A CV32 MANAGER CORE">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="THE SYSTEM" target="A PROGRAMMABLE ACCELERATOR SUBSYSTEM">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="THE SYSTEM" target="A SET OF STANDARD PERIPHERALS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="SCHEDULING DECISIONS" target="DELETING OR SWAPPING OUT THE KV CACHE OF SOME REQUESTS FROM GPU MEMORY">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE ALLOCATION" target="THE ACTUAL INPUT OR EVENTUAL OUTPUT LENGTH OF THE REQUEST">
  <data key="d0">IS IRRESPECTIVE OF</data>
</edge>
<edge source="REQUEST A" target="2048">
  <data key="d0">HAS MAXIMUM POSSIBLE SEQUENCE LENGTH</data>
</edge>
<edge source="REQUEST B" target="512">
  <data key="d0">HAS MAXIMUM POSSIBLE SEQUENCE LENGTH</data>
</edge>
<edge source="THE EXTERNAL FRAGMENTATION" target="GENERATED TOKENS">
  <data key="d0">WILL NEVER BE USED FOR</data>
</edge>
<edge source="THE EXTERNAL FRAGMENTATION" target="BEFORE SERVING A REQUEST">
  <data key="d0">IS KNOWN</data>
</edge>
<edge source="RESERVING THIS SPACE" target="THE SPACE THAT COULD OTHERWISE BE USED TO PROCESS OTHER REQUESTS">
  <data key="d0">OCCUPIES</data>
</edge>
<edge source="RESERVING THIS SPACE" target="THE ENTIRE REQUESTS DURATION">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE RESERVED MEMORY" target="EVENTUALLY">
  <data key="d0">IS USED</data>
</edge>
<edge source="THE RESERVED SPACE" target="LARGE">
  <data key="d0">IS</data>
</edge>
<edge source="THE ACTUAL EFFECTIVE MEMORY IN PREVIOUS SYSTEMS" target="20.4">
  <data key="d0">CAN BE AS LOW AS</data>
</edge>
<edge source="THE ARCHITECTURE OF VLLM" target="FIG.">
  <data key="d0">IS SHOWN IN</data>
</edge>
<edge source="SYSTEM DESIGN OF VLLM" target="DISTRIBUTED SETTING">
  <data key="d0">WORKS IN</data>
</edge>
<edge source="COMPACTION 54" target="A POTENTIAL SOLUTION TO FRAGMENTATION">
  <data key="d0">HAS BEEN PROPOSED AS</data>
</edge>
<edge source="PERFORMING COMPACTION IN A PERFORMANCE-SENSITIVE LLM SERVING SYSTEM" target="IMPRATICAL DUE TO THE MASSIVE KV CACHE">
  <data key="d0">IS</data>
</edge>
<edge source="THE PRE-ALLOCATED CHUNK SPACE FOR EACH REQUEST" target="MEMORY SHARING SPECIFIC TO DECODING ALGORITHMS IN EXISTING MEMORY MANAGEMENT SYSTEMS">
  <data key="d0">PREVENTS</data>
</edge>
<edge source="A CENTRALIZED SCHEDULER" target="THE EXECUTION OF DISTRIBUTED GPU WORKERS">
  <data key="d0">COORDINATES</data>
</edge>
<edge source="EACH GPU WORKER" target="THE SAME PHYSICAL BLOCK IDS">
  <data key="d0">HAS</data>
</edge>
<edge source="A WORKER" target="A PORTION OF THE KV CACHE FOR ITS CORRESPONDING ATTENTION HEADS">
  <data key="d0">ONLY STORES</data>
</edge>
<edge source="GPU WORKERS" target="KV CACHE">
  <data key="d0">READ</data>
</edge>
<edge source="GPU WORKERS" target="BLOCK TABLE">
  <data key="d0">READ ACCORDING TO</data>
</edge>
<edge source="GPU WORKERS" target="SAMPLED TOKENS OF THIS ITERATION BACK TO THE SCHEDULER">
  <data key="d0">SEND</data>
</edge>
<edge source="GPU WORKERS" target="THE MODEL">
  <data key="d0">START TO EXECUTE</data>
</edge>
<edge source="GPU WORKERS" target="THE INPUT TOKEN IDS">
  <data key="d0">EXECUTE WITH</data>
</edge>
<edge source="GPU WORKERS" target="INTERMEDIATE RESULTS">
  <data key="d0">SYNCHRONIZE</data>
</edge>
<edge source="GPU WORKERS" target="ALL-REDUCE COMMUNICATION PRIMITIVE">
  <data key="d0">SYNCHRONIZE WITH</data>
</edge>
<edge source="GPU WORKERS" target="MEMORY MANAGEMENT">
  <data key="d0">DO NOT NEED TO SYNCHRONIZE ON</data>
</edge>
<edge source="GPU WORKERS" target="ALL THE MEMORY MANAGEMENT INFORMATION AT THE BEGINNING OF EACH DECODING ITERATION">
  <data key="d0">NEED TO RECEIVE</data>
</edge>
<edge source="GPU WORKERS" target="STEP INPUTS">
  <data key="d0">NEED TO RECEIVE</data>
</edge>
<edge source="BLOCK TABLE" target="CONTROL MESSAGE">
  <data key="d0">IS IN</data>
</edge>
<edge source="BLOCK TABLE" target="VLLM">
  <data key="d0">IS IN</data>
</edge>
<edge source="THE PAGEDATTENTION ALGORITHM" target="4.1">
  <data key="d0">IS DESCRIBED IN</data>
</edge>
<edge source="THE PAGEDATTENTION ALGORITHM" target="PHYSICAL BLOCKS 7 AND 1">
  <data key="d0">OPERATES ON</data>
</edge>
<edge source="THE FIXED NUMBER OF TOKENS" target="KV 1 IN TRANSFORMER">
  <data key="d0">IS DENOTED AS</data>
</edge>
<edge source="EACH TOKEN" target="A SET OF KEY AND VALUE VECTORS ACROSS LAYERS AND ATTENTION HEADS WITHIN A LAYER">
  <data key="d0">HAS</data>
</edge>
<edge source="ALL THE KEY AND VALUE VECTORS" target="TOGETHER WITHIN A SINGLE KV BLOCK">
  <data key="d0">CAN BE MANAGED</data>
</edge>
<edge source="THE KEY AND VALUE VECTORS AT DIFFERENT HEADS AND LAYERS" target="A SEPARATE BLOCK">
  <data key="d0">CAN EACH HAVE</data>
</edge>
<edge source="THE KEY AND VALUE VECTORS AT DIFFERENT HEADS AND LAYERS" target="IN SEPARATE BLOCK TABLES">
  <data key="d0">CAN BE MANAGED</data>
</edge>
<edge source="THE TWO DESIGNS" target="NO PERFORMANCE DIFFERENCE">
  <data key="d0">HAVE</data>
</edge>
<edge source="THE SECOND ONE" target="EASY IMPLEMENTATION">
  <data key="d0">IS CHOSEN FOR</data>
</edge>
<edge source="OUR FATHERS" target="FOUR SCORE AND SEVEN KEY AND VALUE VECTORS">
  <data key="d0">BROUGHT FORTH</data>
</edge>
<edge source="KEY BLOCK" target="((1)1">
  <data key="d0">IS DENOTED BY</data>
</edge>
<edge source="THE ATTENTION COMPUTATION" target="EQ.">
  <data key="d0">IS IN</data>
</edge>
<edge source="4" target="THE FOLLOWING BLOCK-WISE COMPUTATION">
  <data key="d0">CAN BE TRANSFORMED INTO</data>
</edge>
<edge source="ROW VECTOR" target="ATTENTION SCORE ON -TH KV BLOCK">
  <data key="d0">IS</data>
</edge>
<edge source="THE KEY AND VALUE VECTORS" target="THREE BLOCKS">
  <data key="d0">ARE SPREAD ACROSS</data>
</edge>
<edge source="THE THREE BLOCKS" target="THE PHYSICAL MEMORY">
  <data key="d0">ARE NOT CONTIGUOUS ON</data>
</edge>
<edge source="THE KERNEL" target="THE QUERY VECTOR OF THE QUERY TOKEN (FORTH) AND THE KEY VECTORS IN A BLOCK">
  <data key="d0">MULTIPLIES</data>
</edge>
<edge source="THE KERNEL" target="THE ATTENTION SCORE">
  <data key="d0">COMPUTES</data>
</edge>
<edge source="THE KERNEL" target="THE VALUE VECTORS IN A BLOCK">
  <data key="d0">MULTIPLIES WITH</data>
</edge>
<edge source="THE KERNEL" target="THE FINAL ATTENTION OUTPUT">
  <data key="d0">DERIVES</data>
</edge>
<edge source="OS" target="MEMORY INTO FIXED-SIZED PAGES">
  <data key="d0">PARTITIONS</data>
</edge>
<edge source="OS" target="USER PROGRAMS LOGICAL PAGES TO PHYSICAL PAGES">
  <data key="d0">MAPS</data>
</edge>
<edge source="OS" target="PHYSICAL PAGES DYNAMICALLY">
  <data key="d0">CAN ALLOCATE</data>
</edge>
<edge source="OS" target="SHARED LIBRARY ACROSS PROCESSES">
  <data key="d0">HANDLES</data>
</edge>
<edge source="CONTIGUOUS LOGICAL PAGES" target="NON-CONTIGUOUS PHYSICAL MEMORY PAGES">
  <data key="d0">CAN CORRESPOND TO</data>
</edge>
<edge source="USER PROGRAMS" target="MEMORY AS THOUGH IT WERE CONTIGUOUS">
  <data key="d0">CAN ACCESS</data>
</edge>
<edge source="PHYSICAL MEMORY SPACE" target="FULLY RESERVED IN ADVANCE">
  <data key="d0">NEEDS NOT TO BE</data>
</edge>
<edge source="THE LAST KV BLOCKS UNFILLED POSITIONS" target="FUTURE GENERATIONS">
  <data key="d0">ARE RESERVED FOR</data>
</edge>
<edge source="THE KV BLOCK MANAGER" target="BLOCK TABLES">
  <data key="d0">MAINTAINS</data>
</edge>
<edge source="BLOCK TABLES" target="THE MAPPING BETWEEN LOGICAL AND PHYSICAL KV BLOCKS OF EACH REQUEST">
  <data key="d0">ARE</data>
</edge>
<edge source="EACH BLOCK TABLE ENTRY" target="THE CORRESPONDING PHYSICAL BLOCKS OF A LOGICAL BLOCK">
  <data key="d0">RECORDS</data>
</edge>
<edge source="EACH BLOCK TABLE ENTRY" target="THE NUMBER OF FILLED POSITIONS">
  <data key="d0">RECORDS</data>
</edge>
<edge source="SEPARATING LOGICAL AND PHYSICAL KV BLOCKS" target="VLLM TO DYNAMICALLY GROW THE KV CACHE MEMORY WITHOUT RESERVING IT FOR ALL POSITIONS IN ADVANCE">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="SEPARATING LOGICAL AND PHYSICAL KV BLOCKS" target="MOST MEMORY WASTE IN EXISTING SYSTEMS">
  <data key="d0">ELIMINATES</data>
</edge>
<edge source="ALL THE BLOCKS" target="LEFT TO RIGHT">
  <data key="d0">ARE FILLED FROM</data>
</edge>
<edge source="A NEW PHYSICAL BLOCK" target="ALL PREVIOUS BLOCKS ARE FULL">
  <data key="d0">IS ONLY ALLOCATED WHEN</data>
</edge>
<edge source="THE FINAL LOGICAL BLOCK" target="A COPY-ON-WRITE MECHANISM">
  <data key="d0">IS MANAGED BY</data>
</edge>
<edge source="FREQUENT MEMORY COPY OVERHEAD" target="VLLMS PHYSICAL BLOCK SHARING">
  <data key="d0">IS REDUCED BY</data>
</edge>
<edge source="A COMMON MAPPING LAYER" target="LOGICAL BLOCKS TO PHYSICAL BLOCKS">
  <data key="d0">TRANSLATES</data>
</edge>
<edge source="INTRODUCING THE VLLMS TECHNIQUES" target="THE PERFORMANCE">
  <data key="d0">MAY DEGRADE</data>
</edge>
<edge source="THE PERFORMANCE" target="PRIMARILY COMPUTE-BOUND">
  <data key="d0">IS</data>
</edge>
<edge source="THE DEGRADATION" target="THE EXTRA OVERHEAD OF MEMORY INDIRECTION AND NON-CONTIGUOUS BLOCK MEMORY">
  <data key="d0">IS DUE TO</data>
</edge>
<edge source="EXAMPLE" target="IN FIG">
  <data key="d0">IS SHOWN</data>
</edge>
<edge source="THE FIRST AUTOREGRESSIVE DECODING STEP" target="VLLM GENERATING THE NEW TOKEN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="4.3" target="HOW PAGEDATTENTION AND VLLM HANDLE BASIC DECODING ALGORITHMS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="BASIC DECODING ALGORITHMS" target="GREEDY DECODING AND SAMPLING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="BASIC DECODING ALGORITHMS" target="ONE USER PROMPT AS INPUT">
  <data key="d0">TAKE</data>
</edge>
<edge source="BASIC DECODING ALGORITHMS" target="A SINGLE OUTPUT SEQUENCE">
  <data key="d0">GENERATE</data>
</edge>
<edge source="THE NECESSARY KV BLOCKS" target="THE KV CACHE GENERATED DURING PROMPT COMPUTATION">
  <data key="d0">ACCOMMODATE</data>
</edge>
<edge source="THE PROMPT" target="7 TOKENS">
  <data key="d0">HAS</data>
</edge>
<edge source="TOKENS AND THEIR KV CACHE" target="MORE">
  <data key="d0">ARE GENERATED</data>
</edge>
<edge source="THE REMAINING SLOT" target="THE SUBSEQUENT AUTOREGRESSIVE GENERATION PHASE">
  <data key="d0">IS RESERVED FOR</data>
</edge>
<edge source="ONE SLOT" target="AVAILABLE IN THE LAST LOGICAL BLOCK">
  <data key="d0">REMAINS</data>
</edge>
<edge source="THE NEWLY GENERATED KV CACHE" target="THERE">
  <data key="d0">IS STORED</data>
</edge>
<edge source="STORING MULTIPLE TOKENS WITHIN A KV BLOCK (BLOCK SIZE 1)" target="THE PAGEDATTENTION KERNEL TO PROCESS THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL">
  <data key="d0">ENABLES</data>
</edge>
<edge source="PROCESSING THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL" target="THE HARDWARE UTILIZATION">
  <data key="d0">INCREASES</data>
</edge>
<edge source="PROCESSING THE KV CACHE ACROSS MORE POSITIONS IN PARALLEL" target="LATENCY">
  <data key="d0">REDUCES</data>
</edge>
<edge source="A LARGER BLOCK SIZE" target="MEMORY FRAGMENTATION">
  <data key="d0">INCREASES</data>
</edge>
<edge source="THE LOGICAL BLOCKS OF THE TWO SEQUENCES" target="DIFFERENT PHYSICAL BLOCKS">
  <data key="d0">ARE MAPPED TO</data>
</edge>
<edge source="DIFFERENT PHYSICAL BLOCKS" target="THE SPACE RESERVED BY THE BLOCK ENGINE IN GPU WORKERS">
  <data key="d0">ARE WITHIN</data>
</edge>
<edge source="THE NEIGHBORING LOGICAL BLOCKS OF BOTH SEQUENCES" target="CONTIGUOUS IN PHYSICAL GPU MEMORY">
  <data key="d0">DO NOT NEED TO BE</data>
</edge>
<edge source="THE SPACE OF PHYSICAL BLOCKS" target="BOTH SEQUENCES">
  <data key="d0">CAN BE EFFECTIVELY UTILIZED BY</data>
</edge>
<edge source="AN LLM" target="MULTIPLE SAMPLED OUTPUTS FOR A SINGLE INPUT PROMPT">
  <data key="d0">GENERATES</data>
</edge>
<edge source="GENERATES" target="A SINGLE SEQUENCE">
  <data key="d0">GENERATES</data>
</edge>
<edge source="A REQUEST" target="MULTIPLE SEQUENCES">
  <data key="d0">GENERATES</data>
</edge>
<edge source="A REQUEST" target="ONCE">
  <data key="d0">COMPLETES</data>
</edge>
<edge source="ONE REQUEST" target="MULTIPLE SAMPLES">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MULTIPLE SAMPLES" target="THE SAME INPUT PROMPT">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE KV CACHE OF THE PROMPT" target="SHARED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="ALL PARALLEL SEQUENCES IN A REQUEST" target="THE KV CACHE FOR THE PROMPT">
  <data key="d0">CAN SHARE</data>
</edge>
<edge source="8" target="AN EXAMPLE OF PARALLEL DECODING FOR TWO OUTPUTS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="OUTPUTS" target="THE SAME PROMPT">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE LOGICAL BLOCKS FOR THE PROMPTS OF BOTH SEQUENCES" target="THE SAME PHYSICAL BLOCKS">
  <data key="d0">ARE MAPPED TO</data>
</edge>
<edge source="THE LOGICAL BLOCK 0 AND 1 OF BOTH SEQUENCES" target="PHYSICAL BLOCKS 7 AND 1">
  <data key="d0">ARE MAPPED TO</data>
</edge>
<edge source="A SINGLE PHYSICAL BLOCK" target="MULTIPLE LOGICAL BLOCKS">
  <data key="d0">CAN BE MAPPED TO</data>
</edge>
<edge source="REFERENCE COUNTS FOR PHYSICAL BLOCK 7" target="2">
  <data key="d0">ARE</data>
</edge>
<edge source="2" target="AN OVERVIEW OF THE DESIGN">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="THE TWO OUTPUTS" target="DIFFERENT OUTPUT TOKENS">
  <data key="d0">SAMPLE</data>
</edge>
<edge source="THE TWO OUTPUTS" target="SEPARATE STORAGE FOR KV CACHE">
  <data key="d0">NEED</data>
</edge>
<edge source="AT THE GENERATION PHASE" target="THE TWO OUTPUTS SAMPLE DIFFERENT OUTPUT TOKENS">
  <data key="d0">IS CONTEXT FOR</data>
</edge>
<edge source="AT THE GENERATION PHASE" target="THE TWO OUTPUTS NEED SEPARATE STORAGE FOR KV CACHE">
  <data key="d0">IS CONTEXT FOR</data>
</edge>
<edge source="SAMPLE A2" target="PHYSICAL BLOCK 1">
  <data key="d0">WRITES TO</data>
</edge>
<edge source="REFERENCE COUNT" target="1">
  <data key="d0">IS REDUCED TO</data>
</edge>
<edge source="A2" target="NEWLY GENERATED KV CACHE TO PHYSICAL BLOCK 1">
  <data key="d0">WRITES</data>
</edge>
<edge source="SHARING PHYSICAL BLOCKS ACROSS MULTIPLE SAMPLES" target="MEMORY USAGE">
  <data key="d0">CAN REDUCE</data>
</edge>
<edge source="LLM TASKS LIKE MACHINE TRANSLATION 59" target="MACHINE TRANSLATION 59">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE ALGORITHM" target="THE BEAM WIDTH PARAMETER">
  <data key="d0">RELIES ON</data>
</edge>
<edge source="THE BEAM WIDTH PARAMETER" target="THE NUMBER OF TOP CANDIDATES RETAINED AT EVERY STEP">
  <data key="d0">DETERMINES</data>
</edge>
<edge source="SHARING PATTERNS" target="AS THE DECODING PROCESS ADVANCES">
  <data key="d0">DYNAMICALLY CHANGE</data>
</edge>
<edge source="SHARING PATTERNS" target="THE PROCESS TREE IN THE OS CREATED BY COMPOUND FORKS">
  <data key="d0">ARE SIMILAR TO</data>
</edge>
<edge source="THE KV BLOCKS" target="A BEAM SEARCH EXAMPLE">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="A BEAM SEARCH EXAMPLE" target="4">
  <data key="d0">HAS</data>
</edge>
<edge source="EACH CANDIDATE SEQUENCE" target="4 FULL LOGICAL BLOCKS">
  <data key="d0">HAS USED</data>
</edge>
<edge source="THE ITERATION ILLUSTRATED AS THE DOTTED LINE" target="EACH CANDIDATE SEQUENCE USING 4 FULL LOGICAL BLOCKS">
  <data key="d0">IS PRIOR TO</data>
</edge>
<edge source="ALL BEAM CANDIDATES" target="THE FIRST BLOCK 0 (I.E., PROMPT)">
  <data key="d0">SHARE</data>
</edge>
<edge source="CANDIDATE 3" target="OTHERS FROM THE SECOND BLOCK">
  <data key="d0">DIGRESSES FROM</data>
</edge>
<edge source="CANDIDATE 3" target="A LARGE PORTION OF CANDIDATE 2S KV CACHE">
  <data key="d0">WOULD NEED TO COPY</data>
</edge>
<edge source="CANDIDATE 3" target="A LARGE PORTION OF CANDIDATE 2S KV CACHE TO CONTINUE GENERATION">
  <data key="d0">WOULD NEED TO COPY</data>
</edge>
<edge source="CANDIDATES 0-2" target="THE FIRST 3 BLOCKS">
  <data key="d0">SHARE</data>
</edge>
<edge source="CANDIDATES 0-2" target="AT THE FOURTH BLOCK">
  <data key="d0">DIVERGE</data>
</edge>
<edge source="ALL CANDIDATES" target="BLOCKS 0, 1, 3">
  <data key="d0">SHARE</data>
</edge>
<edge source="CANDIDATES 0 AND 1" target="BLOCK 6">
  <data key="d0">SHARE</data>
</edge>
<edge source="ORIGINAL CANDIDATES 0 AND 3" target="TOP CANDIDATES">
  <data key="d0">ARE NO LONGER AMONG</data>
</edge>
<edge source="THEIR LOGICAL BLOCKS" target="FREED">
  <data key="d0">ARE</data>
</edge>
<edge source="REFERENCE COUNTS OF CORRESPONDING PHYSICAL BLOCKS" target="REDUCED">
  <data key="d0">ARE</data>
</edge>
<edge source="NEW PHYSICAL BLOCKS (BLOCKS 9-12)" target="THE NEW KV CACHE">
  <data key="d0">ARE ALLOCATED TO STORE</data>
</edge>
<edge source="THE NEW KV CACHE" target="THE NEW CANDIDATES">
  <data key="d0">IS FROM</data>
</edge>
<edge source="MOST BLOCKS OF DIFFERENT BEAM CANDIDATES" target="SHARED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="MOST BLOCKS" target="DIFFERENT BEAM CANDIDATES">
  <data key="d0">ARE OF</data>
</edge>
<edge source="THE SAME STRATEGY" target="BEAM SEARCH">
  <data key="d0">IS APPLIED IN</data>
</edge>
<edge source="THE SAME STRATEGY" target="PREFIX SHARING">
  <data key="d0">IS APPLIED IN</data>
</edge>
<edge source="PREFIX SHARING" target="VLLM">
  <data key="d0">IS APPLIED BY</data>
</edge>
<edge source="THE COPY-ON-WRITE MECHANISM" target="THE NEWLY GENERATED TOKENS ARE WITHIN AN OLD SHARED BLOCK">
  <data key="d0">IS APPLIED ONLY WHEN</data>
</edge>
<edge source="THE COPY-ON-WRITE MECHANISM" target="PARALLEL DECODING">
  <data key="d0">IS APPLIED IN</data>
</edge>
<edge source="LLM USER" target="DESCRIPTION OF THE TASK">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="DESCRIPTION OF THE TASK" target="INSTRUCTIONS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="DESCRIPTION OF THE TASK" target="EXAMPLE INPUTS AND OUTPUTS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="DESCRIPTION OF THE TASK" target="SYSTEM PROMPT 36">
  <data key="d0">IS ALSO KNOWN AS</data>
</edge>
<edge source="THE DESCRIPTION" target="THE ACTUAL TASK INPUT">
  <data key="d0">IS CONCATENATED WITH</data>
</edge>
<edge source="THE DESCRIPTION AND THE ACTUAL TASK INPUT" target="THE PROMPT OF THE REQUEST">
  <data key="d0">FORM</data>
</edge>
<edge source="SHARED PROMPT EXAMPLE" target="MACHINE TRANSLATION">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE EXAMPLES" target="5">
  <data key="d0">ARE ADOPTED FROM</data>
</edge>
<edge source="10" target="AN EXAMPLE">
  <data key="d0">SHOWS</data>
</edge>
<edge source="10" target="DANIEL CRANKSHAW">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="GUR-EYAL SELA">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="XIANGXI MO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="COREY ZUMAR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="ION STOICA">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="JOSEPH GONZALEZ">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="10" target="ALEXEY TUMANOV">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="SHARED PREFIX" target="PROMPT ENGINEERING">
  <data key="d0">CAN BE TUNED VIA</data>
</edge>
<edge source="TUNING SHARED PREFIX" target="ACCURACY OF DOWNSTREAM TASKS">
  <data key="d0">IMPROVES</data>
</edge>
<edge source="MANY USER PROMPTS" target="A PREFIX">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE LLM SERVICE PROVIDER" target="THE KV CACHE OF THE PREFIX">
  <data key="d0">CAN STORE</data>
</edge>
<edge source="STORING THE KV CACHE OF THE PREFIX" target="THE REDUNDANT COMPUTATION SPENT ON THE PREFIX">
  <data key="d0">REDUCES</data>
</edge>
<edge source="A USER INPUT PROMPT WITH THE SHARED PREFIX" target="ITS LOGICAL BLOCKS TO THE CACHED PHYSICAL BLOCKS">
  <data key="d0">CAN MAP</data>
</edge>
<edge source="THE LAST BLOCK" target="COPY-ON-WRITE">
  <data key="d0">IS MARKED</data>
</edge>
<edge source="THE PROMPT PHASE COMPUTATION" target="THE USERS TASK INPUT">
  <data key="d0">ONLY NEEDS TO EXECUTE ON</data>
</edge>
<edge source="THE DECODING METHODS DISCUSSED EARLIER" target="DIVERSE MEMORY SHARING AND ACCESSING PATTERNS">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="THE LLM AND ITS EXECUTION KERNEL" target="A LIST OF PHYSICAL BLOCK IDS FOR EACH SEQUENCE">
  <data key="d0">SEE</data>
</edge>
<edge source="THE LLM AND ITS EXECUTION KERNEL" target="SHARING PATTERNS ACROSS SEQUENCES">
  <data key="d0">DO NOT NEED TO HANDLE</data>
</edge>
<edge source="THIS APPROACH" target="THE BATCHING OPPORTUNITIES FOR REQUESTS WITH DIFFERENT SAMPLING REQUIREMENTS">
  <data key="d0">BROADENS</data>
</edge>
<edge source="THIS APPROACH" target="THE SYSTEMS OVERALL THROUGHPUT">
  <data key="d0">INCREASES</data>
</edge>
<edge source="THIS APPROACH" target="THE LATENCY ADVANTAGE OF HW VECTORING">
  <data key="d0">LOSES</data>
</edge>
<edge source="THIS APPROACH" target="AN SW EMULATION THEREOF">
  <data key="d0">RUNS</data>
</edge>
<edge source="THIS APPROACH" target="LOWERING INTERRUPT LATENCY AND TASK CONTEXT SWITCHES DRAMATICALLY">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THIS APPROACH" target="FLEXIBILITY">
  <data key="d0">LACKS</data>
</edge>
<edge source="THIS APPROACH" target="REPLICATING HARDWARE RESOURCES PER TASK">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="REQUEST TRAFFIC" target="THE SYSTEMS CAPACITY">
  <data key="d0">SURPASSES</data>
</edge>
<edge source="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY" target="ALL REQUESTS">
  <data key="d0">APPLIES TO</data>
</edge>
<edge source="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY" target="FAIRNESS">
  <data key="d0">ENSURES</data>
</edge>
<edge source="FIRST-COME-FIRST-SERVE (FCFS) SCHEDULING POLICY" target="STARVATION">
  <data key="d0">PREVENTS</data>
</edge>
<edge source="BLOCK SIZE" target="TOO SMALL">
  <data key="d0">IS</data>
</edge>
<edge source="BLOCK SIZE" target="1 2 4 8 16 32 64 128 256">
  <data key="d0">INCLUDES VALUES</data>
</edge>
<edge source="TWO CLASSIC QUESTIONS" target="WHICH BLOCKS SHOULD IT EVICT">
  <data key="d0">ARE</data>
</edge>
<edge source="EVICTED BLOCKS" target="IF NEEDED AGAIN">
  <data key="d0">CAN BE RECOVERED</data>
</edge>
<edge source="TWO TECHNIQUES" target="SWAPPING">
  <data key="d0">ARE</data>
</edge>
<edge source="SWAPPING" target="BLOCK SIZE IS LARGE">
  <data key="d0">IS MORE EFFICIENT WHEN</data>
</edge>
<edge source="SWAPPING" target="EXCESSIVE OVERHEAD WITH SMALL BLOCK SIZES">
  <data key="d0">INCURS</data>
</edge>
<edge source="EVICTION POLICIES" target="HEURISTICS">
  <data key="d0">USE</data>
</edge>
<edge source="EVICTION POLICIES" target="THAT BLOCK">
  <data key="d0">EVICT</data>
</edge>
<edge source="HEURISTICS" target="WHICH BLOCK WILL BE ACCESSED FURTHEST IN THE FUTURE">
  <data key="d0">PREDICT</data>
</edge>
<edge source="ALL BLOCKS OF A SEQUENCE" target="TOGETHER">
  <data key="d0">ARE ACCESSED</data>
</edge>
<edge source="ALL-OR-NOTHING EVICTION POLICY" target="EITHER EVICT ALL OR NONE OF THE BLOCKS OF A SEQUENCE">
  <data key="d0">MEANS</data>
</edge>
<edge source="MULTIPLE SEQUENCES WITHIN ONE REQUEST" target="A SEQUENCE GROUP">
  <data key="d0">ARE GANG-SCHEDULED AS</data>
</edge>
<edge source="THE SEQUENCES WITHIN ONE SEQUENCE GROUP" target="TRUE">
  <data key="d0">ARE RESCHEDULED TOGETHER</data>
</edge>
<edge source="THE SEQUENCES WITHIN ONE SEQUENCE GROUP" target="DUE TO POTENTIAL MEMORY SHARING ACROSS THOSE SEQUENCES">
  <data key="d0">ARE PREEMPTED OR RESCHEDULED TOGETHER</data>
</edge>
<edge source="CLASSIC TECHNIQUE" target="MOST VIRTUAL MEMORY IMPLEMENTATIONS">
  <data key="d0">IS USED BY</data>
</edge>
<edge source="MOST VIRTUAL MEMORY IMPLEMENTATIONS" target="EVICTED PAGES TO A SWAP SPACE ON THE DISK">
  <data key="d0">COPY</data>
</edge>
<edge source="CPU BLOCK ALLOCATOR" target="PHYSICAL BLOCKS SWAPPED TO CPU RAM">
  <data key="d0">MANAGES</data>
</edge>
<edge source="ITS BLOCKS" target="MEMORY">
  <data key="d0">ARE FREED FROM</data>
</edge>
<edge source="THE BLOCKS OF A PREEMPTED SEQUENCE" target="CONTINUE THE PROCESSING OF THAT SEQUENCE">
  <data key="d0">ARE BROUGHT BACK IN TO</data>
</edge>
<edge source="NUMBER OF BLOCKS SWAPPED TO THE CPU RAM" target="NUMBER OF TOTAL PHYSICAL BLOCKS IN THE GPU RAM">
  <data key="d0">NEVER EXCEEDS</data>
</edge>
<edge source="SWAP SPACE ON THE CPU RAM" target="GPU MEMORY ALLOCATED FOR THE KV CACHE">
  <data key="d0">IS BOUNDED BY</data>
</edge>
<edge source="RECOMPUTATION LATENCY" target="SIGNIFICANTLY LOWER THAN THE ORIGINAL LATENCY">
  <data key="d0">CAN BE</data>
</edge>
<edge source="TOKENS GENERATED AT DECODING" target="THE ORIGINAL USER PROMPT AS A NEW PROMPT">
  <data key="d0">CAN BE CONCATENATED WITH</data>
</edge>
<edge source="THEIR KV CACHE AT ALL POSITIONS" target="ONE PROMPT PHASE ITERATION">
  <data key="d0">CAN BE GENERATED IN</data>
</edge>
<edge source="PERFORMANCES OF SWAPPING AND RECOMPUTATION" target="BANDWIDTH BETWEEN CPU RAM AND GPU MEMORY">
  <data key="d0">DEPEND ON</data>
</edge>
<edge source="PERFORMANCES OF SWAPPING AND RECOMPUTATION" target="COMPUTATION POWER OF THE GPU">
  <data key="d0">DEPEND ON</data>
</edge>
<edge source="RECOMPUTATION" target="THE KV BLOCKS">
  <data key="d0">DOES NOT UTILIZE</data>
</edge>
<edge source="RECOMPUTATION" target="BLOCK SIZE IS SMALL">
  <data key="d0">IS MORE EFFICIENT WHEN</data>
</edge>
<edge source="RECOMPUTATION AND SWAPPING" target="RECOVERY MECHANISMS">
  <data key="d0">ARE</data>
</edge>
<edge source="MANY LLMS" target="PARAMETER SIZES EXCEEDING THE CAPACITY OF A SINGLE GPU">
  <data key="d0">HAVE</data>
</edge>
<edge source="MEGATRON-LM STYLE TENSOR MODEL PARALLELISM STRATEGY" target="TRANSFORMERS 47">
  <data key="d0">IS USED ON</data>
</edge>
<edge source="THIS STRATEGY" target="AN SPMD (SINGLE PROGRAM MULTIPLE DATA) EXECUTION SCHEDULE">
  <data key="d0">ADHERES TO</data>
</edge>
<edge source="THE LINEAR LAYERS" target="618 TABLE 1">
  <data key="d0">ARE PARTITIONED</data>
</edge>
<edge source="MODEL SIZES" target="SERVER CONFIGURATIONS">
  <data key="d0">AND</data>
</edge>
<edge source="THE DETAILED MODEL SIZES AND SERVER CONFIGURATIONS" target="TABLE 1">
  <data key="d0">ARE SHOWN IN</data>
</edge>
<edge source="KV CACHE SLOTS" target="15.7K 9.7K 60.1K">
  <data key="d0">ARE</data>
</edge>
<edge source="GPUS" target="INTERMEDIATE RESULTS">
  <data key="d0">SYNCHRONIZE</data>
</edge>
<edge source="INTERMEDIATE RESULTS" target="AN ALL-REDUCE OPERATION">
  <data key="d0">ARE SYNCHRONIZED VIA</data>
</edge>
<edge source="BLOCK-WISE MATRIX MULTIPLICATION" target="KV CACHE SLOTS">
  <data key="d0">IS PERFORMED BY</data>
</edge>
<edge source="ATTENTION OPERATOR" target="ATTENTION HEAD DIMENSION">
  <data key="d0">IS SPLIT ON</data>
</edge>
<edge source="EACH SPMD PROCESS" target="A SUBSET OF ATTENTION HEADS IN MULTI-HEAD ATTENTION">
  <data key="d0">TAKES CARE OF</data>
</edge>
<edge source="MODEL SHARD" target="THE SAME SET OF INPUT TOKENS">
  <data key="d0">PROCESSES</data>
</edge>
<edge source="MODEL SHARD" target="THE KV CACHE FOR THE SAME POSITIONS">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="MODEL PARALLEL EXECUTION" target="EACH MODEL SHARD PROCESSING THE SAME SET OF INPUT TOKENS">
  <data key="d0">DOES NOT PREVENT</data>
</edge>
<edge source="KV CACHE MANAGER" target="THE CENTRALIZED SCHEDULER">
  <data key="d0">IS WITHIN</data>
</edge>
<edge source="DIFFERENT GPU WORKERS" target="THE MANAGER">
  <data key="d0">SHARE</data>
</edge>
<edge source="DIFFERENT GPU WORKERS" target="THE MAPPING FROM LOGICAL BLOCKS TO PHYSICAL BLOCKS">
  <data key="d0">SHARE</data>
</edge>
<edge source="THIS COMMON MAPPING" target="GPU WORKERS TO EXECUTE THE MODEL WITH THE PHYSICAL BLOCKS PROVIDED BY THE SCHEDULER FOR EACH INPUT REQUEST">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THE SCHEDULER" target="THE MESSAGE WITH INPUT TOKEN IDS FOR EACH REQUEST IN THE BATCH">
  <data key="d0">PREPARES</data>
</edge>
<edge source="THE SCHEDULER" target="THE BLOCK TABLE FOR EACH REQUEST">
  <data key="d0">PREPARES</data>
</edge>
<edge source="THE SCHEDULER" target="THIS CONTROL MESSAGE TO THE GPU WORKERS">
  <data key="d0">BROADCASTS</data>
</edge>
<edge source="THE SCHEDULER" target="EARLIEST ARRIVED REQUESTS">
  <data key="d0">TAKES UP TO NUMBER OF</data>
</edge>
<edge source="THE SCHEDULER" target="THE BATCH TO FASTERTRANS-FORMER FOR PROCESSING">
  <data key="d0">SENDS</data>
</edge>
<edge source="SYNCHRONIZATION" target="COORDINATION OF THE SCHEDULER">
  <data key="d0">OCCURS WITHOUT</data>
</edge>
<edge source="THE FRONTEND" target="THE OPENAI API 34 INTERFACE">
  <data key="d0">EXTENDS</data>
</edge>
<edge source="THE FRONTEND" target="USERS TO CUSTOMIZE SAMPLING PARAMETERS FOR EACH REQUEST">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="SAMPLING PARAMETERS" target="THE MAXIMUM SEQUENCE LENGTH">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="SAMPLING PARAMETERS" target="THE BEAM WIDTH">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE VLLM ENGINE" target="8.5K LINES OF PYTHON">
  <data key="d0">IS WRITTEN IN</data>
</edge>
<edge source="THE VLLM ENGINE" target="2K LINES OF CCUDA CODE">
  <data key="d0">IS WRITTEN IN</data>
</edge>
<edge source="CONTROL-RELATED COMPONENTS" target="THE SCHEDULER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CONTROL-RELATED COMPONENTS" target="THE BLOCK MANAGER">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CUSTOM CUDA KERNELS" target="KEY OPERATIONS">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="KEY OPERATIONS" target="PAGEDATTENTION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="INPUT AND OUTPUT LENGTH DISTRIBUTIONS" target="THE (A) SHAREGPT DATASET">
  <data key="d0">ARE OF</data>
</edge>
<edge source="INPUT AND OUTPUT LENGTH DISTRIBUTIONS" target="THE (B) ALPACA DATASET">
  <data key="d0">ARE OF</data>
</edge>
<edge source="THE SHAREGPT DATASET" target="THE ALPACA DATASET">
  <data key="d0">HAS OUTPUTS 5.8 TIMES LONGER THAN</data>
</edge>
<edge source="THE SHAREGPT DATASET" target="A COLLECTION OF USER-SHARED CONVERSATIONS WITH CHATGPT 35">
  <data key="d0">IS</data>
</edge>
<edge source="THE ALPACA DATASET" target="A SIMILAR TREND TO THE SHAREGPT DATASET">
  <data key="d0">FOLLOWS</data>
</edge>
<edge source="PYTORCH" target="39">
  <data key="d0">VERSION</data>
</edge>
<edge source="PYTORCH" target="AN IMPERATIVE STYLE HIGH-PERFORMANCE DEEP LEARNING LIBRARY">
  <data key="d0">IS</data>
</edge>
<edge source="TRANSFORMERS" target="58">
  <data key="d0">VERSION</data>
</edge>
<edge source="TRANSFORMERS" target="STATE-OF-THE-ART NATURAL LANGUAGE PROCESSING">
  <data key="d0">ARE</data>
</edge>
<edge source="MEMORY ACCESS PATTERNS" target="EXISTING SYSTEMS">
  <data key="d0">ARE NOT EFFICIENTLY SUPPORTED BY</data>
</edge>
<edge source="SEVERAL GPU KERNELS" target="OPTIMIZING PAGEDATTENTION">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="THE DYNAMIC BLOCK MAPPING IN PAGEDATTENTION" target="THE PERFORMANCE OF THE GPU OPERATIONS INVOLVING THE STORED KV CACHE">
  <data key="d0">AFFECTS</data>
</edge>
<edge source="THE GPU OPERATIONS" target="THE STORED KV CACHE">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="THE GPU OPERATIONS" target="BLOCK READWRITES AND ATTENTION">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="FUSED RE-SHAPE" target="BLOCK WRITE">
  <data key="d0">AND</data>
</edge>
<edge source="FUSED BLOCK COPY" target="3">
  <data key="d0">IS</data>
</edge>
<edge source="NEW KV CACHE" target="BLOCKS">
  <data key="d0">ARE SPLIT INTO</data>
</edge>
<edge source="NEW KV CACHE" target="A MEMORY LAYOUT OPTIMIZED FOR BLOCK READ">
  <data key="d0">ARE RESHAPED TO</data>
</edge>
<edge source="NEW KV CACHE" target="POSITIONS SPECIFIED BY THE BLOCK TABLE">
  <data key="d0">ARE SAVED AT</data>
</edge>
<edge source="FUSING THEM INTO A SINGLE KERNEL" target="KERNEL LAUNCH OVERHEADS">
  <data key="d0">MINIMIZES</data>
</edge>
<edge source="A KERNEL" target="THE COPY OPERATIONS FOR DIFFERENT BLOCKS">
  <data key="d0">BATCHES</data>
</edge>
<edge source="THE COPY OPERATIONS FOR DIFFERENT BLOCKS" target="A SINGLE KERNEL LAUNCH">
  <data key="d0">ARE BATCHED INTO</data>
</edge>
<edge source="BLOCK COPY OPERATIONS" target="THE COPY-ON-WRITE MECHANISM">
  <data key="d0">ARE ISSUED BY</data>
</edge>
<edge source="BLOCK COPY OPERATIONS" target="DISCONTINUOUS BLOCKS">
  <data key="d0">MAY OPERATE ON</data>
</edge>
<edge source="THE FORK METHOD" target="A NEW SEQUENCE FROM AN EXISTING ONE">
  <data key="d0">CREATES</data>
</edge>
<edge source="THE APPEND METHOD" target="A NEW TOKEN TO THE SEQUENCE">
  <data key="d0">APPENDS</data>
</edge>
<edge source="THE FREE METHOD" target="THE SEQUENCE">
  <data key="d0">DELETES</data>
</edge>
<edge source="FUTURE DECODING ALGORITHMS" target="COMBINING THESE METHODS">
  <data key="d0">CAN BE SUPPORTED BY</data>
</edge>
<edge source="PARALLEL GENERATION" target="2">
  <data key="d0">HAS PARALLEL SIZE</data>
</edge>
<edge source="NORMALIZED LATENCY" target="STOKEN">
  <data key="d0">MEASURED IN</data>
</edge>
<edge source="NORMALIZED LATENCY" target="THE MEAN OF EVERY REQUESTS END-TO-END LATENCY DIVIDED BY ITS OUTPUT LENGTH">
  <data key="d0">IS</data>
</edge>
<edge source="REQUEST RATE" target="REQS">
  <data key="d0">MEASURED IN</data>
</edge>
<edge source="REQUEST RATE" target="CAPACITY OF THE SERVING SYSTEM">
  <data key="d0">SURPASSES</data>
</edge>
<edge source="FIGURE" target="14">
  <data key="d0">NUMBER</data>
</edge>
<edge source="FIGURE" target="19">
  <data key="d0">NUMBER</data>
</edge>
<edge source="14" target="RESULTS FOR BEAM SEARCH WITH DIFFERENT BEAM WIDTHS">
  <data key="d0">SHOWS</data>
</edge>
<edge source="FIGURE 17" target="REQUEST RATE (REQS)">
  <data key="d0">DISPLAYS</data>
</edge>
<edge source="FIGURE 17" target="NORMALIZED LATENCY (STOKEN)">
  <data key="d0">DISPLAYS</data>
</edge>
<edge source="FIGURE 17" target="ORCA (MAX)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="FIGURE 17" target="ORCA (POW2)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="FIGURE 17" target="ORCA (ORACLE)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="FIGURE 17" target="VLLM">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="NORMALIZED LATENCY (STOKEN)" target="0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="SINGLE SEQUENCE GENERATION" target="OPT MODELS">
  <data key="d0">IS DONE WITH</data>
</edge>
<edge source="OPT MODELS" target="SHAREGPT DATASET">
  <data key="d0">ARE APPLIED ON</data>
</edge>
<edge source="OPT MODELS" target="ALPACA DATASET">
  <data key="d0">ARE APPLIED ON</data>
</edge>
<edge source="SHAREGPT DATASET" target="16.2 - 30.5">
  <data key="d0">HAS MEMORY SAVING ON PARALLEL SAMPLING</data>
</edge>
<edge source="SHAREGPT DATASET" target="44.3 - 66.3">
  <data key="d0">HAS MEMORY SAVING ON BEAM SEARCH</data>
</edge>
<edge source="SHAREGPT DATASET" target="MANY LONG CONVERSATIONS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="ALPACA DATASET" target="SHORT SEQUENCES">
  <data key="d0">HAS</data>
</edge>
<edge source="BATCHED REQUESTS" target="0 5 10 15 20 25 30 35 FOR SHAREGPT">
  <data key="d0">HAVE VALUES</data>
</edge>
<edge source="BATCHED REQUESTS" target="0 25 50 75 100 125 150 FOR ALPACA">
  <data key="d0">HAVE VALUES</data>
</edge>
<edge source="AVERAGE NUMBER OF BATCHED REQUESTS" target="WHEN SERVING OPT-13B FOR THE SHAREGPT (2 REQSS) AND ALPACA (30 REQSS) TRACES">
  <data key="d0">IS</data>
</edge>
<edge source="6.1" target="EXPERIMENTAL SETUP MODEL AND SERVER CONFIGURATIONS">
  <data key="d0">IS</data>
</edge>
<edge source="OPT 62 MODELS" target="13B PARAMETERS">
  <data key="d0">HAVE</data>
</edge>
<edge source="OPT 62 MODELS" target="66B PARAMETERS">
  <data key="d0">HAVE</data>
</edge>
<edge source="OPT 62 MODELS" target="175B PARAMETERS">
  <data key="d0">HAVE</data>
</edge>
<edge source="13B" target="POPULAR SIZES FOR LLMS">
  <data key="d0">ARE</data>
</edge>
<edge source="13B" target="THE RESULTS ON THE ALPACA DATASET">
  <data key="d0">SHOWS</data>
</edge>
<edge source="66B" target="POPULAR SIZES FOR LLMS">
  <data key="d0">ARE</data>
</edge>
<edge source="13B AND 66B" target="LLM LEADERBOARD 38">
  <data key="d0">ARE SHOWN IN</data>
</edge>
<edge source="175B" target="THE SIZE OF THE FAMOUS GPT-3 5 MODEL">
  <data key="d0">IS</data>
</edge>
<edge source="SHAREGPT 51 AND ALPACA 50 DATASETS" target="INPUT AND OUTPUT TEXTS OF REAL LLM SERVICES">
  <data key="d0">CONTAIN</data>
</edge>
<edge source="THEIR INPUT AND OUTPUT LENGTHS" target="CLIENT REQUESTS">
  <data key="d0">TO SYNTHESIZE</data>
</edge>
<edge source="REQUEST ARRIVAL TIMES" target="POISSON DISTRIBUTION">
  <data key="d0">ARE GENERATED USING</data>
</edge>
<edge source="POISSON DISTRIBUTION" target="DIFFERENT REQUEST RATES">
  <data key="d0">HAS</data>
</edge>
<edge source="THESE DATASETS" target="TIMESTAMPS">
  <data key="d0">DO NOT INCLUDE</data>
</edge>
<edge source="BASELINE 1" target="FASTERTRANSFORMER">
  <data key="d0">IS</data>
</edge>
<edge source="CUSTOM SCHEDULER" target="A DYNAMIC BATCHING MECHANISM">
  <data key="d0">HAS</data>
</edge>
<edge source="DYNAMIC BATCHING MECHANISM" target="EXISTING SERVING SYSTEMS SUCH AS TRITON 30">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="MAXIMUM BATCH SIZE" target="THE GPU MEMORY CAPACITY">
  <data key="d0">IS SET ACCORDING TO</data>
</edge>
<edge source="BASELINE 2" target="ORCA">
  <data key="d0">IS</data>
</edge>
<edge source="THE THREE ORCA BASELINES" target="SIMILARLY">
  <data key="d0">BEHAVE</data>
</edge>
<edge source="BUDDY ALLOCATION ALGORITHM" target="MEMORY ADDRESS TO STORE KV CACHE">
  <data key="d0">DETERMINES</data>
</edge>
<edge source="BUDDY ALLOCATION ALGORITHM" target="ORCA BASELINES TO RESERVE THE SPACE FOR 1024 TOKENS FOR THE REQUEST OUTPUTS">
  <data key="d0">CAUSES</data>
</edge>
<edge source="THE UPPER-BOUND PERFORMANCE OF ORCA" target="INFEASIBLE TO ACHIEVE IN PRACTICE">
  <data key="d0">IS</data>
</edge>
<edge source="TRUE OUTPUT LENGTH" target="25">
  <data key="d0">IS</data>
</edge>
<edge source="USING THE WORKLOADS WITH DIFFERENT REQUEST RATES" target="SPECIFICALLY">
  <data key="d0">IS</data>
</edge>
<edge source="A HIGH-THROUGHPUT SERVING SYSTEM" target="LOW NORMALIZED LATENCY">
  <data key="d0">SHOULD RETAIN</data>
</edge>
<edge source="LOW NORMALIZED LATENCY" target="HIGH REQUEST RATES">
  <data key="d0">IS RETAINED AGAINST</data>
</edge>
<edge source="15-MINUTE TRACES" target="AS AN EXCEPTION">
  <data key="d0">ARE USED</data>
</edge>
<edge source="15-MINUTE TRACES" target="DUE TO THE COST LIMIT">
  <data key="d0">ARE USED</data>
</edge>
<edge source="PARALLEL GENERATION AND BEAM SEARCH" target="OPT-13B">
  <data key="d0">IS USED WITH</data>
</edge>
<edge source="OPT-13B" target="THE ALPACA DATASET">
  <data key="d0">IS USED ON</data>
</edge>
<edge source="BASIC SAMPLING" target="ONE SAMPLE PER REQUEST">
  <data key="d0">IS</data>
</edge>
<edge source="12" target="THE RESULTS ON THE SHAREGPT DATASET">
  <data key="d0">SHOWS</data>
</edge>
<edge source="THE CURVES" target="AS THE REQUEST RATE INCREASES, THE LATENCY INITIALLY INCREASES AT A GRADUAL PACE BUT THEN SUDDENLY EXPLODES">
  <data key="d0">ILLUSTRATE</data>
</edge>
<edge source="QUEUE LENGTH" target="INFINITELY">
  <data key="d0">CONTINUES TO GROW</data>
</edge>
<edge source="LATENCY OF THE REQUESTS" target="SO">
  <data key="d0">DOES</data>
</edge>
<edge source="13A, FOR OPT-13B VLLM" target="2.2 MORE REQUESTS AT THE SAME TIME THAN ORCA (ORACLE)">
  <data key="d0">PROCESSES</data>
</edge>
<edge source="13A, FOR OPT-13B VLLM" target="4.3 MORE REQUESTS THAN ORCA (MAX)">
  <data key="d0">PROCESSES</data>
</edge>
<edge source="VLLMS PAGEDATTENTION" target="MEMORY USAGE EFFICIENTLY">
  <data key="d0">CAN MANAGE</data>
</edge>
<edge source="VLLMS PAGEDATTENTION" target="BATCHING MORE REQUESTS THAN ORCA">
  <data key="d0">ENABLES</data>
</edge>
<edge source="ONE EXCEPTION" target="FIG.">
  <data key="d0">IS</data>
</edge>
<edge source="VLLMS" target="ORCA (ORACLE)">
  <data key="d0">HAS ADVANTAGE OVER</data>
</edge>
<edge source="VLLMS" target="ORCA (POW2)">
  <data key="d0">HAS ADVANTAGE OVER</data>
</edge>
<edge source="ADVANTAGE OF VLLMS OVER ORCA (ORACLE) AND ORCA (POW2)" target="LESS PRONOUNCED">
  <data key="d0">IS</data>
</edge>
<edge source="MODEL AND SERVER CONFIGURATION FOR OPT-175B" target="LARGE GPU MEMORY SPACE AVAILABLE TO STORE KV CACHE">
  <data key="d0">ALLOWS FOR</data>
</edge>
<edge source="ORCA (ORACLE) AND ORCA (POW2)" target="A LARGE NUMBER OF REQUESTS">
  <data key="d0">CAN BATCH</data>
</edge>
<edge source="ORCA (ORACLE) AND ORCA (POW2)" target="REQUESTS DESPITE THE INEFFICIENCIES IN THEIR MEMORY MANAGEMENT">
  <data key="d0">CAN BATCH</data>
</edge>
<edge source="OUTPUT SEQUENCES" target="2 4 6">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="OUTPUT SEQUENCES" target="0 4 8 12">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MEMORY SAVING" target="6.09">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="8.53">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="9.79">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="37.56">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="53.13">
  <data key="d0">VALUE</data>
</edge>
<edge source="MEMORY SAVING" target="55.16">
  <data key="d0">VALUE</data>
</edge>
<edge source="BEAM WIDTH" target="2 4 6">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="BEAM WIDTH" target="0 20 40 60">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="AVERAGE AMOUNT OF MEMORY SAVING" target="SHARING KV BLOCKS">
  <data key="d0">IS FROM</data>
</edge>
<edge source="SHARING KV BLOCKS" target="SERVING OPT-13B FOR THE ALPACA TRACE">
  <data key="d0">OCCURS WHEN</data>
</edge>
<edge source="PARALLEL SAMPLING AND BEAM SEARCH" target="TWO POPULAR SAMPLING METHODS">
  <data key="d0">ARE</data>
</edge>
<edge source="2 HIGHER REQUEST RATES" target="THE THREE ORCA BASELINES">
  <data key="d0">ARE COMPARED TO</data>
</edge>
<edge source="15" target="THE AMOUNT OF MEMORY SAVING">
  <data key="d0">PLOTS</data>
</edge>
<edge source="THE AMOUNT OF MEMORY SAVING" target="THE NUMBER OF BLOCKS WE SAVED BY SHARING DIVIDED BY THE NUMBER OF TOTAL BLOCKS WITHOUT SHARING">
  <data key="d0">IS COMPUTED BY</data>
</edge>
<edge source="INPUT PROMPTS" target="A COMMON PREFIX">
  <data key="d0">SHARE</data>
</edge>
<edge source="THE PREFIX" target="(A) 1 EXAMPLE WITH 80 TOKENS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE PREFIX" target="(B) 5 EXAMPLES WITH 341 TOKENS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PERFORMANCE" target="CHATBOT WORKLOAD">
  <data key="d0">ON</data>
</edge>
<edge source="PERFORMANCE" target="OPT-13B WITH THE SHAREGPT TRACES AT THE SAME REQUEST RATE">
  <data key="d0">IS WHEN SERVING</data>
</edge>
<edge source="LLAMA-13B 52" target="THE MODEL">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="LLAMA-13B 52" target="MULTILINGUAL">
  <data key="d0">IS</data>
</edge>
<edge source="TWO PREFIXES" target="AN INSTRUCTION AND A FEW TRANSLATION EXAMPLES">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="THE FIRST PREFIX" target="A SINGLE EXAMPLE (I.E., ONE-SHOT)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE OTHER PREFIX" target="5 EXAMPLES (I.E., FEW-SHOT)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="CHATBOT" target="ONE OF THE MOST IMPORTANT APPLICATIONS OF LLMS">
  <data key="d0">IS</data>
</edge>
<edge source="A RESPONSE" target="CONCATENATING THE CHATTING HISTORY AND THE LAST USER QUERY INTO A PROMPT">
  <data key="d0">IS GENERATED BY</data>
</edge>
<edge source="THE CONTEXT LENGTH OF THE OPT-13B MODEL" target="LIMITED">
  <data key="d0">IS</data>
</edge>
<edge source="DOING THIS" target="THE SPACE FOR OTHER REQUESTS BETWEEN THE CONVERSATION ROUNDS">
  <data key="d0">WOULD OCCUPY</data>
</edge>
<edge source="INPUT PROMPTS FOR MOST REQUESTS" target="1024 TOKENS">
  <data key="d0">HAVE</data>
</edge>
<edge source="ORCA BASELINES" target="SPACE FOR 1024 TOKENS FOR THE REQUEST OUTPUTS">
  <data key="d0">RESERVE</data>
</edge>
<edge source="ORCA BASELINES" target="REGARDLESS OF HOW THEY PREDICT THE OUTPUT LENGTHS">
  <data key="d0">RESERVE SPACE</data>
</edge>
<edge source="SHAREGPT ALPACA (B)" target="END-TO-END LATENCY">
  <data key="d0">MEASURES</data>
</edge>
<edge source="END-TO-END LATENCY" target="DIFFERENT BLOCK SIZES">
  <data key="d0">VARIES WITH</data>
</edge>
<edge source="THE DESIGN CHOICES" target="ABLATION EXPERIMENTS">
  <data key="d0">ARE MADE WITH</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="EXTRA OVERHEADS OF ACCESSING THE BLOCK TABLE">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="EXECUTING EXTRA BRANCHES">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="HANDLING VARIABLE SEQUENCE LENGTHS">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="OUR GPU KERNELS (5)" target="THE EXISTING SYSTEMS">
  <data key="d0">ARE COMPARED TO</data>
</edge>
<edge source="18A" target="2026 HIGHER ATTENTION KERNEL LATENCY">
  <data key="d0">LEADS TO</data>
</edge>
<edge source="2026 HIGHER ATTENTION KERNEL LATENCY" target="HIGHLY-OPTIMIZED FASTERTRANSFORMER IMPLEMENTATION">
  <data key="d0">IS COMPARED TO</data>
</edge>
<edge source="THE OVERHEAD" target="SMALL">
  <data key="d0">IS</data>
</edge>
<edge source="THE OVERHEAD" target="THE ATTENTION OPERATOR">
  <data key="d0">AFFECTS</data>
</edge>
<edge source="THE OVERHEAD" target="THE OTHER OPERATORS IN THE MODEL">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="THE OTHER OPERATORS IN THE MODEL" target="LINEAR">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CHOICE OF BLOCK SIZE" target="SUBSTANTIAL IMPACT ON THE PERFORMANCE OF VLLM">
  <data key="d0">CAN HAVE</data>
</edge>
<edge source="THE PERFORMANCE OF VLLM" target="FIXED REQUEST RATES">
  <data key="d0">IS EVALUATED UNDER</data>
</edge>
<edge source="BLOCK SIZES FROM 16 TO 128" target="THE BEST PERFORMANCE">
  <data key="d0">LEAD TO</data>
</edge>
<edge source="BLOCK SIZE 16 AND 32" target="WELL IN THE ALPACA TRACE">
  <data key="d0">WORK</data>
</edge>
<edge source="LARGER BLOCK SIZES" target="THE PERFORMANCE">
  <data key="d0">DEGRADE</data>
</edge>
<edge source="SEQUENCES" target="SHORTER THAN THE BLOCK SIZES">
  <data key="d0">BECOME</data>
</edge>
<edge source="BLOCK SIZE 16" target="EFFICIENTLY UTILIZE THE GPU">
  <data key="d0">IS LARGE ENOUGH TO</data>
</edge>
<edge source="BLOCK SIZE 16" target="AVOID SIGNIFICANT INTERNAL FRAGMENTATION IN MOST WORKLOADS">
  <data key="d0">IS SMALL ENOUGH TO</data>
</edge>
<edge source="TIME" target="MS">
  <data key="d0">MEASURED IN</data>
</edge>
<edge source="MICROBENCHMARK" target="RECOMPUTE SWAP IN SWAP OUT SWAP IN OUT">
  <data key="d0">HAS LABEL</data>
</edge>
<edge source="MICROBENCHMARK" target="(A)">
  <data key="d0">LABELLED AS</data>
</edge>
<edge source="END-TO-END PERFORMANCE" target="(B)">
  <data key="d0">LABELLED AS</data>
</edge>
<edge source="OVERHEAD" target="RECOMPUTATION AND SWAPPING">
  <data key="d0">IS FOR</data>
</edge>
<edge source="OVERHEAD" target="DIFFERENT BLOCK SIZES">
  <data key="d0">IS FOR</data>
</edge>
<edge source="OVERHEAD" target="O(N) AND O(LOG(N)) RESPECTIVELY">
  <data key="d0">IS</data>
</edge>
<edge source="THE OVERHEAD OF RECOMPUTATION" target="CONSTANT ACROSS DIFFERENT BLOCK SIZES">
  <data key="d0">REMAINS</data>
</edge>
<edge source="RECOMPUTATION OVERHEAD" target="20 OF SWAPPINGS LATENCY">
  <data key="d0">IS NEVER HIGHER THAN</data>
</edge>
<edge source="SMALL BLOCK SIZES" target="NUMEROUS SMALL DATA TRANSFERS BETWEEN CPU AND GPU">
  <data key="d0">RESULT IN</data>
</edge>
<edge source="NUMEROUS SMALL DATA TRANSFERS BETWEEN CPU AND GPU" target="EFFECTIVE PCIE BANDWIDTH">
  <data key="d0">LIMIT</data>
</edge>
<edge source="THE TWO METHODS" target="COMPARABLE END-TO-END PERFORMANCE FOR MEDIUM BLOCK SIZES FROM 16 TO 64">
  <data key="d0">EXHIBIT</data>
</edge>
<edge source="VIRTUAL MEMORY AND PAGING TECHNIQUE" target="OTHER GPU WORKLOADS">
  <data key="d0">APPLIED TO</data>
</edge>
<edge source="TENSOR SHAPES" target="TYPICALLY STATIC">
  <data key="d0">ARE</data>
</edge>
<edge source="MEMORY ALLOCATION" target="OPTIMIZED AHEAD OF TIME">
  <data key="d0">CAN BE</data>
</edge>
<edge source="AN INCREASE IN MEMORY EFFICIENCY" target="ANY PERFORMANCE IMPROVEMENT">
  <data key="d0">MAY NOT RESULT IN</data>
</edge>
<edge source="LLM-SPECIFIC OPTIMIZATIONS" target="VIRTUAL MEMORY AND PAGING">
  <data key="d0">ARE APPLIED IN</data>
</edge>
<edge source="VLLMS ALL-OR-NOTHING SWAP-OUT POLICY" target="ONE EXAMPLE">
  <data key="d0">IS</data>
</edge>
<edge source="VLLMS ALL-OR-NOTHING SWAP-OUT POLICY" target="THE FACT THAT PROCESSING A REQUEST REQUIRES ALL OF ITS CORRESPONDING TOKEN STATES TO BE STORED IN GPU MEMORY">
  <data key="d0">EXPLOITS</data>
</edge>
<edge source="RECOMPUTATION METHOD" target="ANOTHER EXAMPLE">
  <data key="d0">IS</data>
</edge>
<edge source="RECOMPUTATION METHOD" target="EVICTED BLOCKS">
  <data key="d0">RECOVERS</data>
</edge>
<edge source="RECOMPUTATION METHOD" target="FEASIBLE IN OS">
  <data key="d0">IS NOT</data>
</edge>
<edge source="RELATED WORK" target="GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">IS ABOUT</data>
</edge>
<edge source="MODEL SERVING" target="AN ACTIVE AREA OF RESEARCH">
  <data key="d0">HAS BEEN</data>
</edge>
<edge source="NUMEROUS SYSTEMS" target="DIVERSE ASPECTS OF DEEP LEARNING MODEL DEPLOYMENT">
  <data key="d0">ARE PROPOSED TO TACKLE</data>
</edge>
<edge source="CLIPPER 11" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="TENSORFLOW SERVING 33" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="NEXUS 45" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="INFERLINE 10" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="CLOCKWORK 20" target="EARLIER GENERAL MODEL SERVING SYSTEMS">
  <data key="d0">ARE</data>
</edge>
<edge source="BATCH-ING" target="SERVING SINGLE OR MULTIPLE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="CACHING" target="SERVING SINGLE OR MULTIPLE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="PLACEMENT" target="SERVING SINGLE OR MULTIPLE MODELS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="DVABATCH 12" target="MULTI-ENTRY MULTI-EXIT BATCHING">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="REEF 21" target="PREEMPTION FOR SERVING">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="SHEP-HERD 61" target="PREEMPTION FOR SERVING">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="ALPASERVE 28" target="MODEL PARALLELISM">
  <data key="d0">UTILIZES</data>
</edge>
<edge source="MODEL PARALLELISM" target="STATISTICAL MULTIPLEXING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="MODEL PARALLELISM" target="EFFICIENT SERVING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="ALPASERVE" target="STATISTICAL MULTIPLEXING WITH MODEL PARALLELISM FOR DEEP LEARNING SERVING">
  <data key="d0">IS</data>
</edge>
<edge source="GENERAL SYSTEMS" target="AUTO-REGRESSIVE PROPERTY AND TOKEN STATE OF LLM INFERENCE">
  <data key="d0">FAIL TO TAKE INTO ACCOUNT</data>
</edge>
<edge source="SPECIALIZED SERVING SYSTEMS" target="TRANSFORMERS">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="NUMEROUS SPECIALIZED SERVING SYSTEMS" target="THE TRANSFORMER ARCHITECTURE">
  <data key="d0">HAVE BEEN DEVELOPED FOR</data>
</edge>
<edge source="THESE SYSTEMS" target="GPU KERNEL OPTIMIZATIONS">
  <data key="d0">UTILIZE</data>
</edge>
<edge source="THESE SYSTEMS" target="ADVANCED BATCHING MECHANISMS">
  <data key="d0">UTILIZE</data>
</edge>
<edge source="THESE SYSTEMS" target="MODEL PARALLELISM">
  <data key="d0">UTILIZE</data>
</edge>
<edge source="THESE SYSTEMS" target="PARAMETER SHARING">
  <data key="d0">UTILIZE</data>
</edge>
<edge source="GPU KERNEL OPTIMIZATIONS" target="EFFICIENT SERVING">
  <data key="d0">ARE USED FOR</data>
</edge>
<edge source="ADVANCED BATCHING MECHANISMS" target="EFFICIENT SERVING">
  <data key="d0">ARE USED FOR</data>
</edge>
<edge source="PARAMETER SHARING" target="EFFICIENT SERVING">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="FINE-GRAINED SCHEDULING AND INTERLEAVING OF THE REQUESTS LIKE IN ORCA" target="MEMORY MANAGEMENT MORE CHALLENGING">
  <data key="d0">MAKES</data>
</edge>
<edge source="TECHNIQUES PROPOSED IN VLLM" target="MORE CRUCIAL">
  <data key="d0">ARE</data>
</edge>
<edge source="THE WIDENING GAP BETWEEN THE COMPUTE CAPABILITY AND MEMORY CAPACITY OF ACCELERATORS" target="MEMORY TO BECOME A BOTTLENECK FOR BOTH TRAINING AND INFERENCE">
  <data key="d0">HAS CAUSED</data>
</edge>
<edge source="SWAPPING 23, 42, 55, RECOMPUTATION 7, 24 AND THEIR COMBINATION 40" target="REDUCE THE PEAK MEMORY OF TRAINING">
  <data key="d0">HAVE BEEN UTILIZED TO</data>
</edge>
<edge source="FLEXGEN 46 STUDIES" target="HOW TO SWAP WEIGHTS AND TOKEN STATES FOR LLM INFERENCE WITH 623 LIMITED GPU MEMORY">
  <data key="d0">STUDIES</data>
</edge>
<edge source="OLLA 48" target="THE LIFETIME AND LOCATION OF TENSORS">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="OLLA 48" target="FRAGMENTATION">
  <data key="d0">REDUCES</data>
</edge>
<edge source="OLLA 48" target="FINE-GRAINED BLOCK-LEVEL MANAGEMENT">
  <data key="d0">DOES NOT DO</data>
</edge>
<edge source="OLLA 48" target="ONLINE SERVING">
  <data key="d0">DOES NOT DO</data>
</edge>
<edge source="FLASHAT-TENTION 13" target="TILING AND KERNEL OPTIMIZATIONS">
  <data key="d0">APPLIES</data>
</edge>
<edge source="FLASHAT-TENTION 13" target="THE PEAK MEMORY OF ATTENTION COMPUTATION">
  <data key="d0">REDUCES</data>
</edge>
<edge source="FLASHAT-TENTION 13" target="IO COSTS">
  <data key="d0">REDUCES</data>
</edge>
<edge source="ZHIFENG CHEN" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="OUR SHEPHERD" target="LIDONG ZHOU">
  <data key="d0">IS</data>
</edge>
<edge source="XIAOXUAN LIU, ZHIFENG CHEN, YAN-PING HUANG, ANONYMOUS SOSP REVIEWERS, AND LIDONG ZHOU" target="INSIGHTFUL FEEDBACK">
  <data key="d0">PROVIDE</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM ANDREESSEN HOROWITZ">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM ANYSCALE">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM ASTRONOMER">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM GOOGLE">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM IBM">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM INTEL">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM LACEWORK">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM MICROSOFT">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM MOHAMED BIN ZAYED UNIVERSITY OF ARTIFICIAL INTELLIGENCE">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM SAMSUNG SDS">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM UBER">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="THIS RESEARCH" target="GIFTS FROM VMWARE">
  <data key="d0">IS PARTLY SUPPORTED BY</data>
</edge>
<edge source="REFERENCES" target="REZA YAZDANI AMINABADI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="SAMYAM RAJBHANDARI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="MINJIA ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="AMMAR AHMAD AWAN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="CHENG LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="DU LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="ELTON ZHENG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="JEFF RASLEY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="SHADEN SMITH">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="OLATUNJI RUWASE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="REFERENCES" target="TABLE III">
  <data key="d0">ARE PROVIDED IN</data>
</edge>
<edge source="DEEPSPEED INFERENCE" target="EFFICIENT INFERENCE OF TRANSFORMER MODELS AT UNPRECEDENTED SCALE">
  <data key="d0">ENABLES</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2207.00032">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2022">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1607.06450">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2016">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2107.03374">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2021">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1604.06174">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2204.02311">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2104.08691">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2101.00190">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2302.11665">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2023">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1712.06139">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2017">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2211.05102">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2303.06865">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1909.08053">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="2019">
  <data key="d0">YEAR</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2302.13971">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2212.10560">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:1609.08144">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="ARXIV PREPRINT" target="ARXIV:2205.01068">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="JIMMY LEI BA" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JAMIE RYAN KIROS" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="GEOFFREY E HINTON" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="LAYER NORMALIZATION" target="A TECHNIQUE">
  <data key="d0">IS</data>
</edge>
<edge source="YOSHUA BENGIO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="RJEAN DUCHARME" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="PASCAL VINCENT" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="NEURAL PROBABILISTIC LANGUAGE MODEL" target="A MODEL">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="13">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="33">
  <data key="d0">IS VOLUME</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="35">
  <data key="d0">IS VOLUME</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="27TH EDITION">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="30TH EDITION">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS" target="32ND EDITION">
  <data key="d0">IS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13" target="2000">
  <data key="d0">YEAR</data>
</edge>
<edge source="4 OND REJ BOJAR" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="RAJEN CHATTERJEE" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="CHRISTIAN FEDERMANN" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="YVETTE GRAHAM" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="BARRY HADDOW" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MATTHIAS HUCK" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="ANTONIO JIMENO YEPES" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="PHILIPP KOEHN" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="VARVARA LOGACHEVA" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="CHRISTOF MONZ" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MATTEO NEGRI" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="AURELIE NEVEOL" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MARIANA NEVES" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MARTIN POPEL" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MATT POST" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="RAPHAEL RUBINO" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="CAROLINA SCARTON" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="LUCIA SPECIA" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MARCO TURCHI" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="KARIN VERSPOOR" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="MARCOS ZAMPIERI" target="PERSON">
  <data key="d0">IS A</data>
</edge>
<edge source="2016 CONFERENCE" target="MACHINE TRANSLATION">
  <data key="d0">IS ON</data>
</edge>
<edge source="PROCEEDINGS" target="THE FIRST CONFERENCE ON MACHINE TRANSLATION">
  <data key="d0">ARE OF</data>
</edge>
<edge source="PROCEEDINGS" target="THE IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION">
  <data key="d0">ARE OF</data>
</edge>
<edge source="PROCEEDINGS" target="2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
  <data key="d0">ARE OF</data>
</edge>
<edge source="PROCEEDINGS" target="HUMAN LANGUAGE TECHNOLOGIES: INDUSTRY PAPERS">
  <data key="d0">ARE TITLED</data>
</edge>
<edge source="PROCEEDINGS" target="THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS">
  <data key="d0">ARE OF</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS" target="BERLIN, GERMANY">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS" target="131198">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="TOM BROWN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="BENJAMIN MANN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="NICK RYDER">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="MELANIE SUBBIAH">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="JARED D KAPLAN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="PRAFULLA DHARIWAL">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="ARVIND NEELAKANTAN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="PRANAV SHYAM">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="GIRISH SASTRY">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTP:WWW.ACLWEB.ORGANTHOLOGYWW16W16-2301" target="AMANDA ASKELL">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="LANGUAGE MODELS" target="FEW-SHOT LEARNERS">
  <data key="d0">ARE</data>
</edge>
<edge source="33" target="CHRISTOPHER OLSTON">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="NOAH FIEDEL">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="KIRIL GOROVOY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="JEREMIAH HARMSEN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="LI LAO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="FANGWEI LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="VINU RAJASHEKHAR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="SUKRITI RAMESH">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="33" target="JORDAN SOYKE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 33" target="2020">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 33" target="1877-1901">
  <data key="d0">HAS PAGE RANGE</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35" target="2022">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 35" target="16344-16359">
  <data key="d0">HAS PAGE RANGE</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27" target="2014">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30" target="2017">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="6 MARK CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JERRY TWOREK" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HEEWOO JUN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="QIMING YUAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HENRIQUE PONDE DE OLIVEIRA PINTO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JARED KAPLAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HARRI EDWARDS" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="YURI BURDA" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="NICHOLAS JOSEPH" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="GREG BROCKMAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="LARGE LANGUAGE MODELS" target="CODE">
  <data key="d0">ARE TRAINED ON</data>
</edge>
<edge source="7 TIANQI CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="BING XU" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="CHIYUAN ZHANG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="CARLOS GUESTRIN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="14 JIARUI FANG" target="YANG YU">
  <data key="d0">AND</data>
</edge>
<edge source="14 JIARUI FANG" target="CHENGDUO ZHAO">
  <data key="d0">AND</data>
</edge>
<edge source="14 JIARUI FANG" target="JIE ZHOU">
  <data key="d0">AND</data>
</edge>
<edge source="21 MINGCONG HAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HANZE ZHANG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="RONG CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HAIBO CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="22" target="KAIMING HE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="22" target="XIANGYU ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="22" target="SHAOQING REN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="22" target="JIAN SUN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="23" target="CHIEN-CHIN HUANG, GU JIN, AND JINYANG LI">
  <data key="d0">IS</data>
</edge>
<edge source="54 JING WANG" target="YOUYOU LU">
  <data key="d0">AND</data>
</edge>
<edge source="54 JING WANG" target="QING WANG">
  <data key="d0">AND</data>
</edge>
<edge source="54 JING WANG" target="MINHUI XIE">
  <data key="d0">AND</data>
</edge>
<edge source="54 JING WANG" target="KEJI HUANG">
  <data key="d0">AND</data>
</edge>
<edge source="54 JING WANG" target="JIWU SHU">
  <data key="d0">AND</data>
</edge>
<edge source="56" target="XIAOHUI WANG">
  <data key="d0">IS</data>
</edge>
<edge source="56" target="YING XIONG">
  <data key="d0">IS</data>
</edge>
<edge source="56" target="YANG WEI">
  <data key="d0">IS</data>
</edge>
<edge source="56" target="MINGXUAN WANG">
  <data key="d0">IS</data>
</edge>
<edge source="56" target="LEI LI">
  <data key="d0">IS</data>
</edge>
<edge source="64" target="ZHE ZHOU">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="64" target="XUECHAO WEI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="64" target="JIEJING ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="64" target="GUANGYU SUN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="TRAINING DEEP NETS" target="SUBLINEAR MEMORY COST">
  <data key="d0">HAS</data>
</edge>
<edge source="8 WEI-LIN CHIANG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ZHUOHAN LI" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ZI LIN" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="YING SHENG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ZHANGHAO WU" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="HAO ZHANG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="LIANMIN ZHENG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="SIYUAN ZHUANG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="YONGHAO ZHUANG" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="JOSEPH E. GONZALEZ" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ION STOICA" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="ION STOICA" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="ION STOICA" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="ERIC P. XING" target="TEXT">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="12 WEIHAO CUI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HAN ZHAO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="QUAN CHEN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="HAO WEI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="ZIRUI LI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="DEZE ZENG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="CHAO LI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="MINYI GUO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="28" target="ZHUOHAN LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="LIANMIN ZHENG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="YINMIN ZHONG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="VINCENT LIU">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="YING SHENG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="XIN JIN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="YANPING HUANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="ZHIFENG CHEN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="HAO ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="JOSEPH E GONZALEZ">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="ET AL">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="28" target="A VALID-BASED MECHANISM IN HW TO BLOCK CONTEXT SWITCH ON SELECTED REGISTERS">
  <data key="d0">ADOPTS</data>
</edge>
<edge source="28" target="REGISTER MOVEMENT BY ALMOST 50">
  <data key="d0">REDUCES</data>
</edge>
<edge source="JOSEPH E GONZALEZ" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="59" target="NUMBER">
  <data key="d0">IS A</data>
</edge>
<edge source="YONGHUI WU" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="MIKE SCHUSTER" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="QUOC V LE" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="MOHAMMAD NOROUZI" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="WOLFGANG MACHEREY" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="MAXIM KRIKUN" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="YUAN CAO" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="QIN GAO" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="KLAUS MACHEREY" target="AUTHOR">
  <data key="d0">IS AN</data>
</edge>
<edge source="63 LIANMIN ZHENG, ZHUOHAN LI, HAO ZHANG, YONGHAO ZHUANG, ZHIFENG CHEN, YANPING HUANG, YIDA WANG, YUANZHONG XU, DANYANG ZHUO, ERIC P XING" target="ET AL.">
  <data key="d0">IS</data>
</edge>
<edge source="VICUNA" target="AN OPEN-SOURCE CHATBOT">
  <data key="d0">IS</data>
</edge>
<edge source="VICUNA" target="GPT-4">
  <data key="d0">IMPRESSES</data>
</edge>
<edge source="VICUNA" target="90 CHATGPT QUALITY">
  <data key="d0">HAS</data>
</edge>
<edge source="GPT-4" target="TECHNICAL REPORT">
  <data key="d0">IS</data>
</edge>
<edge source="ORGBLOG2023-03-30-VICUNA 9" target="AAKANKSHA CHOWDHERY, SHARAN NARANG, JACOB DEVLIN, MAARTEN BOSMA, GAURAV MISHRA, ADAM ROBERTS, PAUL BARHAM, HYUNG WON CHUNG, CHARLES SUTTON, SEBASTIAN GEHRMANN, ET AL.">
  <data key="d0">HAS AUTHORS</data>
</edge>
<edge source="PALM" target="SCALING LANGUAGE MODELING WITH PATHWAYS">
  <data key="d0">IS</data>
</edge>
<edge source="DANIEL CRANKSHAW" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JOSEPH GONZALEZ" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="XIN WANG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="GUILIO ZHOU" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="MICHAEL J FRANKLIN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="INFERLINE" target="LATENCY-AWARE PROVISIONING AND SCALING FOR PREDICTION SERVING PIPELINES">
  <data key="d0">IS</data>
</edge>
<edge source="11TH ACM SYMPOSIUM" target="CLOUD COMPUTING">
  <data key="d0">IS ON</data>
</edge>
<edge source="CLIPPER" target="A LOW-LATENCY ONLINE PREDICTION SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION" target="NSDI 17">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="20TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION" target="NSDI 23">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="DVABATCH" target="DIVERSITY-AWARE MULTI-ENTRY MULTI-EXIT BATCHING">
  <data key="d0">IS</data>
</edge>
<edge source="DVABATCH" target="EFFICIENT PROCESSING OF DNN SERVICES ON GPUS">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="USENIX ANNUAL TECHNICAL CONFERENCE" target="2022">
  <data key="d0">OCCURRED IN</data>
</edge>
<edge source="USENIX ANNUAL TECHNICAL CONFERENCE" target="USENIX ATC 22">
  <data key="d0">ABBREVIATED AS</data>
</edge>
<edge source="13 TRI DAO, DAN FU, STEFANO ERMON, ATRI RUDRA, AND CHRISTOPHER R." target="IN 2022">
  <data key="d0">PUBLISHED</data>
</edge>
<edge source="FLASHATTENTION" target="FAST AND MEMORY-EFFICIENT EXACT ATTENTION WITH IO-AWARENESS">
  <data key="d0">IS</data>
</edge>
<edge source="TURBOTRANSFORMERS" target="AN EFFICIENT GPU SERVING SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="TURBOTRANSFORMERS" target="TRANSFORMER MODELS">
  <data key="d0">FOR</data>
</edge>
<edge source="26TH ACM SIGPLAN SYMPOSIUM" target="PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING">
  <data key="d0">IS ON</data>
</edge>
<edge source="THE 23RD ACM SIGPLAN SYMPOSIUM" target="PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING">
  <data key="d0">IS ON</data>
</edge>
<edge source="FASTAPI" target="15">
  <data key="d0">IS</data>
</edge>
<edge source="FASTAPI" target="A WEB FRAMEWORK">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS:GITHUB.COM" target="TIANGOLOFASTAPI">
  <data key="d0">HOSTS</data>
</edge>
<edge source="HTTPS:GITHUB.COM" target="NVIDIA FASTERTRANSFORMER">
  <data key="d0">HOSTS</data>
</edge>
<edge source="16" target="PIN GAO">
  <data key="d0">IS</data>
</edge>
<edge source="16" target="LINGFAN YU">
  <data key="d0">IS</data>
</edge>
<edge source="16" target="YONGWEI WU">
  <data key="d0">IS</data>
</edge>
<edge source="16" target="JINYANG LI">
  <data key="d0">IS</data>
</edge>
<edge source="LOW LATENCY RNN INFERENCE" target="CELLULAR BATCHING">
  <data key="d0">IS WITH</data>
</edge>
<edge source="THIRTEENTH EUROSYS CONFERENCE" target="PROCEEDINGS">
  <data key="d0">IS</data>
</edge>
<edge source="17 AMIR GHOLAMI" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="ZHEWEI YAO" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="SEHOON KIM" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="MICHAEL W MAHONEY" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="KURT KEUTZER" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="AI" target="MEMORY WALL">
  <data key="d0">AND</data>
</edge>
<edge source="RISELAB MEDIUM POST 1" target="2021">
  <data key="d0">HAS YEAR</data>
</edge>
<edge source="RISELAB MEDIUM POST 1" target="6">
  <data key="d0">HAS PAGE</data>
</edge>
<edge source="GITHUB" target="18">
  <data key="d0">HAS NUMBER</data>
</edge>
<edge source="GITHUB" target="COPILOT">
  <data key="d0">HAS FEATURE</data>
</edge>
<edge source="COPILOT" target="GITHUB">
  <data key="d0">IS FEATURE OF</data>
</edge>
<edge source="GOOGLE" target="TEXT">
  <data key="d0">IS MENTIONED</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="ARPAN GUJARATI">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="REZA KARIMI">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="SAFYA ALZAYAT">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="WEI HAO">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="ANTOINE KAUFMANN">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="YMIR VIGFUSSON">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="HTTPS:BARD.GOOGLE.COM" target="JONATHAN MACE">
  <data key="d0">INCLUDES AUTHORS</data>
</edge>
<edge source="SERVING DNNS" target="LIKE CLOCKWORK">
  <data key="d0">IS</data>
</edge>
<edge source="PERFORMANCE PREDICTABILITY" target="FROM THE BOTTOM UP">
  <data key="d0">IS</data>
</edge>
<edge source="OSDI 20" target="14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS HELD IN</data>
</edge>
<edge source="14TH USENIX CONFERENCE" target="OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS ON</data>
</edge>
<edge source="OSDI 22" target="16TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION">
  <data key="d0">IS</data>
</edge>
<edge source="MICROSECOND-SCALE PREEMPTION" target="CONCURRENT GPU-ACCELERATED DNN INFERENCES">
  <data key="d0">IS FOR</data>
</edge>
<edge source="DEEP RESIDUAL LEARNING" target="IMAGE RECOGNITION">
  <data key="d0">IS FOR</data>
</edge>
<edge source="SWAPADVISOR" target="DEEP LEARNING BEYOND THE GPU MEMORY LIMIT VIA SMART SWAPPING">
  <data key="d0">PUSHES</data>
</edge>
<edge source="THE TWENTY-FIFTH INTERNATIONAL CONFERENCE" target="ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS">
  <data key="d0">IS ON</data>
</edge>
<edge source="24 PARAS JAIN" target="AJAY JAIN">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="ANIRUDDHA NRUSIMHA">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="AMIR GHOLAMI">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="PIETER ABBEEL">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="JOSEPH GONZALEZ">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="KURT KEUTZER">
  <data key="d0">AND</data>
</edge>
<edge source="24 PARAS JAIN" target="ION STOICA">
  <data key="d0">AND</data>
</edge>
<edge source="40 SHISHIR G PATIL" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="PARAS JAIN" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="PRABAL DUTTA" target="PERSON">
  <data key="d0">IS</data>
</edge>
<edge source="CHECK-MATE" target="BREAKING THE MEMORY WALL WITH OPTIMAL TENSOR REMATERIALIZATION">
  <data key="d0">IS</data>
</edge>
<edge source="PROCEEDINGS OF MACHINE LEARNING AND SYSTEMS" target="2">
  <data key="d0">VOLUME</data>
</edge>
<edge source="PROCEEDINGS OF MACHINE LEARNING AND SYSTEMS" target="2020">
  <data key="d0">YEAR</data>
</edge>
<edge source="PROCEEDINGS OF MACHINE LEARNING AND SYSTEMS" target="497-511">
  <data key="d0">PAGE NUMBERS</data>
</edge>
<edge source="TOM KILBURN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="DAVID BG EDWARDS" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="MICHAEL J LANIGAN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="FRANK H SUMNER" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="1962" target="YEAR">
  <data key="d0">IS A</data>
</edge>
<edge source="ONE-LEVEL STORAGE SYSTEM" target="SYSTEM">
  <data key="d0">IS</data>
</edge>
<edge source="IRE TRANSACTIONS ON ELECTRONIC COMPUTERS" target="2">
  <data key="d0">VOLUME</data>
</edge>
<edge source="IRE TRANSACTIONS ON ELECTRONIC COMPUTERS" target="1962">
  <data key="d0">YEAR</data>
</edge>
<edge source="IRE TRANSACTIONS ON ELECTRONIC COMPUTERS" target="223-235">
  <data key="d0">PAGE NUMBERS</data>
</edge>
<edge source="26" target="BRIAN LESTER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="26" target="RAMI AL-RFOU">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="26" target="NOAH CONSTANT">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="26" target="A FULL HW SOLUTION BASED ON A HARDWARE SCHEDULING ENGINE (HSE)">
  <data key="d0">PROPOSE</data>
</edge>
<edge source="POWER OF SCALE" target="PARAMETER-EFFICIENT PROMPT TUNING">
  <data key="d0">IS FOR</data>
</edge>
<edge source="XIANG LISA LI" target="PERCY LIANG">
  <data key="d0">AND</data>
</edge>
<edge source="PREFIX-TUNING" target="OPTIMIZING CONTINUOUS PROMPTS FOR GENERATION">
  <data key="d0">IS</data>
</edge>
<edge source="29 LINGXIAO MA" target="ZHIQIANG XIE">
  <data key="d0">AND</data>
</edge>
<edge source="ZHIQIANG XIE" target="ZHI YANG">
  <data key="d0">AND</data>
</edge>
<edge source="ZHI YANG" target="JILONG XUE">
  <data key="d0">AND</data>
</edge>
<edge source="JILONG XUE" target="YOUSHAN MIAO">
  <data key="d0">AND</data>
</edge>
<edge source="YOUSHAN MIAO" target="WEI CUI">
  <data key="d0">AND</data>
</edge>
<edge source="WEI CUI" target="WENXIANG HU">
  <data key="d0">AND</data>
</edge>
<edge source="WENXIANG HU" target="FAN YANG">
  <data key="d0">AND</data>
</edge>
<edge source="FAN YANG" target="LINTAO ZHANG">
  <data key="d0">AND</data>
</edge>
<edge source="LINTAO ZHANG" target="LIDONG ZHOU">
  <data key="d0">AND</data>
</edge>
<edge source="RAMMER" target="HOLISTIC DEEP LEARNING COMPILER OPTIMIZATIONS WITH RTASKS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="NVIDIA" target="30">
  <data key="d0">HAS VALUE</data>
</edge>
<edge source="NVIDIA" target="31">
  <data key="d0">IS</data>
</edge>
<edge source="NVIDIA" target="32">
  <data key="d0">IS</data>
</edge>
<edge source="N. D." target="TRITON INFERENCE SERVER">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS://DEVELOPER.NVIDIA.COM" target="NVIDIA-TRITON-INFERENCE-SERVER">
  <data key="d0">HOSTS</data>
</edge>
<edge source="HTTPS" target="DEVELOPER.NVIDIA.COMNCCL">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="OPENAI.COM/BLOG/CUSTOM-INSTRUCTIONS-FOR-CHATGPT">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS" target="AVAILABLE AT CV32E40P">
  <data key="d0">IS</data>
</edge>
<edge source="NCCL" target="THE NVIDIA COLLECTIVE COMMUNICATION LIBRARY">
  <data key="d0">IS</data>
</edge>
<edge source="TENSORFLOW-SERVING" target="FLEXIBLE ML SERVING">
  <data key="d0">IS</data>
</edge>
<edge source="TENSORFLOW-SERVING" target="HIGH-PERFORMANCE ML SERVING">
  <data key="d0">IS</data>
</edge>
<edge source="OPENAI" target="34">
  <data key="d0">IS</data>
</edge>
<edge source="OPENAI" target="HTTPS://OPENAI.COM/BLOG/OPENAI-API">
  <data key="d0">HAS WEBSITE</data>
</edge>
<edge source="OPENAI" target="HTTPS://OPENAI.COM/BLOG/CHATGPT">
  <data key="d0">HAS WEBSITE</data>
</edge>
<edge source="OPENAI" target="BLOG CUSTOM INSTRUCTIONS FOR CHATGPT">
  <data key="d0">HAS</data>
</edge>
<edge source="CHATBOT ARENA LEADERBOARD" target="WEEK 8">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="WEEK 8" target="MT-BENCH">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="WEEK 8" target="VICUNA-33B">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="HTTPS:LMSYS.ORG" target="BLOG POST DATE 2023-06-22">
  <data key="d0">HAS</data>
</edge>
<edge source="HTTPS:LMSYS.ORG" target="LEADERBOARD">
  <data key="d0">HAS</data>
</edge>
<edge source="ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32" target="2019">
  <data key="d0">WAS PUBLISHED IN</data>
</edge>
<edge source="POET" target="NEURAL NETWORKS">
  <data key="d0">TRAINS</data>
</edge>
<edge source="POET" target="INTEGRATED REMATERIALIZATION AND PAGING">
  <data key="d0">USES</data>
</edge>
<edge source="NEURAL NETWORKS" target="TINY DEVICES">
  <data key="d0">ARE TRAINED ON</data>
</edge>
<edge source="INTERNATIONAL CONFERENCE ON MACHINE LEARNING" target="AN EVENT">
  <data key="d0">IS</data>
</edge>
<edge source="AN EVENT" target="AN INTERRUPT REQUEST">
  <data key="d0">IS SUCH AS</data>
</edge>
<edge source="PMLR" target="1757317583">
  <data key="d0">HAS IDENTIFIER</data>
</edge>
<edge source="41" target="REINER POPE">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="SHOLTO DOUGLAS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="AAKANKSHA CHOWDHERY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="JACOB DEVLIN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="JAMES BRADBURY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="ANSELM LEVSKAYA">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="JONATHAN HEEK">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="KEFAN XIAO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="SHIVANI AGRAWAL">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="41" target="JEFF DEAN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ZERO-OFFLOAD" target="DEMOCRATIZING BILLION-SCALE MODEL TRAINING">
  <data key="d0">IS</data>
</edge>
<edge source="AMAZON WEB SERVICES" target="HTTPS://WWW.REUTERS.COM/TECHNOLOGY/TECH-GIANTS-AI-LIKE-BING-BARD-POSES-BILLION-DOLLAR-SEARCH-PROBLEM-2023-02-22">
  <data key="d0">IS MENTIONED IN</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="HAICHEN SHEN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="LEQUN CHEN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="YUCHEN JIN">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="LIANGYU ZHAO">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="BINGYU KONG">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="MATTHAI PHILIPOSE">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="ARVIND KRISHNAMURTHY">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="HTTPS:AWS.AMAZON.COMBEDROCK" target="RAVI SUNDARAM">
  <data key="d0">HAS_AUTHOR</data>
</edge>
<edge source="NEXUS" target="A GPU CLUSTER ENGINE">
  <data key="d0">IS</data>
</edge>
<edge source="NEXUS" target="ACCELERATING DNN-BASED VIDEO ANALYSIS">
  <data key="d0">PURPOSE</data>
</edge>
<edge source="27TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES" target="PROCEEDINGS">
  <data key="d0">IS</data>
</edge>
<edge source="HIGH-THROUGHPUT GENERATIVE INFERENCE" target="LARGE LANGUAGE MODELS">
  <data key="d0">IS OF</data>
</edge>
<edge source="HIGH-THROUGHPUT GENERATIVE INFERENCE" target="A SINGLE GPU">
  <data key="d0">USES</data>
</edge>
<edge source="47" target="MOHAMMAD SHOEYBI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="MOSTOFA PATWARY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="RAUL PURI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="PATRICK LEGRESLEY">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="JARED CASPER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="47" target="BRYAN CATANZARO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MEGATRON-LM" target="TRAINING MULTI-BILLION PARAMETER LANGUAGE MODELS">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="MEGATRON-LM" target="MODEL PARALLELISM">
  <data key="d0">USES</data>
</edge>
<edge source="48" target="BENOIT STEINER">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="48" target="MOSTAFA ELHOUSHI">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="48" target="JACOB KAHN">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="48" target="JAMES HEGARTY">
  <data key="d0">IS ASSOCIATED WITH</data>
</edge>
<edge source="OLLA" target="THE LIFETIME OF ARRAYS">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="OLLA" target="THE LOCATION OF ARRAYS">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="OLLA" target="THE MEMORY USAGE OF NEURAL NETWORKS">
  <data key="d0">REDUCES</data>
</edge>
<edge source="DOI.ORG/10.48550/ARXIV.2210.12924" target="ILYA SUTSKEVER">
  <data key="d0">HAS AUTHOR</data>
</edge>
<edge source="DOI.ORG/10.48550/ARXIV.2210.12924" target="ORIOL VINYALS">
  <data key="d0">HAS AUTHOR</data>
</edge>
<edge source="DOI.ORG/10.48550/ARXIV.2210.12924" target="QUOC V LE">
  <data key="d0">HAS AUTHOR</data>
</edge>
<edge source="SEQUENCE TO SEQUENCE LEARNING" target="NEURAL NETWORKS">
  <data key="d0">IS DONE WITH</data>
</edge>
<edge source="50" target="ROHAN TAORI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="ISHAAN GULRAJANI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="TIANYI ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="YANN DUBOIS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="XUECHEN LI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="CARLOS GUESTRIN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="PERCY LIANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="50" target="TATSUNORI B. HASHIMOTO">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="55" target="LINNAN WANG">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="JINMIAN YE">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="YIYANG ZHAO">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="WEI WU">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="ANG LI">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="SHUAI-WEN LEON SONG">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="ZENGLIN XU">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="55" target="TIM KRASKA">
  <data key="d0">IS AUTHOR NUMBER</data>
</edge>
<edge source="57 YIZHONG WANG" target="YEGANEH KORDI">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="SWAROOP MISHRA">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="ALISA LIU">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="NOAH A SMITH">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="DANIEL KHASHABI">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="57 YIZHONG WANG" target="HANNANEH HAJISHIRZI">
  <data key="d0">IS AUTHOR</data>
</edge>
<edge source="STANFORD ALPACA" target="AN INSTRUCTION-FOLLOWING LLAMA MODEL">
  <data key="d0">IS</data>
</edge>
<edge source="GITHUB.COM/TATSU-LABS" target="STANFORD ALPACA">
  <data key="d0">HOSTS</data>
</edge>
<edge source="SHAREGPT TEAM" target="51">
  <data key="d0">HAS NUMBER</data>
</edge>
<edge source="HTTPS:SHAREGPT.COM" target="HUGO TOUVRON, THIBAUT LAVRIL, GAUTIER IZACARD, XAVIER MARTINET, MARIE-ANNE LACHAUX, TIMOTHE LACROIX, BAPTISTE ROZIRE, NAMAN GOYAL, ERIC HAMBRO, FAISAL AZHAR, ET AL.">
  <data key="d0">HAS AUTHORS</data>
</edge>
<edge source="LLAMA" target="OPEN AND EFFICIENT FOUNDATION LANGUAGE MODELS">
  <data key="d0">IS</data>
</edge>
<edge source="53" target="ASHISH VASWANI">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="NOAM SHAZEER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="NIKI PARMAR">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="JAKOB USZKOREIT">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="LLION JONES">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="AIDAN N GOMEZ">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="UKASZ KAISER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="53" target="ILLIA POLOSUKHIN">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="ATTENTION" target="ALL YOU NEED">
  <data key="d0">IS</data>
</edge>
<edge source="PACMAN" target="AN EFFICIENT COMPACTION APPROACH">
  <data key="d0">IS</data>
</edge>
<edge source="PACMAN" target="LOG-STRUCTURED KEY-VALUE STORE">
  <data key="d0">APPLIES TO</data>
</edge>
<edge source="LOG-STRUCTURED KEY-VALUE STORE" target="PERSISTENT MEMORY">
  <data key="d0">OPERATES ON</data>
</edge>
<edge source="SUPERNEURONS" target="DYNAMIC GPU MEMORY MANAGEMENT FOR TRAINING DEEP NEURAL NETWORKS">
  <data key="d0">IS</data>
</edge>
<edge source="LIGHTSEQ" target="A HIGH PERFORMANCE INFERENCE LIBRARY FOR TRANSFORMERS">
  <data key="d0">IS</data>
</edge>
<edge source="2021 CONFERENCE" target="NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
  <data key="d0">IS OF</data>
</edge>
<edge source="NORTH AMERICAN CHAPTER" target="ASSOCIATION FOR COMPUTATIONAL LINGUISTICS">
  <data key="d0">IS OF</data>
</edge>
<edge source="SELF-INSTRUCT" target="ALIGNING LANGUAGE MODEL WITH SELF GENERATED INSTRUCTIONS">
  <data key="d0">IS ABOUT</data>
</edge>
<edge source="GOOGLE'S NEURAL MACHINE TRANSLATION SYSTEM" target="THE GAP BETWEEN HUMAN AND MACHINE TRANSLATION">
  <data key="d0">BRIDGES</data>
</edge>
<edge source="60 GYEONG-IN YU" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="JOO SEONG JEONG" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="GEON-WOO KIM" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="SOOJEONG KIM" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="BYUNG-GON CHUN" target="AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="61" target="HONG ZHANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="61" target="YUPENG TANG">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="61" target="ANURAG KHANDELWAL">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="61" target="ION STOICA">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="SHEPHERD" target="DNNS IN THE WILD">
  <data key="d0">SERVES</data>
</edge>
<edge source="USENIX ASSOCIATION" target="BOSTON, MA">
  <data key="d0">LOCATION</data>
</edge>
<edge source="USENIX ASSOCIATION" target="787808">
  <data key="d0">ZIP CODE</data>
</edge>
<edge source="SUSAN ZHANG" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="STEPHEN ROLLER" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="NAMAN GOYAL" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="MIKEL ARTETXE" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="MOYA CHEN" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="SHUOHUI CHEN" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="CHRISTOPHER DEWAN" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="MONA DIAB" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="XIAN LI" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="XI VICTORIA LIN" target="PRESENTER AT NSDI23">
  <data key="d0">IS A</data>
</edge>
<edge source="NSDI23" target="CONFERENCE">
  <data key="d0">IS A</data>
</edge>
<edge source="HTTPS://WWW.USENIX.ORG/CONFERENCE/NSDI23/PRESENTATION/ZHANG-HONG" target="URL">
  <data key="d0">IS A</data>
</edge>
<edge source="ALPA" target="AUTOMATING INTER-AND INTRA-OPERATOR PARALLELISM FOR DISTRIBUTED DEEP LEARNING">
  <data key="d0">IS</data>
</edge>
<edge source="PETS" target="A UNIFIED FRAMEWORK FOR PARAMETER-EFFICIENT TRANSFORMERS SERVING">
  <data key="d0">IS</data>
</edge>
<edge source="1032 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS" target="A JOURNAL VOLUME">
  <data key="d0">IS</data>
</edge>
<edge source="1034 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS" target="VOL.">
  <data key="d0">IS</data>
</edge>
<edge source="IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS" target="VOL.">
  <data key="d0">HAS VOLUME</data>
</edge>
<edge source="1038 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS" target="VOL.">
  <data key="d0">IS</data>
</edge>
<edge source="1040 IEEE TRANSACTIONS" target="VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS">
  <data key="d0">IS ON</data>
</edge>
<edge source="1040 IEEE TRANSACTIONS" target="VOL.">
  <data key="d0">IS</data>
</edge>
<edge source="1042 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS" target="VOL.">
  <data key="d0">IS</data>
</edge>
<edge source="1044 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS" target="A JOURNAL">
  <data key="d0">IS</data>
</edge>
<edge source="1044 IEEE TRANSACTIONS ON VERY LARGE SCALE INTEGRATION (VLSI) SYSTEMS" target="VOL.">
  <data key="d0">HAS VOLUME</data>
</edge>
<edge source="PROCESSORS" target="OPEN RISC-V INSTRUCTION SET ARCHITECTURE (ISA)">
  <data key="d0">USE</data>
</edge>
<edge source="OPEN RISC-V INSTRUCTION SET ARCHITECTURE (ISA)" target="FINDING INCREASING ADOPTION">
  <data key="d0">IS</data>
</edge>
<edge source="OPEN RISC-V INSTRUCTION SET ARCHITECTURE (ISA)" target="EMBEDDED WORLD">
  <data key="d0">IS USED IN</data>
</edge>
<edge source="CV32E40P" target="AN INDUSTRIALLY SUPPORTED OPEN-SOURCE 32-BIT MICROCONTROLLER UNIT (MCU)-CLASS RISC-V CORE">
  <data key="d0">IS</data>
</edge>
<edge source="CV32E40P" target="AN INDUSTRIALLY SUPPORTED OPEN-SOURCE CORE">
  <data key="d0">IS</data>
</edge>
<edge source="FASTIRQ" target="A CUSTOM EXTENSION">
  <data key="d0">IS</data>
</edge>
<edge source="FASTIRQ" target="INTERRUPT LATENCY AS LOW AS SIX CYCLES">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="FASTIRQ" target="THE EXTENSION PROPOSED IN THIS WORK">
  <data key="d0">IS</data>
</edge>
<edge source="FASTIRQ" target="CV32RTFASTIRQ">
  <data key="d0">IS ALSO KNOWN AS</data>
</edge>
<edge source="FASTIRQ" target="A FAST INTERRUPT EXTENSION">
  <data key="d0">IS</data>
</edge>
<edge source="FASTIRQ" target="RISC-V EMBEDDED SYSTEMS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="FASTIRQ" target="INTERRUPT LATENCY">
  <data key="d0">REDUCES</data>
</edge>
<edge source="FASTIRQ" target="HIDING THE LATENCY THROUGH MEMORY BANKS">
  <data key="d0">REDUCES INTERRUPT LATENCY BY</data>
</edge>
<edge source="FASTIRQ" target="A BACKGROUND-SAVING MECHANISM">
  <data key="d0">REDUCES INTERRUPT LATENCY BY</data>
</edge>
<edge source="FASTIRQ" target="BY MOVING THE INTERRUPT STATE SAVING LOGIC IN HW">
  <data key="d0">IMPROVES UPON THAT</data>
</edge>
<edge source="FASTIRQ" target="EMRET TO HANDLE REDUNDANT INTERRUPT CONTEXT SEQUENCES">
  <data key="d0">ADDS</data>
</edge>
<edge source="FASTIRQ" target="A WRAPPER AROUND THE CORES RF">
  <data key="d0">IS</data>
</edge>
<edge source="FASTIRQ" target="US TO SKIP AHEAD THE SAVING OF THE GENERAL-PURPOSE REGISTERS">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="FASTIRQ" target="AN SW INTERRUPT">
  <data key="d0">TRIGGERS</data>
</edge>
<edge source="FASTIRQ" target="20 FASTER THAN AN SW-ONLY APPROACH">
  <data key="d0">IS</data>
</edge>
<edge source="CV32RT" target="A 32-BIT RISC-V CORE">
  <data key="d0">IS</data>
</edge>
<edge source="CV32RT" target="THE INTERRUPT HANDLING CAPABILITIES OF CV32E40P">
  <data key="d0">EXTENDS</data>
</edge>
<edge source="CV32RT" target="BEST-IN-CLASS INTERRUPT LATENCY">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="CV32RT" target="FAST CONTEXT SWITCHING">
  <data key="d0">ACHIEVES</data>
</edge>
<edge source="CV32RT" target="THE ROAD FOR RISC-V ARCHITECTURES IN TIME-CRITICAL SYSTEMS">
  <data key="d0">PAVES</data>
</edge>
<edge source="CV32RT" target="CV32 CORE AS THE BASELINE">
  <data key="d0">STARTS WITH</data>
</edge>
<edge source="CV32RT" target="CLINT INTERRUPT CONTROLLER WITH CLIC">
  <data key="d0">REPLACES</data>
</edge>
<edge source="CV32RT" target="FASTIRQ INTO THE CORE MICROARCHITECTURE">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="CV32RT" target="A 32-BIT, IN-ORDER, SINGLE-ISSUE CORE">
  <data key="d0">IS</data>
</edge>
<edge source="CV32RT" target="THE RISC-V CLIC FAST INTERRUPT CONTROLLER">
  <data key="d0">IS DESIGNED WITH</data>
</edge>
<edge source="CV32RT" target="THE FIRST FULLY OPEN-SOURCE RV32 CORE">
  <data key="d0">IS</data>
</edge>
<edge source="CV32RT" target="COMPETITIVE INTERRUPT-HANDLING FEATURES">
  <data key="d0">HAS</data>
</edge>
<edge source="CV32RT" target="FAST INTERRUPT AND CONTEXT SWITCHING">
  <data key="d0">ENABLES</data>
</edge>
<edge source="CV32RT" target="FAST INTERRUPT AND CONTEXT SWITCHING 1043 INSTRUCTION IN MACHINE MODE">
  <data key="d0">ENABLES</data>
</edge>
<edge source="CV32RT" target="CONTROLPULP">
  <data key="d0">IS PART OF</data>
</edge>
<edge source="FAST CONTEXT SWITCHING" target="ARCHITECTURES SUCH AS SUPERSCALAR CENTRAL PROCESSING UNITS (CPUS)">
  <data key="d0">IS OFTEN REQUIRED IN</data>
</edge>
<edge source="FAST CONTEXT SWITCHING" target="HIDE LATENCY IN GRAPHIC PROCESSING UNITS (GPUS)">
  <data key="d0">IS OFTEN REQUIRED TO</data>
</edge>
<edge source="FULL-SYSTEM PLATFORM" target="PROPOSED INTERRUPT EXTENSION">
  <data key="d0">IS USED TO DESIGN AND IMPLEMENT</data>
</edge>
<edge source="PROPOSED INTERRUPT EXTENSION" target="SECTION II-A">
  <data key="d0">IS DESCRIBED IN</data>
</edge>
<edge source="RELEVANT TARGET METRICS" target="SECTION II-B3">
  <data key="d0">ARE MOTIVATED AND EXPLAINED IN</data>
</edge>
<edge source="CURRENT STATUS OF INTERRUPT HANDLING IN RISC-V" target="SECTION II-C">
  <data key="d0">IS DESCRIBED IN</data>
</edge>
<edge source="CV32E40P CORE" target="CV32">
  <data key="d0">IS ABBREVIATED TO</data>
</edge>
<edge source="CV32E40P CORE" target="AN OPEN-SOURCE, INDUSTRY-GRADE, 32-BIT, IN-ORDER, FOUR-STAGE RISC-V CORE">
  <data key="d0">IS</data>
</edge>
<edge source="CV32E40P CORE" target="IMPLEMENTING OUR EXTENSIONS">
  <data key="d0">IS RELIED ON AS BASIS FOR</data>
</edge>
<edge source="CV32 CORE" target="CLINT INTERRUPT CONTROLLER">
  <data key="d0">HAS NATIVE</data>
</edge>
<edge source="CLIC" target="CV32RTCLIC">
  <data key="d0">IS ALSO KNOWN AS</data>
</edge>
<edge source="CLIC" target="THE PROPOSED">
  <data key="d0">IS IMPLEMENTED IN</data>
</edge>
<edge source="CV32RTCLIC" target="CLIC FAST INTERRUPT CONTROLLER">
  <data key="d0">IS</data>
</edge>
<edge source="CV32RTFASTIRQ" target="INTERRUPT LATENCY">
  <data key="d0">OPTIMIZES</data>
</edge>
<edge source="CV32RTFASTIRQ" target="THE NON-NESTED INTERRUPT CASE">
  <data key="d0">OPTIMIZES INTERRUPT LATENCY IN</data>
</edge>
<edge source="CV32RTFASTIRQ" target="BANK SWITCHING AND THE NESTED INTERRUPT CASE">
  <data key="d0">COMBINES</data>
</edge>
<edge source="CV32RTFASTIRQ" target="AN AUTOMATIC CONTEXT-SAVING MECHANISM IN THE BACKGROUND">
  <data key="d0">USES</data>
</edge>
<edge source="CLIC INTERRUPT CONTROLLER" target="DRAFT SPECIFICATION">
  <data key="d0">IS ACCORDING TO</data>
</edge>
<edge source="DRAFT SPECIFICATION" target="RISC-V PRIVILEGED SPECIFICATION 14">
  <data key="d0">IS TO BE INCLUDED IN</data>
</edge>
<edge source="FASTIRQ EXTENSION" target="WEAKNESSES OF RISC-V CORE-SPECIFIC INTERRUPT CONTROLLERS">
  <data key="d0">ADDRESSES</data>
</edge>
<edge source="FASTIRQ EXTENSION" target="CLIC BASE CAPABILITIES">
  <data key="d0">EXTENDS</data>
</edge>
<edge source="FASTIRQ EXTENSION" target="INTERRUPT LATENCY">
  <data key="d0">LOWERS</data>
</edge>
<edge source="FASTIRQ EXTENSION" target="HW VECTORED INTERRUPTS">
  <data key="d0">KEEPS</data>
</edge>
<edge source="FASTIRQ EXTENSION" target="SKIPPING OF REDUNDANT CONTEXT RESTORE OPERATIONS">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="FASTIRQ EXTENSION" target="INTERRUPT LATENCY TO SIX CYCLES">
  <data key="d0">IS ABLE TO REDUCE</data>
</edge>
<edge source="INTERRUPT LATENCY" target="SW-DEPENDENT CONTRIBUTIONS">
  <data key="d0">BREAKS DOWN INTO</data>
</edge>
<edge source="INTERRUPT LATENCY" target="HW-DEPENDENT CONTRIBUTIONS">
  <data key="d0">BREAKS DOWN INTO</data>
</edge>
<edge source="INTERRUPT LATENCY" target="VARIOUS FLAVORS OF RISC-V CORE-SPECIFIC INTERRUPT CONTROLLERS">
  <data key="d0">IS OF</data>
</edge>
<edge source="INTERRUPT LATENCY" target="ITS HW-CONTRIBUTED PART">
  <data key="d0">MEAN</data>
</edge>
<edge source="INTERRUPT LATENCY" target="THE TIME IT TAKES FROM AN INTERRUPT EDGE ARRIVING AT THE HW TO THE EXECUTION OF THE FIRST INSTRUCTION OF THE CORRESPONDING INTERRUPT HANDLER ROUTINE">
  <data key="d0">IS DEFINED AS</data>
</edge>
<edge source="INTERRUPT LATENCY" target="NUMBER OF CYCLES IT TAKES FOR AN INTERRUPT TO ARRIVE AT THE INTERRUPT CONTROLLER INPUT TO THE FIRST INSTRUCTION OF AN INTERRUPT HANDLER THAT ALLOWS THE CALLING OF A C-FUNCTION">
  <data key="d0">IS MEASURED AS</data>
</edge>
<edge source="INTERRUPT LATENCY" target="HIGHER INTERRUPT LOADS">
  <data key="d0">IS IMPACTED ON</data>
</edge>
<edge source="INTERRUPT LATENCY" target="A SINGLE-CYCLE MEMORY 24">
  <data key="d0">IS GIVEN</data>
</edge>
<edge source="INTERRUPT LATENCY" target="DEFINITION PRESENTED IN SECTION II-B">
  <data key="d0">HAS DEFINITION</data>
</edge>
<edge source="INTERRUPT LATENCY" target="ARM CORTEX-M PROCESSORS">
  <data key="d0">IS RELATED TO</data>
</edge>
<edge source="THE EXTENSION" target="CV32RT">
  <data key="d0">IS IMPLEMENTED ON</data>
</edge>
<edge source="THE EXTENSION" target="NEGLIGIBLE AREA OVERHEAD IN A MODERN TECHNOLOGY NODE">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="THE EXTENSION" target="PERFORMANCE BENEFITS">
  <data key="d0">PROVIDES</data>
</edge>
<edge source="27 B. MAO, N. TAN, T. CHONG, AND L. LI" target="A CLIC EXTENSION BASED FAST INTERRUPT SYSTEM FOR EMBEDDED RISC-V PROCESSORS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="A CLIC EXTENSION BASED FAST INTERRUPT SYSTEM" target="EMBEDDED RISC-V PROCESSORS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="A CLIC EXTENSION BASED FAST INTERRUPT SYSTEM" target="PROC.">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="MANY EMBEDDED USE CASES" target="REAL-TIME CONSTRAINTS">
  <data key="d0">HAVE</data>
</edge>
<edge source="MANY EMBEDDED USE CASES" target="FLEXIBLE, PREDICTABLE, AND FAST REACTIVE HANDLING OF INCOMING EVENTS">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="RISC-V PROCESSORS" target="THIS AREA">
  <data key="d0">ARE LAGGING IN</data>
</edge>
<edge source="RISC-V PROCESSORS" target="MORE MATURE PROPRIETARY ARCHITECTURES">
  <data key="d0">ARE LAGGING COMPARED TO</data>
</edge>
<edge source="MORE MATURE PROPRIETARY ARCHITECTURES" target="ARM CORTEX-M">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="MORE MATURE PROPRIETARY ARCHITECTURES" target="TRICORE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="ARM CORTEX-M" target="16 CORE REGISTERS">
  <data key="d0">HAS</data>
</edge>
<edge source="ARM CORTEX-M AND TRICORE" target="TUNED FOR YEARS">
  <data key="d0">HAVE BEEN</data>
</edge>
<edge source="THE DEFAULT INTERRUPT CONTROLLER" target="RISC-V">
  <data key="d0">IS STANDARDIZED BY</data>
</edge>
<edge source="THE DEFAULT INTERRUPT CONTROLLER" target="THE CORE LOCAL INTERRUPTOR (CLINT)">
  <data key="d0">IS CALLED</data>
</edge>
<edge source="RISC-V" target="PLATFORM-LEVEL INTERRUPT CONTROLLER SPECIFICATION">
  <data key="d0">HAS</data>
</edge>
<edge source="RISC-V" target="RV32E AS A VARIANT OF RV32I">
  <data key="d0">DEFINES</data>
</edge>
<edge source="THE CORE LOCAL INTERRUPTOR (CLINT)" target="CONFIGURABILITY IN PRIORITIZATION AND PREEMPTION OF INTERRUPTS">
  <data key="d0">LACKS</data>
</edge>
<edge source="THE RISC-V CORE LOCAL INTERRUPT CONTROLLER (CLIC) SPECIFICATION" target="THIS CONCERN">
  <data key="d0">ADDRESSES</data>
</edge>
<edge source="THE RISC-V CORE LOCAL INTERRUPT CONTROLLER (CLIC) SPECIFICATION" target="PREEMPTIBLE, LOW-LATENCY VECTORED INTERRUPTS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="THE RISC-V CORE LOCAL INTERRUPT CONTROLLER (CLIC) SPECIFICATION" target="OPTIONAL EXTENSIONS TO IMPROVE INTERRUPT LATENCY">
  <data key="d0">ENVISIONS</data>
</edge>
<edge source="THE RISC-V COMMUNITY" target="AN EXTENSION TO THE PRIVILEGED SPECIFICATIONS 14">
  <data key="d0">HAS BEEN DEVELOPING</data>
</edge>
<edge source="THE RISC-V COMMUNITY" target="THE RISC-V CORE LOCAL INTERRUPT CONTROLLER (CLIC) 15">
  <data key="d0">HAS PROPOSED</data>
</edge>
<edge source="THE RISC-V CORE LOCAL INTERRUPT CONTROLLER (CLIC) 15" target="CURRENTLY UNDER RATIFICATION BY THE COMMUNITY">
  <data key="d0">IS</data>
</edge>
<edge source="THE RISC-V CORE LOCAL INTERRUPT CONTROLLER (CLIC) 15" target="REAL-TIME SCENARIOS">
  <data key="d0">IS DEVELOPED TO HANDLE</data>
</edge>
<edge source="CLICS" target="LOCAL TO EACH HARDWARE THREAD (HART)">
  <data key="d0">ARE</data>
</edge>
<edge source="PLATFORM LEVEL INTERRUPT CONTROLLERS (PLICS)" target="CENTRALIZED INTERRUPT CONTROLLERS">
  <data key="d0">ARE</data>
</edge>
<edge source="PLATFORM LEVEL INTERRUPT CONTROLLERS (PLICS)" target="MANAGING MULTIPLE HARTS">
  <data key="d0">ARE CAPABLE OF</data>
</edge>
<edge source="PLATFORM-LEVEL INTERRUPT CONTROLLERS" target="THIS CATEGORY">
  <data key="d0">BELONG TO CATEGORY</data>
</edge>
<edge source="PLATFORM-LEVEL INTERRUPT CONTROLLERS" target="DISTRIBUTE TIME-CRITICAL INTERRUPTS TO THE RUNNING HARTS">
  <data key="d0">ARE NOT DESIGNED TO</data>
</edge>
<edge source="RISC-V PLIC" target="PLATFORM-LEVEL INTERRUPT CONTROLLER">
  <data key="d0">IS A</data>
</edge>
<edge source="ADVANCED PLIC (APLIC)" target="PLATFORM-LEVEL INTERRUPT CONTROLLER">
  <data key="d0">IS A</data>
</edge>
<edge source="RISC-V INCOMING MESSAGE SIGNALED INTERRUPT CONTROLLER (IMSIC)" target="PLATFORM-LEVEL INTERRUPT CONTROLLER">
  <data key="d0">IS A</data>
</edge>
<edge source="RISC-V INCOMING MESSAGE SIGNALED INTERRUPT CONTROLLER (IMSIC)" target="MESSAGE-SIGNALED INTERRUPT COMMUNICATION">
  <data key="d0">IS USED FOR</data>
</edge>
<edge source="RISC-V PLIC AND ADVANCED PLIC (APLIC)" target="WIRE-BASED INTERRUPT COMMUNICATION">
  <data key="d0">ARE USED FOR</data>
</edge>
<edge source="B. CORE-LOCAL INTERRUPT CONTROLLERS CLICS" target="PROVIDING FAST INTERRUPT-HANDLING CAPABILITIES">
  <data key="d0">ARE OFTEN SPECIALIZED IN</data>
</edge>
<edge source="B. CORE-LOCAL INTERRUPT CONTROLLERS CLICS" target="REAL-TIME EMBEDDED APPLICATION DOMAINS">
  <data key="d0">ARE SPECIALIZED IN</data>
</edge>
<edge source="RISC-V SMCLIC CORE-LOCAL INTERRUPT CONTROLLER (CLIC)" target="RISC-V PRIVILEGED ARCHITECTURE EXTENSION">
  <data key="d0">IS</data>
</edge>
<edge source="DOCUMENT" target="HTTP://WWW2.EECS.BERKELEY.EDU/PUB/STECHRPTS/2016/EECS-2016-129.HTML">
  <data key="d0">IS AVAILABLE AT</data>
</edge>
<edge source="DOCUMENT" target="HTTP:WEB.ENGR.OREGONSTATE.EDU/TRAYLORECE473 PDFSMINIMIZEINTERRUPTRESPONSETIME.PDF">
  <data key="d0">AVAILABLE AT</data>
</edge>
<edge source="DOCUMENT" target="352">
  <data key="d0">HAS PAGE</data>
</edge>
<edge source="DOCUMENT" target="10.11451146909.1147001">
  <data key="d0">HAS DOI</data>
</edge>
<edge source="DOCUMENT" target="HTTPS:DOC.NUCLEISYS.COMNUCLEISPECISAINTRODUCTION.HTML">
  <data key="d0">IS AVAILABLE AT</data>
</edge>
<edge source="COMPETITIVE INTERRUPT-HANDLING FEATURES" target="ARM CORTEX-M SERIES">
  <data key="d0">ARE COMPARED TO</data>
</edge>
<edge source="COMPETITIVE INTERRUPT-HANDLING FEATURES" target="TRICORE">
  <data key="d0">ARE COMPARED TO</data>
</edge>
<edge source="ARM CORTEX-M SERIES" target="A REFERENCE EXAMPLE IN THE FIELD">
  <data key="d0">IS</data>
</edge>
<edge source="ARM CORTEX-M SERIES" target="NESTED VECTORED INTERRUPT CONTROLLER (NVIC)">
  <data key="d0">INTEGRATES</data>
</edge>
<edge source="VARIOUS CV32RT VERSIONS" target="INTERRUPT LATENCY AND CONTEXT SWITCH TIMES">
  <data key="d0">PERFORM IN TERMS OF</data>
</edge>
<edge source="THESE ADDITIONS" target="OVERHEAD IN TERMS OF AREA AND TIMING">
  <data key="d0">INCUR</data>
</edge>
<edge source="THE PROPOSED EXTENSIONS" target="TASK CONTEXT SWITCHING IN REAL-TIME OPERATING SYSTEMS (RTOSS)">
  <data key="d0">ARE DEMONSTRATED TO IMPROVE</data>
</edge>
<edge source="INDEX TERMS" target="CONTEXT SWITCHING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="INDEX TERMS" target="EMBEDDED">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="INDEX TERMS" target="INTERRUPT LATENCY">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="INDEX TERMS" target="MICROCONTROLLER UNIT (MCU)">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="INDEX TERMS" target="REAL-TIME">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="INDEX TERMS" target="RISC-V">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="CONTEXT SWITCHING" target="EMBEDDED PROCESSORS">
  <data key="d0">OCCURS ON</data>
</edge>
<edge source="SEVERAL MARKETS" target="REAL-TIME AN SW-BASED SOLUTION">
  <data key="d0">RELY ON</data>
</edge>
<edge source="THE AUTOMOTIVE INDUSTRY" target="HUNDREDS OF ELECTRONIC CONTROL UNITS (ECUS)">
  <data key="d0">EMPLOYS</data>
</edge>
<edge source="ELECTRONIC CONTROL UNITS (ECUS)" target="REAL-TIME APPLICATIONS">
  <data key="d0">ARE USED FOR</data>
</edge>
<edge source="REAL-TIME APPLICATIONS" target="ELECTRONIC ENGINE CONTROL">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="REAL-TIME APPLICATIONS" target="GEARBOX CONTROL">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="REAL-TIME APPLICATIONS" target="CRUISE CONTROL">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="REAL-TIME APPLICATIONS" target="ANTI-LOCK BRAKE SYSTEMS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="REAL-TIME APPLICATIONS" target="MANY OTHER TASKS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="GENERAL-PURPOSE OPERATING SYSTEMS (GPOSS)" target="AVERAGE THROUGHPUT">
  <data key="d0">ARE TYPICALLY TUNED FOR</data>
</edge>
<edge source="THE HORIZON KEY DIGITAL TECHNOLOGIES JOINT UNDERTAKING (KDT JU) PROGRAMME" target="THE TRISTAN PROJECT">
  <data key="d0">SUPPORTED THROUGH</data>
</edge>
<edge source="THE TRISTAN PROJECT" target="101095947">
  <data key="d0">IS UNDER GRANT</data>
</edge>
<edge source="ROBERT BALAS" target="CORRESPONDING AUTHOR">
  <data key="d0">IS</data>
</edge>
<edge source="ROBERT BALAS" target="INTEGRATED SYSTEMS LABORATORY (IIS)">
  <data key="d0">IS WITH</data>
</edge>
<edge source="ROBERT BALAS" target="GRADUATE STUDENT MEMBER OF IEEE">
  <data key="d0">IS</data>
</edge>
<edge source="ROBERT BALAS" target="B.SC.">
  <data key="d0">RECEIVED</data>
</edge>
<edge source="INTEGRATED SYSTEMS LABORATORY (IIS)" target="ETH ZURICH">
  <data key="d0">IS AT</data>
</edge>
<edge source="ALESSANDRO OTTAVIANO" target="INTEGRATED SYSTEMS LABORATORY (IIS)">
  <data key="d0">IS WITH</data>
</edge>
<edge source="ALESSANDRO OTTAVIANO" target="THE B.SC.">
  <data key="d0">RECEIVED</data>
</edge>
<edge source="ETH ZURICH" target="8092 ZURICH">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="ETH ZURICH" target="ZURICH, SWITZERLAND">
  <data key="d0">IS IN</data>
</edge>
<edge source="8092 ZURICH" target="SWITZERLAND">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="LUCA BENINI" target="THE INTEGRATED SYSTEMS LABORATORY (IIS)">
  <data key="d0">IS WITH</data>
</edge>
<edge source="LUCA BENINI" target="THE DEPARTMENT OF ELECTRICAL, ELECTRONIC AND INFORMATION ENGINEERING (DEI)">
  <data key="d0">IS WITH</data>
</edge>
<edge source="LUCA BENINI" target="FELLOW OF IEEE">
  <data key="d0">IS</data>
</edge>
<edge source="LUCA BENINI" target="PH.D.">
  <data key="d0">RECEIVED</data>
</edge>
<edge source="THE DEPARTMENT OF ELECTRICAL, ELECTRONIC AND INFORMATION ENGINEERING (DEI)" target="UNIVERSITY OF BOLOGNA">
  <data key="d0">IS AT</data>
</edge>
<edge source="UNIVERSITY OF BOLOGNA" target="40126 BOLOGNA, ITALY">
  <data key="d0">IS IN</data>
</edge>
<edge source="COLOR VERSIONS OF ONE OR MORE FIGURES IN THIS ARTICLE" target="HTTPS:DOI.ORG10.1109TVLSI.2024.3377130">
  <data key="d0">ARE AVAILABLE AT</data>
</edge>
<edge source="DIGITAL OBJECT IDENTIFIER" target="10.1109TVLSI.2024.3377130">
  <data key="d0">IS</data>
</edge>
<edge source="THE DEVELOPMENT OF LINUX" target="AVERAGE PERFORMANCE">
  <data key="d0">IS FOCUSED ON</data>
</edge>
<edge source="LINUX" target="POPULAR OPEN-SOURCE GPOS KERNEL">
  <data key="d0">IS A</data>
</edge>
<edge source="LINUX" target="REAL-TIME APPLICATIONS">
  <data key="d0">IS LESS SUITABLE TO BE USED FOR</data>
</edge>
<edge source="EXTENSIONS AND MODIFICATIONS" target="IMPROVING DETERMINISM AND LATENCIES OF CRITICAL OPERATIONS IN LINUX">
  <data key="d0">AIM AT</data>
</edge>
<edge source="EXTENSIONS AND MODIFICATIONS" target="PROPOSED AND IMPLEMENTED">
  <data key="d0">HAVE BEEN</data>
</edge>
<edge source="EXTENSIONS AND MODIFICATIONS" target="STRICT BOUNDS ON MAXIMUM LATENCIES OF OPERATIONS">
  <data key="d0">DO NOT GUARANTEE</data>
</edge>
<edge source="EXTENSIONS AND MODIFICATIONS" target="INDUSTRY-GRADE MATURITY">
  <data key="d0">LACK</data>
</edge>
<edge source="INDUSTRY-GRADE MATURITY" target="EMPLOYED IN HARD REAL-TIME SCENARIOS">
  <data key="d0">IS REQUIRED TO BE</data>
</edge>
<edge source="REAL-TIME OPERATING SYSTEMS (RTOSS) KERNELS" target="SPECIAL-PURPOSE OPERATING SYSTEMS (OSS)">
  <data key="d0">ARE</data>
</edge>
<edge source="REAL-TIME OPERATING SYSTEMS (RTOSS) KERNELS" target="REAL-TIME GUARANTEES">
  <data key="d0">ARE DESIGNED TO PROVIDE</data>
</edge>
<edge source="REAL-TIME GUARANTEES" target="TASK SCHEDULING ACCORDING TO A GIVEN EXPECTED COMPLETION DEADLINE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="REAL-TIME GUARANTEES" target="DETERMINISTIC LATENCIES OF VARIOUS OPERATIONS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="A. STANKOVIC" target="SCHEDULING ALGORITHMS AND OPERATING SYSTEMS SUPPORT FOR REAL-TIME SYSTEMS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="SCHEDULING ALGORITHMS AND OPERATING SYSTEMS SUPPORT FOR REAL-TIME SYSTEMS" target="PROC.">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="THE RTOS SCHEDULER" target="A SIGNIFICANT OVERHEAD">
  <data key="d0">MIGHT ADD</data>
</edge>
<edge source="THE SIGNIFICANT OVERHEAD" target="THE COMBINED EFFECT OF BOTH THE CONTEXT SWITCHES REQUIRED TO HANDLE THE TRANSITION FROM A FOREGROUND TO A BACKGROUND TASK AND THE AMOUNT OF TIME ELAPSED FROM THE SOURCE EVENT THAT CAUSES THE PREEMPTION AND THE FIRST INSTRUCTION OF THE AWAKENED TASK">
  <data key="d0">IS DUE TO</data>
</edge>
<edge source="THE SIGNIFICANT OVERHEAD" target="THE WORST CASE EXECUTION TIME (WCET) 2, 8, 9">
  <data key="d0">THUS INCREASES</data>
</edge>
<edge source="THE FIRST INSTRUCTION OF THE AWAKENED TASK" target="INTERRUPT LATENCY 7">
  <data key="d0">IS KNOWN AS</data>
</edge>
<edge source="THE COST OF SAVING AND RESTORING THE TASK STATE DURING A CONTEXT SWITCH" target="A SIGNIFICANT CONCERN">
  <data key="d0">IS</data>
</edge>
<edge source="THE COST OF SAVING AND RESTORING THE TASK STATE DURING A CONTEXT SWITCH" target="RELATIVELY HIGH">
  <data key="d0">REMAINS</data>
</edge>
<edge source="LONG CONTEXT SWITCH TIMES" target="AVAILABLE TASK UTILIZATION">
  <data key="d0">REDUCE</data>
</edge>
<edge source="LONG CONTEXT SWITCH TIMES" target="THE MINIMUM VIABLE SWITCHING GRANULARITY">
  <data key="d0">REDUCE</data>
</edge>
<edge source="PROCESS STATE" target="CONTEXT SWITCH">
  <data key="d0">NEEDS TO BE SAVED ON</data>
</edge>
<edge source="PROCESS STATE" target="PROGRAM COUNTER">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PROCESS STATE" target="REGISTER FILES (RFS)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PROCESS STATE" target="STATUS REGISTERS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PROCESS STATE" target="ADDRESS SPACE MAPPING">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="MEMORY ACCESS OPERATIONS" target="STORE THE STATE OF THE PREEMPTED TASK">
  <data key="d0">NEED TO BE PERFORMED TO</data>
</edge>
<edge source="MEMORY ACCESS OPERATIONS" target="RESTORE THE STATE OF THE NEW TASK TO BE EXECUTED">
  <data key="d0">NEED TO BE PERFORMED TO</data>
</edge>
<edge source="A SWITCH INTO AN INTERRUPT CONTEXT" target="NORMAL PROGRAM EXECUTION">
  <data key="d0">HAPPENS FROM</data>
</edge>
<edge source="A SWITCH INTO AN INTERRUPT CONTEXT" target="AN ASYNCHRONOUS EVENT IS TRIGGERED">
  <data key="d0">HAPPENS EACH TIME</data>
</edge>
<edge source="AN ASYNCHRONOUS EVENT" target="AN IO PERIPHERAL DEVICE">
  <data key="d0">IS TRIGGERED FROM</data>
</edge>
<edge source="HW-INDUCED INTERRUPT LATENCY" target="ONLY ONE PART OF THE PROBLEM">
  <data key="d0">IS</data>
</edge>
<edge source="SW-INDUCED INTERRUPT LATENCY" target="PART OF THE PROBLEM">
  <data key="d0">IS</data>
</edge>
<edge source="PERSONAL USE" target="PERMITTED">
  <data key="d0">IS</data>
</edge>
<edge source="REPUBLICATIONREDISTRIBUTION" target="IEEE PERMISSION">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="HTTPS://WWW.IEEE.ORG/PUBLICATIONS/RIGHTS/INDEX.HTML" target="MORE INFORMATION">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="IEEE" target="39TH INT.">
  <data key="d0">IS</data>
</edge>
<edge source="AUTHORIZED LICENSED USE" target="CALIFORNIA POLYTECHNIC STATE UNIVERSITY SAN LUIS OBISPO">
  <data key="d0">IS LIMITED TO</data>
</edge>
<edge source="THE CLIC DESIGN" target="AUTHORIZED LICENSED USE LIMITED TO CALIFORNIA POLYTECHNIC STATE UNIVERSITY SAN LUIS OBISPO">
  <data key="d0">IS PROPOSED IN</data>
</edge>
<edge source="INTERRUPT STATE" target="PUSHED">
  <data key="d0">GETS</data>
</edge>
<edge source="THE LATTER" target="THE EXECUTION TIME OF THE AUTHORIZED LICENSED USE LIMITED TO CALIFORNIA POLYTECHNIC STATE UNIVERSITY SAN LUIS OBISPO">
  <data key="d0">REDUCES</data>
</edge>
<edge source="THE LATTER" target="THE RECEIVING SIDE OF THE INTERRUPT TO CLEAR THE SOURCE OFTEN THROUGH ACCESSING APPROPRIATE HW REGISTERS">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="THE LATTER" target="RESTORING THE INTERRUPT CONTEXT AND THE REGULAR INTERRUPT LATENCY">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="DATA" target="MAY 28, 2025 AT 17:50:39 UTC">
  <data key="d0">WAS DOWNLOADED ON</data>
</edge>
<edge source="DATA" target="IEEE XPLORE">
  <data key="d0">WAS DOWNLOADED FROM</data>
</edge>
<edge source="DATA" target="MAY 28, 2025">
  <data key="d0">WAS DOWNLOADED ON</data>
</edge>
<edge source="DATA" target="17:50:39 UTC">
  <data key="d0">WAS DOWNLOADED AT</data>
</edge>
<edge source="FAST INTERRUPT AND CONTEXT SWITCHING" target="GPOSRTOS SCHEDULER AND THE USER CODE">
  <data key="d0">IS INTRODUCED BY</data>
</edge>
<edge source="FAST INTERRUPT AND CONTEXT SWITCHING" target="CAPABILITY OF THE SYSTEM TO PROVIDE TIMELY RESPONSES TO ASYNCHRONOUS EVENTS">
  <data key="d0">IMPACTS</data>
</edge>
<edge source="TABLE I" target="NESTED INTERRUPT PREEMPTION SCHEME">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="TABLE I" target="PREEMPTION CONDITIONS OF TWO NESTED INTERRUPTS IRQ2 AND IRQ1">
  <data key="d0">SHOWS</data>
</edge>
<edge source="NESTED INTERRUPT PREEMPTION SCHEME" target="RISC-V CLIC">
  <data key="d0">IS ACCORDING TO</data>
</edge>
<edge source="C. CV32RTFASTIRQ" target="FAST INTERRUPT EXTENSION">
  <data key="d0">IS</data>
</edge>
<edge source="A BLOCK DIAGRAM OF CV32RTFASTIRQ" target="FIG.">
  <data key="d0">IS SHOWN IN</data>
</edge>
<edge source="SW INTERRUPT" target="WRITING TO CLICS MEMORY MAP">
  <data key="d0">IS TRIGGERED BY</data>
</edge>
<edge source="POINTER" target="APPROPRIATELY BEFORE TRIGGERING AN SW INTERRUPT">
  <data key="d0">IS SET</data>
</edge>
<edge source="FAST INTERRUPT AND CONTEXT SWITCHING 1043 INSTRUCTION IN MACHINE MODE" target="EMRET">
  <data key="d0">IS</data>
</edge>
<edge source="EMRET" target="REDUNDANT CONTEXT SAVING AND RESTORING SEQUENCES">
  <data key="d0">CAN SKIP</data>
</edge>
<edge source="EMRET" target="DIRECTLY JUMPING TO THE NEXT AVAILABLE INTERRUPT HANDLER">
  <data key="d0">SKIPS BY</data>
</edge>
<edge source="EMRET" target="TO DIFFERENTIATE BETWEEN A REGULAR RETURN FROM AN INTERRUPT HANDLER USING MRET">
  <data key="d0">IS ADDED</data>
</edge>
<edge source="EMRET" target="THE CONTROL FLOW TO THE PENDING INTERRUPTS HANDLER">
  <data key="d0">REDIRECTS</data>
</edge>
<edge source="LOW INTERRUPT LATENCY" target="CRUCIAL METRICS">
  <data key="d0">ARE</data>
</edge>
<edge source="CRUCIAL METRICS" target="A WIDE RANGE OF PLATFORMS">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="CONTEXT SWITCH TIME" target="CRUCIAL METRICS">
  <data key="d0">ARE</data>
</edge>
<edge source="CONTEXT SWITCH TIME" target="IN FIG.">
  <data key="d0">IS SHOWN</data>
</edge>
<edge source="PLATFORMS" target="COMMODITY MCU-CLASS EMBEDDED SYSTEMS">
  <data key="d0">RANGE FROM</data>
</edge>
<edge source="PLATFORMS" target="MORE ADVANCED AND COMPLEX APPLICATION-CLASS MIXED CRITICALITY SYSTEMS (MCSS)">
  <data key="d0">RANGE TO</data>
</edge>
<edge source="TIMESAFETY-CRITICAL AND NON-CRITICAL APPLICATIONS" target="DIFFERENT ISOLATED PARTITIONS">
  <data key="d0">COEXIST ON</data>
</edge>
<edge source="DIFFERENT ISOLATED PARTITIONS" target="THE SAME HW PLATFORM 12">
  <data key="d0">ARE OF</data>
</edge>
<edge source="RESPONSE AND CONTEXT SWITCH TIME MINIMIZATION" target="A CHALLENGE TO BE TACKLED AT THE HWSW INTERFACE">
  <data key="d0">BECOME</data>
</edge>
<edge source="SW PROGRAMMING TECHNIQUES AND HW INTERRUPT CONTROLLER ARCHITECTURES" target="TO ENSURE MINIMAL RESPONSE TIME">
  <data key="d0">CAN COOPERATE</data>
</edge>
<edge source="COMMERCIAL VENDORS AND IP PROVIDERS" target="SUCH FEATURES AS IN-HOUSE SOLUTIONS">
  <data key="d0">OFFER</data>
</edge>
<edge source="SUCH FEATURES AS IN-HOUSE SOLUTIONS" target="OFTEN PROPRIETARY">
  <data key="d0">ARE</data>
</edge>
<edge source="SUCH FEATURES AS IN-HOUSE SOLUTIONS" target="TIGHTLY COUPLED WITH THE VENDORS INSTRUCTION SET ARCHITECTURE (ISA)">
  <data key="d0">ARE</data>
</edge>
<edge source="SUCH FEATURES AS IN-HOUSE SOLUTIONS" target="TIGHTLY COUPLED WITH THE VENDORS TARGET HW FAMILY">
  <data key="d0">ARE</data>
</edge>
<edge source="SUCH FEATURES AS IN-HOUSE SOLUTIONS" target="TIGHTLY COUPLED WITH THE VENDORS ASSOCIATED SW STACK">
  <data key="d0">ARE</data>
</edge>
<edge source="RISC-V ECOSYSTEM 13" target="A MODULAR, FREE, AND OPEN-SOURCE ISA">
  <data key="d0">HAS BEEN OFFERING</data>
</edge>
<edge source="RISC-V ECOSYSTEM 13" target="THE DE FACTO LINGUA FRANCA OF COMPUTING">
  <data key="d0">IS BECOMING</data>
</edge>
<edge source="RISC-V SUPPORT FOR FAST INTERRUPT AND CONTEXT SWITCH HANDLING" target="INCUMBENT PROPRIETARY ARCHITECTURES">
  <data key="d0">IS NOT MATURE ENOUGH TO COMPETE WITH</data>
</edge>
<edge source="RISC-V SUPPORT FOR FAST INTERRUPT AND CONTEXT SWITCH HANDLING" target="FLEXIBLE INTERRUPT PRIORITIZATION">
  <data key="d0">LACKS</data>
</edge>
<edge source="RISC-V SUPPORT FOR FAST INTERRUPT AND CONTEXT SWITCH HANDLING" target="PREEMPTION MECHANISMS">
  <data key="d0">LACKS</data>
</edge>
<edge source="RISC-V SUPPORT FOR FAST INTERRUPT AND CONTEXT SWITCH HANDLING" target="LOW INTERRUPT LATENCY">
  <data key="d0">LACKS</data>
</edge>
<edge source="MODULAR RISC-V ISA" target="DEVELOPING ORTHOGONAL CUSTOM EXTENSIONS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="FEW PUBLISHED WORKS" target="THE PROBLEM OF MINIMIZING INTERRUPT LATENCY AND CONTEXT SWITCH TIME FROM A HOLISTIC (HW AND SW) VIEWPOINT FOR RISC-V">
  <data key="d0">TACKLE</data>
</edge>
<edge source="FEW PUBLISHED WORKS" target="THE GAP WITH MORE ESTABLISHED PROPRIETARY SOLUTIONS">
  <data key="d0">TRY TO CLOSE</data>
</edge>
<edge source="FEW PUBLISHED WORKS" target="AN OPEN-SOURCE SOLUTION TO BE SHARED WITH THE COMMUNITY">
  <data key="d0">PROVIDE</data>
</edge>
<edge source="CLOSING THE GAP WITH MORE ESTABLISHED PROPRIETARY SOLUTIONS" target="RISC-V AS A VALUABLE CANDIDATE FOR TIME- AND SAFETY-CRITICAL APPLICATION DOMAINS SUCH AS AUTOMOTIVE AND AEROSPACE">
  <data key="d0">PROMOTES</data>
</edge>
<edge source="INTERRUPT HANDLERS" target="NESTING">
  <data key="d0">SUPPORT</data>
</edge>
<edge source="INTERRUPT HANDLERS" target="CALLING OF C-FUNCTIONS WITHIN IT">
  <data key="d0">SUPPORT</data>
</edge>
<edge source="CALLING OF C-FUNCTIONS" target="SAVING AND RESTORING STATE FOLLOWING THE C-ABI">
  <data key="d0">INVOLVES</data>
</edge>
<edge source="OUR DESIGN" target="INTERRUPT LATENCIES OF SIX CLOCK CYCLES">
  <data key="d0">CAN ACHIEVE</data>
</edge>
<edge source="OUR DESIGN" target="EFFICIENT BACK-TO-BACK INTERRUPT HANDLING IN 12 CYCLES">
  <data key="d0">CAN ACHIEVE</data>
</edge>
<edge source="INTERRUPT LATENCIES OF SIX CLOCK CYCLES AND EFFICIENT BACK-TO-BACK INTERRUPT HANDLING IN 12 CYCLES" target="THE FASTEST AVAILABLE APPROACHES CURRENTLY IMPLEMENTED IN THE RISC-V LANDSCAPE">
  <data key="d0">IS AS LOW AS</data>
</edge>
<edge source="THE FASTEST AVAILABLE APPROACHES" target="FULLY OPEN-SOURCE">
  <data key="d0">ARE</data>
</edge>
<edge source="THE FASTEST AVAILABLE APPROACHES" target="COMPETITIVE AGAINST CLOSED-SOURCE AND PROPRIETARY COMMERCIAL SOLUTIONS">
  <data key="d0">ARE</data>
</edge>
<edge source="FAST INTERRUPT EXTENSION (FASTIRQ)" target="BOTH NESTED INTERRUPT CASE SCENARIOS">
  <data key="d0">ACCELERATES</data>
</edge>
<edge source="FAST INTERRUPT EXTENSION (FASTIRQ)" target="NON-NESTED INTERRUPT CASE SCENARIOS">
  <data key="d0">ACCELERATES</data>
</edge>
<edge source="CLINT AND CLIC" target="ABOUT 33 CYCLES INTERRUPT LATENCY">
  <data key="d0">HAVE</data>
</edge>
<edge source="THE SAME MECHANISM" target="ONE TO ACCELERATE CONTEXT SWITCHING THROUGH HWSW COOPERATION">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THE PROPOSED SOLUTION" target="RISC-V AS A COMPETITIVE CANDIDATE FOR BUILDING THE NEXT GENERATION OF TIME-CRITICAL SYSTEMS">
  <data key="d0">PROMOTES</data>
</edge>
<edge source="THIS CORE" target="CONTROLPULP 19">
  <data key="d0">IS EMBEDDED IN</data>
</edge>
<edge source="CONTROLPULP 19" target="A SOC">
  <data key="d0">IS</data>
</edge>
<edge source="CONTROLPULP 19" target="RUNNING REAL-TIME WORKLOADS">
  <data key="d0">IS SPECIALIZED IN</data>
</edge>
<edge source="A PROGRAMMABLE ACCELERATOR SUBSYSTEM" target="EIGHT CV32 CORES">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="A SET OF STANDARD PERIPHERALS" target="QUAD SERIAL PERIPHERAL INTERFACE (QSPI)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="A SET OF STANDARD PERIPHERALS" target="INTER-INTEGRATED CIRCUIT (I2C)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="A SET OF STANDARD PERIPHERALS" target="UNIVERSAL ASYNCHRONOUS RECEIVER-TRANSMITTER (UART)">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE MANAGER CORE" target="SCHEDULING TASKS">
  <data key="d0">IS RESPONSIBLE FOR</data>
</edge>
<edge source="THE MANAGER CORE" target="COMMUNICATING WITH THE PERIPHERALS">
  <data key="d0">IS RESPONSIBLE FOR</data>
</edge>
<edge source="THE MANAGER CORE" target="OFFLOADING TASKS TO THE ACCELERATOR SUBSYSTEM">
  <data key="d0">IS RESPONSIBLE FOR</data>
</edge>
<edge source="THE MANAGER CORE" target="ASYNCHRONOUS EXTERNAL EVENTS">
  <data key="d0">IS RESPONSIVE TO</data>
</edge>
<edge source="ASYNCHRONOUS EXTERNAL EVENTS" target="INTERRUPTS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="INTERRUPTS" target="ASYNCHRONOUS EVENTS">
  <data key="d0">ARE</data>
</edge>
<edge source="INTERRUPTS" target="THE NORMAL PROGRAM ORDER EXECUTION">
  <data key="d0">ALTER</data>
</edge>
<edge source="INTERRUPTS" target="A SWITCH TO A DIFFERENT CONTEXT TO HANDLE THE EVENT">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="INTERRUPTS" target="GLOBALLY DURING THE EXECUTION OF AN ISR">
  <data key="d0">ARE DISABLED</data>
</edge>
<edge source="INTERRUPTS" target="SEQUENTIALLY">
  <data key="d0">ARE SERVED</data>
</edge>
<edge source="INTERRUPTS" target="GLOBALLY ENABLED">
  <data key="d0">ARE</data>
</edge>
<edge source="INTERRUPTS" target="THE SCOPE OF AN EXECUTING ISR">
  <data key="d0">ARE ENABLED WITHIN</data>
</edge>
<edge source="INTERRUPTS" target="BELOW THE THRESHOLD">
  <data key="d0">ARE DISABLED</data>
</edge>
<edge source="INTERRUPTS" target="A PRIORITY">
  <data key="d0">CAN BE ASSIGNED</data>
</edge>
<edge source="INTERRUPTS" target="A LEVEL">
  <data key="d0">CAN BE ASSIGNED</data>
</edge>
<edge source="INTERRUPTS" target="SO-CALLED LEVELS AND PRIORITIES">
  <data key="d0">ARE PRIORITIZED BY</data>
</edge>
<edge source="INTERRUPTS" target="THE GATEWAY">
  <data key="d0">ARRIVE AT</data>
</edge>
<edge source="INTERRUPTS" target="PROGRAMMABLE CONFIGURATION INFORMATION">
  <data key="d0">ARE COMBINED WITH</data>
</edge>
<edge source="INTERRUPTS" target="WRITE-BACK STAGE OF THE CORE">
  <data key="d0">ARE ONLY ACTED UPON IN</data>
</edge>
<edge source="INTERRUPTS" target="THE DESIGN AT THE INTERRUPT CONTROLLER INPUTS">
  <data key="d0">ARE INJECTED INTO</data>
</edge>
<edge source="CONTROLPULP" target="A SET OF SCRATCHPAD MEMORIES (SPMS)">
  <data key="d0">HOSTS</data>
</edge>
<edge source="SCRATCHPAD MEMORIES (SPMS)" target="SINGLE-CYCLE ACCESS TIME">
  <data key="d0">GUARANTEE</data>
</edge>
<edge source="SINGLE-CYCLE ACCESS TIME" target="THE CV32 MANAGER CORE">
  <data key="d0">IS FROM</data>
</edge>
<edge source="THIS DESIGN CHOICE" target="DETERMINISTIC MEMORY ACCESS LATENCY FOR BOTH DATA LOAD, STORE, AND INSTRUCTION FETCH">
  <data key="d0">ENABLES</data>
</edge>
<edge source="THIS DESIGN CHOICE" target="THE WORST CASE LATENCY WHEN HANDLING UNPREDICTABLE EVENTS">
  <data key="d0">BOUNDS</data>
</edge>
<edge source="APPLICATIONS" target="FREERTOS 20">
  <data key="d0">RUN ON TOP OF</data>
</edge>
<edge source="FREERTOS 20" target="AN OPEN-SOURCE PRIORITY-BASED PREEMPTIVE RTOS">
  <data key="d0">IS</data>
</edge>
<edge source="FREERTOS 20" target="THE MANAGER CORE">
  <data key="d0">RUNS IN</data>
</edge>
<edge source="TASKS" target="THE MANAGER CORE">
  <data key="d0">ARE SCHEDULED AND RUN FROM</data>
</edge>
<edge source="DATE" target="6 JUNE 2024">
  <data key="d0">IS</data>
</edge>
<edge source="DATE" target="136, JAN. 2021">
  <data key="d0">IS</data>
</edge>
<edge source="INTERRUPT SOURCES" target="INTERRUPTS">
  <data key="d0">CAN SIGNAL</data>
</edge>
<edge source="INTERRUPT SOURCES" target="LEVEL CHANGE OF THE INTERRUPT LINE">
  <data key="d0">CAN SIGNAL INTERRUPTS THROUGH</data>
</edge>
<edge source="INTERRUPT SOURCES" target="LOGIC LEVEL ITSELF">
  <data key="d0">CAN SIGNAL INTERRUPTS THROUGH</data>
</edge>
<edge source="LEVEL CHANGE OF THE INTERRUPT LINE" target="EDGE-TRIGGERED INTERRUPT">
  <data key="d0">IS CALLED</data>
</edge>
<edge source="LOGIC LEVEL ITSELF" target="LEVEL-TRIGGERED INTERRUPT">
  <data key="d0">IS</data>
</edge>
<edge source="NESTED INTERRUPTS" target="PREEMPTION OF A LOW-LEVEL INTERRUPT BY A HIGH-LEVEL INTERRUPT">
  <data key="d0">ENTAILS</data>
</edge>
<edge source="TRANSITION" target="DIFFERENT INTERRUPT LEVELS">
  <data key="d0">IS BETWEEN</data>
</edge>
<edge source="THE FORMER" target="UNIDIRECTIONAL NOTIFICATION WITHOUT CONFIRMATION">
  <data key="d0">IS</data>
</edge>
<edge source="A PROCESSOR" target="VECTORED INTERRUPTS">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="VECTORED INTERRUPTS" target="FAST INTERRUPT RESPONSE">
  <data key="d0">GRANT</data>
</edge>
<edge source="EACH INTERRUPT" target="A SPECIFIC INTERRUPT SERVICE ROUTINE (ISR)">
  <data key="d0">TRAPS TO</data>
</edge>
<edge source="EACH INTERRUPT" target="AN INTERRUPT VECTOR TABLE">
  <data key="d0">TRAPS ACCORDING TO</data>
</edge>
<edge source="EACH INTERRUPT" target="A PRIORITY">
  <data key="d0">CAN BE ASSIGNED</data>
</edge>
<edge source="FAST INTERRUPT RESPONSE" target="INCREASED CODE SIZE">
  <data key="d0">IS AT THE COST OF</data>
</edge>
<edge source="INTERRUPT VECTORING" target="IMPROVE INTERRUPT LATENCIES">
  <data key="d0">IS SUPPORTED TO</data>
</edge>
<edge source="NON-VECTORED OR DIRECT INTERRUPTS" target="A SHARED ISR">
  <data key="d0">TRAP TO</data>
</edge>
<edge source="THE LATTER APPROACH" target="CODE SIZE OFF FOR A SLOWER INTERRUPT RESPONSE">
  <data key="d0">TRADES</data>
</edge>
<edge source="THE LATTER APPROACH" target="LEVERAGING RISC-VS RV32E BASE INSTRUCTION SET">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="THE LATTER APPROACH" target="THE LOWER 16 ARCHITECTURAL REGISTERS OF THE RF">
  <data key="d0">RE-USES</data>
</edge>
<edge source="THE LATTER APPROACH" target="THIS WORK">
  <data key="d0">IS PROPOSED IN</data>
</edge>
<edge source="THE OVERHEAD OF RESOLVING THE INTERRUPTION CAUSE AND JUMPING TO THE CORRECT ISR" target="EXPLICIT INSTRUCTIONS">
  <data key="d0">ARE HANDLED IN</data>
</edge>
<edge source="THE INTERRUPT TABLE" target="MUCH MORE COMPACT">
  <data key="d0">CAN BE MADE</data>
</edge>
<edge source="MULTIPLE SOURCES" target="INTERRUPT LATENCY IN A SYSTEM">
  <data key="d0">DETERMINE</data>
</edge>
<edge source="UNDERLYING HW" target="INTERRUPT LATENCY">
  <data key="d0">IS A SOURCE OF</data>
</edge>
<edge source="SCHEDULER OR OS" target="INTERRUPT LATENCY">
  <data key="d0">IS A SOURCE OF</data>
</edge>
<edge source="APPLICATION RUNNING ON TOP" target="INTERRUPT LATENCY">
  <data key="d0">IS A SOURCE OF</data>
</edge>
<edge source="THIS BREAKDOWN" target="FIG.">
  <data key="d0">IS DETAILED IN</data>
</edge>
<edge source="INTERRUPT EDGE" target="THE HW">
  <data key="d0">ARRIVES AT</data>
</edge>
<edge source="THE HW" target="THE LINK REGISTER A VALUE (EXCRETURN)">
  <data key="d0">ENCODES IN</data>
</edge>
<edge source="HW" target="THE INTERRUPT CONTROLLER">
  <data key="d0">USUALLY IS</data>
</edge>
<edge source="HW" target="ENOUGH INFORMATION">
  <data key="d0">STORES</data>
</edge>
<edge source="HW" target="THE PREVIOUS REGISTER STATE TO MEMORY IN THE BACKGROUND">
  <data key="d0">PUSHES OUT</data>
</edge>
<edge source="HW" target="A PRIORI WHICH CALLER-SAVE REGISTERS NEED TO BE SAVED">
  <data key="d0">DOES NOT KNOW</data>
</edge>
<edge source="TO MAKE A FAIR COMPARISON BETWEEN SW-BASED AND MORE HW-ORIENTED INTERRUPT SOLUTIONS" target="WHAT EXACTLY CONSTITUTES THE FIRST INSTRUCTION OF THE INTERRUPT HANDLER">
  <data key="d0">WE HAVE TO DELINEATE</data>
</edge>
<edge source="EACH HW CONFIGURATION" target="A HANDWRITTEN OPTIMIZED INTERRUPT HANDLER">
  <data key="d0">HAS</data>
</edge>
<edge source="HANDWRITTEN OPTIMIZED INTERRUPT HANDLER" target="ALL REQUIRED GENERAL-PURPOSE AND MACHINE-SPECIFIC REGISTERS FOR NESTING INTERRUPTS">
  <data key="d0">STORES</data>
</edge>
<edge source="THE FIRST INSTRUCTION" target="THE ONE AFTER ALL NECESSARY INTERRUPT CONTEXT HAS BEEN SAVED ON THE STACK TO BE ABLE TO CALL A FUNCTION">
  <data key="d0">IS COUNTED AS</data>
</edge>
<edge source="THE FIRST INSTRUCTION" target="RE-ENABLING GLOBAL INTERRUPTS">
  <data key="d0">WILL BE</data>
</edge>
<edge source="WHAT EXACTLY ENTAILS A FUNCTION CALL" target="THE USED APPLICATION BINARY INTERFACE (ABI)">
  <data key="d0">IS DICTATED BY</data>
</edge>
<edge source="WHAT EXACTLY ENTAILS A FUNCTION CALL" target="ITS CALLING CONVENTION">
  <data key="d0">IS DICTATED BY</data>
</edge>
<edge source="INTERRUPT HANDLER AND INTERRUPT CONTEXT SAVING CODE" target="TRUE">
  <data key="d0">CAN BE INTERLEAVED</data>
</edge>
<edge source="SOME OF THE CONTEXT SAVING CODE" target="TRUE">
  <data key="d0">CAN BE REMOVED</data>
</edge>
<edge source="SOME OF THE CONTEXT SAVING CODE" target="REDUNDANT">
  <data key="d0">MIGHT BE</data>
</edge>
<edge source="THE ACTIVE INTERRUPT HANDLERS CONTEXT" target="RESTORED ON INTERRUPT RETURN">
  <data key="d0">MUST BE</data>
</edge>
<edge source="THE ACTIVE INTERRUPT HANDLERS CONTEXT" target="IMMEDIATELY SAVED AGAIN">
  <data key="d0">IS</data>
</edge>
<edge source="THE ACTIVE INTERRUPT HANDLERS CONTEXT" target="THE NEXT PENDING INTERRUPT FIRING">
  <data key="d0">IS SAVED AGAIN DUE TO</data>
</edge>
<edge source="RESTORING THE PRE-INTERRUPT CONTEXT" target="SW FOR THE NESTED INTERRUPT CASE">
  <data key="d0">IS HANDLED IN</data>
</edge>
<edge source="CONTEXT SWITCHING TIME" target="THE RESPONSIVENESS OF THE ARCHITECTURE IN SWAPPING FROM ONE EXECUTION CONTEXT TO ANOTHER">
  <data key="d0">DETERMINES</data>
</edge>
<edge source="THE EXECUTION CONTEXT" target="THE OS BEING USED">
  <data key="d0">IS DEPENDENT ON</data>
</edge>
<edge source="THE EXECUTION CONTEXT" target="THE STATE OF THE ARCHITECTURAL REGISTERS OF THE ISA">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="THE EXECUTION CONTEXT" target="THE CHOSEN ABI">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="PREEMPTION" target="AN EVENT">
  <data key="d0">REFERS TO</data>
</edge>
<edge source="PREEMPTION" target="A CURRENT TASK">
  <data key="d0">TEMPORARILY INTERRUPTS</data>
</edge>
<edge source="PREEMPTION" target="PRIVILEGE MODE (VERTICAL INTERRUPTS)">
  <data key="d0">IS REGULATED BY</data>
</edge>
<edge source="PREEMPTION" target="INTERRUPT LEVEL WHEN THE PRIVILEGE MODE IS THE SAME (HORIZONTAL INTERRUPTS)">
  <data key="d0">IS REGULATED BY</data>
</edge>
<edge source="PURPOSE OF PREEMPTION" target="RESUMING ITS EXECUTION LATER">
  <data key="d0">IS</data>
</edge>
<edge source="THE SIMPLEST CASE FOR PREEMPTION" target="NON-NESTED INTERRUPT HANDLERS">
  <data key="d0">OCCURS WITH</data>
</edge>
<edge source="A MORE COMPLEX CASE FOR PREEMPTION" target="NESTED INTERRUPT HANDLERS">
  <data key="d0">OCCURS WITH</data>
</edge>
<edge source="NESTED INTERRUPT HANDLERS" target="THE CASE OF MULTIPLE INTERRUPTS AT A TIME">
  <data key="d0">HANDLE</data>
</edge>
<edge source="THIS SITUATION" target="PREEMPTION">
  <data key="d0">DOES NOT RESULT IN</data>
</edge>
<edge source="THIS SITUATION" target="PENDING INTERRUPTS TO BE SERVICED IN SEQUENCE ACCORDING TO INCREASING PRIORITY">
  <data key="d0">CAUSES</data>
</edge>
<edge source="THIS SITUATION" target="DIRECTLY CHECKING FOR OTHER INTERRUPTS PENDING ON THE SAME LEVEL BEFORE RESTORING THE EXECUTIONS INTERRUPT CONTEXT">
  <data key="d0">CAN BE OPTIMIZED BY</data>
</edge>
<edge source="THIS SITUATION" target="THE RISC-V ECOSYSTEM">
  <data key="d0">IS IN</data>
</edge>
<edge source="THIS SITUATION" target="FIG.">
  <data key="d0">IS VISUALIZED IN</data>
</edge>
<edge source="LEVELPRIORITY ARBITRATION" target="SW-DRIVEN">
  <data key="d0">IS</data>
</edge>
<edge source="PRIORITY SIMPLESTANDARD INTERRUPT HANDLERS" target="LEVELPRIORITY ARBITRATION">
  <data key="d0">ARE USED WITH</data>
</edge>
<edge source="HIGHEST PRIORITY INTERRUPT IDENTIFICATION CODE" target="NOT EXECUTED">
  <data key="d0">IS</data>
</edge>
<edge source="LEVELPRIORITY ARBITRATION LOGIC" target="INTERRUPT CONTROLLER">
  <data key="d0">IS DESIGNED WITHIN</data>
</edge>
<edge source="HIGHEST LEVELPRIORITY INTERRUPT" target="PENDING BUT DISABLED">
  <data key="d0">IS</data>
</edge>
<edge source="HIGHEST LEVELPRIORITY INTERRUPT" target="CORE">
  <data key="d0">IS NOT PROPAGATED TO</data>
</edge>
<edge source="CORE" target="AN ACTIVE HANDLER">
  <data key="d0">IS WITHIN</data>
</edge>
<edge source="LEVELPRIORITY INTERRUPT SCHEME" target="ADDITIONAL MASKING OF INCOMING INTERRUPTS OF EQUAL OR LOWER LEVELPRIORITY THAN THE EXECUTING ISR">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="LEVELPRIORITY INTERRUPT SCHEME" target="ADDITIONAL MASKING OF INCOMING INTERRUPTS SOMETIMES LARGER THAN A CONFIGURABLE LEVELPRIORITY THRESHOLD">
  <data key="d0">INTRODUCES</data>
</edge>
<edge source="THIS SCENARIO" target="REAL-TIME AND COMPLEX EMBEDDED SYSTEMS">
  <data key="d0">IS NOT IDEAL FOR</data>
</edge>
<edge source="ATTEMPTS TO HANDLE A MORE COMPLEX INTERRUPT SCHEME" target="SW EMULATION">
  <data key="d0">WOULD REQUIRE</data>
</edge>
<edge source="SW EMULATION" target="UNTENABLE INTERRUPT LATENCIES">
  <data key="d0">INCURS</data>
</edge>
<edge source="A HIGH-PRIORITY INTERRUPT" target="A LOWER-PRIORITY INTERRUPT TO FINISH">
  <data key="d0">HAS TO WAIT FOR</data>
</edge>
<edge source="GLOBAL INTERRUPTS" target="DISABLED WHENEVER AN INTERRUPT HANDLER IS ENTERED">
  <data key="d0">ARE</data>
</edge>
<edge source="THE ISR" target="CARE DESIGNED TO ENSURE THEY ARE REENTRANT">
  <data key="d0">NEED TO</data>
</edge>
<edge source="THE NESTING" target="HIGHER PRIORITY INTERRUPTS TO PREEMPT A CURRENT LOWER PRIORITY ISR EXECUTING">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="REDUNDANT INTERRUPT CONTEXT" target="BACK-TO-BACK INTERRUPTS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="BACK-TO-BACK INTERRUPTS" target="INTERRUPTS THAT NEED TO BE SERVED SEQUENTIALLY">
  <data key="d0">ARE</data>
</edge>
<edge source="BACK-TO-BACK INTERRUPTS" target="WHENEVER THERE ARE MULTIPLE INTERRUPTS PENDING">
  <data key="d0">CAN HAPPEN</data>
</edge>
<edge source="THE TRANSITION FROM ONE INTERRUPT TO THE NEXT ONE" target="A REDUNDANT SEQUENCE OF CONTEXT RESTORES AND CONTEXT SAVES">
  <data key="d0">CAUSES</data>
</edge>
<edge source="REDUNDANT CONTEXT RESTORE SEQUENCES" target="INTERRUPT LATENCY">
  <data key="d0">NEGATIVELY IMPACT</data>
</edge>
<edge source="REDUNDANT CONTEXT RESTORE" target="UNWANTED ADDITIONAL INTERRUPT LATENCY">
  <data key="d0">CAN INTRODUCE</data>
</edge>
<edge source="REDUNDANT CONTEXT RESTORE" target="NON-NESTED OR NESTED HORIZONTAL INTERRUPTS">
  <data key="d0">OCCURS WITH</data>
</edge>
<edge source="REDUNDANT CONTEXT RESTORE" target="TWO NON-PREEMPTIVE INTERRUPTS">
  <data key="d0">OF</data>
</edge>
<edge source="TWO NON-PREEMPTIVE INTERRUPTS" target="TWO INTERRUPTS WITH SAME LEVEL BUT DIFFERENT PRIORITIES">
  <data key="d0">ARE</data>
</edge>
<edge source="TAIL-CHAINING" target="OPTIMIZE IT">
  <data key="d0">IS USED TO</data>
</edge>
<edge source="INTERRUPT CONTEXT RESTORE AND STORE SEQUENCE BETWEEN BACK-TO-BACK INTERRUPTS" target="REDUNDANT">
  <data key="d0">CAN BE CONSIDERED</data>
</edge>
<edge source="THE REDUNDANT CONTEXT RESTORING SEQUENCES" target="THE FULL INTERRUPT EXIT CODE SEQUENCE">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="REDUNDANT CONTEXT RESTORE WITH NON-NESTED INTERRUPTS" target="CHAINING TWO BACK-TO-BACK INTERRUPTS">
  <data key="d0">IS ADDRESSED BY</data>
</edge>
<edge source="REDUNDANT CONTEXT RESTORE WITH NON-NESTED INTERRUPTS" target="BYPASSING THE SUPERFLUOUS RESTORESAVE OPERATION">
  <data key="d0">IS ADDRESSED BY</data>
</edge>
<edge source="SECTION III-D" target="THE OPTIMIZATIONS IMPLEMENTED IN THIS WORK TO ADDRESS THIS SCENARIO">
  <data key="d0">EXPLORES</data>
</edge>
<edge source="THE PRIVILEGED SPECIFICATION 14" target="A SIMPLE INTERRUPT SCHEME">
  <data key="d0">DEFINES</data>
</edge>
<edge source="THE PRIVILEGED SPECIFICATION 14" target="THE RISC-V ECOSYSTEM">
  <data key="d0">IS IN</data>
</edge>
<edge source="A SIMPLE INTERRUPT SCHEME" target="A SET OF TIMER AND INTER-PROCESSOR INTERRUPTS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="PENDING AND ENABLED INTERRUPTS" target="A THRESHOLD VALUE REPRESENTING AN INTERRUPT LEVEL">
  <data key="d0">ARE MASKED ACCORDING TO</data>
</edge>
<edge source="PENDING AND ENABLED INTERRUPTS" target="COMPARED TO STANDARD RISC-V PRIVILEGED SPECIFICATIONS">
  <data key="d0">ARE FURTHER SELECTIVELY MASKED</data>
</edge>
<edge source="THRESHOLD VALUE" target="A CSR">
  <data key="d0">IS CONFIGURED THROUGH</data>
</edge>
<edge source="A FIXED PRIORITY INTERRUPT SCHEME" target="16 PREDEFINED OR RESERVED INTERRUPTS">
  <data key="d0">HAS</data>
</edge>
<edge source="A FIXED PRIORITY INTERRUPT SCHEME" target="16 IMPLEMENTATION-DEFINED INTERRUPTS">
  <data key="d0">HAS</data>
</edge>
<edge source="16 IMPLEMENTATION-DEFINED INTERRUPTS" target="OPTIONALLY VECTORED">
  <data key="d0">CAN BE</data>
</edge>
<edge source="THE CLINT" target="PRIORITIZATION OF INTERRUPTS BASED ON PRIVILEGE MODE">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="THE CLINT" target="FINE-GRAINED CONTROL OVER INTERRUPT PRIORITIZATION">
  <data key="d0">LACKS</data>
</edge>
<edge source="THE INFLEXIBLE INTERRUPT SCHEME OF THE CLINT-MODE" target="MUCH MORE WORK TO BE DONE IN MANAGING INTERRUPT MASK (SOMEIRQMASK) AND OTHER MACHINE STATE">
  <data key="d0">CAUSES</data>
</edge>
<edge source="INTERRUPTS WITH LOWER PRIORITY THAN THE CURRENT INTERRUPT RUNNING" target="IN CLINT-MODE WHEN GLOBAL INTERRUPTS ARE RE-ENABLED">
  <data key="d0">CAN FIRE</data>
</edge>
<edge source="PLIC 22" target="CLINT">
  <data key="d0">CAN BE ATTACHED TO</data>
</edge>
<edge source="PLIC 22" target="NUMBER OF CUSTOM INTERRUPTS">
  <data key="d0">INCREASES</data>
</edge>
<edge source="ACTIVE INTERRUPT HANDLER" target="IRQ2">
  <data key="d0">SERVICES</data>
</edge>
<edge source="IRQ2" target="PRIV2">
  <data key="d0">HAS INTERRUPT PRIVILEGE MODE</data>
</edge>
<edge source="IRQ2" target="L2">
  <data key="d0">HAS LEVEL</data>
</edge>
<edge source="IRQ2" target="P2">
  <data key="d0">HAS PRIORITY</data>
</edge>
<edge source="IRQ1" target="PENDING">
  <data key="d0">IS</data>
</edge>
<edge source="IRQ1" target="PRIV1">
  <data key="d0">HAS INTERRUPT PRIVILEGE MODE</data>
</edge>
<edge source="IRQ1" target="L1">
  <data key="d0">HAS LEVEL</data>
</edge>
<edge source="IRQ1" target="P1">
  <data key="d0">HAS PRIORITY</data>
</edge>
<edge source="IRQ1" target="IRQ2">
  <data key="d0">PREEMPTS</data>
</edge>
<edge source="NUMBER OF INTERRUPTS" target="FLEXIBLE (AT DESIGN TIME)">
  <data key="d0">IS</data>
</edge>
<edge source="NUMBER OF INTERRUPTS" target="ONE OR MORE TARGETS">
  <data key="d0">TARGETS</data>
</edge>
<edge source="INTERRUPTS THAT ARE ASSIGNED A HIGHER LEVEL" target="LOWER-LEVEL INTERRUPTS">
  <data key="d0">CAN PRE-EMPT</data>
</edge>
<edge source="EACH TARGET" target="A THRESHOLD">
  <data key="d0">CAN SELECT</data>
</edge>
<edge source="INTERRUPT SELECTION" target="THE CLIC IN HW">
  <data key="d0">IS DRIVEN BY</data>
</edge>
<edge source="THE CLIC IN HW" target="THE HIGHEST LEVEL, HIGHEST PRIORITY PENDING INTERRUPT TO THE CORES INTERFACE">
  <data key="d0">PROPAGATES</data>
</edge>
<edge source="INTERRUPT PRIORITY" target="TIE-BREAKER">
  <data key="d0">SERVES AS</data>
</edge>
<edge source="TIE-BREAKER" target="THE CASE OF MULTIPLE INTERRUPTS PENDING WITH THE SAME LEVEL">
  <data key="d0">IS FOR</data>
</edge>
<edge source="SELECTING THE INTERRUPT WITH MAXIMUM LEVEL AND PRIORITY" target="THREE BINARY TREES">
  <data key="d0">IS IMPLEMENTED WITH</data>
</edge>
<edge source="ENABLED INTERRUPTS AND THEIR LEVEL AND PRIORITY INFORMATION" target="PRIORITIZATION LOGIC">
  <data key="d0">ARE SENT TO</data>
</edge>
<edge source="PRIORITIZATION LOGIC" target="BINARY ARBITRATION TREE">
  <data key="d0">USES</data>
</edge>
<edge source="BINARY ARBITRATION TREE" target="HIGHEST-LEVEL INTERRUPT">
  <data key="d0">SELECTS</data>
</edge>
<edge source="THE PRIORITIES" target="CONCURRENT PENDING INTERRUPTS TO BE TAKEN IN THE ORDER PREFERRED BY THE PROGRAMMER">
  <data key="d0">ALLOW</data>
</edge>
<edge source="THE LEVEL INFORMATION" target="PRE-EMPTION OF SAME-PRIVILEGE LEVEL INTERRUPTS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="SAME-PRIVILEGE LEVEL INTERRUPTS" target="HORIZONTAL INTERRUPTS">
  <data key="d0">ARE ALSO CALLED</data>
</edge>
<edge source="THIS SCHEME" target="INTERRUPTS TO BE DIVIDED ACCORDING TO THEIR PRIORITIES ON THE PLIC-LEVEL">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THIS SCHEME" target="SOME FLEXIBILITY IN TERMS OF PRIORITIZATION">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THIS SCHEME" target="THE FLEXIBILITY PROBLEM ON THE CORE LOCAL-LEVEL">
  <data key="d0">DOES NOT ADDRESS</data>
</edge>
<edge source="CLIC 15" target="THESE LIMITATIONS">
  <data key="d0">ADDRESSES</data>
</edge>
<edge source="CLIC 15" target="INTERRUPTS TO BE PRIORITIZED">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="MULTIPLE HORIZONTAL INTERRUPTS" target="EQUAL LEVELS AND PRIORITIES">
  <data key="d0">HAVE</data>
</edge>
<edge source="THE CASE WHERE MULTIPLE HORIZONTAL INTERRUPTS HAVE EQUAL LEVELS AND PRIORITIES" target="THE CLIC SELECTING THE HIGHEST NUMBERED INTERRUPT 15">
  <data key="d0">RESULTS IN</data>
</edge>
<edge source="INTERRUPT 15" target="AN ARBITRARY ASSIGNMENT">
  <data key="d0">IS</data>
</edge>
<edge source="AN ARBITRARY ASSIGNMENT" target="DECIDED AT DESIGN TIME">
  <data key="d0">IS</data>
</edge>
<edge source="THIS FEATURE" target="RTOSS THAT ONLY WANT TO DISABLE A SUBSET OF ALL INTERRUPTS DURING CRITICAL SECTIONS">
  <data key="d0">IS USEFUL FOR</data>
</edge>
<edge source="THIS FEATURE" target="SEMI-SHADOWING">
  <data key="d0">IS COMBINED WITH</data>
</edge>
<edge source="THIS FEATURE" target="THE TOP (FLIP) AND BOTTOM (FLOP) HALVES OF THE RF AS RF COPIES">
  <data key="d0">USES</data>
</edge>
<edge source="ONLY A SUBSET OF INTERRUPTS" target="DISABLED">
  <data key="d0">NEED TO BE</data>
</edge>
<edge source="INTERRUPTS THAT DO NOT INTERFERE WITH THE DATA ACCESSED IN SUCH A CRITICAL SECTION" target="STILL FIRE">
  <data key="d0">CAN</data>
</edge>
<edge source="IRQ1 LEVEL" target="INTERRUPT THRESHOLD">
  <data key="d0">IS HIGHER THAN</data>
</edge>
<edge source="IRQ1 LEVEL" target="IRQ2 LEVEL">
  <data key="d0">IS HIGHER THAN</data>
</edge>
<edge source="INTERRUPTS FIRED FROM DIFFERENT PRIVILEGE MODES" target="VERTICAL INTERRUPTS">
  <data key="d0">ARE REFERRED TO AS</data>
</edge>
<edge source="INTERRUPTS FIRED FROM THE SAME PRIVILEGE MODE" target="HORIZONTAL INTERRUPTS">
  <data key="d0">ARE REFERRED TO AS</data>
</edge>
<edge source="PREEMPTION CONDITIONS" target="CLIC SPECIFICATION">
  <data key="d0">ARE ACCORDING TO</data>
</edge>
<edge source="THE CLIC SPECIFICATION" target="THE CASE OF REDUNDANT CONTEXT RESTORE">
  <data key="d0">ADDRESSES</data>
</edge>
<edge source="XNXTI" target="A CSR SHORT FOR NEXT INTERRUPT HANDLER ADDRESS">
  <data key="d0">IS</data>
</edge>
<edge source="XNXTI" target="AN INTERRUPT-ENABLE CSR">
  <data key="d0">IS</data>
</edge>
<edge source="XNXTI" target="NON-VECTORED INTERRUPTS">
  <data key="d0">IS MEANT FOR USE WITH</data>
</edge>
<edge source="XNXTI" target="THE INTERRUPT CONTEXT STORING PART ITSELF">
  <data key="d0">DOES NOT TOUCH</data>
</edge>
<edge source="XNXTI" target="A POINTER TO THE ADDRESS OF THE NEXT HANDLER">
  <data key="d0">RETURNS</data>
</edge>
<edge source="XNXTI" target="A SMALL CODE SEQUENCE">
  <data key="d0">NEEDS</data>
</edge>
<edge source="READING FROM THIS CSR" target="TO FAST-TRACK INTERRUPTS THAT ARRIVE LATE">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="READING FROM THIS CSR" target="TO AVOID REDUNDANT CONTEXT SAVERESTORE">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="REDUNDANT CONTEXT SAVERESTORE" target="RUNNING THROUGH PENDING INTERRUPTS BACK-TO-BACK">
  <data key="d0">IS AVOIDED BY</data>
</edge>
<edge source="ARCHITECTURE" target="HW TO STORE ENOUGH INFORMATION">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="ARCHITECTURE" target="TYPICAL CASE SCENARIOS">
  <data key="d0">TACKLES</data>
</edge>
<edge source="ENOUGH INFORMATION" target="RESUME OPERATION CORRECTLY AFTER RETURNING FROM THE AFOREMENTIONED CONTEXT">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE BACKGROUND-SAVING MECHANISM" target="THE STACK POINTER">
  <data key="d0">UPDATES</data>
</edge>
<edge source="THE BACKGROUND-SAVING MECHANISM" target="THE BANK-SWITCHED CONTENTS IN MEMORY">
  <data key="d0">STORES</data>
</edge>
<edge source="THE BACKGROUND-SAVING MECHANISM" target="THE STATE SAVING AND RESTORING PART OF CONTEXT FIG">
  <data key="d0">TO ACCELERATE</data>
</edge>
<edge source="THE STACK POINTER" target="APPROPRIATELY TO MAINTAIN ABI INVARIANTS">
  <data key="d0">IS ADJUSTED</data>
</edge>
<edge source="THE EXECUTION OF THE CORE" target="IN PARALLEL">
  <data key="d0">PROCEEDS</data>
</edge>
<edge source="INCOMING INTERRUPTS" target="A GATEWAY MODULE">
  <data key="d0">ARE FILTERED WITH</data>
</edge>
<edge source="A GATEWAY MODULE" target="THERE IS A PENDING AND ENABLED REQUEST FOR EACH INTERRUPT SOURCE I (IRQ I)">
  <data key="d0">DECIDES WHETHER</data>
</edge>
<edge source="PROGRAMMABLE CONFIGURATION INFORMATION" target="EACH INTERRUPT LINE">
  <data key="d0">IS ABOUT</data>
</edge>
<edge source="EACH INTERRUPT LINE" target="LEVEL">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="EACH INTERRUPT LINE" target="PRIORITY">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="EACH INTERRUPT LINE" target="ENABLE STATUS">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="EACH INTERRUPT LINE" target="SENSITIVITY (LEVELEDGE)">
  <data key="d0">CONSISTS OF</data>
</edge>
<edge source="THE INTERRUPT PRIORITIZATION MODULE" target="THE TREE FROM LEAVES TO THE ROOT">
  <data key="d0">TRAVERSES</data>
</edge>
<edge source="THE INTERRUPT PRIORITIZATION MODULE" target="THE SOUGHT-AFTER MAXIMUM LEVEL AND PRIORITY INTERRUPT">
  <data key="d0">FINDS</data>
</edge>
<edge source="EACH TREE" target="LOW OVERHEAD IN TERMS OF AREA AND DELAY">
  <data key="d0">HAS</data>
</edge>
<edge source="THE INTERRUPT" target="THE CORE">
  <data key="d0">IS PRESENTED TO</data>
</edge>
<edge source="THE INTERRUPT" target="A HANDSHAKE-BASED INTERFACE">
  <data key="d0">IS PRESENTED WITH</data>
</edge>
<edge source="THE ADDITIONAL KILL SIGNAL" target="A HANDSHAKE TO RESTART">
  <data key="d0">IS THERE TO ALLOW FOR</data>
</edge>
<edge source="A HANDSHAKE TO RESTART" target="A POTENTIALLY MORE IMPORTANT INTERRUPT CAN BE PRESENTED TO THE CORE">
  <data key="d0">IS THERE SO THAT</data>
</edge>
<edge source="ADDITIONAL PIPELINE STAGES" target="THE ARBITRATION TREE">
  <data key="d0">CAN BE INSERTED IN</data>
</edge>
<edge source="ADDITIONAL PIPELINE STAGES" target="RELAX TIMING">
  <data key="d0">CAN BE INSERTED TO</data>
</edge>
<edge source="OUR VERSION OF THE CLIC" target="SHV">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="OUR VERSION OF THE CLIC" target="THE XNXTI CSR">
  <data key="d0">SUPPORTS</data>
</edge>
<edge source="THE XNXTI CSR" target="THE CORE">
  <data key="d0">IS IN</data>
</edge>
<edge source="THE CLIC" target="THIS WITH THE LEVELPRIORITY SCHEME">
  <data key="d0">ADDRESSES</data>
</edge>
<edge source="THE CLIC" target="AN HW-ASSISTED SOLUTION">
  <data key="d0">OFFERS</data>
</edge>
<edge source="THE ADDITIONAL PROCESSOR STATE" target="PROPER INTERRUPT NESTING">
  <data key="d0">IS REQUIRED FOR</data>
</edge>
<edge source="A NEW INTERRUPT AT THE CLIC" target="THE INTERRUPT LEVEL EXCEEDS THE CONFIGURED THRESHOLD">
  <data key="d0">WILL FIRST BE CHECKED WHETHER</data>
</edge>
<edge source="THE SAVING LOGIC FINITE STATE MACHINE (FSM)" target="THE EXTENDED RF">
  <data key="d0">IS IN</data>
</edge>
<edge source="THE CORES STATE MACHINE" target="THE PIPELINE">
  <data key="d0">WILL FLUSH</data>
</edge>
<edge source="THE CORES STATE MACHINE" target="THE PROGRAM COUNTER">
  <data key="d0">WILL UPDATE</data>
</edge>
<edge source="THE PROGRAM COUNTER" target="THE VECTOR TABLE ENTRY">
  <data key="d0">IS UPDATED ACCORDING TO</data>
</edge>
<edge source="SAVING LOGIC" target="BANK SWITCH">
  <data key="d0">TRIGGERS</data>
</edge>
<edge source="BANK SWITCH" target="INTERRUPT CONTEXT TO HAVE A FRESH SET OF REGISTERS">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="OTHER BANK CONTENTS" target="SEPARATE PORT TO THE MAIN MEMORY">
  <data key="d0">DRAIN THROUGH</data>
</edge>
<edge source="RF BANKS" target="ON AN INTERRUPT">
  <data key="d0">ARE SWITCHED</data>
</edge>
<edge source="STACK POINTER" target="DURING A BANK SWITCH">
  <data key="d0">NEED TO UPDATE</data>
</edge>
<edge source="STACK POINTER" target="ON AN INTERRUPT">
  <data key="d0">IS DECREMENTED</data>
</edge>
<edge source="STACK POINTER" target="SPACE AVAILABLE">
  <data key="d0">MAKES</data>
</edge>
<edge source="THE RISC-V EMBEDDED AND INTEGER ABI" target="THAT THE STACK POINTER POINTS BELOW THE LAST SAVED REGISTER ON THE STACK">
  <data key="d0">DICTATE</data>
</edge>
<edge source="THE PROGRAM CODE RUNNING IN THE INTERRUPT HANDLER" target="THE VALUES ON THE STACK">
  <data key="d0">COULD CLOBBER</data>
</edge>
<edge source="INTERACTIONS" target="BACKGROUND-SAVING MECHANISM AND REGULAR LOADSTORE INSTRUCTIONS OF THE CORE">
  <data key="d0">ARE BETWEEN</data>
</edge>
<edge source="INTERACTIONS" target="INCORRECT EXECUTION">
  <data key="d0">COULD RESULT IN</data>
</edge>
<edge source="LOAD INSTRUCTION" target="ARCHITECTURAL REGISTER">
  <data key="d0">IS TRYING TO UPDATE</data>
</edge>
<edge source="BACKGROUND-SAVING MECHANISM" target="SAME REGISTER">
  <data key="d0">IS TRYING TO READ</data>
</edge>
<edge source="BACKGROUND-SAVING MECHANISM" target="STACK MEMORY REGIONS">
  <data key="d0">IS WRITING TO</data>
</edge>
<edge source="BACKGROUND-SAVING MECHANISM" target="INTERRUPT STATE WORD BY WORD">
  <data key="d0">STARTS STORING</data>
</edge>
<edge source="NO CONFLICT" target="INTERRUPTS ARE INJECTED INTO THE PIPELINE">
  <data key="d0">ARISES BECAUSE</data>
</edge>
<edge source="UPDATES TO THE RF" target="RESOLVED">
  <data key="d0">ARE</data>
</edge>
<edge source="BANK SWITCHING OPERATION" target="AT THIS POINT">
  <data key="d0">TAKES PLACE</data>
</edge>
<edge source="BANK SWITCHING OPERATION" target="CORRECTNESS OF THE EXECUTION">
  <data key="d0">ENSURES</data>
</edge>
<edge source="INTERRUPT HANDLER" target="ALREADY EXECUTING">
  <data key="d0">IS</data>
</edge>
<edge source="INTERRUPT HANDLER" target="DEDICATED REGISTER BANK">
  <data key="d0">USES</data>
</edge>
<edge source="LOADS OR STORES" target="STACK MEMORY REGIONS">
  <data key="d0">ACCESS</data>
</edge>
<edge source="LOADS OR STORES ACCESSING STACK MEMORY REGIONS WHERE THE BACKGROUND-SAVING MECHANISM IS WRITING TO" target="PROPERLY RESOLVED">
  <data key="d0">NEED TO BE</data>
</edge>
<edge source="EXECUTION OF THE HANDLER" target="IF THIS HAPPENS WHILE THE BACKGROUND-SAVING MECHANISM IS STILL AT WORK">
  <data key="d0">HAS TO WAIT</data>
</edge>
<edge source="A STRAIGHTFORWARD SOLUTION" target="TO STALL THE CORES PIPELINE WHILE THE BACKGROUND-SAVING MECHANISM IS AT WORK">
  <data key="d0">IS</data>
</edge>
<edge source="ADDRESS OFFSET OF THE LAST WORD PUSHED OUT BY THE BACKGROUND-SAVING MECHANISM" target="ANY INCOMING LOAD AND STORES">
  <data key="d0">IS COMPARED AGAINST</data>
</edge>
<edge source="ADDRESS OFFSET OF THE LAST WORD" target="LOAD-STORE UNIT">
  <data key="d0">IS IN</data>
</edge>
<edge source="LOAD AND STORES THAT TRY TO ACCESS DATA THAT IS NOT YET PUSHED TO MEMORY" target="THE CORES PIPELINE TO STALL">
  <data key="d0">CAUSE</data>
</edge>
<edge source="THIS MECHANISM" target="THE CORRECTNESS OF LOADS AND STORES ISSUED BY THE CORE">
  <data key="d0">ENSURES</data>
</edge>
<edge source="A SYSTEM CALL HANDLER" target="THE ECALL INSTRUCTION IN RISC-V">
  <data key="d0">IS ISSUED THROUGH</data>
</edge>
<edge source="A SYSTEM CALL HANDLER" target="USER-PROVIDED ARGUMENTS">
  <data key="d0">WANTS TO ACCESS</data>
</edge>
<edge source="MOST" target="GENERAL-PURPOSE REGISTERS">
  <data key="d0">WILL BE PASSED THROUGH</data>
</edge>
<edge source="SOME" target="THE STACK">
  <data key="d0">MIGHT BE PLACED ON</data>
</edge>
<edge source="SHORT INTERRUPT HANDLERS" target="THE FULL INTERRUPT STATE HAS BEEN SAVED">
  <data key="d0">WANT TO RETURN BEFORE</data>
</edge>
<edge source="EACH OF THESE CASES" target="THE STALLING LOGIC OUTLINED IN SECTION III-C3">
  <data key="d0">WOULD POTENTIALLY ENGAGE</data>
</edge>
<edge source="THE STALLING LOGIC OUTLINED IN SECTION III-C3" target="HIGHER INTERRUPT LATENCIES">
  <data key="d0">CAUSES</data>
</edge>
<edge source="SOME REGISTER VALUES" target="BECAUSE THEY MIGHT NOT HAVE REACHED THE LOAD-STORE UNIT YET">
  <data key="d0">CANNOT BE FORWARDED</data>
</edge>
<edge source="THE APPROACH" target="HW COMPLEXITY">
  <data key="d0">INCREASES</data>
</edge>
<edge source="THE APPROACH" target="A DYNAMIC ADDRESS LOOKUP INTO A QUEUE-LIKE BUFFER">
  <data key="d0">REQUIRES</data>
</edge>
<edge source="AN SW-BASED SOLUTION" target="ANY KIND OF STALLING">
  <data key="d0">DOES NOT CAUSE</data>
</edge>
<edge source="THE PIPELINE STALLING LOGIC" target="ORDERING THE LOADS TO ACCESS THE ALREADY STORED INTERRUPT STATE FIRST">
  <data key="d0">IS ENGAGED BY</data>
</edge>
<edge source="THE CV32S RF" target="ADDITIONAL LOGIC FOR THE BACKGROUND-SAVING MECHANISM">
  <data key="d0">IS EXTENDED WITH</data>
</edge>
<edge source="PARTS OF THE OLD MEMORY BANK (THE INTERRUPT CONTEXT)" target="THE CORES STACK LOCATION">
  <data key="d0">ARE COPIED TO</data>
</edge>
<edge source="EXECUTION" target="THE NEW BANK">
  <data key="d0">CAN GO AHEAD BY USING</data>
</edge>
<edge source="THE MEMORY PORT" target="THE PORT FROM THE LOAD-STORE UNIT">
  <data key="d0">CAN BE SHARED WITH</data>
</edge>
<edge source="THE PROGRAMMER" target="THAT TO ACHIEVE THE BEST POSSIBLE LATENCY">
  <data key="d0">HAS TO BE AWARE OF</data>
</edge>
<edge source="PROGRAMMER" target="COMPILER-SPECIFIC ATTRIBUTES TO WRITE INTERRUPT HANDLERS">
  <data key="d0">USES</data>
</edge>
<edge source="ATTRIBUTE((INTERRUPT))" target="GCC">
  <data key="d0">IS USED IN</data>
</edge>
<edge source="COMPILER" target="THE FACT THAT PROGRAMMER USES COMPILER-SPECIFIC ATTRIBUTES">
  <data key="d0">NEEDS TO BE MADE AWARE OF</data>
</edge>
<edge source="INTERRUPT HANDLER ROUTINES" target="IN FIG.">
  <data key="d0">ARE SHOWN</data>
</edge>
<edge source="INTERRUPT HANDLER ROUTINES" target="SW-BASED MECHANISMS TO SAVE AND RESTORE INTERRUPT STATE">
  <data key="d0">USE</data>
</edge>
<edge source="INTERRUPT HANDLER ROUTINES" target="THE HANDLER CODE">
  <data key="d0">CAN FULLY INLINE</data>
</edge>
<edge source="INTERRUPT HANDLER ROUTINES" target="SOME CALLER-SAVE REGISTERS">
  <data key="d0">CAN SAVE</data>
</edge>
<edge source="ROUTINES" target="SAVING STATE FOR VECTORED NESTING INTERRUPTS">
  <data key="d0">ARE FOR</data>
</edge>
<edge source="ROUTINES" target="(A) CLINT">
  <data key="d0">USE</data>
</edge>
<edge source="ROUTINES" target="(B) CLIC">
  <data key="d0">USE</data>
</edge>
<edge source="ROUTINES" target="(C) PROPOSED FASTIRQ EXTENSION">
  <data key="d0">USE</data>
</edge>
<edge source="RV32E" target="16 REGISTERS">
  <data key="d0">HAS</data>
</edge>
<edge source="RV32I" target="32 REGISTERS">
  <data key="d0">HAS</data>
</edge>
<edge source="REDUCING THE RF SIZE" target="CONTEXT SWITCH TIMES">
  <data key="d0">HELPS LOWER</data>
</edge>
<edge source="REDUCING THE RF SIZE" target="INTERRUPT LATENCY">
  <data key="d0">DOES NOT AFFECT</data>
</edge>
<edge source="THE SET OF CALLER-SAVE REGISTERS" target="THE SAME">
  <data key="d0">REMAINS</data>
</edge>
<edge source="THE SET OF CALLER-SAVE REGISTERS" target="WHEN USING THE EMBEDDED-APPLICATION BINARY INTERFACE (EABI)">
  <data key="d0">REMAINS THE SAME</data>
</edge>
<edge source="OUR IMPLEMENTATION" target="THE CORE TO DYNAMICALLY SWITCH BETWEEN RV32I AND RV32E WITH FASTIRQ DEPENDING ON THE WORKLOAD">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="INCREASED PRESSURE ON THE RF" target="NOT ACCEPTABLE">
  <data key="d0">IS</data>
</edge>
<edge source="ONE" target="ADDITIONAL REGISTERS FOR THE SEVEN CALLER-SAVE REGISTERS TO SAVE AREA">
  <data key="d0">COULD ADD</data>
</edge>
<edge source="DOUBLING THE RF SIZE FOR THE BANKING LOGIC" target="ALTERNATIVE TO ADDING ADDITIONAL REGISTERS">
  <data key="d0">IS</data>
</edge>
<edge source="THE INTERRUPT STATE" target="SIMPLY SWITCHING REGISTER BANKS">
  <data key="d0">CAN BE RESTORED BY</data>
</edge>
<edge source="MRET" target="THE INTERRUPT STATE HAS BEEN RESTORED BY SW">
  <data key="d0">ASSUMES</data>
</edge>
<edge source="THIS INSTRUCTION" target="THE SAME FUNCTION AS MRET">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="THIS INSTRUCTION" target="SWITCHING REGISTER BANKS">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="AN HW-ASSISTED SOLUTION" target="SUCH A SCENARIO">
  <data key="d0">TO ADDRESS</data>
</edge>
<edge source="THE HW-ASSISTED SOLUTION" target="THE XNXTI CSRS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="NUCLEIS ENHANCED CLIC (ECLIC) 18" target="XNXTI">
  <data key="d0">EXTENDS</data>
</edge>
<edge source="NUCLEIS ENHANCED CLIC (ECLIC) 18" target="THE JUMP TO THE QUEUING INTERRUPT HANDLER IN THE XNXTI HW (JALXNXTI)">
  <data key="d0">EMBEDS</data>
</edge>
<edge source="NUCLEI SYSTEM TECHNOLOGY ECLIC 39" target="TRADITIONAL XNXTI">
  <data key="d0">EXTENDS</data>
</edge>
<edge source="NUCLEI SYSTEM TECHNOLOGY ECLIC 39" target="A NOVEL CSR FOR MACHINE PRIVILEGE MODE">
  <data key="d0">HAS</data>
</edge>
<edge source="JALMNXTI 18" target="THIS WORK IN SECTION IV">
  <data key="d0">IS DISCUSSED IN</data>
</edge>
<edge source="SOME HW" target="THIS CONCEPT OF REMOVING REDUNDANT CONTEXT RESTORES AS TAIL-CHAINING 23">
  <data key="d0">REFERS TO</data>
</edge>
<edge source="INTERRUPT RESPONSE TIME" target="MINIMIZED">
  <data key="d0">IS</data>
</edge>
<edge source="A HIGH-LEVEL INTERRUPT" target="THE CURRENT RUNNING INTERRUPT HANDLER">
  <data key="d0">COULD PREEMPT</data>
</edge>
<edge source="THIS COMBINATION OF LATENCY HIDING AND BACKGROUND SAVING" target="THE CORE TO QUICKLY ENTER A FIRST-LEVEL INTERRUPT HANDLER">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="MIE" target="TO PREVENT THAT">
  <data key="d0">HAS TO BE MANUALLY ADJUSTED</data>
</edge>
<edge source="LOWER-PRIORITY INTERRUPTS" target="DISABLED">
  <data key="d0">ARE</data>
</edge>
<edge source="THIS RETURN PATH" target="HW BY ADDING AN ADDITIONAL WRITE PORT TO THE CORES RF">
  <data key="d0">COULD BE HANDLED IN</data>
</edge>
<edge source="EXITING AN INTERRUPT HANDLER" target="LESS TIME-CRITICAL">
  <data key="d0">IS</data>
</edge>
<edge source="CONTEXT SWITCHES" target="OS-SPECIFIC PARTS">
  <data key="d0">CAN BE DIVIDED INTO</data>
</edge>
<edge source="CONTEXT SWITCHES" target="HW-SPECIFIC PARTS">
  <data key="d0">CAN BE DIVIDED INTO</data>
</edge>
<edge source="THE OS PART" target="ALL CONTRIBUTIONS TO THE CONTEXT SWITCH TIME THAT IS SPECIFIC TO THE OS ITSELF">
  <data key="d0">ENTAILS</data>
</edge>
<edge source="THE OS PART" target="COMPUTING THE NEXT TASK TO BE SCHEDULED">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE OS PART" target="BOOKKEEPING OPERATIONS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="THE REMAINDER" target="THE HW-DEPENDENT SAVING AND RESTORING OF THE STATE BELONGING TO THE NEW CONTEXT">
  <data key="d0">IS</data>
</edge>
<edge source="THE IDEA" target="THE BACKGROUND-SAVING MECHANISM">
  <data key="d0">IS TO USE</data>
</edge>
<edge source="SWITCHES" target="INTERLEAVING THE LOADING OF A NEW STATE WITH THE AUTOMATIC SAVING HW">
  <data key="d0">BY</data>
</edge>
<edge source="THE HW MECHANISM" target="THE REGISTERS">
  <data key="d0">TO SWAP</data>
</edge>
<edge source="THE HW MECHANISM" target="THEM IN THE BACKGROUND">
  <data key="d0">TO SAVE</data>
</edge>
<edge source="THE INITIAL PART OF THE CONTEXT SWITCH ROUTINE" target="FOR THAT">
  <data key="d0">CHANGES</data>
</edge>
<edge source="ANY" target="CONTEXT SWITCHING">
  <data key="d0">IMPACT ON</data>
</edge>
<edge source="ADDITIONAL RISC-V EXTENSIONS" target="MORE CONTEXT SWITCHING STATE">
  <data key="d0">INTRODUCE</data>
</edge>
<edge source="MORE CONTEXT SWITCHING STATE" target="CONTRARY TO THE GOALS OF FASTIRQ REGARDING LATENCIES">
  <data key="d0">RUN</data>
</edge>
<edge source="ADDING MORE STATE TO FASTIRQ" target="NO TECHNICAL LIMITATIONS">
  <data key="d0">HAS</data>
</edge>
<edge source="THE RESULTING DESIGN" target="A SIGNIFICANT INCREASE IN AREA AND POWER">
  <data key="d0">WOULD INCUR</data>
</edge>
<edge source="A DIRTY BIT" target="KEEP THE FAST PATH COMPETITIVE">
  <data key="d0">COULD HELP TO</data>
</edge>
<edge source="A DIRTY BIT" target="CONTEXT LAZY SWITCHABLE">
  <data key="d0">IS TO MAKE</data>
</edge>
<edge source="A SPECIFIC EXTENSION" target="REQUIRED">
  <data key="d0">IS STILL</data>
</edge>
<edge source="A FUNCTIONAL AND QUANTITATIVE EVALUATION" target="THE VARIOUS FLAVORS OF THE CV32RT">
  <data key="d0">OF</data>
</edge>
<edge source="INTERRUPT LINES" target="A HARDWIRED PRIORITIZATION SCHEME">
  <data key="d0">HAVE</data>
</edge>
<edge source="THE BASELINE CLIC" target="THESE WEAKNESSES">
  <data key="d0">ADDRESSES</data>
</edge>
<edge source="THE BASELINE CLIC" target="THE REGULAR CLINT">
  <data key="d0">DOES NOT DIFFER FROM</data>
</edge>
<edge source="THE BASELINE CLIC" target="STORING AND RESTORING THE INTERRUPT CONTEXT">
  <data key="d0">DOES NOT DIFFER FROM THE REGULAR CLINT IN TERMS OF</data>
</edge>
<edge source="A LEVEL THRESHOLD REGISTER PER PRIVILEGE LEVEL (XINTTHRESH)" target="THE SET OF ALLOWED HORIZONTAL INTERRUPTS">
  <data key="d0">CONTROLS</data>
</edge>
<edge source="THE LEVEL THRESHOLD REGISTER PER PRIVILEGE LEVEL (XINTTHRESH)" target="THE SET OF ALLOWED HORIZONTAL INTERRUPTS TO THOSE WHOSE LEVEL EXCEEDS THE GIVEN VALUE IN THE REGISTER">
  <data key="d0">LIMITS</data>
</edge>
<edge source="VECTORING" target="SELECTIVELY ENABLED OR DISABLED PER INTERRUPT LINE">
  <data key="d0">CAN BE</data>
</edge>
<edge source="THIS SOLUTION" target="BOTH VECTORED AND NON-VECTORED INTERRUPTS">
  <data key="d0">WORKS FOR</data>
</edge>
<edge source="STORING AND RESTORING THE INTERRUPT CONTEXT" target="SW">
  <data key="d0">IS FULLY HANDLED IN</data>
</edge>
<edge source="THE OPTIONAL XNXTI EXTENSION" target="MULTIPLE HORIZONTAL INTERRUPTS TO BE SERVICED IN SEQUENCE WITHOUT REDUNDANT CONTEXT-RESTORING OPERATIONS IN BETWEEN">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THE FIRST INTERRUPT" target="THE FULL LATENCY COST">
  <data key="d0">HAS TO PAY</data>
</edge>
<edge source="READING THE XNXTI CSR" target="A POINTER TO THE VECTOR TABLE ENTRY FOR THE NEXT PENDING AND QUALIFYING INTERRUPT">
  <data key="d0">YIELDS</data>
</edge>
<edge source="THE POINTER" target="A DIRECT JUMP THERE">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THE DISCUSSED DIFFERENCES" target="TABLE II">
  <data key="d0">ARE SUMMARIZED IN</data>
</edge>
<edge source="MEASUREMENTS" target="RUNNING RTL SIMULATIONS">
  <data key="d0">ARE TAKEN BY</data>
</edge>
<edge source="RTL SIMULATIONS" target="DIFFERENT VERSIONS OF CV32RT">
  <data key="d0">ARE OF</data>
</edge>
<edge source="THE MEMORY BANK WE ARE USING" target="OTHER BUS MASTERS">
  <data key="d0">IS NOT CONTENDED BY</data>
</edge>
<edge source="ADDITIONAL LATENCIES" target="ON INTERRUPT LINES BETWEEN INTERRUPT SOURCES AND THE CLIC">
  <data key="d0">ARE NOT INTRODUCED</data>
</edge>
<edge source="HW CONTRIBUTED INTERRUPT LATENCY" target="SECTION II-B3">
  <data key="d0">IS DESCRIBED IN</data>
</edge>
<edge source="CV32 AND CV32RT VARIATIONS" target="STANDARD CLIC, XNXTI, JALXNXTI">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="9" target="THE AREA BREAKDOWN COMPARISON OF CV32RT WITH BASELINE CLIC (INCLUDING MNXTI AND JALMNXTI CSRS) AND FASTIRQ EXTENSIONS WITH 256 INPUT INTERRUPTS">
  <data key="d0">REPORTS</data>
</edge>
<edge source="ALL CALLER-SAVE REGISTERS" target="SAVED">
  <data key="d0">NEED TO BE</data>
</edge>
<edge source="ALL CALLER-SAVE REGISTERS" target="THE RESPECTIVE ABI">
  <data key="d0">ARE IN</data>
</edge>
<edge source="ALL CALLER-SAVE REGISTERS NEED TO BE SAVED" target="THE GENERAL CASE">
  <data key="d0">IS</data>
</edge>
<edge source="THE SAVED GENERAL-PURPOSE REGISTERS" target="ALL CALLER-SAVE REGISTERS">
  <data key="d0">ARE</data>
</edge>
<edge source="INTERRUPT HANDLER ROUTINES THAT SAVE THE INTERRUPT CONTEXT IN SW" target="THE MINIMUM STATE">
  <data key="d0">CAN ONLY SAVE</data>
</edge>
<edge source="THE COMPILER" target="FULLY INLINE THE HANDLERS FUNCTION BODY">
  <data key="d0">IS ABLE TO</data>
</edge>
<edge source="THE COMPILER" target="FULLY INLINE THE HANDLER CODE">
  <data key="d0">PERMITS</data>
</edge>
<edge source="ONLY ONE CALLER-SAVE REGISTER" target="SAVING FOR SW-BASED INTERRUPT HANDLERS">
  <data key="d0">NEEDS</data>
</edge>
<edge source="THIS AMOUNT OF STATE" target="FOR OUR FASTIRQ EXTENSION">
  <data key="d0">IS ALWAYS SAVED</data>
</edge>
<edge source="TABLE III" target="THE MAIN TECHNIQUES FOR OPTIMIZING INTERRUPT CONTEXT AND TASK CONTEXT SAVE-RESTORE WITH NESTED AND NON-NESTED INTERRUPTS">
  <data key="d0">COMPARES</data>
</edge>
<edge source="TABLE III" target="THE OVERVIEW">
  <data key="d0">SUMMARIZES</data>
</edge>
<edge source="THE MAIN TECHNIQUES" target="INDUSTRY AND ACADEMIA">
  <data key="d0">ARE EMPLOYED BY</data>
</edge>
<edge source="THE MAIN TECHNIQUES" target="EMBEDDED AND REAL-TIME APPLICATION DOMAINS">
  <data key="d0">ARE USED IN</data>
</edge>
<edge source="THE RESULTS" target="FIG.">
  <data key="d0">ARE SUMMARIZED IN</data>
</edge>
<edge source="SEVERAL DESIGNS" target="THE INTERRUPT CONTEXT">
  <data key="d0">SAVE AND RESTORE</data>
</edge>
<edge source="SEVERAL DESIGNS" target="THE INTERRUPT CONTEXT DIRECTLY IN HW">
  <data key="d0">SAVE AND RESTORE</data>
</edge>
<edge source="AUTOMATIC INTERRUPT CONTEXT SAVERESTORE" target="SW HOUSEKEEPING OVERHEAD BEFORE AND AFTER HANDLING THE INTERRUPT ROUTINE">
  <data key="d0">REDUCES</data>
</edge>
<edge source="AUTOMATIC INTERRUPT CONTEXT SAVERESTORE" target="ACCELERATION OF THE COMPLETE TASK CONTEXT SWITCH">
  <data key="d0">ONLY PARTIALLY ADDRESSES</data>
</edge>
<edge source="PRESENTED SOLUTIONS" target="OPTIMIZING CONTEXT">
  <data key="d0">ARE EFFECTIVE IN</data>
</edge>
<edge source="PRESENTED SOLUTIONS" target="A COHESIVE APPROACH TO ADDRESS BOTH INTERRUPT CONTEXT AND TASK CONTEXT SWITCH ACCELERATION">
  <data key="d0">LACK</data>
</edge>
<edge source="SAVERESTORE WITH HW AND SW COOPERATION" target="OPTIMIZING CONTEXT">
  <data key="d0">ARE EFFECTIVE IN</data>
</edge>
<edge source="EXISTING RISC-V-BASED APPROACHES" target="THE GAP WITH WELL-ESTABLISHED INDUSTRY VENDORS">
  <data key="d0">CANNOT CLOSE</data>
</edge>
<edge source="INTERRUPT CONTEXT SAVERESTORE" target="THE FORMER">
  <data key="d0">TAKES ADVANTAGE OF</data>
</edge>
<edge source="INTERRUPT CONTEXT SAVERESTORE" target="THE HW">
  <data key="d0">DEFERS OPERATIONS TO</data>
</edge>
<edge source="XNXTI AND JALXNXTI" target="EVEN WORSE">
  <data key="d0">PERFORM</data>
</edge>
<edge source="XNXTI AND JALXNXTI" target="42 AND 35 CYCLES RESPECTIVELY">
  <data key="d0">REQUIRE</data>
</edge>
<edge source="XNXTI AND JALXNXTI" target="ADDITIONAL INSTRUCTIONS IN THE CODE PATH BETWEEN THE HANDLER AND INTERRUPT EVENT">
  <data key="d0">INSERT</data>
</edge>
<edge source="XNXTI AND JALXNXTI" target="THIS SITUATION">
  <data key="d0">IMPROVE UPON</data>
</edge>
<edge source="XNXTI AND JALXNXTI" target="PENDING INTERRUPTS">
  <data key="d0">CHECK FOR</data>
</edge>
<edge source="XNXTI AND JALXNXTI" target="THE RESPECTIVE HANDLERS">
  <data key="d0">JUMP TO</data>
</edge>
<edge source="THE SMALL CODE SEQUENCE" target="LOAD, JUMP, AND RETRY LOOP">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="JALXNXTI" target="THESE OPERATIONS INTO ONE INSTRUCTION">
  <data key="d0">FUSES</data>
</edge>
<edge source="FUSING THESE OPERATIONS INTO ONE INSTRUCTION" target="SAVING NINE CYCLES">
  <data key="d0">RESULTS IN</data>
</edge>
<edge source="ARM CORTEX-M4" target="INTERRUPT LATENCY OF 12 CYCLES">
  <data key="d0">HAS</data>
</edge>
<edge source="THE ARM CORTEX-M4" target="THE SAME TASK IN SIX CYCLES">
  <data key="d0">IS ABLE TO DO</data>
</edge>
<edge source="THE ARM CORTEX-M4" target="SINGLE-CYCLE MEMORY 24">
  <data key="d0">HAS ACCESS TO</data>
</edge>
<edge source="BASELINE CLIC" target="68 CYCLES WHEN USING INTEGER ABI">
  <data key="d0">TAKES</data>
</edge>
<edge source="BASELINE CLIC" target="50 CYCLES WHEN USING EMBEDDED ABI">
  <data key="d0">TAKES</data>
</edge>
<edge source="THE EMRET MECHANISM OF FASTIRQ" target="SIMILARLY">
  <data key="d0">WORKS</data>
</edge>
<edge source="THE EMRET MECHANISM OF FASTIRQ" target="EIGHT CLOCK CYCLES">
  <data key="d0">COSTS</data>
</edge>
<edge source="THE EMRET MECHANISM OF FASTIRQ" target="NON-VECTORED INTERRUPTS">
  <data key="d0">IS NOT RESTRICTED TO</data>
</edge>
<edge source="THE CONTEXT SWITCH TIME" target="THE BASELINE CV32RT AGAINST CV32RTFASTIRQ">
  <data key="d0">IS SHOWN WHEN COMPARING</data>
</edge>
<edge source="AVERAGE CONTEXT SWITCH TIME" target="FREERTOS">
  <data key="d0">IS IN</data>
</edge>
<edge source="AVERAGE CONTEXT SWITCH TIME" target="TWO TASKS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="FREERTOS" target="REAL-TIME OPERATING SYSTEM FOR MICROCONTROLLERS">
  <data key="d0">IS</data>
</edge>
<edge source="TWO TASKS" target="VARIOUS FLAVORS OF CV32RT">
  <data key="d0">ARE OF</data>
</edge>
<edge source="ALL COMPILE TIME OPTIONS SUCH AS TRACING, STACK OVERFLOW SIGNALING, AND THE MORE GENERIC TASK SELECTION MECHANISM" target="TURNED OFF">
  <data key="d0">WERE</data>
</edge>
<edge source="ALL COMPILE TIME OPTIONS SUCH AS TRACING, STACK OVERFLOW SIGNALING, AND THE MORE GENERIC TASK SELECTION MECHANISM" target="MINIMIZE THE CONTEXT SWITCH CODE">
  <data key="d0">WERE TURNED OFF TO</data>
</edge>
<edge source="GENERIC CORTEX-M4 CORE" target="SINGLE-CYCLE ACCESS TO MEMORY">
  <data key="d0">HAS</data>
</edge>
<edge source="AN SW INTERRUPT" target="THE SAVE SEQUENCE">
  <data key="d0">IS PART OF</data>
</edge>
<edge source="THE SW INTERRUPT" target="THE FASTIRQ MECHANISM">
  <data key="d0">WILL TRIGGER</data>
</edge>
<edge source="THE FASTIRQ MECHANISM" target="SAVING THE GENERAL-PURPOSE REGISTERS TO MEMORY">
  <data key="d0">STARTS</data>
</edge>
<edge source="USING THE I-EXTENSION" target="31 CYCLES FOR A CONTEXT SWITCH">
  <data key="d0">CAN SAVE UP TO</data>
</edge>
<edge source="USING THE E-EXTENSION" target="16 CYCLES FOR A CONTEXT SWITCH">
  <data key="d0">CAN SAVE UP TO</data>
</edge>
<edge source="NOT TO USE REGISTERS THAT ARE STILL BEING SAVED BY THE BACKGROUND-SAVING MECHANISM" target="SAVING CYCLES FOR A CONTEXT SWITCH">
  <data key="d0">RESULTS IN</data>
</edge>
<edge source="SOME SOCS SUCH AS THE STM32L476RG" target="HIGHER LATENCIES">
  <data key="d0">HAVE</data>
</edge>
<edge source="HIGHER LATENCIES" target="MEMORY ACCESS STALLS AND OTHER IMPLEMENTATION CHOICES IN THE MEMORY SUBSYSTEM">
  <data key="d0">ARE DUE TO</data>
</edge>
<edge source="THE RISC-V E-EXTENSION" target="THE AVAILABLE GENERAL-PURPOSE REGISTERS FROM 32 TO 16">
  <data key="d0">REDUCES</data>
</edge>
<edge source="THE RISC-V E-EXTENSION" target="THE CONTEXT SWITCH STATE THAT NEEDS TO BE SAVED AND RESTORED">
  <data key="d0">LOWERS</data>
</edge>
<edge source="RISC-V CLIC AREA" target="VARYING NUMBERS OF INTERRUPT SOURCES">
  <data key="d0">BREAKDOWN AT</data>
</edge>
<edge source="REPORTS" target="THE CLIC IMPLEMENTED">
  <data key="d0">THE AREA BREAKDOWN OF</data>
</edge>
<edge source="THE PROPOSED" target="DIFFERENT INTERRUPT SOURCES">
  <data key="d0">HAS</data>
</edge>
<edge source="AREA OVERHEAD" target="CV32RT">
  <data key="d0">OF</data>
</edge>
<edge source="AREA OVERHEAD OF CV32RT" target="THE TWO MAIN CONFIGURATIONS">
  <data key="d0">IN</data>
</edge>
<edge source="THE OVERHEAD OF FASTIRQ IN CV32RTFASTIRQ CORE" target="A MINIMAL 10 AREA INCREASE">
  <data key="d0">RESULTS IN</data>
</edge>
<edge source="A MINIMAL 10 AREA INCREASE" target="THE ID STAGE">
  <data key="d0">IS CONCENTRATED AROUND</data>
</edge>
<edge source="THE ID STAGE" target="THE HW BLOCK WHERE THE ADDITIONAL REGISTERS AND THE AUTOMATIC STACKING-UNSTACKING LOGIC ARE LOCALIZED">
  <data key="d0">IS</data>
</edge>
<edge source="THE DESIGN" target="GF12LP TECHNOLOGY">
  <data key="d0">HAS BEEN SYNTHESIZED IN</data>
</edge>
<edge source="GF12LP TECHNOLOGY" target="500 MHZ">
  <data key="d0">OPERATES AT</data>
</edge>
<edge source="GF12LP TECHNOLOGY" target="TT CORNER">
  <data key="d0">USES</data>
</edge>
<edge source="GF12LP TECHNOLOGY" target="25 C">
  <data key="d0">OPERATES AT TEMPERATURE</data>
</edge>
<edge source="GF12LP TECHNOLOGY" target="0.8 V">
  <data key="d0">OPERATES AT VOLTAGE</data>
</edge>
<edge source="GF12LP TECHNOLOGY" target="SUPER LOW VT STANDARD CELLS">
  <data key="d0">INCLUDES</data>
</edge>
<edge source="CV32E40PRT ID STAGE" target="AREA BREAKDOWN WITH THE PROPOSED HW EXTENSIONS">
  <data key="d0">HAS</data>
</edge>
<edge source="MORE THAN HALF OF THE RESOURCES" target="THE CONFIGURATION REGISTERS REQUIRED TO CONTROL THE CLIC">
  <data key="d0">IMPLEMENT</data>
</edge>
<edge source="THE SIZE" target="LINEARLY WITH THE NUMBER OF INPUT INTERRUPTS">
  <data key="d0">INCREASES</data>
</edge>
<edge source="THE REMAINING AREA" target="THE GATEWAY AND BINARY TREE ARBITRATION LOGIC AT THE CORE OF THE CLIC WORKING PRINCIPLE">
  <data key="d0">IMPLEMENTS</data>
</edge>
<edge source="THE REMAINING AREA" target="ADDITIONAL HOUSEKEEPING CONTROL LOGIC">
  <data key="d0">IMPLEMENTS</data>
</edge>
<edge source="THE GATEWAY AND BINARY TREE ARBITRATION LOGIC" target="SECTION III-B">
  <data key="d0">IS DISCUSSED IN</data>
</edge>
<edge source="ADDITIONAL HOUSEKEEPING CONTROL LOGIC" target="LINEARLY WITH THE NUMBER OF INTERRUPT SOURCES">
  <data key="d0">SCALES</data>
</edge>
<edge source="THE FRACTION OF THE DESIGN OCCUPIED BY THE ARBITRATION TREE" target="WHEN INCREASING THE NUMBER OF SOURCES">
  <data key="d0">IS KEPT CONSTANT</data>
</edge>
<edge source="THE GAIN IN FLEXIBILITY" target="A BROADER APPLICATION SCOPE WITH TIME-CRITICAL SYSTEMS">
  <data key="d0">ENABLES</data>
</edge>
<edge source="INSTRUCTION DECODE (ID) STAGE" target="AN AREA OVERHEAD OF 21 COMPARED TO CV32RTCLIC">
  <data key="d0">INCURS</data>
</edge>
<edge source="OTHER HW BLOCKS OF THE CORE" target="PRIMARILY UNAFFECTED">
  <data key="d0">REMAIN</data>
</edge>
<edge source="A BREAKDOWN OF THE ID STAGE" target="FIG.">
  <data key="d0">IS SHOWN IN</data>
</edge>
<edge source="ADDITIONAL STORAGE SPACE FOR AUTOMATIC CONTEXT SAVE AND RESTORE IN HW" target="AREA OF THE RF BY ABOUT 36 IN THE PROPOSED IMPLEMENTATION">
  <data key="d0">INCREASES</data>
</edge>
<edge source="THE LOGIC FOR MANAGING THE SHADOW REGISTERS" target="AN OVERHEAD OF 40 ON THE BASELINE ID STAGE CONTROLLER">
  <data key="d0">ACCOUNTS FOR</data>
</edge>
<edge source="HW OVERHEAD" target="NEGLIGIBLE">
  <data key="d0">IS</data>
</edge>
<edge source="HW OVERHEAD" target="ADDITIONAL EMRET INSTRUCTION">
  <data key="d0">COMES FROM</data>
</edge>
<edge source="ADDITIONAL EMRET INSTRUCTION" target="SECTION IV">
  <data key="d0">IS DISCUSSED IN</data>
</edge>
<edge source="INCREASED SIZE OF THE ID STAGE" target="BENEFITS OF A SIMPLIFIED PROGRAMMING MODEL THAT MOVES SEVERAL SW OPERATIONS IN HW">
  <data key="d0">TRADES OFF</data>
</edge>
<edge source="INCREASED SIZE OF THE ID STAGE" target="BENEFITS OF SIGNIFICANTLY LOWERED INTERRUPT LATENCY THAN STANDARD RISC-V">
  <data key="d0">TRADES OFF</data>
</edge>
<edge source="INCREASED SIZE OF THE ID STAGE" target="CRITICAL PATH OF THE BASE CORE DESIGN">
  <data key="d0">DOES NOT IMPACT</data>
</edge>
<edge source="TIME-CRITICAL SYSTEMS" target="AREA EFFICIENCY">
  <data key="d0">SHIFT DESIGN PRIORITIES FROM</data>
</edge>
<edge source="TIME-CRITICAL SYSTEMS" target="SAFETY, SECURITY, AND RELIABILITY">
  <data key="d0">SHIFT DESIGN PRIORITIES TO</data>
</edge>
<edge source="SOLUTIONS" target="INTERRUPT CONTEXT SAVERESTORE TECHNIQUES">
  <data key="d0">ADDRESS</data>
</edge>
<edge source="SOLUTIONS" target="CONTEXT SWITCH TECHNIQUES">
  <data key="d0">ADDRESS</data>
</edge>
<edge source="SOLUTIONS" target="DEDICATED STRATEGIES TO OPTIMIZE REDUNDANT CONTEXT RESTORE WITH BACK-TO-BACK INTERRUPTS">
  <data key="d0">ADDRESS</data>
</edge>
<edge source="RETURN AUTHORIZED LICENSED USE" target="CALIFORNIA POLYTECHNIC STATE UNIVERSITY SAN LUIS OBISPO">
  <data key="d0">IS LIMITED TO</data>
</edge>
<edge source="ARMS GENERIC INTERRUPT CONTROLLER (GIC)" target="INCOMING ASYNCHRONOUS EVENTS">
  <data key="d0">REDISTRIBUTES</data>
</edge>
<edge source="INCOMING ASYNCHRONOUS EVENTS" target="NON-CRITICAL (IRQ) OR CRITICAL INTERRUPTS (FAST IRQ, OR FIQ)">
  <data key="d0">ARE REDISTRIBUTED AS</data>
</edge>
<edge source="DEDICATED REGISTER BANK" target="UP TO EIGHT REGISTERS">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="UP TO EIGHT REGISTERS" target="MINIMIZE CONTEXT SWITCHING">
  <data key="d0">ARE EMPLOYED TO</data>
</edge>
<edge source="STATE MACHINE 32" target="CALLER-SAVE REGISTER STACKING IN THE BACKGROUND">
  <data key="d0">PERFORMS</data>
</edge>
<edge source="THE VALUE (EXCRETURN)" target="THE CORE TO START UNWINDING THE STACK TO RETURN TO NORMAL PROGRAM EXECUTION">
  <data key="d0">NOTIFIES</data>
</edge>
<edge source="INTERRUPT CONTROL UNIT (ICU) 33, 34, 35" target="INFINEON AURIX MCU-CLASS TRICORE FAMILYS">
  <data key="d0">IS IN</data>
</edge>
<edge source="THE CONTEXT OF THE CALLING ROUTINE" target="MEMORY AUTONOMOUSLY">
  <data key="d0">IS SAVED IN</data>
</edge>
<edge source="RESTORING THE CONTEXT" target="THE RET INSTRUCTION">
  <data key="d0">IS EMBEDDED IN</data>
</edge>
<edge source="RESTORING THE CONTEXT" target="THE RETURN JUMP 36">
  <data key="d0">HAPPENS IN PARALLEL WITH</data>
</edge>
<edge source="27" target="EXTENSIONS FOR THE RISC-V CLIC">
  <data key="d0">ARE THE FIRST TO PROPOSE</data>
</edge>
<edge source="INTERRUPT HANDLING" target="AUTOMATIC STACKING IN HW">
  <data key="d0">IS ENHANCED WITH</data>
</edge>
<edge source="AUTOMATIC STACKING IN HW" target="CORES HARVARD ARCHITECTURE">
  <data key="d0">BENEFITS FROM</data>
</edge>
<edge source="AUTOMATIC STACKING IN HW" target="SIMULTANEOUS DATA AND INSTRUCTION MEMORY ACCESS">
  <data key="d0">BENEFITS FROM</data>
</edge>
<edge source="REGISTER BANKING" target="A TECHNIQUE">
  <data key="d0">IS</data>
</edge>
<edge source="REGISTER BANKING" target="SEVERAL ARCHITECTURES">
  <data key="d0">IS ADOPTED BY</data>
</edge>
<edge source="REGISTER BANKING" target="TASKS CONTEXT">
  <data key="d0">SWAPS</data>
</edge>
<edge source="REGISTER BANKING" target="REGISTER VALUES TO THE STACK">
  <data key="d0">DOES NOT PUSH OR POP</data>
</edge>
<edge source="REGISTER BANKING" target="ADDITIONAL AREA OVERHEAD IN THE DESIGN">
  <data key="d0">HAS COST</data>
</edge>
<edge source="A TASKS CONTEXT SWITCH" target="QUICKLY TRANSFERRING THE SUSPENDED CONTEXT TO THE DEDICATED REGISTER BANK">
  <data key="d0">BENEFITS FROM</data>
</edge>
<edge source="A TASKS CONTEXT SWITCH" target="ALREADY RESTORING THE NEXT TASK TO BE EXECUTED">
  <data key="d0">BENEFITS FROM</data>
</edge>
<edge source="A SIMILAR APPROACH" target="THE RENESAS M32C80 SERIES 30">
  <data key="d0">IS IMPLEMENTED IN</data>
</edge>
<edge source="A DUAL REGISTER BANK" target="QUICKLY SWAPPING THE CONTEXT WITHOUT SAVING OR RESTORING TO OR FROM THE STACK">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="THE SECOND REGISTER BANK" target="HIGH-SPEED INTERRUPTS">
  <data key="d0">IS RESERVED FOR</data>
</edge>
<edge source="THE AURIX FAMILY" target="AN SW MANAGED SOLUTION">
  <data key="d0">IMPLEMENTS</data>
</edge>
<edge source="THE SW MANAGED SOLUTION" target="A SPECIFIC ORGANIZATION OF THE CONTEXT LAYOUT IN THE SYSTEM MEMORY">
  <data key="d0">FEATURES</data>
</edge>
<edge source="THE CONTEXT LAYOUT IN THE SYSTEM MEMORY" target="CONTEXT SAVE AREA (CSA) CHAINED IN A LINKED LIST FASHION">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="A MORE COMPLEX APPROACH" target="THE INTUITION THAT OFTEN THE CONTENT OF SOME REGISTER REMAINS UNTOUCHED AFTER A CONTEXT SWITCH">
  <data key="d0">IS BASED ON</data>
</edge>
<edge source="A MORE COMPLEX APPROACH" target="HUANG ET AL.">
  <data key="d0">IS INTRODUCED BY</data>
</edge>
<edge source="VALID-BASED MECHANISM IN HW" target="CONTEXT SWITCH ON SELECTED REGISTERS">
  <data key="d0">BLOCKS</data>
</edge>
<edge source="SEMI-SHADOWING" target="REGISTER BANKING">
  <data key="d0">IS SIMILAR TO</data>
</edge>
<edge source="THE LOWER 16 ARCHITECTURAL REGISTERS" target="THE RF">
  <data key="d0">ARE OF</data>
</edge>
<edge source="THE EVALUATION IN 28" target="HW IMPLEMENTATION">
  <data key="d0">LACKS</data>
</edge>
<edge source="THE EVALUATION IN 28" target="AREA OVERHEAD ASSESSMENT">
  <data key="d0">LACKS</data>
</edge>
<edge source="THE EVALUATION IN 28" target="CONTEXT SWITCHING OVERHEAD">
  <data key="d0">REDUCES</data>
</edge>
<edge source="THE EVALUATION IN 28" target="24">
  <data key="d0">REDUCES CONTEXT SWITCHING OVERHEAD BY</data>
</edge>
<edge source="THE EVALUATION IN 28" target="THE DSPSTONE BENCHMARK">
  <data key="d0">REDUCES CONTEXT SWITCHING OVERHEAD BY 24 ON</data>
</edge>
<edge source="24" target="ARM">
  <data key="d0">IS</data>
</edge>
<edge source="THESE APPROACHES" target="HW- AND SW-INDUCED LATENCIES WHEN HANDLING ASYNCHRONOUS EVENTS">
  <data key="d0">TRADE-OFF</data>
</edge>
<edge source="HARDWARE SCHEDULING ENGINE (HSE)" target="INTERRUPTS TO RUNNING TASKS">
  <data key="d0">DIRECTLY ATTACHES</data>
</edge>
<edge source="HARDWARE SCHEDULING ENGINE (HSE)" target="WITHOUT THE NEED FOR A SPECIALIZED INTERRUPT CONTROLLER">
  <data key="d0">OPERATES</data>
</edge>
<edge source="ITS AREA OVERHEAD" target="A HIGH NUMBER OF TASKS">
  <data key="d0">GROWS FOR</data>
</edge>
<edge source="SUCH WORKS" target="RF CACHING">
  <data key="d0">ADOPT</data>
</edge>
<edge source="RF CACHING" target="REGISTER SHADOWING OR BANKING">
  <data key="d0">IS ADOPTED RATHER THAN</data>
</edge>
<edge source="RF CACHING" target="PERFORMANCE REASONS">
  <data key="d0">IS ADOPTED FOR</data>
</edge>
<edge source="PERFORMANCE REASONS" target="LOWER ACCESS LATENCY TO THE RF">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="PERFORMANCE REASONS" target="HIGHER THREAD-LEVEL PARALLELISM (TLP)">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="SUCH TECHNIQUES" target="THE SYSTEMS PRE-DICTABILITY">
  <data key="d0">MAY DETERIORATE</data>
</edge>
<edge source="RISC-V AIA WITHOUT APLIC" target="MNXTI WITH THE XTOPI CSR">
  <data key="d0">HAS A SIMILAR APPROACH TO</data>
</edge>
<edge source="RISC-V AIA WITHOUT APLIC" target="BOTH LATE ARRIVAL AND REDUNDANT CONTEXT RESTORE MECHANISMS">
  <data key="d0">ALLOWS</data>
</edge>
<edge source="XTOPI CSR" target="THE HIGHEST-PRIORITY, PENDING, AND ENABLED INTERRUPT">
  <data key="d0">REPORTS</data>
</edge>
<edge source="THE HIGHEST-PRIORITY, PENDING, AND ENABLED INTERRUPT" target="A SPECIFIC PRIVILEGE MODE">
  <data key="d0">IS FOR</data>
</edge>
<edge source="THE AUTHORS" target="SOME RESEARCH DIRECTIONS FOR FUTURE WORK">
  <data key="d0">CONSIDER</data>
</edge>
<edge source="SOME RESEARCH DIRECTIONS" target="ANALYZING FASTIRQS IMPACT ON TIMING CHANNELS">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="SOME RESEARCH DIRECTIONS" target="ITS INTEGRATION WITH DIFFERENT RISC-V EXTENSIONS">
  <data key="d0">INVOLVE</data>
</edge>
<edge source="C. ROCHANGE, S. UHRIG, AND P. SAINRAT" target="TIME-PREDICTABLE ARCHITECTURES (FOCUS COMPUTER ENGINEERING SERIES)">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="WILEY" target="IN 2014">
  <data key="d0">PUBLISHED</data>
</edge>
<edge source="WILEY" target="HOBOKEN, NJ, USA">
  <data key="d0">LOCATION</data>
</edge>
<edge source="L. M. PINHO ET AL." target="HIGH-PERFORMANCE AND TIME-PREDICTABLE EMBEDDED COMPUTING">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="HIGH-PERFORMANCE AND TIME-PREDICTABLE EMBEDDED COMPUTING" target="HTTP:EU.WILEY.COMWILEYCDA WILEYTITLEPRODUCTCD-1848215932.HTML">
  <data key="d0">AVAILABLE AT</data>
</edge>
<edge source="RIVER" target="WHARTON, TX, USA">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="RIVER" target="2018">
  <data key="d0">YEAR</data>
</edge>
<edge source="3 F. REGHENZANI, G. MASSARI, AND W. FORNACIARI" target="THE REAL-TIME LINUX KERNEL: A SURVEY ON PREEMPT">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="136" target="FEB. 2019">
  <data key="d0">IS</data>
</edge>
<edge source="DOI" target="10.11453297714">
  <data key="d0">IS</data>
</edge>
<edge source="DOI" target="10.11453419973">
  <data key="d0">IS</data>
</edge>
<edge source="M. LIU, D. LIU, Y. WANG, M. WANG, AND Z. SHAO" target="ON IMPROVING REAL-TIME INTERRUPT LATENCIES OF HYBRID OPERATING SYSTEMS WITH TWO-LEVEL HARDWARE INTERRUPTS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="ON IMPROVING REAL-TIME INTERRUPT LATENCIES OF HYBRID OPERATING SYSTEMS WITH TWO-LEVEL HARDWARE INTERRUPTS" target="IEEE TRANS">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="P. MANTEGAZZA, E. L. DOZIO, AND S. PAPACHARALAMBOUS" target="RTAI: REAL TIME APPLICATION INTERFACE">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="6 K. RAMAMRITHAM AND J." target="TEXT">
  <data key="d0">IS</data>
</edge>
<edge source="J. VALVANO" target="INTRODUCTION TO EMBEDDED SYSTEMS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="INTRODUCTION TO EMBEDDED SYSTEMS" target="PDF">
  <data key="d0">FORMAT</data>
</edge>
<edge source="CREATESPACE" target="SCOTTS VALLEY, CA, USA">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="CREATESPACE" target="AUG. 2016">
  <data key="d0">DATE</data>
</edge>
<edge source="9 Y. HUANG, L. SHI, J. LI, Q. LI, AND C. J. XUE" target="WCET-AWARE RE-SCHEDULING REGISTER ALLOCATION FOR REAL-TIME EMBEDDED SYSTEMS WITH CLUSTERED VLIW ARCHITECTURE">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="WCET-AWARE RE-SCHEDULING REGISTER ALLOCATION FOR REAL-TIME EMBEDDED SYSTEMS WITH CLUSTERED VLIW ARCHITECTURE" target="IEEE TRANS.">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="VERY LARGE SCALE INTEGR" target="TEXT">
  <data key="d0">IS</data>
</edge>
<edge source="X. ZHOU AND P. PETROV" target="RAPID AND LOW-COST CONTEXT-SWITCH THROUGH EMBEDDED PROCESSOR CUSTOMIZATION FOR REAL-TIME AND CONTROL APPLICATIONS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="RAPID AND LOW-COST CONTEXT-SWITCH THROUGH EMBEDDED PROCESSOR CUSTOMIZATION FOR REAL-TIME AND CONTROL APPLICATIONS" target="PROC.">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTING MACHINERY" target="NEW YORK, NY, USA">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="ASSOCIATION FOR COMPUTING MACHINERY" target="2006">
  <data key="d0">PUBLISHED</data>
</edge>
<edge source="I. BEHNKE, L. PIRL, L. THAMSEN, R. DANICKI, A. POLZE, AND O. KAO" target="INTERRUPTING REAL-TIME IOT TASKS: HOW BAD CAN IT BE TO CONNECT YOUR CRITICAL EMBEDDED SYSTEM TO THE INTERNET?">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="12 F. REHM ET AL." target="THE ROAD TOWARDS PREDICTABLE AUTOMOTIVE HIGH PERFORMANCE PLATFORMS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="THE ROAD TOWARDS PREDICTABLE AUTOMOTIVE HIGH PERFORMANCE PLATFORMS" target="PROC.">
  <data key="d0">APPEARED_IN</data>
</edge>
<edge source="TEST" target="EUR">
  <data key="d0">IS</data>
</edge>
<edge source="13 K. ASANOVIC AND D. A. PATTERSON" target="INSTRUCTION SETS SHOULD BE FREE: THE CASE FOR RISC-V, DEPT.">
  <data key="d0">WROTE</data>
</edge>
<edge source="A. WATERMAN, Y. LEE, R. AVIZIENIS, D. A. PATTERSON, AND K. ASANOVIC" target="THE RISC-V INSTRUCTION SET MANUAL VOLUME II: PRIVILEGED ARCHITECTURE VERSION 1.9">
  <data key="d0">ARE AUTHORS OF</data>
</edge>
<edge source="THE RISC-V INSTRUCTION SET MANUAL VOLUME II: PRIVILEGED ARCHITECTURE VERSION 1.9" target="DEPT.">
  <data key="d0">IS FROM</data>
</edge>
<edge source="EECS" target="UNIV.">
  <data key="d0">IS PART OF</data>
</edge>
<edge source="CALIFORNIA" target="BERKELEY">
  <data key="d0">CONTAINS</data>
</edge>
<edge source="BERKELEY" target="CA">
  <data key="d0">IS_LOCATED_IN</data>
</edge>
<edge source="BERKELEY" target="TECH">
  <data key="d0">HAS</data>
</edge>
<edge source="CA" target="USA">
  <data key="d0">IS_LOCATED_IN</data>
</edge>
<edge source="ACCESSED" target="JUN.">
  <data key="d0">DATE</data>
</edge>
<edge source="ACCESSED" target="AUG. 4, 2023">
  <data key="d0">DATE</data>
</edge>
<edge source="RISCV-FAST-INTERRUPT-BLOB" target="HTTPS://GITHUB.COM/RISCV/RISCV-FAST-INTERRUPT-BLOB/MASTER/CLIC">
  <data key="d0">IS AVAILABLE AT</data>
</edge>
<edge source="ADOC 16 M. GAUTSCHI ET AL." target="NEAR-THRESHOLD RISC-V CORE WITH DSP EXTENSIONS FOR SCALABLE IOT ENDPOINT DEVICES">
  <data key="d0">IS</data>
</edge>
<edge source="ADOC 16 M. GAUTSCHI ET AL." target="IEEE TRANS.">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="17" target="D. SCHIAVONE">
  <data key="d0">IS</data>
</edge>
<edge source="OPENHW GROUP" target="CV32E40P USER MANUAL">
  <data key="d0">HAS</data>
</edge>
<edge source="OPENHW GROUP" target="ORGANIZATION">
  <data key="d0">IS</data>
</edge>
<edge source="READTHEDOCS.IOEN" target="18 NUCLEI SYSTEM TECHNOLOGY">
  <data key="d0">LATEST</data>
</edge>
<edge source="39 NUCLEI SYSTEM TECHNOLOGY CO. LTD." target="ECLIC UNIT">
  <data key="d0">HAS UNIT</data>
</edge>
<edge source="ECLIC UNIT" target="INTRODUCTION">
  <data key="d0">IS</data>
</edge>
<edge source="NUCLEI" target="SPEC">
  <data key="d0">ISA</data>
</edge>
<edge source="CONTROL PULP" target="A RISC-V ON-CHIP PARALLEL POWER CONTROLLER">
  <data key="d0">IS</data>
</edge>
<edge source="CONTROL PULP" target="MANY-CORE HPC PROCESSORS">
  <data key="d0">IS FOR</data>
</edge>
<edge source="CONTROL PULP" target="FPGA-BASED HARDWARE-IN-THE-LOOP POWER AND THERMAL EMULATION">
  <data key="d0">USES</data>
</edge>
<edge source="PARALLEL PROGRAM." target="FEB. 2024">
  <data key="d0">HAS PUBLICATION DATE</data>
</edge>
<edge source="PARALLEL PROGRAM." target="10.1007S10766-024-00761-4">
  <data key="d0">HAS DOI</data>
</edge>
<edge source="REAL TIME ENGINEERS LTD." target="ONLINE">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS://WWW.FREERTOS.ORG/INDEX.HTML" target="AVAILABLE">
  <data key="d0">IS</data>
</edge>
<edge source="C.-M. LIN" target="NESTED INTERRUPT ANALYSIS OF LOW COST AND HIGH PERFORMANCE EMBEDDED SYSTEMS USING GSPN FRAMEWORK">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="NESTED INTERRUPT ANALYSIS OF LOW COST AND HIGH PERFORMANCE EMBEDDED SYSTEMS USING GSPN FRAMEWORK" target="IEICE TRANS.">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="HTTPS:GITHUB.COMRISCVRISCV-PLIC-SPECBLOBMASTERRISCV-PLIC-1.0.0.PDF" target="AVAILABLE">
  <data key="d0">IS</data>
</edge>
<edge source="J. YIU" target="THE DEFINITIVE GUIDE TO ARM CORTEX-M3 CORTEX-M4 PROCESSORS">
  <data key="d0">IS AUTHOR OF</data>
</edge>
<edge source="THE DEFINITIVE GUIDE TO ARM CORTEX-M3 CORTEX-M4 PROCESSORS" target="3RD ED.">
  <data key="d0">HAS EDITION</data>
</edge>
<edge source="NEWNES" target="BOSTON, MA, USA">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="NEWNES" target="2013">
  <data key="d0">PUBLISHED_IN_YEAR</data>
</edge>
<edge source="CORTEX-M4" target="TECH">
  <data key="d0">IS</data>
</edge>
<edge source="DOCUMENTATION" target="HTTPS://DEVELOPER.ARM.COM/DOCUMENTATION/1001660001">
  <data key="d0">AVAILABLE AT</data>
</edge>
<edge source="DOCUMENTATION" target="J. YIU">
  <data key="d0">AUTHOR</data>
</edge>
<edge source="CIRCUITS MICROSYSTEMS (ICICM)" target="OCT. 2021">
  <data key="d0">DATE</data>
</edge>
<edge source="CIRCUITS MICROSYSTEMS (ICICM)" target="PP.">
  <data key="d0">PAGE</data>
</edge>
<edge source="LI AND J. K. LEE" target="PAGED REGISTER FILES">
  <data key="d0">SUPPORT</data>
</edge>
<edge source="PAGED REGISTER FILES" target="CONTEXT SWITCHING">
  <data key="d0">IMPROVE</data>
</edge>
<edge source="29 R. BALAS AND L. BENINI" target="RISC-V FOR REAL-TIME MCU SOFTWARE OPTIMIZATION AND MICROARCHITECTURAL GAP ANALYSIS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="RISC-V FOR REAL-TIME MCU SOFTWARE OPTIMIZATION AND MICROARCHITECTURAL GAP ANALYSIS" target="PROC.">
  <data key="d0">PUBLISHED_IN</data>
</edge>
<edge source="HARDWARE MANUAL" target="RENESAS MCU M16C">
  <data key="d0">DESCRIBES</data>
</edge>
<edge source="HARDWARE MANUAL" target="2021">
  <data key="d0">YEAR</data>
</edge>
<edge source="HTTPS://WWW.RENESAS.COM/US/ENDOCUMENT/MAHM32C87-GROUP-M32C87-M32C87A-M32C87B-HARDWARE-MANUAL" target="TRUE">
  <data key="d0">IS AVAILABLE</data>
</edge>
<edge source="SIFIVE INC." target="HARDWARE MANUAL">
  <data key="d0">PUBLISHED</data>
</edge>
<edge source="SIFIVE E21 CORE COMPLEX" target="MANUAL">
  <data key="d0">IS</data>
</edge>
<edge source="HTTPS:SIFIVE.CDN.PRISMIC.IOSIFIVE7C22C2EC-8AF4-4B6C-A5FE-9327D91E7808E21CORECOMPLEXMANUAL21G1.PDF" target="AVAILABLE">
  <data key="d0">IS</data>
</edge>
<edge source="STMICROELECTRONICS" target="MENTIONED">
  <data key="d0">IS</data>
</edge>
<edge source="STM32L5-SYSTEM-NESTEDVECTORED INTERRUPTCONTROLNVIC.PDF" target="HTTPS://WWW.ST.COM/CONTENT/CCC/RESOURCE/TRAINING/TECHNICAL/PRODUCTTRAINING/GROUP16135D207346F4E83/STM32L5-SYSTEM-NESTEDVECTOREDINTERRUPTCONTROLNVICFILES/STM32L5-SYSTEM-NESTEDVECTOREDINTERRUPTCONTROLNVIC.PDF">
  <data key="d0">IS AVAILABLE AT</data>
</edge>
<edge source="INFINEON TECHNOLOGIES AG" target="STM32L5-SYSTEM-NESTEDVECTORED INTERRUPTCONTROLNVIC.PDF">
  <data key="d0">IS THE COPYRIGHT HOLDER OF</data>
</edge>
<edge source="INFINEON TECHNOLOGIES AG" target="HC163TUE6HC16SESS7PRES1BW.PDF">
  <data key="d0">IS MENTIONED IN</data>
</edge>
<edge source="INFINEON TECHNOLOGIES AG" target="U.S. PATENT 7 434 222 B2">
  <data key="d0">HAS PATENT</data>
</edge>
<edge source="POWERTRAIN MICROCONTROLLER" target="FAST">
  <data key="d0">IS</data>
</edge>
<edge source="HC163TUE6HC16SESS7PRES1BW.PDF" target="HTTPS://OLD.HOTCHIPS.ORG/WP-CONTENT/UPLOADS/HCARCHIVES/HC163TUE6HC16SESS7PRES1BW.PDF">
  <data key="d0">IS AVAILABLE AT</data>
</edge>
<edge source="FILE" target="HTTPS:HITEX.CO.UKFILEADMINUK-FILESDOWNLOADSSHIELDBUDDYTC27XDUMV2.2.PDF">
  <data key="d0">AVAILABLE_AT</data>
</edge>
<edge source="FILE" target="35">
  <data key="d0">SIZE</data>
</edge>
<edge source="FILE" target="INFINEON TECHNOLOGIES AG">
  <data key="d0">RELATED_TO</data>
</edge>
<edge source="TC27X D-STEP" target="32-BIT SINGLE-CHIP MICROCONTROLLER">
  <data key="d0">IS</data>
</edge>
<edge source="TRICORE V1.6" target="CORE ARCHITECTURE">
  <data key="d0">IS</data>
</edge>
<edge source="U.S. PATENT 7 434 222 B2" target="TASK CONTEXT SWITCHING RTOS">
  <data key="d0">IS ABOUT</data>
</edge>
<edge source="U.S. PATENT 7 434 222 B2" target="OCT. 2008">
  <data key="d0">WAS ISSUED</data>
</edge>
<edge source="H. ZENG AND K. GHOSE" target="REGISTER FILE CACHING FOR ENERGY EFFICIENCY">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="REGISTER FILE CACHING FOR ENERGY EFFICIENCY" target="ISLPED PROC">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="LOW POWER" target="ELECTRON">
  <data key="d0">IS</data>
</edge>
<edge source="DESIGN" target="OCT. 2006">
  <data key="d0">IS</data>
</edge>
<edge source="DESIGN" target="PP.">
  <data key="d0">HAS PAGE</data>
</edge>
<edge source="38 M. SADROSADATI ET AL." target="HIGHLY CONCURRENT LATENCY-TOLERANT REGISTER FILES FOR GPUS">
  <data key="d0">AUTHORED</data>
</edge>
<edge source="HIGHLY CONCURRENT LATENCY-TOLERANT REGISTER FILES FOR GPUS" target="ACM TRANS.">
  <data key="d0">PUBLISHED IN</data>
</edge>
<edge source="DEGREE" target="THE DIGITAL CIRCUITS AND SYSTEMS GROUP OF PROF. BENINI">
  <data key="d0">IS AT</data>
</edge>
<edge source="DEGREE" target="PHYSICAL ENGINEERING">
  <data key="d0">IS IN</data>
</edge>
<edge source="DEGREE" target="POLITECNICO DI TURINO">
  <data key="d0">IS FROM</data>
</edge>
<edge source="DEGREE" target="2018">
  <data key="d0">WAS OBTAINED IN</data>
</edge>
<edge source="DEGREE" target="ELECTRICAL ENGINEERING">
  <data key="d0">IS IN</data>
</edge>
<edge source="DEGREE" target="GRENOBLE INP-PHELMA">
  <data key="d0">IS FROM</data>
</edge>
<edge source="DEGREE" target="EPFL LAUSANNE">
  <data key="d0">IS FROM</data>
</edge>
<edge source="DEGREE" target="2020">
  <data key="d0">WAS OBTAINED IN</data>
</edge>
<edge source="DEGREE" target="STANFORD UNIVERSITY">
  <data key="d0">FROM</data>
</edge>
<edge source="DEGREE" target="1997">
  <data key="d0">YEAR</data>
</edge>
<edge source="HIS RESEARCH INTERESTS" target="REAL-TIME COMPUTING">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="HIS RESEARCH INTERESTS" target="COMPILERS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="HIS RESEARCH INTERESTS" target="OPERATING SYSTEMS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="HIS RESEARCH INTERESTS" target="POWER MANAGEMENT OF HPC PROCESSORS">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="HIS RESEARCH INTERESTS" target="ENERGY-EFFICIENT PROCESSOR ARCHITECTURE">
  <data key="d0">INCLUDE</data>
</edge>
<edge source="POLITECNICO DI TURINO" target="TURIN">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="TURIN" target="ITALY">
  <data key="d0">IS LOCATED IN</data>
</edge>
<edge source="GRENOBLE INP-PHELMA" target="GRENOBLE, FRANCE">
  <data key="d0">IS IN</data>
</edge>
<edge source="EPFL LAUSANNE" target="LAUSANNE, SWITZERLAND">
  <data key="d0">IS IN</data>
</edge>
<edge source="STANFORD UNIVERSITY" target="STANFORD, CA, USA">
  <data key="d0">LOCATION</data>
</edge>
<edge source="DR. BENINI" target="THE ACM">
  <data key="d0">IS A FELLOW OF</data>
</edge>
<edge source="DR. BENINI" target="THE ACADEMIA EUROPAEA">
  <data key="d0">IS A MEMBER OF</data>
</edge>
<edge source="MCCLUSKEY AWARD" target="AWARD">
  <data key="d0">IS</data>
</edge>
</graph></graphml>