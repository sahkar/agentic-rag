[
  {
    "subject": "This paper",
    "predicate": "is included in",
    "object": "Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "date",
    "object": "November 4-6, 2020"
  },
  {
    "subject": "Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "ISBN",
    "object": "978-1-939133-19-9"
  },
  {
    "subject": "Open access to the Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "is sponsored by",
    "object": "USENIX"
  },
  {
    "subject": "PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications",
    "predicate": "authors",
    "object": "Zhihao Bai, Zhen Zhang, Yibo Zhu, Xin Jin"
  },
  {
    "subject": "Zhihao Bai",
    "predicate": "affiliation",
    "object": "Johns Hopkins University"
  },
  {
    "subject": "Zhen Zhang",
    "predicate": "affiliation",
    "object": "Johns Hopkins University"
  },
  {
    "subject": "Yibo Zhu",
    "predicate": "affiliation",
    "object": "ByteDance Inc."
  },
  {
    "subject": "Xin Jin",
    "predicate": "affiliation",
    "object": "Johns Hopkins University"
  },
  {
    "subject": "Deep learning workloads",
    "predicate": "include",
    "object": "throughput-intensive training tasks and latency-sensitive inference tasks"
  },
  {
    "subject": "The dominant practice today",
    "predicate": "is",
    "object": "to provision dedicated GPU clusters for training and inference separately"
  },
  {
    "subject": "GPU Clusters",
    "predicate": "are",
    "object": "Shared GPU clusters"
  },
  {
    "subject": "training and inference",
    "predicate": "use",
    "object": "GPUs"
  },
  {
    "subject": "the current practice",
    "predicate": "is to build",
    "object": "dedicated clusters for training and inference separately"
  },
  {
    "subject": "We",
    "predicate": "envision to build",
    "object": "GPU clusters"
  },
  {
    "subject": "GPU clusters",
    "predicate": "can be shared across",
    "object": "different applications including training and inference"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are often over-provisioned",
    "object": "based on the peak load"
  },
  {
    "subject": "GPU clusters",
    "predicate": "have",
    "object": "limited sharing between applications and task types"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are over-provisioned due to",
    "object": "the need to meet strict Service-Level Objectives (SLOs)"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is",
    "object": "a system"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "unused cycles of an inference application to be filled by training or other inference applications"
  },
  {
    "subject": "multiple DL applications",
    "predicate": "time-share",
    "object": "the same GPU"
  },
  {
    "subject": "the same GPU",
    "predicate": "has",
    "object": "the entire GPU memory"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can improve",
    "object": "GPU utilization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can improve GPU utilization",
    "object": "significantly"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can improve GPU utilization",
    "object": "without sacrificing SLOs"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "incurs",
    "object": "a task startup overhead of 3.66.6 ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "incurs",
    "object": "a total overhead of 5.434.6 ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is",
    "object": "1050 better than NVIDIA MPS"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "near 100 GPU utilization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is focused on",
    "object": "single-GPU tasks for training and inference"
  },
  {
    "subject": "Multi-GPU inference tasks",
    "predicate": "can be supported by",
    "object": "performing PipeSwitch on each GPU with transactions"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "single-GPU training for training tasks"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "supports",
    "object": "asynchronous multi-GPU training for data parallel strategies"
  },
  {
    "subject": "preempting one GPU",
    "predicate": "does not affect",
    "object": "other GPUs"
  },
  {
    "subject": "a significant fraction of tasks in real-world workloads",
    "predicate": "use",
    "object": "a single GPU"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is applicable to",
    "object": "a significant fraction of tasks in real-world workloads"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can be used for",
    "object": "synchronous multi-GPU training"
  },
  {
    "subject": "elastic synchronous training",
    "predicate": "allows",
    "object": "dynamic changing of the number of GPUs used for training"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "high throughput close to the upper bound"
  },
  {
    "subject": "We",
    "predicate": "demonstrate",
    "object": "the performance of PipeSwitch"
  },
  {
    "subject": "experiments",
    "predicate": "are on",
    "object": "a variety of DNN models and GPU cards"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can significantly increase",
    "object": "GPU utilization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can improve",
    "object": "the agility of DL applications"
  },
  {
    "subject": "We",
    "predicate": "achieve",
    "object": "introducing pipelined context switching"
  },
  {
    "subject": "The key idea",
    "predicate": "is to leverage",
    "object": "the layered structure of neural network models and their layer-by-layer computation pattern"
  },
  {
    "subject": "The key idea",
    "predicate": "is to pipeline",
    "object": "model transmission over the PCIe and task execution in the GPU with model-aware grouping"
  },
  {
    "subject": "we",
    "predicate": "design",
    "object": "a pipelined model transmission mechanism"
  },
  {
    "subject": "a pipelined model transmission mechanism",
    "predicate": "pipelines",
    "object": "model transmission over the PCIe and model computation in the GPU"
  },
  {
    "subject": "Transmitting a task from CPU to GPU",
    "predicate": "is bounded by",
    "object": "the PCIe bandwidth"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "unified memory management mechanisms"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "active-standby worker switching mechanisms"
  },
  {
    "subject": "unified memory management and active-standby worker switching mechanisms",
    "predicate": "accompany",
    "object": "the pipelining"
  },
  {
    "subject": "unified memory management and active-standby worker switching mechanisms",
    "predicate": "ensure",
    "object": "process-level isolation"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "an active-standby mechanism for fast worker switching and process-level isolation"
  },
  {
    "subject": "We",
    "predicate": "have built",
    "object": "a PipeSwitch prototype"
  },
  {
    "subject": "We",
    "predicate": "have integrated",
    "object": "it with PyTorch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "does not modify",
    "object": "the model structure"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "adds",
    "object": "hooks for PyTorch to wait for transmission or synchronize the execution"
  },
  {
    "subject": "Deep learning (DL)",
    "predicate": "powers",
    "object": "an emerging family of intelligent applications"
  },
  {
    "subject": "intelligent applications",
    "predicate": "are in",
    "object": "many domains"
  },
  {
    "subject": "many domains",
    "predicate": "include",
    "object": "retail"
  },
  {
    "subject": "many domains",
    "predicate": "include",
    "object": "transportation"
  },
  {
    "subject": "many domains",
    "predicate": "include",
    "object": "finance"
  },
  {
    "subject": "many domains",
    "predicate": "include",
    "object": "healthcare"
  },
  {
    "subject": "GPUs",
    "predicate": "are",
    "object": "one of the most widely-used classes of accelerators for DL"
  },
  {
    "subject": "DL workloads",
    "predicate": "include",
    "object": "throughput-intensive training tasks"
  },
  {
    "subject": "DL workloads",
    "predicate": "include",
    "object": "latency-sensitive inference tasks"
  },
  {
    "subject": "Inference tasks",
    "predicate": "cannot be served with",
    "object": "training clusters under ash crowds"
  },
  {
    "subject": "Training tasks",
    "predicate": "cannot utilize",
    "object": "inference clusters when the inference load is low"
  },
  {
    "subject": "training cluster",
    "predicate": "cannot preempt",
    "object": "training tasks for inference tasks"
  },
  {
    "subject": "inference clusters",
    "predicate": "are often over-provisioned for",
    "object": "the peak load"
  },
  {
    "subject": "Inference clusters",
    "predicate": "are over-provisioned for",
    "object": "the peak load"
  },
  {
    "subject": "Inference clusters",
    "predicate": "directly serve",
    "object": "user requests"
  },
  {
    "subject": "Inference clusters",
    "predicate": "need to meet",
    "object": "strict SLOs"
  },
  {
    "subject": "production systems",
    "predicate": "are provisioned to",
    "object": "each application on per-GPU granularity"
  },
  {
    "subject": "provisioning on per-GPU granularity",
    "predicate": "limits",
    "object": "the interference between applications"
  },
  {
    "subject": "production systems",
    "predicate": "allocate",
    "object": "GPUs to applications on per-GPU granularity"
  },
  {
    "subject": "production systems",
    "predicate": "bind",
    "object": "GPUs to the VMs, containers or processes of an application"
  },
  {
    "subject": "production systems",
    "predicate": "limit",
    "object": "the interference between different applications"
  },
  {
    "subject": "production systems",
    "predicate": "satisfy",
    "object": "the SLO requirements"
  },
  {
    "subject": "multiple DL applications",
    "predicate": "should be able to be packed to",
    "object": "the same GPU server"
  },
  {
    "subject": "packing multiple DL applications to the same GPU server",
    "predicate": "maximizes",
    "object": "GPU utilization via time-sharing"
  },
  {
    "subject": "operating systems",
    "predicate": "achieve",
    "object": "high CPU utilization"
  },
  {
    "subject": "high CPU utilization",
    "predicate": "achieved via",
    "object": "task scheduling and context switching"
  },
  {
    "subject": "The idea of ne-grained CPU time-sharing",
    "predicate": "has been further extended to",
    "object": "cluster scheduling"
  },
  {
    "subject": "ne-grained time-sharing",
    "predicate": "is similar to",
    "object": "CPU workloads"
  },
  {
    "subject": "ne-grained time-sharing",
    "predicate": "can provide",
    "object": "better utilization than provisioning dedicated resources"
  },
  {
    "subject": "ne-grained time-sharing",
    "predicate": "provides",
    "object": "necessary process-level isolation"
  },
  {
    "subject": "scheduling cycles",
    "predicate": "are",
    "object": "ne-grained enabled"
  },
  {
    "subject": "Google Borg 1",
    "predicate": "packs",
    "object": "online services and batch jobs"
  },
  {
    "subject": "Google Borg 1",
    "predicate": "saves",
    "object": "20-30 machines"
  },
  {
    "subject": "20-30 machines",
    "predicate": "are saved compared with",
    "object": "not packing them"
  },
  {
    "subject": "GPU",
    "predicate": "has",
    "object": "high overhead when switching between tasks"
  },
  {
    "subject": "The gap",
    "predicate": "is",
    "object": "the precious GPU memory and slow switching"
  },
  {
    "subject": "naively using GPUs in the same way as CPUs",
    "predicate": "will not satisfy",
    "object": "the requirements of DL inference that have strict SLOs in the range of tens to hundreds of milliseconds"
  },
  {
    "subject": "a GPU",
    "predicate": "switches to",
    "object": "a DNN model (e.g., ResNet)"
  },
  {
    "subject": "a DNN model (e.g., ResNet)",
    "predicate": "has not been preloaded onto",
    "object": "the GPU"
  },
  {
    "subject": "state-of-the-art tricks like CUDA unified memory",
    "predicate": "do not prevent",
    "object": "multiple seconds delay before serving the first inference request"
  },
  {
    "subject": "CPU applications",
    "predicate": "can be switched in",
    "object": "milliseconds or even microseconds"
  },
  {
    "subject": "the existing solution",
    "predicate": "is to",
    "object": "spatially share the GPU memory"
  },
  {
    "subject": "this approach",
    "predicate": "does not provide",
    "object": "strong GPU memory isolation between applications"
  },
  {
    "subject": "NVIDIA Multiple Process Sharing (MPS) 6",
    "predicate": "allow",
    "object": "multiple processes to use the same GPU"
  },
  {
    "subject": "Salus 7",
    "predicate": "allow",
    "object": "multiple processes to use the same GPU"
  },
  {
    "subject": "NVIDIA Multiple Process Sharing (MPS) 6 and Salus 7",
    "predicate": "require",
    "object": "all processes data to be preloaded into the GPU memory"
  },
  {
    "subject": "multi-process support from NVIDIA",
    "predicate": "allows",
    "object": "the inference process to share the GPU with the training process"
  },
  {
    "subject": "NVIDIA MPS 6",
    "predicate": "provides",
    "object": "official support for sharing a GPU between multiple processes"
  },
  {
    "subject": "GPU memory",
    "predicate": "is",
    "object": "much more limited than host memory"
  },
  {
    "subject": "GPU memory",
    "predicate": "cannot preload",
    "object": "many applications"
  },
  {
    "subject": "one single memory-intensive training task",
    "predicate": "may consume",
    "object": "all the GPU memory"
  },
  {
    "subject": "The training task",
    "predicate": "stops",
    "object": "its GPU environment"
  },
  {
    "subject": "The training task",
    "predicate": "cleans",
    "object": "its GPU environment"
  },
  {
    "subject": "its GPU environment",
    "predicate": "includes",
    "object": "freeing the GPU memory"
  },
  {
    "subject": "The training task",
    "predicate": "occupies",
    "object": "the entire GPU memory"
  },
  {
    "subject": "The training task",
    "predicate": "does not stop",
    "object": "when inference tasks come"
  },
  {
    "subject": "memory footprints of inference tasks",
    "predicate": "are increasing",
    "object": "memory footprints"
  },
  {
    "subject": "models",
    "predicate": "are getting",
    "object": "larger"
  },
  {
    "subject": "request batching",
    "predicate": "is used",
    "object": "to increase throughput"
  },
  {
    "subject": "increasing throughput",
    "predicate": "increases",
    "object": "GPU memory requirement of inference applications"
  },
  {
    "subject": "a context switching design",
    "predicate": "minimizes",
    "object": "the switching overhead"
  },
  {
    "subject": "a context switching design",
    "predicate": "quickly switches",
    "object": "the contents on GPU memory"
  },
  {
    "subject": "a context switching design",
    "predicate": "is",
    "object": "a better approach for efficiently time-sharing GPUs"
  },
  {
    "subject": "no existing solution",
    "predicate": "offers",
    "object": "such context switching abstraction for GPU"
  },
  {
    "subject": "We",
    "predicate": "introduce",
    "object": "a new technology called pipelined context switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "exploits",
    "object": "the characteristics of DL applications"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "achieves",
    "object": "millisecond-scale overhead for switching tasks on GPUs"
  },
  {
    "subject": "we",
    "predicate": "face",
    "object": "a major challenge fast GPU context switching between different processes"
  },
  {
    "subject": "pipeline",
    "predicate": "overlaps",
    "object": "computation and GPU memory swapping"
  },
  {
    "subject": "pipeline",
    "predicate": "is possible to build for",
    "object": "fast context switching"
  },
  {
    "subject": "application",
    "predicate": "is already loaded in",
    "object": "GPU"
  },
  {
    "subject": "context switching",
    "predicate": "is not needed",
    "object": "if the application is already loaded in the GPU"
  },
  {
    "subject": "We",
    "predicate": "introduce",
    "object": "pipelined context switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "minimizes",
    "object": "task switching overhead on GPUs for DL applications"
  },
  {
    "subject": "DNN models",
    "predicate": "can be held in",
    "object": "host memory"
  },
  {
    "subject": "host memory",
    "predicate": "is",
    "object": "much larger and cheaper than GPU memory"
  },
  {
    "subject": "GPU",
    "predicate": "can quickly context-switch between",
    "object": "the models"
  },
  {
    "subject": "context-switching",
    "predicate": "is for",
    "object": "training or inference"
  },
  {
    "subject": "enterprises",
    "predicate": "build",
    "object": "GPU clusters"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are shared by",
    "object": "multiple users"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are either",
    "object": "privately shared"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are either",
    "object": "publicly shared"
  },
  {
    "subject": "11 M. Jeon, S. Venkataraman, A. Phanishayee, u. Qian, W. Xiao, and F. Yang",
    "predicate": "authored",
    "object": "Analysis of large-scale multi-tenant GPU clusters for DNN training workloads"
  },
  {
    "subject": "Analysis of large-scale multi-tenant GPU clusters for DNN training workloads",
    "predicate": "published in",
    "object": "USENIX ATC"
  },
  {
    "subject": "Analysis of large-scale multi-tenant GPU clusters for DNN training workloads",
    "predicate": "published in year",
    "object": "2019"
  },
  {
    "subject": "Analysis of large-scale multi-tenant GPU clusters for DNN training workloads",
    "predicate": "was published in",
    "object": "USENIX ATC, 2019"
  },
  {
    "subject": "512 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "is organized by",
    "object": "USENIX Association"
  },
  {
    "subject": "the number of applications that can be multiplexed",
    "predicate": "is not limited by",
    "object": "the GPU memory size"
  },
  {
    "subject": "each application",
    "predicate": "is able to use",
    "object": "the entire GPU compute and memory resources during its time slice"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "GPU-efficient multiplexing of many DL applications on GPU servers via fine-grained time-sharing"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "millisecond-scale latencies and high throughput as dedicated servers"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "GPU-efficient fine-grained time-sharing for multiple DL applications"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "millisecond-scale context switching latencies"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "high throughput"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "combines",
    "object": "all the ideas into our system"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "closes",
    "object": "the gap of GPU memory sharing and switching"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "the design of an efficient time-sharing GPU cluster for DL workloads"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "GPU-efficient multiplexing of multiple DL applications on GPU servers"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is able to achieve",
    "object": "millisecond-scale task switching time"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "DL applications on time-sharing GPUs to meet strict SLOs"
  },
  {
    "subject": "small switching overhead",
    "predicate": "is critical for",
    "object": "DL applications"
  },
  {
    "subject": "DL applications",
    "predicate": "to satisfy",
    "object": "strict SLO requirements"
  },
  {
    "subject": "we",
    "predicate": "perform",
    "object": "a measurement study"
  },
  {
    "subject": "a measurement study",
    "predicate": "profiles",
    "object": "the task switching overhead"
  },
  {
    "subject": "a measurement study",
    "predicate": "analyzes",
    "object": "the overhead of each component"
  },
  {
    "subject": "measurement study",
    "predicate": "is performed to",
    "object": "profile the task switching overhead"
  },
  {
    "subject": "switching overhead",
    "predicate": "is divided into",
    "object": "four components"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "old task cleaning"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "new task initialization"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "GPU memory allocation"
  },
  {
    "subject": "four components",
    "predicate": "are",
    "object": "model transmission via PCIe from CPU to GPU"
  },
  {
    "subject": "Instance Type",
    "predicate": "includes",
    "object": "g4dn.2xlarge"
  },
  {
    "subject": "Instance Type",
    "predicate": "includes",
    "object": "p3.2xlarge"
  },
  {
    "subject": "GPU Type of g4dn.2xlarge",
    "predicate": "is",
    "object": "NVIDIA T4"
  },
  {
    "subject": "GPU Type of p3.2xlarge",
    "predicate": "is",
    "object": "NVIDIA V100"
  },
  {
    "subject": "Task Cleaning time on g4dn.2xlarge",
    "predicate": "is",
    "object": "155 ms"
  },
  {
    "subject": "Task Cleaning time on p3.2xlarge",
    "predicate": "is",
    "object": "165 ms"
  },
  {
    "subject": "Task Initialization time on g4dn.2xlarge",
    "predicate": "is",
    "object": "5530 ms"
  },
  {
    "subject": "Task Initialization time on p3.2xlarge",
    "predicate": "is",
    "object": "7290 ms"
  },
  {
    "subject": "Memory Allocation time on g4dn.2xlarge",
    "predicate": "is",
    "object": "10 ms"
  },
  {
    "subject": "Memory Allocation time on p3.2xlarge",
    "predicate": "is",
    "object": "13 ms"
  },
  {
    "subject": "Model Transmission time on g4dn.2xlarge",
    "predicate": "is",
    "object": "91 ms"
  },
  {
    "subject": "Model Transmission time on p3.2xlarge",
    "predicate": "is",
    "object": "81 ms"
  },
  {
    "subject": "Total Overhead on g4dn.2xlarge",
    "predicate": "is",
    "object": "5787 ms"
  },
  {
    "subject": "Total Overhead on p3.2xlarge",
    "predicate": "is",
    "object": "7551 ms"
  },
  {
    "subject": "Inference Time on g4dn.2xlarge",
    "predicate": "is",
    "object": "105 ms"
  },
  {
    "subject": "Inference Time on p3.2xlarge",
    "predicate": "is",
    "object": "32 ms"
  },
  {
    "subject": "Table 1",
    "predicate": "shows",
    "object": "Measurement results of task switching overhead and the breakdown of individual components"
  },
  {
    "subject": "Every component",
    "predicate": "takes",
    "object": "a considerable amount of time"
  },
  {
    "subject": "amount of time",
    "predicate": "varies from",
    "object": "tens of milliseconds to seconds"
  },
  {
    "subject": "inference task",
    "predicate": "takes",
    "object": "tens of milliseconds on a GPU"
  },
  {
    "subject": "latency SLOs",
    "predicate": "are",
    "object": "typically a small multiple of the inference time"
  },
  {
    "subject": "One source of the overhead",
    "predicate": "is",
    "object": "the contentions both on the computation and memory of the GPU"
  },
  {
    "subject": "the training task",
    "predicate": "do not stop",
    "object": "when an inference task comes"
  },
  {
    "subject": "We",
    "predicate": "take",
    "object": "a holistic approach"
  },
  {
    "subject": "We",
    "predicate": "exploit",
    "object": "the characteristics of DL applications"
  },
  {
    "subject": "the characteristics of DL applications",
    "predicate": "minimize",
    "object": "the overhead of all the components"
  },
  {
    "subject": "Our design",
    "predicate": "is based on",
    "object": "a key observation"
  },
  {
    "subject": "DNN models",
    "predicate": "have",
    "object": "a layered structure"
  },
  {
    "subject": "DNN models",
    "predicate": "have",
    "object": "a layer-by-layer computation pattern"
  },
  {
    "subject": "DNN models",
    "predicate": "are",
    "object": "usually deep"
  },
  {
    "subject": "DNN models",
    "predicate": "consist of",
    "object": "multiple layers stacking one on another"
  },
  {
    "subject": "computation of DNN models",
    "predicate": "takes place",
    "object": "layer by layer"
  },
  {
    "subject": "there",
    "predicate": "is no need to wait for",
    "object": "the entire model to be transmitted to the GPU before starting computation"
  },
  {
    "subject": "a task",
    "predicate": "does not need to wait for",
    "object": "the entire model to be transmitted to the GPU before beginning the computation"
  },
  {
    "subject": "Naive pipelining on per-layer granularity",
    "predicate": "introduces",
    "object": "high overhead on tensor transmission and synchronization"
  },
  {
    "subject": "Pipelining on per-layer granularity",
    "predicate": "requires",
    "object": "synchronization for every layer"
  },
  {
    "subject": "We",
    "predicate": "divide",
    "object": "layers into groups"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "an optimal model-aware grouping algorithm"
  },
  {
    "subject": "an optimal model-aware grouping algorithm",
    "predicate": "finds",
    "object": "the best grouping strategy for a given model"
  },
  {
    "subject": "we",
    "predicate": "design",
    "object": "an algorithm to find the optimal grouping strategy for a given model"
  },
  {
    "subject": "The computation of a DL task",
    "predicate": "is",
    "object": "layer by layer"
  },
  {
    "subject": "The computation of a DL task",
    "predicate": "has",
    "object": "a simple, regular pattern for memory allocation"
  },
  {
    "subject": "A DL task",
    "predicate": "stores",
    "object": "two important types of data in the GPU memory"
  },
  {
    "subject": "two important types of data",
    "predicate": "are",
    "object": "the DNN model (including the model parameters) and the intermediate results"
  },
  {
    "subject": "The default general-purpose GPU memory management",
    "predicate": "is",
    "object": "an overkill"
  },
  {
    "subject": "The default general-purpose GPU memory management",
    "predicate": "incurs",
    "object": "unnecessary overhead"
  },
  {
    "subject": "NVIDIA",
    "predicate": "provides",
    "object": "CUDA unified memory 4"
  },
  {
    "subject": "CUDA unified memory 4",
    "predicate": "handles",
    "object": "memory movement between the host memory and the GPU memory"
  },
  {
    "subject": "CUDA unified memory 4",
    "predicate": "handles memory movement for",
    "object": "applications"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "unified memory management with a dedicated memory daemon"
  },
  {
    "subject": "unified memory management with a dedicated memory daemon",
    "predicate": "minimize",
    "object": "the overhead"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "unified memory management with the memory daemon"
  },
  {
    "subject": "unified memory management with the memory daemon",
    "predicate": "achieve",
    "object": "minimal memory footprint"
  },
  {
    "subject": "unified memory management with the memory daemon",
    "predicate": "eliminate",
    "object": "extra memory copies"
  },
  {
    "subject": "The daemon",
    "predicate": "pre-allocates",
    "object": "the GPU memory"
  },
  {
    "subject": "The daemon",
    "predicate": "re-allocates",
    "object": "it to each task"
  },
  {
    "subject": "The daemon",
    "predicate": "does not involve",
    "object": "the expensive GPU memory manager"
  },
  {
    "subject": "the memory daemon",
    "predicate": "uses",
    "object": "cudaMalloc to obtain the GPU memory when the system starts"
  },
  {
    "subject": "the memory daemon",
    "predicate": "allocates",
    "object": "the memory to the workers at runtime"
  },
  {
    "subject": "the memory daemon",
    "predicate": "does not replace",
    "object": "the GPU memory manager"
  },
  {
    "subject": "the memory daemon",
    "predicate": "is compatible with",
    "object": "the existing system"
  },
  {
    "subject": "the memory daemon",
    "predicate": "incurs",
    "object": "minimal changes"
  },
  {
    "subject": "The DNN models",
    "predicate": "are stored",
    "object": "only once in the memory daemon"
  },
  {
    "subject": "The DNN models",
    "predicate": "are not stored",
    "object": "in every worker"
  },
  {
    "subject": "Storing the DNN models only once in the memory daemon",
    "predicate": "minimizes",
    "object": "memory footprint"
  },
  {
    "subject": "We",
    "predicate": "exploit",
    "object": "that the memory allocation for a DNN model is deterministic"
  },
  {
    "subject": "memory allocation for a DNN model",
    "predicate": "is",
    "object": "deterministic"
  },
  {
    "subject": "We",
    "predicate": "eliminate",
    "object": "extra memory copies between the daemon and the workers"
  },
  {
    "subject": "We",
    "predicate": "reduce",
    "object": "the IPC overhead"
  },
  {
    "subject": "no unified memory management",
    "predicate": "requires",
    "object": "each worker to keep a copy for each DNN model"
  },
  {
    "subject": "keeping a copy for each DNN model",
    "predicate": "increases",
    "object": "the memory footprint"
  },
  {
    "subject": "Each server",
    "predicate": "contains",
    "object": "an active worker"
  },
  {
    "subject": "Each server",
    "predicate": "contains",
    "object": "multiple standby workers"
  },
  {
    "subject": "A server",
    "predicate": "has",
    "object": "one or more standby workers"
  },
  {
    "subject": "The active worker",
    "predicate": "executes",
    "object": "the current task on the GPU"
  },
  {
    "subject": "The standby workers",
    "predicate": "stay",
    "object": "on the CPU"
  },
  {
    "subject": "The standby workers",
    "predicate": "wait for",
    "object": "the next task"
  },
  {
    "subject": "The active worker",
    "predicate": "is",
    "object": "the worker that currently executes a task in the GPU"
  },
  {
    "subject": "worker",
    "predicate": "is",
    "object": "a process that executes tasks on one GPU"
  },
  {
    "subject": "active worker",
    "predicate": "completes or stops",
    "object": "current task"
  },
  {
    "subject": "controller",
    "predicate": "notifies",
    "object": "memory daemon"
  },
  {
    "subject": "controller",
    "predicate": "notifies",
    "object": "standby worker"
  },
  {
    "subject": "standby worker",
    "predicate": "loads",
    "object": "task to GPU"
  },
  {
    "subject": "standby worker",
    "predicate": "executes",
    "object": "task with pipelined model transmission"
  },
  {
    "subject": "Our mechanism",
    "predicate": "parallelizes",
    "object": "old task cleaning in the active worker and new task initialization in the standby worker"
  },
  {
    "subject": "Our mechanism",
    "predicate": "aims to minimize",
    "object": "worker switching overhead"
  },
  {
    "subject": "Table 2",
    "predicate": "compares",
    "object": "worker switching mechanisms"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "an active and standby worker switching mechanism"
  },
  {
    "subject": "active and standby worker switching mechanism",
    "predicate": "hides",
    "object": "the overhead of both task cleaning and task initialization"
  },
  {
    "subject": "active and standby worker switching mechanism",
    "predicate": "ensures",
    "object": "process-level isolation"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enforces",
    "object": "process-level isolation"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "requires",
    "object": "us to address new technical challenges on memory management and worker switching across different processes"
  },
  {
    "subject": "pipelining",
    "predicate": "is for",
    "object": "the same task"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "aims to provide",
    "object": "fast task switching"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "aims to ensure",
    "object": "process-level isolation"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "an active worker"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "multiple standby workers"
  },
  {
    "subject": "we",
    "predicate": "keep",
    "object": "all other components of PipeSwitch the same"
  },
  {
    "subject": "we",
    "predicate": "compare",
    "object": "the following mechanisms discussed in 4.4"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is",
    "object": "the active-standby worker switching mechanism used by PipeSwitch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "uses",
    "object": "an active-standby worker switching mechanism"
  },
  {
    "subject": "an active-standby worker switching mechanism",
    "predicate": "is used to",
    "object": "parallelize old task cleaning and new task initialization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "incurs",
    "object": "minimal overhead"
  },
  {
    "subject": "Pipelining",
    "predicate": "is",
    "object": "a canonical technique"
  },
  {
    "subject": "Pipelining",
    "predicate": "is widely used in",
    "object": "computer systems"
  },
  {
    "subject": "Pipelining",
    "predicate": "improves",
    "object": "system performance"
  },
  {
    "subject": "Pipelining",
    "predicate": "maximizes",
    "object": "resource utilization"
  },
  {
    "subject": "Pipelining",
    "predicate": "brings",
    "object": "two sources of system overheads"
  },
  {
    "subject": "Prior work in DL systems such as PipeDream 8 and ByteScheduler 9",
    "predicate": "has applied",
    "object": "pipelining to distributed training"
  },
  {
    "subject": "These solutions",
    "predicate": "focus on",
    "object": "inter-batch pipelining"
  },
  {
    "subject": "inter-batch pipelining",
    "predicate": "overlaps",
    "object": "computation and gradient transmission of different batches"
  },
  {
    "subject": "computation and gradient transmission",
    "predicate": "are for",
    "object": "training workloads of the same DNN model"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "introduces",
    "object": "intra-batch pipelining"
  },
  {
    "subject": "intra-batch pipelining",
    "predicate": "overlaps",
    "object": "model transmission and computation"
  },
  {
    "subject": "intra-batch pipelining",
    "predicate": "reduces",
    "object": "the overhead of switching between different DNN models"
  },
  {
    "subject": "different DNN models",
    "predicate": "can be",
    "object": "either inference or training"
  },
  {
    "subject": "We",
    "predicate": "design",
    "object": "new techniques"
  },
  {
    "subject": "new techniques",
    "predicate": "support",
    "object": "training"
  },
  {
    "subject": "new techniques",
    "predicate": "support",
    "object": "inference that has strict SLOs"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "pipelined model transmission"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "unified memory management"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "leverages",
    "object": "active-standby worker switching"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "minimizes",
    "object": "switching overhead"
  },
  {
    "subject": "pipelined context switching",
    "predicate": "enforces",
    "object": "process-level isolation"
  },
  {
    "subject": "Pipelined context switching",
    "predicate": "includes",
    "object": "three key techniques"
  },
  {
    "subject": "three key techniques",
    "predicate": "are",
    "object": "pipelined model transmission, unified memory management and active-standby worker switching"
  },
  {
    "subject": "We",
    "predicate": "implement",
    "object": "a system prototype"
  },
  {
    "subject": "We",
    "predicate": "integrate",
    "object": "it with Py-Torch"
  },
  {
    "subject": "this section",
    "predicate": "identifies",
    "object": "inefficiencies in today's shared GPU clusters"
  },
  {
    "subject": "this section",
    "predicate": "motivates",
    "object": "running DL workloads on GPUs in the fine-grained time-sharing model"
  },
  {
    "subject": "We",
    "predicate": "propose",
    "object": "to pack multiple DL applications onto the same GPU via fine-grained time-sharing abstraction"
  },
  {
    "subject": "fine-grained time-sharing abstraction",
    "predicate": "maximizes",
    "object": "GPU utilization"
  },
  {
    "subject": "fast task switching",
    "predicate": "enables",
    "object": "more flexible fine-grained scheduling"
  },
  {
    "subject": "more flexible fine-grained scheduling",
    "predicate": "improves",
    "object": "GPU utilization for dynamic workloads"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "dedicated physical forms and power supplies"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "high speed networks"
  },
  {
    "subject": "GPU clusters",
    "predicate": "are designed with",
    "object": "specialized task schedulers"
  },
  {
    "subject": "500 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "is organized by",
    "object": "USENIX Association"
  },
  {
    "subject": "USENIX Association",
    "predicate": "organized",
    "object": "14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "includes",
    "object": "Kubernetes"
  },
  {
    "subject": "514",
    "predicate": "is part of",
    "object": "14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "The main reason",
    "predicate": "is",
    "object": "to bring down the cost"
  },
  {
    "subject": "The demand of training",
    "predicate": "is not",
    "object": "well predictable"
  },
  {
    "subject": "The demand of training",
    "predicate": "would depend on",
    "object": "the progress of different developers"
  },
  {
    "subject": "The demand of inference",
    "predicate": "is",
    "object": "more predictable"
  },
  {
    "subject": "an inference task for a particular application",
    "predicate": "usually has",
    "object": "a daily periodical pattern based on the application usage"
  },
  {
    "subject": "the patterns",
    "predicate": "can vary",
    "object": "across different tasks"
  },
  {
    "subject": "a shared cluster by different tasks",
    "predicate": "would increase",
    "object": "the resource utilization via time-sharing"
  },
  {
    "subject": "a shared cluster by different tasks",
    "predicate": "is like",
    "object": "traditional CPU workloads"
  },
  {
    "subject": "training",
    "predicate": "has no sharing with",
    "object": "inference"
  },
  {
    "subject": "shared clusters",
    "predicate": "are not shared between",
    "object": "training and inference"
  },
  {
    "subject": "inference clusters",
    "predicate": "are not always",
    "object": "running at high utilization"
  },
  {
    "subject": "inference clusters",
    "predicate": "cannot be utilized by",
    "object": "training"
  },
  {
    "subject": "Training clusters",
    "predicate": "are equipped with",
    "object": "powerful GPUs"
  },
  {
    "subject": "Training clusters",
    "predicate": "run",
    "object": "training tasks"
  },
  {
    "subject": "training tasks",
    "predicate": "are",
    "object": "often elastic"
  },
  {
    "subject": "training tasks",
    "predicate": "do not have",
    "object": "strict deadlines"
  },
  {
    "subject": "GPUs designed for inference tasks",
    "predicate": "might be",
    "object": "too wimpy for training tasks"
  },
  {
    "subject": "the arrival of new GPU hardware",
    "predicate": "has started to change",
    "object": "this"
  },
  {
    "subject": "new GPU hardware",
    "predicate": "includes",
    "object": "NVIDIA T4"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "has",
    "object": "up to 32GB GPU memory"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "has",
    "object": "15.7 TFLOPS (single-precision)"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "has",
    "object": "16GB GPU memory"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "has",
    "object": "8.1 TFLOPS (single-precision)"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "has",
    "object": "comparable performance with NVIDIA V100"
  },
  {
    "subject": "new algorithms and systems for distributed training",
    "predicate": "enable",
    "object": "multiple GPUs to accelerate training"
  },
  {
    "subject": "Our industry collaborator",
    "predicate": "is",
    "object": "a leading online service provider"
  },
  {
    "subject": "Our industry collaborator",
    "predicate": "confirms",
    "object": "this observation"
  },
  {
    "subject": "This service provider",
    "predicate": "runs",
    "object": "more than 10K V100 GPUs for training"
  },
  {
    "subject": "This service provider",
    "predicate": "runs",
    "object": "at least 5 as many T4 GPUs for inference"
  },
  {
    "subject": "The computation power on both sides",
    "predicate": "is within",
    "object": "the same order of magnitude"
  },
  {
    "subject": "The inference workload",
    "predicate": "fluctuates in correlation with",
    "object": "the number of active users"
  },
  {
    "subject": "The inference workload",
    "predicate": "shows",
    "object": "clear peaks and valleys within each day"
  },
  {
    "subject": "The peak demand during daytime",
    "predicate": "is",
    "object": "2 of the valley at midnight"
  },
  {
    "subject": "inference GPUs",
    "predicate": "can be utilized",
    "object": "during less busy times"
  },
  {
    "subject": "training models",
    "predicate": "require",
    "object": "daily updates with latest data"
  },
  {
    "subject": "A good example",
    "predicate": "is to",
    "object": "ne-tune BERT using daily news"
  },
  {
    "subject": "Borg-like 1 systems for GPUs",
    "predicate": "have",
    "object": "great opportunity in improving GPU utilization"
  },
  {
    "subject": "inference and training workloads",
    "predicate": "have",
    "object": "complementary usage patterns"
  },
  {
    "subject": "Online inference services",
    "predicate": "are",
    "object": "more idle during midnight"
  },
  {
    "subject": "many training developers",
    "predicate": "would start",
    "object": "a time-consuming job at night"
  },
  {
    "subject": "inference loads on different models",
    "predicate": "have",
    "object": "different patterns"
  },
  {
    "subject": "different patterns",
    "predicate": "benefit from",
    "object": "time sharing"
  },
  {
    "subject": "any server",
    "predicate": "would be able to run",
    "object": "any task"
  },
  {
    "subject": "switch between different applications",
    "predicate": "has",
    "object": "low overhead"
  },
  {
    "subject": "A modern server",
    "predicate": "can be equipped with",
    "object": "several TB of host memory"
  },
  {
    "subject": "several TB of host memory",
    "predicate": "enables",
    "object": "it to load many applications"
  },
  {
    "subject": "task execution on GPUs",
    "predicate": "requires",
    "object": "GPU memory"
  },
  {
    "subject": "GPU memory",
    "predicate": "is",
    "object": "very limited even on high-end GPUs"
  },
  {
    "subject": "T4",
    "predicate": "has GPU memory of",
    "object": "16 GB"
  },
  {
    "subject": "V100",
    "predicate": "has GPU memory of",
    "object": "32 GB"
  },
  {
    "subject": "GPU memory",
    "predicate": "is purposed for",
    "object": "task execution"
  },
  {
    "subject": "GPU memory",
    "predicate": "is not purposed for",
    "object": "storing the state of idle applications"
  },
  {
    "subject": "DL tasks, especially training",
    "predicate": "require",
    "object": "a large amount, or even all of the memory on a GPU"
  },
  {
    "subject": "DL applications",
    "predicate": "have",
    "object": "large models"
  },
  {
    "subject": "DL applications",
    "predicate": "generate",
    "object": "large amounts of intermediate results"
  },
  {
    "subject": "large models and large amounts of intermediate results",
    "predicate": "require",
    "object": "a lot of GPU memory"
  },
  {
    "subject": "Salus 7",
    "predicate": "cannot support",
    "object": "training tasks which are memory-intensive"
  },
  {
    "subject": "Salus 7",
    "predicate": "cannot support",
    "object": "multiple inference tasks which have large models"
  },
  {
    "subject": "models",
    "predicate": "are stored in",
    "object": "the GPU"
  },
  {
    "subject": "state-of-the-art models",
    "predicate": "are getting",
    "object": "deeper and larger"
  },
  {
    "subject": "even idle applications",
    "predicate": "can occupy",
    "object": "large memory space"
  },
  {
    "subject": "the active application",
    "predicate": "should be able to utilize",
    "object": "the entire GPU memory"
  },
  {
    "subject": "the number of applications that can be served by a GPU server",
    "predicate": "should be limited by",
    "object": "its host memory size"
  },
  {
    "subject": "switching a task",
    "predicate": "would require",
    "object": "heavy memory swapping"
  },
  {
    "subject": "many online inference workloads",
    "predicate": "require",
    "object": "strict SLOs"
  },
  {
    "subject": "naive memory swapping between the host memory and the GPU memory",
    "predicate": "cannot meet",
    "object": "strict SLOs"
  },
  {
    "subject": "strict SLOs",
    "predicate": "require",
    "object": "requests to be handled in small batches for low latency"
  },
  {
    "subject": "we",
    "predicate": "test",
    "object": "the strawman scenario"
  },
  {
    "subject": "the strawman scenario",
    "predicate": "involves",
    "object": "stop a training task and then start an inference task"
  },
  {
    "subject": "The rst inference batch",
    "predicate": "would require",
    "object": "several seconds to nish (4.1)"
  },
  {
    "subject": "Existing support such as NVIDIA MPS",
    "predicate": "is not optimized for",
    "object": "DL workloads"
  },
  {
    "subject": "Existing support such as NVIDIA MPS",
    "predicate": "incurs",
    "object": "hundreds of milliseconds overhead"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "has",
    "object": "lower overhead compared to stop-and-start"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "incurs",
    "object": "several hundred milliseconds overhead"
  },
  {
    "subject": "several hundred milliseconds overhead",
    "predicate": "prevents",
    "object": "MPS from meeting strict SLOs"
  },
  {
    "subject": "USENIX Association",
    "predicate": "hosts",
    "object": "14th USENIX Symposium on Operating Systems Design and Implementation"
  },
  {
    "subject": "Figure 1",
    "predicate": "depicts",
    "object": "PipeSwitch architecture"
  },
  {
    "subject": "Throughput",
    "predicate": "measured in",
    "object": "batches per second"
  },
  {
    "subject": "Throughput",
    "predicate": "measured for",
    "object": "eight p3.2xlarge instances"
  },
  {
    "subject": "Throughput",
    "predicate": "includes",
    "object": "Upper bound"
  },
  {
    "subject": "Throughput",
    "predicate": "includes",
    "object": "PipeSwitch"
  },
  {
    "subject": "Throughput",
    "predicate": "includes",
    "object": "MPS"
  },
  {
    "subject": "Throughput",
    "predicate": "includes",
    "object": "Stop-and-start"
  },
  {
    "subject": "intra-batch pipelining",
    "predicate": "fast start",
    "object": "both training and inference tasks"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "fast switching across tasks"
  },
  {
    "subject": "DL workloads",
    "predicate": "have",
    "object": "well-defined structures"
  },
  {
    "subject": "the structure and computation pattern of DNN models",
    "predicate": "allow",
    "object": "us to highly optimize task switching"
  },
  {
    "subject": "the structure and computation pattern of DNN models",
    "predicate": "allow",
    "object": "us to achieve millisecond-scale overhead"
  },
  {
    "subject": "pipeline",
    "predicate": "is",
    "object": "feasible and effective"
  },
  {
    "subject": "we",
    "predicate": "need to resolve",
    "object": "other challenges like memory management and worker switching"
  },
  {
    "subject": "we",
    "predicate": "provide",
    "object": "an overview of the architecture and task execution"
  },
  {
    "subject": "Figure 1",
    "predicate": "shows",
    "object": "the architecture of a PipeSwitch server"
  },
  {
    "subject": "PipeSwitch pipelines",
    "predicate": "model",
    "object": "transmission and task execution"
  },
  {
    "subject": "This server",
    "predicate": "contains",
    "object": "four types of components"
  },
  {
    "subject": "four types of components",
    "predicate": "are",
    "object": "a controller"
  },
  {
    "subject": "four types of components",
    "predicate": "are",
    "object": "a memory daemon"
  },
  {
    "subject": "four types of components",
    "predicate": "are",
    "object": "an active worker"
  },
  {
    "subject": "four types of components",
    "predicate": "are",
    "object": "multiple standby workers"
  },
  {
    "subject": "The controller",
    "predicate": "is",
    "object": "the central component"
  },
  {
    "subject": "the memory daemon and the workers",
    "predicate": "execute",
    "object": "the tasks"
  },
  {
    "subject": "Memory",
    "predicate": "is a",
    "object": "daemon"
  },
  {
    "subject": "Controller",
    "predicate": "is",
    "object": "memory daemon"
  },
  {
    "subject": "The memory daemon",
    "predicate": "manages",
    "object": "the GPU memory"
  },
  {
    "subject": "The memory daemon",
    "predicate": "manages",
    "object": "the DNN models"
  },
  {
    "subject": "The server",
    "predicate": "stores",
    "object": "the DNN models in the host memory"
  },
  {
    "subject": "worker",
    "predicate": "is",
    "object": "active"
  },
  {
    "subject": "All components",
    "predicate": "should be optimized to meet",
    "object": "the SLOs"
  },
  {
    "subject": "worker",
    "predicate": "is",
    "object": "standby"
  },
  {
    "subject": "A standby worker",
    "predicate": "is",
    "object": "idle"
  },
  {
    "subject": "A standby worker",
    "predicate": "is",
    "object": "initializing a new task"
  },
  {
    "subject": "A standby worker",
    "predicate": "is",
    "object": "cleaning its environment for the previous task"
  },
  {
    "subject": "The standby worker",
    "predicate": "becomes",
    "object": "the new active worker to execute the new task"
  },
  {
    "subject": "The active worker",
    "predicate": "becomes",
    "object": "a standby worker"
  },
  {
    "subject": "The active worker",
    "predicate": "cleans",
    "object": "the environment for the previous task"
  },
  {
    "subject": "a new task",
    "predicate": "arrives before",
    "object": "a standby worker finishes cleaning a previous task"
  },
  {
    "subject": "the new task",
    "predicate": "needs to",
    "object": "wait"
  },
  {
    "subject": "waiting",
    "predicate": "increases",
    "object": "its startup time"
  },
  {
    "subject": "The controller",
    "predicate": "queues",
    "object": "a set of tasks received from the clients"
  },
  {
    "subject": "a scheduling policy",
    "predicate": "decides",
    "object": "which task to execute next"
  },
  {
    "subject": "The scheduling",
    "predicate": "is",
    "object": "preemptive"
  },
  {
    "subject": "the controller",
    "predicate": "can preempt",
    "object": "the current task for the next one based on the scheduling policy"
  },
  {
    "subject": "We",
    "predicate": "focus on",
    "object": "fast context switching"
  },
  {
    "subject": "the specific scheduling algorithm",
    "predicate": "is",
    "object": "orthogonal to this paper"
  },
  {
    "subject": "the controller",
    "predicate": "can preempt",
    "object": "the current task"
  },
  {
    "subject": "the current task",
    "predicate": "is",
    "object": "a training task"
  },
  {
    "subject": "an inference task",
    "predicate": "has",
    "object": "a strict latency SLO"
  },
  {
    "subject": "the controller",
    "predicate": "noties",
    "object": "an idle standby worker"
  },
  {
    "subject": "an idle standby worker",
    "predicate": "to initialize",
    "object": "its environment for the new task"
  },
  {
    "subject": "the controller",
    "predicate": "schedules",
    "object": "a task"
  },
  {
    "subject": "The memory daemon",
    "predicate": "allocates",
    "object": "the memory to the standby worker (4.3)"
  },
  {
    "subject": "The memory daemon",
    "predicate": "transmits",
    "object": "the model used by the new task from the host memory to the GPU memory"
  },
  {
    "subject": "the memory daemon",
    "predicate": "transmits",
    "object": "the model from the host memory to the GPU memory"
  },
  {
    "subject": "transmitting the model from the host memory to the GPU memory",
    "predicate": "eliminates",
    "object": "the extra memory copy from the memory daemon to the worker"
  },
  {
    "subject": "the memory daemon",
    "predicate": "needs to notify",
    "object": "the worker"
  },
  {
    "subject": "the memory daemon",
    "predicate": "needs to export",
    "object": "the relevant GPU memory handlers to the worker"
  },
  {
    "subject": "the worker",
    "predicate": "can access",
    "object": "the model"
  },
  {
    "subject": "the worker",
    "predicate": "can execute",
    "object": "its task"
  },
  {
    "subject": "The memory daemon",
    "predicate": "handles",
    "object": "GPU memory allocation"
  },
  {
    "subject": "The memory daemon",
    "predicate": "handles",
    "object": "model transmission"
  },
  {
    "subject": "The memory daemon",
    "predicate": "creates and sends",
    "object": "GPU memory handlers to workers"
  },
  {
    "subject": "The primary goal of this paper",
    "predicate": "is to",
    "object": "design a set of techniques based on the characteristics of DL applications"
  },
  {
    "subject": "The set of techniques",
    "predicate": "aims to",
    "object": "minimize the task switching overhead in this process"
  },
  {
    "subject": "task switching overhead",
    "predicate": "is broken down to",
    "object": "individual components"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "end-to-end experiments to demonstrate the benefits of PipeSwitch"
  },
  {
    "subject": "we",
    "predicate": "show",
    "object": "the effectiveness of the design choices on each component"
  },
  {
    "subject": "we",
    "predicate": "describe",
    "object": "our design"
  },
  {
    "subject": "our design",
    "predicate": "minimize",
    "object": "the overhead of each component"
  },
  {
    "subject": "a server",
    "predicate": "stops",
    "object": "a training task running on the GPU"
  },
  {
    "subject": "a server",
    "predicate": "starts",
    "object": "an inference task"
  },
  {
    "subject": "The DNN model used in the measurement",
    "predicate": "is",
    "object": "ResNet152 17"
  },
  {
    "subject": "The measurement",
    "predicate": "covers",
    "object": "two types of instances on Amazon AWS"
  },
  {
    "subject": "two types of instances on Amazon AWS",
    "predicate": "are",
    "object": "g4dn.2xlarge with NVIDA T4"
  },
  {
    "subject": "two types of instances on Amazon AWS",
    "predicate": "are",
    "object": "p3.2xlarge with NVIDIA V100"
  },
  {
    "subject": "inference task",
    "predicate": "has arrived at",
    "object": "the server"
  },
  {
    "subject": "We",
    "predicate": "focus on",
    "object": "measuring the time to start and execute it on the GPU"
  },
  {
    "subject": "We",
    "predicate": "exclude",
    "object": "the network time"
  },
  {
    "subject": "We",
    "predicate": "exclude",
    "object": "the task queueing time"
  },
  {
    "subject": "total times to start the inference task on the GPUs",
    "predicate": "are",
    "object": "5787 ms and 7551 ms, respectively"
  },
  {
    "subject": "We",
    "predicate": "break down",
    "object": "the overhead into the four components"
  },
  {
    "subject": "task cleaning",
    "predicate": "takes",
    "object": "time"
  },
  {
    "subject": "The inference task",
    "predicate": "creates and initializes",
    "object": "its environment"
  },
  {
    "subject": "its environment",
    "predicate": "includes",
    "object": "process launching"
  },
  {
    "subject": "its environment",
    "predicate": "includes",
    "object": "PyTorch CUDA runtime loading"
  },
  {
    "subject": "its environment",
    "predicate": "includes",
    "object": "CUDA context initialization"
  },
  {
    "subject": "The inference task",
    "predicate": "allocates",
    "object": "GPU memory"
  },
  {
    "subject": "GPU memory",
    "predicate": "is allocated for",
    "object": "its neural network model"
  },
  {
    "subject": "The inference task",
    "predicate": "transmits",
    "object": "the model from the host memory to the GPU memory"
  },
  {
    "subject": "Model",
    "predicate": "is",
    "object": "transmission"
  },
  {
    "subject": "Grouped transmission",
    "predicate": "is",
    "object": "a concept"
  },
  {
    "subject": "inference time on V100",
    "predicate": "is lower than",
    "object": "inference time on T4"
  },
  {
    "subject": "inference time on V100 and inference time on T4",
    "predicate": "are significantly lower than",
    "object": "total overheads"
  },
  {
    "subject": "lower overhead on T4",
    "predicate": "is because",
    "object": "task switching largely depends on CPU"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "is equipped with",
    "object": "better CPU than p3.2xlarge"
  },
  {
    "subject": "better CPU",
    "predicate": "is",
    "object": "Intel Platinum 8259CL"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has CPU",
    "object": "Intel Xeon E5-2686 v4"
  },
  {
    "subject": "A strawman solution",
    "predicate": "stops",
    "object": "the old task"
  },
  {
    "subject": "A strawman solution",
    "predicate": "starts",
    "object": "the new task"
  },
  {
    "subject": "A strawman solution that simply stops the old task and starts the new task",
    "predicate": "would violate",
    "object": "SLOs"
  },
  {
    "subject": "all the components",
    "predicate": "take",
    "object": "considerable time compared to the inference time"
  },
  {
    "subject": "all the components",
    "predicate": "should be optimized to achieve",
    "object": "minimal switching overhead"
  },
  {
    "subject": "The PCIe bandwidth",
    "predicate": "is",
    "object": "the physical limit on how fast an arbitrary task can be loaded to the GPU"
  },
  {
    "subject": "We",
    "predicate": "exploit the characteristics of DL applications",
    "object": "to circumvent this physical limit"
  },
  {
    "subject": "The computation",
    "predicate": "is performed",
    "object": "layer by layer"
  },
  {
    "subject": "An inference task",
    "predicate": "performs",
    "object": "a forward pass from the first layer to the final layer"
  },
  {
    "subject": "An inference task",
    "predicate": "performs",
    "object": "a forward pass to make a prediction"
  },
  {
    "subject": "each iteration in a training task",
    "predicate": "performs",
    "object": "a forward pass"
  },
  {
    "subject": "each iteration in a training task",
    "predicate": "performs",
    "object": "a backward pass"
  },
  {
    "subject": "the task",
    "predicate": "can start",
    "object": "the computation of a layer as soon as the layer is loaded in the GPU and the input of the layer is ready"
  },
  {
    "subject": "the input of the layer",
    "predicate": "is ready",
    "object": "the previous layers have finished their computation"
  },
  {
    "subject": "the task",
    "predicate": "can start computation",
    "object": "regardless of its following layers"
  },
  {
    "subject": "Figure 2",
    "predicate": "illustrates",
    "object": "the advantage of pipelining over the strawman solution"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "requires",
    "object": "the knowledge of models"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "pipelining mechanism with optimal model-aware grouping"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "uses",
    "object": "model-aware grouping"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "the best trade-off between pipeline overhead and efficiency"
  },
  {
    "subject": "model",
    "predicate": "transmit over",
    "object": "PCIe"
  },
  {
    "subject": "task execution",
    "predicate": "occur on",
    "object": "GPU"
  },
  {
    "subject": "model",
    "predicate": "transmit to",
    "object": "GPU"
  },
  {
    "subject": "task",
    "predicate": "execute on",
    "object": "GPU"
  },
  {
    "subject": "PCIe GPU E0 E1 En-1 E2",
    "predicate": "is",
    "object": "pipeline model transmission and task execution"
  },
  {
    "subject": "The example",
    "predicate": "shows",
    "object": "an inference task that only has a forward pass in task execution"
  },
  {
    "subject": "Adding hooks",
    "predicate": "can be",
    "object": "automated"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can be implemented as a part of",
    "object": "the DNN framework"
  },
  {
    "subject": "the DNN framework",
    "predicate": "includes example",
    "object": "PyTorch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "can gather",
    "object": "the model structure information"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "remains",
    "object": "transparent to users and cluster managers"
  },
  {
    "subject": "The basic way for pipelining",
    "predicate": "is",
    "object": "to pipeline on per-layer granularity"
  },
  {
    "subject": "the system",
    "predicate": "transmits",
    "object": "the layers to the GPU memory one by one"
  },
  {
    "subject": "the computation for a layer",
    "predicate": "is blocked before",
    "object": "the layer is transmitted"
  },
  {
    "subject": "One",
    "predicate": "is",
    "object": "the overhead to invoke multiple calls to PCIe to transmit the data"
  },
  {
    "subject": "transmission overhead",
    "predicate": "is dominated by",
    "object": "data size"
  },
  {
    "subject": "dividing the model into many layers",
    "predicate": "causes",
    "object": "significant extra overhead when invoking a PCIe call for each layer"
  },
  {
    "subject": "some layers",
    "predicate": "can be",
    "object": "very small"
  },
  {
    "subject": "synchronization overhead",
    "predicate": "is",
    "object": "between transmission and computation"
  },
  {
    "subject": "synchronization overhead",
    "predicate": "is necessary for",
    "object": "the computation to know when a layer is ready to compute"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "grouping"
  },
  {
    "subject": "grouping",
    "predicate": "minimize",
    "object": "these two sources of overhead"
  },
  {
    "subject": "We",
    "predicate": "combine",
    "object": "multiple layers into a group"
  },
  {
    "subject": "the pipelining",
    "predicate": "is performed on",
    "object": "per-group granularity"
  },
  {
    "subject": "pipelining overhead",
    "predicate": "is paid",
    "object": "once for each group"
  },
  {
    "subject": "pipelining overhead",
    "predicate": "is paid",
    "object": "instead of each layer"
  },
  {
    "subject": "Grouping",
    "predicate": "introduces",
    "object": "a trade-off between pipelining efficiency and pipelining overhead"
  },
  {
    "subject": "using small groups",
    "predicate": "enables",
    "object": "more overlap between transmission and computation"
  },
  {
    "subject": "more overlap between transmission and computation",
    "predicate": "improves",
    "object": "pipelining efficiency"
  },
  {
    "subject": "using small groups",
    "predicate": "pays",
    "object": "more pipelining overhead"
  },
  {
    "subject": "using big groups",
    "predicate": "has",
    "object": "minimal pipelining overhead"
  },
  {
    "subject": "using big groups",
    "predicate": "reduces",
    "object": "the chance for overlapping"
  },
  {
    "subject": "Grouping",
    "predicate": "must be",
    "object": "model-aware"
  },
  {
    "subject": "models",
    "predicate": "have",
    "object": "different structures"
  },
  {
    "subject": "different structures",
    "predicate": "are in terms of",
    "object": "the number of layers"
  },
  {
    "subject": "different structures",
    "predicate": "are in terms of",
    "object": "the size of each layer"
  },
  {
    "subject": "we",
    "predicate": "can enumerate",
    "object": "all possible combinations"
  },
  {
    "subject": "we",
    "predicate": "can find",
    "object": "the optimal grouping strategy"
  },
  {
    "subject": "we",
    "predicate": "introduce",
    "object": "two pruning techniques"
  },
  {
    "subject": "two pruning techniques",
    "predicate": "are based on",
    "object": "two insights"
  },
  {
    "subject": "large models",
    "predicate": "can have",
    "object": "hundreds of layers"
  },
  {
    "subject": "the time complexity for enumeration",
    "predicate": "is",
    "object": "exponential"
  },
  {
    "subject": "PCIe GPU",
    "predicate": "has",
    "object": "lower bound of F(Group(0, i), i1)"
  },
  {
    "subject": "Group(0, i)",
    "predicate": "is related to",
    "object": "Group(i1, j)"
  },
  {
    "subject": "j",
    "predicate": "ranges from",
    "object": "j1 to n-1"
  },
  {
    "subject": "case",
    "predicate": "is pruned if",
    "object": "lower bound current optimal time"
  },
  {
    "subject": "cases",
    "predicate": "group from",
    "object": "i to j"
  },
  {
    "subject": "batch",
    "predicate": "at least from layer",
    "object": "i1 to j"
  },
  {
    "subject": "Figure 3",
    "predicate": "shows",
    "object": "examples for two pruning techniques"
  },
  {
    "subject": "we",
    "predicate": "can prune",
    "object": "the cases that group from layer (i 1) to j j"
  },
  {
    "subject": "we",
    "predicate": "can only search for",
    "object": "j j"
  },
  {
    "subject": "number of layers",
    "predicate": "be",
    "object": "n"
  },
  {
    "subject": "F(B,i)",
    "predicate": "be",
    "object": "a function"
  },
  {
    "subject": "F(B,i)",
    "predicate": "returns",
    "object": "the total time of the optimal grouping strategy from layer i to n-1"
  },
  {
    "subject": "layer 0 to i-1",
    "predicate": "have formed",
    "object": "groups represented by B"
  },
  {
    "subject": "F(,0)",
    "predicate": "is defined as",
    "object": "min i F(group(0,i),i1)"
  },
  {
    "subject": "optimal grouping strategy",
    "predicate": "is for",
    "object": "the entire model (i.e., F(, 0))"
  },
  {
    "subject": "all possible combinations",
    "predicate": "are divided into",
    "object": "n cases"
  },
  {
    "subject": "cases",
    "predicate": "are based on",
    "object": "how the first group is formed"
  },
  {
    "subject": "case i",
    "predicate": "means",
    "object": "the first group contains layer 0 to i"
  },
  {
    "subject": "This formula",
    "predicate": "can be applied to",
    "object": "compute F(group(0,i),i1)"
  },
  {
    "subject": "This formula",
    "predicate": "can be applied",
    "object": "recursively"
  },
  {
    "subject": "the rst group",
    "predicate": "contains",
    "object": "too many layers"
  },
  {
    "subject": "the computation of the rst group",
    "predicate": "would be delayed",
    "object": "too much"
  },
  {
    "subject": "the delay",
    "predicate": "would compensate",
    "object": "the pipeline efciency"
  },
  {
    "subject": "we",
    "predicate": "can safely pack",
    "object": "multiple layers in a group based on the progress of computation"
  },
  {
    "subject": "packing multiple layers in a group based on the progress of computation",
    "predicate": "does not affect",
    "object": "pipeline efficiency"
  },
  {
    "subject": "other than the rst group",
    "predicate": "is excluded from",
    "object": "packing multiple layers in a group"
  },
  {
    "subject": "T(i, j)",
    "predicate": "is",
    "object": "the transmission time for a group from layer i to j"
  },
  {
    "subject": "E(i, j)",
    "predicate": "is",
    "object": "the execution time for a group from layer i to j"
  },
  {
    "subject": "T(i, j)",
    "predicate": "is calculated based on",
    "object": "the size of layer i to j and PCIe bandwidth"
  },
  {
    "subject": "E(i, j)",
    "predicate": "is profiled on",
    "object": "the GPU"
  },
  {
    "subject": "the overhead of invoking multiple calls",
    "predicate": "is included in",
    "object": "T(i, j)"
  },
  {
    "subject": "we",
    "predicate": "compute",
    "object": "a lower bound for the total time for each case in Equation 1"
  },
  {
    "subject": "The lower bound",
    "predicate": "considers",
    "object": "the best case that all the remaining layers are combined in one group for transmission and computation"
  },
  {
    "subject": "The lower bound",
    "predicate": "assumes",
    "object": "the computation and communication can be perfectly overlapped"
  },
  {
    "subject": "Its computation",
    "predicate": "can happen",
    "object": "right after the computation of the first group finishes"
  },
  {
    "subject": "Figure 3(b)",
    "predicate": "shows",
    "object": "an example for this insight"
  },
  {
    "subject": "the first group",
    "predicate": "is from",
    "object": "layer 0 to i"
  },
  {
    "subject": "Equation 1",
    "predicate": "is applied recursively to",
    "object": "enumerate the cases for the PCIe GPU B.delay"
  },
  {
    "subject": "Group",
    "predicate": "is defined as",
    "object": "Group(a, i), Group(i1, n-1), Group(x, i), Group(i1, n-1)"
  },
  {
    "subject": "Figure 4",
    "predicate": "illustrates",
    "object": "general case for the two pruning techniques"
  },
  {
    "subject": "We",
    "predicate": "can hide",
    "object": "the transmission of the second group into the computation of the first group"
  },
  {
    "subject": "the transmission",
    "predicate": "finishes",
    "object": "no later than the computation of the first group"
  },
  {
    "subject": "The least number of layers to group",
    "predicate": "can be computed using",
    "object": "the following equation"
  },
  {
    "subject": "j argmax j T(i1, j) E(0,i)",
    "predicate": "is",
    "object": "Group from layer (i1) to j"
  },
  {
    "subject": "Group from layer (i1) to j",
    "predicate": "is no better than",
    "object": "grouping from (i-1) to j"
  },
  {
    "subject": "grouping from (i-1) to j",
    "predicate": "does not increase",
    "object": "pipeline efficiency"
  },
  {
    "subject": "grouping from (i-1) to j",
    "predicate": "has",
    "object": "higher pipeline overhead"
  },
  {
    "subject": "this algorithm",
    "predicate": "runs",
    "object": "offline to find the strategy"
  },
  {
    "subject": "the resulting strategy",
    "predicate": "is used online by",
    "object": "PipeSwitch for context switching"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "shows",
    "object": "the pseudo code"
  },
  {
    "subject": "The function FindOptGrouping",
    "predicate": "finds",
    "object": "the optimal grouping strategy"
  },
  {
    "subject": "The function FindOptGrouping",
    "predicate": "finds recursively",
    "object": "the optimal grouping strategy"
  },
  {
    "subject": "the optimal grouping strategy",
    "predicate": "is based on",
    "object": "Equation 1 (line 1-27)"
  },
  {
    "subject": "B",
    "predicate": "represents",
    "object": "the groups that have already formed"
  },
  {
    "subject": "x",
    "predicate": "is",
    "object": "the first layer that have not formed a group"
  },
  {
    "subject": "the rst group",
    "predicate": "contains",
    "object": "all layers from x to n1"
  },
  {
    "subject": "optgroups",
    "predicate": "store",
    "object": "the best grouping strategy from layer x given B"
  },
  {
    "subject": "the best grouping strategy from layer x given B",
    "predicate": "is initialized to",
    "object": "none (line 2)"
  },
  {
    "subject": "The algorithm",
    "predicate": "applies",
    "object": "the second pruning insight"
  },
  {
    "subject": "The algorithm",
    "predicate": "forms",
    "object": "the rst group from layer x"
  },
  {
    "subject": "The algorithm",
    "predicate": "divides",
    "object": "the problem into k - 1 cases"
  },
  {
    "subject": "case i (0  i  k)",
    "predicate": "forms",
    "object": "the first group from layer x to x_i"
  },
  {
    "subject": "Equation 3 and Figure 3(b)",
    "predicate": "illustrate",
    "object": "this insight with a special example"
  },
  {
    "subject": "B",
    "predicate": "contains",
    "object": "one group from layer 0 to i"
  },
  {
    "subject": "B",
    "predicate": "can contain",
    "object": "multiple groups formed by previous layers"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "B.delay to denote the time to which the group can be formed"
  },
  {
    "subject": "The algorithm",
    "predicate": "finds j",
    "object": "based on B.delay (line 4-9)"
  },
  {
    "subject": "The enumeration for i",
    "predicate": "can skip",
    "object": "the layers from x to j-1 (line 11)"
  },
  {
    "subject": "the algorithm",
    "predicate": "applies",
    "object": "the rst insight"
  },
  {
    "subject": "the algorithm",
    "predicate": "computes",
    "object": "the lower bound"
  },
  {
    "subject": "the lower bound",
    "predicate": "is computed at",
    "object": "line 12-17"
  },
  {
    "subject": "the computation from x",
    "predicate": "has to wait for",
    "object": "both its transmission (T(x,i)) and the computation of the previous groups (B.delay)"
  },
  {
    "subject": "the lower bound",
    "predicate": "is bigger than",
    "object": "the current optimal time"
  },
  {
    "subject": "case i",
    "predicate": "is pruned",
    "object": "line 18-19"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "a heuristic that bootstraps optgroups with a relative good strategy"
  },
  {
    "subject": "good strategy",
    "predicate": "is",
    "object": "group every ten layers"
  },
  {
    "subject": "The two pruning techniques",
    "predicate": "are able to prune",
    "object": "most of the strategies"
  },
  {
    "subject": "The two pruning techniques",
    "predicate": "can find",
    "object": "the optimal one quickly"
  },
  {
    "subject": "The algorithm",
    "predicate": "uses",
    "object": "two pruning techniques"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "finds",
    "object": "the optimal grouping strategy"
  },
  {
    "subject": "the optimal grouping strategy",
    "predicate": "minimizes",
    "object": "the total time for the pipeline"
  },
  {
    "subject": "m n x",
    "predicate": "is",
    "object": "the number of layers the function considers"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "induction on m"
  },
  {
    "subject": "FindOptGrouping(B,x)",
    "predicate": "outputs",
    "object": "the optimal grouping strategy from layer x to n 1"
  },
  {
    "subject": "previous layers",
    "predicate": "have formed",
    "object": "groups represented by B"
  },
  {
    "subject": "FindOptGrouping(B,x)",
    "predicate": "outputs",
    "object": "the optimal strategy"
  },
  {
    "subject": "the function",
    "predicate": "examines",
    "object": "one layer"
  },
  {
    "subject": "m",
    "predicate": "is",
    "object": "1"
  },
  {
    "subject": "layer x itself",
    "predicate": "is",
    "object": "one group"
  },
  {
    "subject": "this strategy",
    "predicate": "is",
    "object": "the optimal strategy"
  },
  {
    "subject": "the algorithm",
    "predicate": "considers",
    "object": "k1 layers"
  },
  {
    "subject": "The optimal strategy for this case",
    "predicate": "is",
    "object": "one group"
  },
  {
    "subject": "these cases",
    "predicate": "are",
    "object": "exclusive"
  },
  {
    "subject": "these cases",
    "predicate": "cover",
    "object": "the entire search space"
  },
  {
    "subject": "the algorithm",
    "predicate": "outputs",
    "object": "the optimal grouping strategy for m k 1"
  },
  {
    "subject": "the algorithm",
    "predicate": "chooses",
    "object": "the optimal grouping strategy from these cases"
  },
  {
    "subject": "The rst technique",
    "predicate": "prunes",
    "object": "the cases"
  },
  {
    "subject": "their lower bounds",
    "predicate": "are no better than",
    "object": "the current found optimal"
  },
  {
    "subject": "this technique",
    "predicate": "does not affect",
    "object": "the optimality"
  },
  {
    "subject": "The second technique",
    "predicate": "prunes",
    "object": "the case"
  },
  {
    "subject": "their rst groups",
    "predicate": "are from",
    "object": "layer x to j j"
  },
  {
    "subject": "these cases",
    "predicate": "cannot advance",
    "object": "the computation to an earlier point than grouping from x to at least j"
  },
  {
    "subject": "pruning these cases",
    "predicate": "do not affect",
    "object": "the optimality"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "achieves",
    "object": "optimality for a given list of layers"
  },
  {
    "subject": "layers or operators in a DNN model",
    "predicate": "can be connected as",
    "object": "an arbitrary computation graph"
  },
  {
    "subject": "Models like ResNet and Inception",
    "predicate": "are",
    "object": "technically non-linear directed acyclic graphs (DAGs)"
  },
  {
    "subject": "layersoperators in the DAG",
    "predicate": "are issued to",
    "object": "the GPU one by one"
  },
  {
    "subject": "execution order",
    "predicate": "exists for",
    "object": "layersoperators in the DAG"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "does not have",
    "object": "any special assumptions on the execution order"
  },
  {
    "subject": "grouping the layers",
    "predicate": "aims to achieve",
    "object": "high pipelining efficiency and low pipelining overhead"
  },
  {
    "subject": "the order",
    "predicate": "is based on",
    "object": "the first time an operator is executed"
  },
  {
    "subject": "The order",
    "predicate": "does not affect",
    "object": "correctness"
  },
  {
    "subject": "an operator",
    "predicate": "can be executed only when",
    "object": "it is transmitted to the GPU and the input is ready"
  },
  {
    "subject": "our pipelined model transmission",
    "predicate": "is applicable to",
    "object": "the general case"
  },
  {
    "subject": "Pipelined Model Transmission",
    "predicate": "is evaluated by",
    "object": "keeping all other components of PipeSwitch the same and comparing mechanisms discussed in 4.2"
  },
  {
    "subject": "Figure 7",
    "predicate": "shows",
    "object": "Effectiveness of pipelined model transmission"
  },
  {
    "subject": "Task execution in a GPU",
    "predicate": "requires",
    "object": "GPU memory"
  },
  {
    "subject": "A GPU",
    "predicate": "has",
    "object": "its own memory management system"
  },
  {
    "subject": "A GPU",
    "predicate": "provides",
    "object": "a malloc function"
  },
  {
    "subject": "malloc function",
    "predicate": "example",
    "object": "cudaMalloc for NVIDIA GPUs"
  },
  {
    "subject": "malloc function",
    "predicate": "is similar to",
    "object": "CPUs for memory allocation"
  },
  {
    "subject": "each task",
    "predicate": "uses",
    "object": "the native cudaMallocManaged function for GPU memory allocation"
  },
  {
    "subject": "each task",
    "predicate": "delegates",
    "object": "model transmission to CUDA unified memory"
  },
  {
    "subject": "we",
    "predicate": "add functions for",
    "object": "allocating GPU memory"
  },
  {
    "subject": "we",
    "predicate": "add functions for",
    "object": "sharing the GPU memory to workers through CUDA IPC API"
  },
  {
    "subject": "we",
    "predicate": "add functions for",
    "object": "getting the shared GPU memory"
  },
  {
    "subject": "Each worker",
    "predicate": "uses",
    "object": "cudaMalloc"
  },
  {
    "subject": "cudaMalloc",
    "predicate": "allocates",
    "object": "GPU memory"
  },
  {
    "subject": "Each worker",
    "predicate": "transmits",
    "object": "the model to GPU"
  },
  {
    "subject": "the model",
    "predicate": "is transmitted by",
    "object": "its own worker"
  },
  {
    "subject": "Each worker",
    "predicate": "allocates",
    "object": "GPU memory with cudaMallocManaged"
  },
  {
    "subject": "CUDA",
    "predicate": "automatically transmits",
    "object": "the model to GPU when needed"
  },
  {
    "subject": "This solution",
    "predicate": "incurs",
    "object": "high overhead for DL applications"
  },
  {
    "subject": "native cudaMalloc function",
    "predicate": "is designed for",
    "object": "general-purpose applications"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "is designed for",
    "object": "general-purpose applications"
  },
  {
    "subject": "native cudaMalloc function and CUDA unified memory",
    "predicate": "may incur",
    "object": "unnecessary overhead for DL applications"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "is not optimized for",
    "object": "DL applications"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "introduces",
    "object": "more than one hundred milliseconds overhead than PipeSwitch"
  },
  {
    "subject": "We",
    "predicate": "exploit",
    "object": "two characteristics of DL applications"
  },
  {
    "subject": "two characteristics of DL applications",
    "predicate": "are exploited to",
    "object": "minimize GPU memory management overhead"
  },
  {
    "subject": "The general-purpose GPU memory management",
    "predicate": "does not consider",
    "object": "these characteristics"
  },
  {
    "subject": "The general-purpose GPU memory management",
    "predicate": "is",
    "object": "too heavy-weight for DL applications that require fast task switching"
  },
  {
    "subject": "the amount of memory allocated to the DNN model",
    "predicate": "is",
    "object": "fixed"
  },
  {
    "subject": "the amount of memory allocated to the DNN model",
    "predicate": "does not change",
    "object": "during task execution"
  },
  {
    "subject": "a training task",
    "predicate": "updates",
    "object": "the model parameters"
  },
  {
    "subject": "a training task",
    "predicate": "does not update",
    "object": "the DNN structure"
  },
  {
    "subject": "the amount of memory needed to store them",
    "predicate": "stays",
    "object": "the same"
  },
  {
    "subject": "inference task",
    "predicate": "uses",
    "object": "the model for inference"
  },
  {
    "subject": "inference task",
    "predicate": "does not change",
    "object": "the model itself"
  },
  {
    "subject": "the intermediate results",
    "predicate": "change in",
    "object": "a simple, regular pattern"
  },
  {
    "subject": "the intermediate results",
    "predicate": "do not cause",
    "object": "memory fragmentation"
  },
  {
    "subject": "intermediate results",
    "predicate": "are",
    "object": "the outputs of each layer"
  },
  {
    "subject": "intermediate results",
    "predicate": "are used by",
    "object": "the next layer"
  },
  {
    "subject": "A training task",
    "predicate": "differs in that",
    "object": "the intermediate results generated in the forward pass cannot be immediately freed"
  },
  {
    "subject": "the intermediate results generated in the forward pass",
    "predicate": "are used by",
    "object": "the backward pass to update the weights"
  },
  {
    "subject": "the backward pass",
    "predicate": "consumes",
    "object": "the intermediate results"
  },
  {
    "subject": "the backward pass",
    "predicate": "consumes in order",
    "object": "reverse order as that the forward pass generates them"
  },
  {
    "subject": "the intermediate results",
    "predicate": "are",
    "object": "first-in-last-out"
  },
  {
    "subject": "The memory allocation and release",
    "predicate": "can be handled by",
    "object": "a simple stack-like mechanism"
  },
  {
    "subject": "a simple stack-like mechanism",
    "predicate": "does not cause",
    "object": "memory fragmentation"
  },
  {
    "subject": "memory allocation overhead",
    "predicate": "should be minimized",
    "object": ""
  },
  {
    "subject": "memory footprint",
    "predicate": "should be",
    "object": "minimized"
  },
  {
    "subject": "extra memory copies",
    "predicate": "should be",
    "object": "avoided"
  },
  {
    "subject": "we",
    "predicate": "design",
    "object": "a memory management mechanism tailored for DL applications"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "uses",
    "object": "a dedicated memory daemon"
  },
  {
    "subject": "a dedicated memory daemon",
    "predicate": "manages",
    "object": "the GPU memory"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "sends",
    "object": "a 64-bit integer offset for the shared GPU memory to workers"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "saves",
    "object": "223 ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "saves time by",
    "object": "eliminating the memory allocation overhead with the memory daemon"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is compared to",
    "object": "no unified memory management"
  },
  {
    "subject": "The memory daemon",
    "predicate": "needs to pass",
    "object": "memory pointers to the workers"
  },
  {
    "subject": "passing memory pointers to the workers",
    "predicate": "is",
    "object": "light-weight"
  },
  {
    "subject": "The daemon",
    "predicate": "ensures",
    "object": "that each time only one worker owns the GPU memory"
  },
  {
    "subject": "one worker",
    "predicate": "owns",
    "object": "the GPU memory"
  },
  {
    "subject": "owning the GPU memory",
    "predicate": "guarantees",
    "object": "memory isolation between workers"
  },
  {
    "subject": "Each worker",
    "predicate": "uses",
    "object": "a memory pool to allocate the memory to store its model and intermediate results"
  },
  {
    "subject": "Each worker",
    "predicate": "recycles",
    "object": "the memory to the pool after the intermediate results are no longer needed"
  },
  {
    "subject": "The memory management of PipeSwitch",
    "predicate": "extends",
    "object": "that of Py-Torch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "inserts",
    "object": "GPU memory blocks to PyTorch GPU memory pool"
  },
  {
    "subject": "PyTorch",
    "predicate": "creates",
    "object": "tensors on them"
  },
  {
    "subject": "memory management in PyTorch",
    "predicate": "handles",
    "object": "memory allocation for a task itself"
  },
  {
    "subject": "Replicating the models in each worker",
    "predicate": "incurs",
    "object": "high memory footprint"
  },
  {
    "subject": "Replicating the models in each worker",
    "predicate": "reduces",
    "object": "the number of models a server can store"
  },
  {
    "subject": "reducing the number of models a server can store",
    "predicate": "consequently reduces",
    "object": "the types of tasks the server can execute"
  },
  {
    "subject": "storing the models in a dedicate process",
    "predicate": "has",
    "object": "minimal memory footprint"
  },
  {
    "subject": "each model",
    "predicate": "is stored",
    "object": "only once"
  },
  {
    "subject": "storing the models in a dedicate process",
    "predicate": "incurs",
    "object": "an extra memory copy from this process to a worker to start a task"
  },
  {
    "subject": "an extra memory copy from this process to a worker to start a task",
    "predicate": "hurts",
    "object": "the task switching time"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "stores",
    "object": "the models in the memory daemon"
  },
  {
    "subject": "the server",
    "predicate": "needs to keep",
    "object": "one copy of each model in the host memory"
  },
  {
    "subject": "IPC overhead",
    "predicate": "should be",
    "object": "minimized"
  },
  {
    "subject": "We",
    "predicate": "leverage",
    "object": "a property of DL applications"
  },
  {
    "subject": "a property of DL applications",
    "predicate": "minimize",
    "object": "the IPC overhead"
  },
  {
    "subject": "IPC APIs",
    "predicate": "include example",
    "object": "cudaIpcOpenMemHandle for NVIDIA GPUs"
  },
  {
    "subject": "We",
    "predicate": "have measured",
    "object": "the performance of these IPC APIs"
  },
  {
    "subject": "these IPC APIs",
    "predicate": "incur",
    "object": "high overhead"
  },
  {
    "subject": "The overhead",
    "predicate": "is exacerbated by",
    "object": "the pipeline"
  },
  {
    "subject": "the pipeline",
    "predicate": "needs to invoke",
    "object": "the IPCs frequently"
  },
  {
    "subject": "the IPCs",
    "predicate": "are invoked to",
    "object": "synchronize model transmission and task execution for every pipeline group"
  },
  {
    "subject": "the pipeline",
    "predicate": "invokes the IPC",
    "object": "only once for the entire model transmission (not)"
  },
  {
    "subject": "memory allocation process for a neural network model",
    "predicate": "is",
    "object": "deterministic"
  },
  {
    "subject": "memory daemon and the worker",
    "predicate": "use",
    "object": "the same order to allocate memory for the model parameters"
  },
  {
    "subject": "memory pointers for the parameters",
    "predicate": "would be",
    "object": "the same"
  },
  {
    "subject": "the neural network model",
    "predicate": "is",
    "object": "known and given"
  },
  {
    "subject": "the memory daemon",
    "predicate": "needs to use",
    "object": "the same order to transmit the model as the worker would"
  },
  {
    "subject": "the memory daemon",
    "predicate": "can minimize",
    "object": "the usage of expensive GPU IPCs"
  },
  {
    "subject": "latency",
    "predicate": "is higher than",
    "object": "no unified memory management without IPC optimization"
  },
  {
    "subject": "Pin memory",
    "predicate": "is",
    "object": "a concept"
  },
  {
    "subject": "pin memory",
    "predicate": "is",
    "object": "no"
  },
  {
    "subject": "The OS",
    "predicate": "would swap",
    "object": "a memory page to disk"
  },
  {
    "subject": "The OS",
    "predicate": "would swap a memory page to disk if",
    "object": "the page is inactive for a certain amount of time"
  },
  {
    "subject": "GPUs",
    "predicate": "require",
    "object": "a page in the host memory to be pinned (or page-locked)"
  },
  {
    "subject": "a page in the host memory",
    "predicate": "needs to be pinned",
    "object": "in order to transmit the data in the page to the GPU memory"
  },
  {
    "subject": "a temporary pinned page",
    "predicate": "is created for",
    "object": "the transmission"
  },
  {
    "subject": "We",
    "predicate": "pin",
    "object": "the pages of the memory daemon to the host memory"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "is",
    "object": "desirable"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "ensures",
    "object": "one task cannot read the memory of another task"
  },
  {
    "subject": "Process-level isolation",
    "predicate": "ensures",
    "object": "the crashing of one task does not affect other tasks or the entire system"
  },
  {
    "subject": "we",
    "predicate": "use",
    "object": "separate processes"
  },
  {
    "subject": "separate processes",
    "predicate": "achieve",
    "object": "process-level isolation"
  },
  {
    "subject": "A naive solution",
    "predicate": "is to use",
    "object": "separate processes"
  },
  {
    "subject": "A naive solution",
    "predicate": "is to start",
    "object": "the new task after the current task is stopped"
  },
  {
    "subject": "sequential execution",
    "predicate": "incurs",
    "object": "long delay"
  },
  {
    "subject": "long delay",
    "predicate": "is due to",
    "object": "old task cleaning and new task initialization"
  },
  {
    "subject": "Another possible solution",
    "predicate": "is",
    "object": "to let the current and new tasks share the same process with a warm CUDA context"
  },
  {
    "subject": "the new task",
    "predicate": "can reuse",
    "object": "the GPU environment of the current task"
  },
  {
    "subject": "The process of the old task",
    "predicate": "cleans",
    "object": "the GPU environment"
  },
  {
    "subject": "another process",
    "predicate": "is created and initialized for",
    "object": "the new task"
  },
  {
    "subject": "The process",
    "predicate": "reuses",
    "object": "the environment for the new task"
  },
  {
    "subject": "a major job",
    "predicate": "is to clear",
    "object": "asynchronous CUDA functions queued on the GPU"
  },
  {
    "subject": "We",
    "predicate": "insert",
    "object": "synchronization points into training tasks"
  },
  {
    "subject": "the number of queued functions",
    "predicate": "are",
    "object": "limited"
  },
  {
    "subject": "the number of queued functions",
    "predicate": "can be",
    "object": "quickly cleared"
  },
  {
    "subject": "Synchronization points",
    "predicate": "are not needed for",
    "object": "inference tasks"
  },
  {
    "subject": "inference tasks",
    "predicate": "are",
    "object": "short"
  },
  {
    "subject": "inference tasks",
    "predicate": "are",
    "object": "not preempted"
  },
  {
    "subject": "Another job",
    "predicate": "is to",
    "object": "free its GPU memory"
  },
  {
    "subject": "cleaning procedure",
    "predicate": "has property",
    "object": "does not modify the content of the memory"
  },
  {
    "subject": "cleaning procedure",
    "predicate": "cleans",
    "object": "metadata"
  },
  {
    "subject": "metadata",
    "predicate": "is",
    "object": "GPU memory pointers"
  },
  {
    "subject": "GPU memory",
    "predicate": "is managed by",
    "object": "PipeSwitch"
  },
  {
    "subject": "cleaning procedure",
    "predicate": "deletes",
    "object": "pointers pointing to the tensor data"
  },
  {
    "subject": "cleaning procedure",
    "predicate": "does not free",
    "object": "the actual data"
  },
  {
    "subject": "the new task",
    "predicate": "transmit",
    "object": "its model to the GPU memory"
  },
  {
    "subject": "we",
    "predicate": "can parallelize",
    "object": "the task cleaning of the current task and the pipelined model transmission of the new task"
  },
  {
    "subject": "parallelizing the task cleaning and pipelined model transmission",
    "predicate": "aims to",
    "object": "hide the task cleaning overhead"
  },
  {
    "subject": "This choice",
    "predicate": "is optimized for",
    "object": "performance"
  },
  {
    "subject": "This choice",
    "predicate": "is not a problem for",
    "object": "a trusted environment"
  },
  {
    "subject": "a latter process",
    "predicate": "can read",
    "object": "the memory data of a previous process"
  },
  {
    "subject": "an additional zero-out operation",
    "predicate": "can be added",
    "object": "if this is a concern"
  },
  {
    "subject": "GPU",
    "predicate": "has",
    "object": "high memory bandwidth"
  },
  {
    "subject": "memory bandwidth of GPU",
    "predicate": "is",
    "object": "900GBs for V100"
  },
  {
    "subject": "zeroing-out most models like ResNet-152 (around 240MB)",
    "predicate": "incurs",
    "object": "sub-millisecond overhead"
  },
  {
    "subject": "the controller",
    "predicate": "signals",
    "object": "the current active worker to stop"
  },
  {
    "subject": "the controller",
    "predicate": "deletes",
    "object": "the GPU memory allocated to the current active worker"
  },
  {
    "subject": "the controller",
    "predicate": "allocates",
    "object": "the GPU memory to the new active worker"
  },
  {
    "subject": "controller",
    "predicate": "will notify",
    "object": "current active worker to stop"
  },
  {
    "subject": "controller",
    "predicate": "transfers",
    "object": "parameters of the new model to the GPU"
  },
  {
    "subject": "controller",
    "predicate": "transfers parameters after",
    "object": "receiving the current active worker's reply"
  },
  {
    "subject": "The controller",
    "predicate": "ensures",
    "object": "only one active worker"
  },
  {
    "subject": "only one active worker",
    "predicate": "guarantees",
    "object": "exclusive occupation of the GPU"
  },
  {
    "subject": "number of standby workers",
    "predicate": "has trade-off with",
    "object": "their GPU memory consumption"
  },
  {
    "subject": "every standby worker",
    "predicate": "needs to maintain",
    "object": "its own CUDA context"
  },
  {
    "subject": "its own CUDA context",
    "predicate": "consumes",
    "object": "a few hundred MB GPU memory"
  },
  {
    "subject": "there",
    "predicate": "is",
    "object": "always at least one idle standby worker"
  },
  {
    "subject": "two standby workers",
    "predicate": "are sufficient to ensure",
    "object": "at least one idle worker"
  },
  {
    "subject": "one idle worker",
    "predicate": "eliminates",
    "object": "the waiting time"
  },
  {
    "subject": "one idle worker",
    "predicate": "has",
    "object": "moderate GPU memory consumption"
  },
  {
    "subject": "A transaction",
    "predicate": "means",
    "object": "a model is switched in or out on all of its GPUs to enable or disable inference on this model"
  },
  {
    "subject": "We",
    "predicate": "have analyzed",
    "object": "a production GPU training trace from Microsoft 19,20"
  },
  {
    "subject": "111,883 tasks in this trace",
    "predicate": "include",
    "object": "96,662 tasks"
  },
  {
    "subject": "96,662 tasks",
    "predicate": "are",
    "object": "single-GPU training tasks"
  },
  {
    "subject": "96,662 tasks",
    "predicate": "represent",
    "object": "86% of all the tasks"
  },
  {
    "subject": "these jobs",
    "predicate": "account for",
    "object": "18 of total GPU hours"
  },
  {
    "subject": "we",
    "predicate": "expect",
    "object": "the share of multi-GPU jobs to increase in the future"
  },
  {
    "subject": "current training frameworks",
    "predicate": "do not have",
    "object": "mature support of elastic training"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "performs",
    "object": "the best"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is close to",
    "object": "the lower bound"
  },
  {
    "subject": "These solutions",
    "predicate": "are complementary to",
    "object": "PipeSwitch"
  },
  {
    "subject": "We",
    "predicate": "add",
    "object": "C and Python functions to the GPU memory management module of PyTorch"
  },
  {
    "subject": "functions",
    "predicate": "insert",
    "object": "the received GPU memory into PyTorch GPU memory pool for a specific CUDA stream"
  },
  {
    "subject": "functions",
    "predicate": "clear",
    "object": "the GPU memory from the pool"
  },
  {
    "subject": "shared GPU memory",
    "predicate": "can be inserted into",
    "object": "PyTorch GPU memory pool"
  },
  {
    "subject": "shared GPU memory",
    "predicate": "can be inserted multiple times for",
    "object": "different CUDA streams"
  },
  {
    "subject": "controller",
    "predicate": "guarantees",
    "object": "only one of these CUDA streams is active"
  },
  {
    "subject": "The controller process",
    "predicate": "consists of",
    "object": "a TCP thread and a scheduler thread"
  },
  {
    "subject": "the scheduler and the memory daemon",
    "predicate": "are implemented together",
    "object": "for better performance"
  },
  {
    "subject": "The TCP thread",
    "predicate": "accepts",
    "object": "task through TCP from clients"
  },
  {
    "subject": "The TCP thread",
    "predicate": "sends",
    "object": "the task to the scheduler thread"
  },
  {
    "subject": "The scheduler thread",
    "predicate": "allocates and shares",
    "object": "the GPU memory with workers"
  },
  {
    "subject": "The scheduler thread",
    "predicate": "activates or deactivates",
    "object": "workers"
  },
  {
    "subject": "The scheduler thread",
    "predicate": "sends",
    "object": "the task to a worker"
  },
  {
    "subject": "The scheduler thread",
    "predicate": "transfers",
    "object": "parameters for the corresponding model to the GPU memory"
  },
  {
    "subject": "the user",
    "predicate": "should register",
    "object": "the model in the scheduler"
  },
  {
    "subject": "the scheduler",
    "predicate": "notifies",
    "object": "the controller"
  },
  {
    "subject": "the controller",
    "predicate": "loads",
    "object": "the model from the disk to the CPU memory"
  },
  {
    "subject": "Parameters",
    "predicate": "are transmitted to",
    "object": "the GPU memory"
  },
  {
    "subject": "Parameters",
    "predicate": "are transmitted in",
    "object": "groups"
  },
  {
    "subject": "Parameters",
    "predicate": "are transmitted in",
    "object": "a pipeline"
  },
  {
    "subject": "controller",
    "predicate": "notifies",
    "object": "worker"
  },
  {
    "subject": "worker",
    "predicate": "starts",
    "object": "computing the corresponding layers"
  },
  {
    "subject": "each group",
    "predicate": "is transferred",
    "object": null
  },
  {
    "subject": "The worker process",
    "predicate": "consists of",
    "object": "two threads"
  },
  {
    "subject": "The termination thread",
    "predicate": "waits for",
    "object": "the termination signal from the controller"
  },
  {
    "subject": "The termination thread",
    "predicate": "notifies",
    "object": "the main thread"
  },
  {
    "subject": "The main thread",
    "predicate": "manages",
    "object": "the DNN models"
  },
  {
    "subject": "The main thread",
    "predicate": "performs",
    "object": "the computation for inference or training"
  },
  {
    "subject": "worker",
    "predicate": "requires",
    "object": "user to register the model before starting a task"
  },
  {
    "subject": "worker",
    "predicate": "can load",
    "object": "the models"
  },
  {
    "subject": "worker",
    "predicate": "can add",
    "object": "the hooks to wait for parameter transmission or terminate on notification"
  },
  {
    "subject": "the worker",
    "predicate": "loads",
    "object": "the model structures"
  },
  {
    "subject": "the model structures",
    "predicate": "is",
    "object": "small"
  },
  {
    "subject": "the worker",
    "predicate": "does not load",
    "object": "the model parameters"
  },
  {
    "subject": "The parameters",
    "predicate": "are stored",
    "object": "once in the memory daemon"
  },
  {
    "subject": "The parameters",
    "predicate": "are stored for",
    "object": "minimal memory footprint"
  },
  {
    "subject": "models",
    "predicate": "are loaded",
    "object": null
  },
  {
    "subject": "models",
    "predicate": "are attached to",
    "object": "different CUDA streams"
  },
  {
    "subject": "their parameters",
    "predicate": "are assigned to",
    "object": "locations in the shared GPU memory"
  },
  {
    "subject": "Different models",
    "predicate": "might use",
    "object": "the same GPU memory location"
  },
  {
    "subject": "the value",
    "predicate": "is not valid until",
    "object": "the controller transfers the corresponding parameters to these locations"
  },
  {
    "subject": "All experiments",
    "predicate": "are conducted on",
    "object": "AWS"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "two EC2 instance types"
  },
  {
    "subject": "One",
    "predicate": "is",
    "object": "p3.2xlarge"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "is configured with",
    "object": "8 vCPUs (Intel Xeon E5-2686 v4)"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has",
    "object": "1 GPU (NVIDIA V100 with 16 GB GPU memory)"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has",
    "object": "PCIe 3.0 16"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has",
    "object": "61 GB memory"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "is configured with",
    "object": "8 vCPUs (Intel Platinum 8259CL)"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has",
    "object": "1 GPU (NVIDIA T4 with 16 GB GPU memory)"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has",
    "object": "PCIe 3.0 8"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has",
    "object": "32 GB memory"
  },
  {
    "subject": "The software environment",
    "predicate": "includes",
    "object": "PyTorch-1.3.0"
  },
  {
    "subject": "The software environment",
    "predicate": "includes",
    "object": "torchvision-0.4.2"
  },
  {
    "subject": "The software environment",
    "predicate": "includes",
    "object": "scipy-1.3.2"
  },
  {
    "subject": "The software environment",
    "predicate": "includes",
    "object": "CUDA-10.1"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "PyTorch with our plugins for all mechanisms in comparison for consistency"
  },
  {
    "subject": "PyTorch with our plugins",
    "predicate": "provides",
    "object": "better results for stop-and-start than native PyTorch from Python-PyPI used in Table 1"
  },
  {
    "subject": "The models",
    "predicate": "include",
    "object": "ResNet152 17"
  },
  {
    "subject": "The models",
    "predicate": "include",
    "object": "Inceptionv3 22"
  },
  {
    "subject": "The models",
    "predicate": "include",
    "object": "Bertbase 23"
  },
  {
    "subject": "ResNet152 17, Inceptionv3 22 and Bertbase 23",
    "predicate": "are",
    "object": "standard benchmarks for evaluating DL systems"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "representative configurations for each model"
  },
  {
    "subject": "The experiments",
    "predicate": "cover",
    "object": "both training and inference"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "single-GPU inference and training tasks"
  },
  {
    "subject": "single-GPU inference and training tasks",
    "predicate": "are discussed in",
    "object": "4.5"
  },
  {
    "subject": "Training tasks",
    "predicate": "check-point",
    "object": "their models to the host memory periodically"
  },
  {
    "subject": "Training tasks",
    "predicate": "restart from",
    "object": "the latest checkpoint after preemption"
  },
  {
    "subject": "The checkpointing frequency of training tasks",
    "predicate": "is set according to",
    "object": "the scheduling cycle"
  },
  {
    "subject": "The checkpointing frequency of training tasks",
    "predicate": "is set to minimize",
    "object": "checkpointing overhead"
  },
  {
    "subject": "default batch size for training",
    "predicate": "is",
    "object": "32"
  },
  {
    "subject": "default batch size for inference",
    "predicate": "is",
    "object": "8"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "throughput and latency as evaluation metrics"
  },
  {
    "subject": "We",
    "predicate": "measure",
    "object": "the end-to-end latency experienced by the client"
  },
  {
    "subject": "Figure 5",
    "predicate": "shows",
    "object": "total latency experienced by the client for different mechanisms"
  },
  {
    "subject": "Figure 5",
    "predicate": "shows",
    "object": "the latency experienced by the client"
  },
  {
    "subject": "Table 3",
    "predicate": "shows",
    "object": "the total overhead"
  },
  {
    "subject": "Each number",
    "predicate": "is reported with",
    "object": "the average of 100 runs"
  },
  {
    "subject": "Figure 6(b)",
    "predicate": "reports",
    "object": "minimum and maximum latencies using the error bar"
  },
  {
    "subject": "latency of the first batch and those of later batches in one scheduling cycle",
    "predicate": "can differ",
    "object": "significantly due to switching overhead"
  },
  {
    "subject": "6.1 End-to-End Experiments",
    "predicate": "minimize",
    "object": "end-to-end overhead"
  },
  {
    "subject": "a client",
    "predicate": "sends",
    "object": "an inference task to a GPU server"
  },
  {
    "subject": "the GPU server",
    "predicate": "preempts",
    "object": "the training task"
  },
  {
    "subject": "the GPU server",
    "predicate": "executes",
    "object": "the inference task"
  },
  {
    "subject": "the GPU server",
    "predicate": "sends",
    "object": "a reply back to the client"
  },
  {
    "subject": "There",
    "predicate": "is",
    "object": "no training task"
  },
  {
    "subject": "The process",
    "predicate": "has",
    "object": "the required model loaded in the GPU"
  },
  {
    "subject": "This solution",
    "predicate": "provides",
    "object": "the lower bound"
  },
  {
    "subject": "the lower bound",
    "predicate": "is",
    "object": "the lowest latency we can achieve for an inference task"
  },
  {
    "subject": "This solution",
    "predicate": "is used by",
    "object": "existing systems like Gandiva 24"
  },
  {
    "subject": "Gandiva 24",
    "predicate": "is used for",
    "object": "task switching"
  },
  {
    "subject": "task switching",
    "predicate": "reported",
    "object": "similar second-scale overhead"
  },
  {
    "subject": "NVIDIA MPS",
    "predicate": "is",
    "object": "NVIDIA MPS"
  },
  {
    "subject": "We",
    "predicate": "initialize",
    "object": "separate processes in advance"
  },
  {
    "subject": "CUDA unified memory",
    "predicate": "is used for",
    "object": "memory swapping"
  },
  {
    "subject": "CUDA",
    "predicate": "has feature",
    "object": "unified memory"
  },
  {
    "subject": "The properties",
    "predicate": "are described in",
    "object": "4"
  },
  {
    "subject": "Latency",
    "predicate": "measured in",
    "object": "milliseconds (ms)"
  },
  {
    "subject": "Latency",
    "predicate": "values",
    "object": "7500 to 10000 ms"
  },
  {
    "subject": "Models",
    "predicate": "include",
    "object": "Ready model, PipeSwitch, MPS, Stop-and-start, ResNet152, Inceptionv3, Bertbase"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "equipped with",
    "object": "NVIDIA V100 GPU"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "connected via",
    "object": "PCIe 3.0 x16 interface"
  },
  {
    "subject": "g4dn.2xlarge",
    "predicate": "has GPU",
    "object": "NVIDIA T4"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "uses interface",
    "object": "PCIe 3.0 x8"
  },
  {
    "subject": "Latency (ms)",
    "predicate": "ranges from",
    "object": "5000 to 10000"
  },
  {
    "subject": "Models",
    "predicate": "include",
    "object": "ResNet152, Inceptionv3, Bertbase"
  },
  {
    "subject": "Latency (ms)",
    "predicate": "measured for",
    "object": "Ready model, PipeSwitch, MPS, Stop-and-start"
  },
  {
    "subject": "ResNet152",
    "predicate": "is mentioned in",
    "object": "the text"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "is mentioned in",
    "object": "the text"
  },
  {
    "subject": "Bertbase",
    "predicate": "is mentioned in",
    "object": "the text"
  },
  {
    "subject": "Latency",
    "predicate": "is measured in",
    "object": "ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "is a",
    "object": "pipeline method"
  },
  {
    "subject": "Per-layer pipeline",
    "predicate": "is a",
    "object": "pipeline method"
  },
  {
    "subject": "Grouped transmission",
    "predicate": "is a",
    "object": "pipeline method"
  },
  {
    "subject": "No optimization",
    "predicate": "is a",
    "object": "pipeline method"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has GPU",
    "object": "NVIDIA V100"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has PCIe version",
    "object": "3.0"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has PCIe lanes",
    "object": "16"
  },
  {
    "subject": "NVIDIA T4",
    "predicate": "uses",
    "object": "PCIe 3.0 8"
  },
  {
    "subject": "510 14th USENIX Symposium on Operating Systems Design and Implementation",
    "predicate": "organized by",
    "object": "USENIX Association"
  },
  {
    "subject": "p3.2xlarge",
    "predicate": "has PCIe version",
    "object": "3.0 16"
  },
  {
    "subject": "Latency (ms)",
    "predicate": "measured for",
    "object": "ResNet152"
  },
  {
    "subject": "Latency (ms)",
    "predicate": "measured for",
    "object": "Inceptionv3"
  },
  {
    "subject": "Latency (ms)",
    "predicate": "measured for",
    "object": "Bertbase"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "No memory management"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "No IPC optimization"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has",
    "object": "No pin memory"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "uses",
    "object": "CUDA unified memory"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has latency range",
    "object": "6000 to 8000 ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "runs on",
    "object": "One process"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "runs on",
    "object": "Two processes"
  },
  {
    "subject": "ResNet152",
    "predicate": "is a model used in",
    "object": "PipeSwitch"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "is a model used in",
    "object": "PipeSwitch"
  },
  {
    "subject": "Bertbase",
    "predicate": "is a model used in",
    "object": "PipeSwitch"
  },
  {
    "subject": "NVIDIA V100",
    "predicate": "uses interface",
    "object": "PCIe 3.0 16"
  },
  {
    "subject": "Latency",
    "predicate": "range",
    "object": "5000 to 7500 ms"
  },
  {
    "subject": "Models",
    "predicate": "include",
    "object": "ResNet152"
  },
  {
    "subject": "Models",
    "predicate": "include",
    "object": "Inceptionv3"
  },
  {
    "subject": "Models",
    "predicate": "include",
    "object": "Bertbase"
  },
  {
    "subject": "ResNet152",
    "predicate": "startup overhead on",
    "object": "p3.2xlarge"
  },
  {
    "subject": "ResNet152",
    "predicate": "startup overhead time on p3.2xlarge",
    "object": "3.62 ms"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "startup overhead on",
    "object": "p3.2xlarge"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "startup overhead time on p3.2xlarge",
    "object": "4.82 ms"
  },
  {
    "subject": "Bertbase",
    "predicate": "startup overhead on",
    "object": "p3.2xlarge"
  },
  {
    "subject": "Bertbase",
    "predicate": "startup overhead time on p3.2xlarge",
    "object": "3.62 ms"
  },
  {
    "subject": "ResNet152",
    "predicate": "startup overhead on",
    "object": "g4dn.2xlarge"
  },
  {
    "subject": "ResNet152",
    "predicate": "startup overhead time on g4dn.2xlarge",
    "object": "2.53 ms"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "startup overhead on",
    "object": "g4dn.2xlarge"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "startup overhead time on g4dn.2xlarge",
    "object": "5.49 ms"
  },
  {
    "subject": "Bertbase",
    "predicate": "startup overhead on",
    "object": "g4dn.2xlarge"
  },
  {
    "subject": "Bertbase",
    "predicate": "startup overhead time on g4dn.2xlarge",
    "object": "6.57 ms"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "has startup overhead to start computing",
    "object": "the first layer"
  },
  {
    "subject": "Table 4",
    "predicate": "shows",
    "object": "task startup overhead for PipeSwitch"
  },
  {
    "subject": "task startup overhead for PipeSwitch",
    "predicate": "is",
    "object": "the difference between the time for ResNet152, Inceptionv3, Bertbase"
  },
  {
    "subject": "ResNet152",
    "predicate": "has layers",
    "object": "464"
  },
  {
    "subject": "Inceptionv3",
    "predicate": "has layers",
    "object": "189"
  },
  {
    "subject": "Bertbase",
    "predicate": "has layers",
    "object": "139"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "has startup overhead time for ResNet152",
    "object": "1.33 s"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "has startup overhead time for Inceptionv3",
    "object": "0.18 s"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "has startup overhead time for Bertbase",
    "object": "0.34 s"
  },
  {
    "subject": "Only Pruning 1",
    "predicate": "has startup overhead time for ResNet152",
    "object": "2.09 s"
  },
  {
    "subject": "Only Pruning 1",
    "predicate": "has startup overhead time for Inceptionv3",
    "object": "0.30 s"
  },
  {
    "subject": "Only Pruning 1",
    "predicate": "has startup overhead time for Bertbase",
    "object": "0.88 s"
  },
  {
    "subject": "Only Pruning 2",
    "predicate": "has startup overhead time for ResNet152",
    "object": "3.44 h"
  },
  {
    "subject": "Only Pruning 2",
    "predicate": "has startup overhead time for Inceptionv3",
    "object": "5.07 s"
  },
  {
    "subject": "Only Pruning 2",
    "predicate": "has startup overhead time for Bertbase",
    "object": "24 h"
  },
  {
    "subject": "No Pruning",
    "predicate": "has startup overhead time for ResNet152",
    "object": "24 h"
  },
  {
    "subject": "No Pruning",
    "predicate": "has startup overhead time for Inceptionv3",
    "object": "24 h"
  },
  {
    "subject": "No Pruning",
    "predicate": "has startup overhead time for Bertbase",
    "object": "24 h"
  },
  {
    "subject": "Table 5",
    "predicate": "shows",
    "object": "effectiveness of two pruning techniques"
  },
  {
    "subject": "Salus 7",
    "predicate": "is not directly comparable",
    "object": ""
  },
  {
    "subject": "Salus 7",
    "predicate": "requires",
    "object": "the models to be preloaded to the GPU"
  },
  {
    "subject": "Salus 7",
    "predicate": "has",
    "object": "several limitations described in 2.2"
  },
  {
    "subject": "Its performance",
    "predicate": "is similar to",
    "object": "the ready model when the model is preloaded"
  },
  {
    "subject": "Its performance",
    "predicate": "is similar to",
    "object": "NVIDIA MPS when the model is in the host memory"
  },
  {
    "subject": "The total overhead",
    "predicate": "is",
    "object": "the difference between the latency of a mechanism and that of the ready model"
  },
  {
    "subject": "stop-and-start",
    "predicate": "performs",
    "object": "the worst"
  },
  {
    "subject": "stop-and-start",
    "predicate": "takes",
    "object": "several seconds"
  },
  {
    "subject": "The main source of the overhead",
    "predicate": "is",
    "object": "CUDA context initialization and rst-time library loading operations in PyTorch"
  },
  {
    "subject": "Another source",
    "predicate": "is",
    "object": "GPU memory swapping"
  },
  {
    "subject": "The overhead of PipeSwitch for most configurations",
    "predicate": "is up to",
    "object": "10ms"
  },
  {
    "subject": "The overhead of PipeSwitch for BERT on T4",
    "predicate": "is due to",
    "object": "the large model size"
  },
  {
    "subject": "The overhead of PipeSwitch for BERT on T4",
    "predicate": "is due to",
    "object": "the smaller PCIe bandwidth on T4 than that on V100"
  },
  {
    "subject": "computing BERT on T4",
    "predicate": "takes",
    "object": "120ms"
  },
  {
    "subject": "relative overhead",
    "predicate": "is",
    "object": "acceptable"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "to start",
    "object": "computing the rst layer"
  },
  {
    "subject": "the ready model",
    "predicate": "to start",
    "object": "computing"
  },
  {
    "subject": "The startup overhead of PipeSwitch",
    "predicate": "is",
    "object": "only a few milliseconds"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "incurs",
    "object": "only a few milliseconds overhead for task switching"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "achieves",
    "object": "low latency close to the lower bound"
  },
  {
    "subject": "we",
    "predicate": "compare",
    "object": "throughput and end-to-end latency of different mechanisms under different scheduling cycles"
  },
  {
    "subject": "We",
    "predicate": "use",
    "object": "ResNet152"
  },
  {
    "subject": "We",
    "predicate": "use ResNet152 for",
    "object": "both training and inference"
  },
  {
    "subject": "training and inference",
    "predicate": "run on",
    "object": "eight p3.2xlarge instances"
  },
  {
    "subject": "We",
    "predicate": "switch between",
    "object": "training and inference"
  },
  {
    "subject": "We",
    "predicate": "switch tasks after",
    "object": "each scheduling cycle"
  },
  {
    "subject": "Figure 6(a)",
    "predicate": "shows",
    "object": "the inference throughput"
  },
  {
    "subject": "The dashed line",
    "predicate": "is",
    "object": "the upper bound"
  },
  {
    "subject": "the upper bound",
    "predicate": "is",
    "object": "the throughput of the ready model assuming no task switching"
  },
  {
    "subject": "The dashed line",
    "predicate": "is",
    "object": "the lower bound"
  },
  {
    "subject": "the lower bound",
    "predicate": "is",
    "object": "the average latency of the ready model assuming no task switching"
  },
  {
    "subject": "throughput of stop-and-start",
    "predicate": "is",
    "object": "nearly zero for scheduling cycles smaller than 10 s"
  },
  {
    "subject": "task switching",
    "predicate": "takes",
    "object": "several seconds"
  },
  {
    "subject": "MPS",
    "predicate": "keeps",
    "object": "poor throughput around 100 batches per second"
  },
  {
    "subject": "GPU utilization",
    "predicate": "is defined as",
    "object": "the ratio to the upper bound"
  },
  {
    "subject": "Figure 6(b)",
    "predicate": "shows",
    "object": "the average latency of the inference tasks"
  },
  {
    "subject": "The error bar",
    "predicate": "indicates",
    "object": "the minimum and maximum latency"
  },
  {
    "subject": "Stop- and-start",
    "predicate": "has",
    "object": "poor latency"
  },
  {
    "subject": "the rst batch",
    "predicate": "has",
    "object": "several seconds overhead"
  },
  {
    "subject": "MPS",
    "predicate": "has",
    "object": "about 80 ms average latency"
  },
  {
    "subject": "MPS",
    "predicate": "has",
    "object": "several hundred milliseconds latency for the rst batch"
  },
  {
    "subject": "Throughput and latency",
    "predicate": "are measured under",
    "object": "different scheduling cycles for ResNet on p3.2xlarge"
  },
  {
    "subject": "Per-layer pipeline",
    "predicate": "is",
    "object": "a concept"
  },
  {
    "subject": "Computation",
    "predicate": "starts",
    "object": "once parameters are transmitted"
  },
  {
    "subject": "Figure 7",
    "predicate": "shows",
    "object": "the total time measured by the client for an inference task to preempt a training task and finish its inference"
  },
  {
    "subject": "Figure 8",
    "predicate": "shows",
    "object": "the total time measured by the client"
  },
  {
    "subject": "No optimization",
    "predicate": "performs",
    "object": "the worst in most cases"
  },
  {
    "subject": "Grouped transmission",
    "predicate": "improves",
    "object": "no optimization"
  },
  {
    "subject": "Grouped transmission",
    "predicate": "combines",
    "object": "the layers of the model into one big tensor"
  },
  {
    "subject": "Grouped transmission",
    "predicate": "transmits",
    "object": "it in one group"
  },
  {
    "subject": "Per-layer pipeline",
    "predicate": "overlaps",
    "object": "transmission and computation"
  },
  {
    "subject": "overlaps",
    "predicate": "occur at granularity of",
    "object": "layer"
  },
  {
    "subject": "models with many layers but relatively light computation such as ResNet152 and Inception",
    "predicate": "can perform worse than",
    "object": "grouped transmission"
  },
  {
    "subject": "models with many layers but relatively light computation such as ResNet152 and Inception",
    "predicate": "can perform worse than",
    "object": "sometimes even no pipeline"
  },
  {
    "subject": "this reduction",
    "predicate": "is",
    "object": "significant"
  },
  {
    "subject": "this reduction",
    "predicate": "is evaluated when",
    "object": "the optimizations on memory management and worker switching have already been applied"
  },
  {
    "subject": "We",
    "predicate": "would like to emphasize",
    "object": "that to meet strict SLOs, it is important to reduce all overheads for task switching, not only the most significant one"
  },
  {
    "subject": "Table 5",
    "predicate": "shows",
    "object": "the running time of Algorithm 1"
  },
  {
    "subject": "Table 5",
    "predicate": "shows",
    "object": "the effects of the two pruning techniques mentioned in 4.2"
  },
  {
    "subject": "the number of layers",
    "predicate": "includes",
    "object": "both weighted and unweighted layers"
  },
  {
    "subject": "both weighted and unweighted layers",
    "predicate": "contribute to",
    "object": "the computation time"
  },
  {
    "subject": "We",
    "predicate": "measure",
    "object": "the parameter size and running time for each layer"
  },
  {
    "subject": "Algorithm 1",
    "predicate": "takes",
    "object": "only several seconds to compute an optimal grouping strategy"
  },
  {
    "subject": "ResNet152",
    "predicate": "has",
    "object": "hundreds of layers"
  },
  {
    "subject": "no pruning",
    "predicate": "does not finish",
    "object": "for all three models after running for 24 hours"
  },
  {
    "subject": "unied memory management",
    "predicate": "is evaluated for",
    "object": "effectiveness"
  },
  {
    "subject": "memory management",
    "predicate": "is",
    "object": "not unified"
  },
  {
    "subject": "we",
    "predicate": "compare",
    "object": "the following five mechanisms discussed in 4.3"
  },
  {
    "subject": "IPC",
    "predicate": "has",
    "object": "no optimization"
  },
  {
    "subject": "the pages of the memory daemon",
    "predicate": "are not pinned to",
    "object": "the main memory"
  },
  {
    "subject": "this experiment",
    "predicate": "demonstrates",
    "object": "all the optimizations on memory management are effective"
  },
  {
    "subject": "unied memory management mechanism",
    "predicate": "is used by",
    "object": "PipeSwitch"
  },
  {
    "subject": "IPC optimization",
    "predicate": "is",
    "object": "important"
  },
  {
    "subject": "IPC optimization",
    "predicate": "reduces",
    "object": "latency by 1648 ms"
  },
  {
    "subject": "pinning the pages to the host memory",
    "predicate": "can reduce",
    "object": "the latency with a few milliseconds"
  },
  {
    "subject": "One process",
    "predicate": "is",
    "object": "a process"
  },
  {
    "subject": "Figure 9",
    "predicate": "shows",
    "object": "Effectiveness of active-standby switching"
  },
  {
    "subject": "Figure 9",
    "predicate": "shows",
    "object": "the results"
  },
  {
    "subject": "Two processes",
    "predicate": "perform",
    "object": "the worst"
  },
  {
    "subject": "The new process",
    "predicate": "needs to create",
    "object": "a new CUDA environment"
  },
  {
    "subject": "a new CUDA environment",
    "predicate": "dominates",
    "object": "the total time"
  },
  {
    "subject": "One process",
    "predicate": "reuses",
    "object": "the CUDA environment"
  },
  {
    "subject": "One process",
    "predicate": "pays",
    "object": "the overhead to clean the environment"
  },
  {
    "subject": "Many frameworks",
    "predicate": "have been developed for",
    "object": "deep learning"
  },
  {
    "subject": "Many frameworks",
    "predicate": "include",
    "object": "TensorFlow"
  },
  {
    "subject": "Many frameworks",
    "predicate": "include",
    "object": "PyTorch"
  },
  {
    "subject": "Many frameworks",
    "predicate": "include",
    "object": "MXNet"
  },
  {
    "subject": "Several algorithms and systems",
    "predicate": "have been designed for",
    "object": "executing and scheduling deep learning tasks on clusters"
  },
  {
    "subject": "Several algorithms and systems",
    "predicate": "include",
    "object": "both training and inference tasks"
  },
  {
    "subject": "These scheduling solutions",
    "predicate": "are",
    "object": "orthogonal and complementary to PipeSwitch"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "focuses on",
    "object": "how to realize a scheduling decision"
  },
  {
    "subject": "PipeSwitch",
    "predicate": "enables",
    "object": "the scheduler to change the resource allocation more often with millisecond-scale task switching"
  },
  {
    "subject": "Many techniques and systems",
    "predicate": "have been proposed to",
    "object": "optimize communication"
  },
  {
    "subject": "Many techniques and systems",
    "predicate": "have been proposed to",
    "object": "improve distributed training"
  },
  {
    "subject": "The most relevant ones",
    "predicate": "are",
    "object": "PipeDream 8, ByteScheduler 9 and Poseidon 40"
  },
  {
    "subject": "vDNN 43 and SwapAdvisor 44",
    "predicate": "have",
    "object": "GPU memory management module"
  },
  {
    "subject": "vDNN 43 and SwapAdvisor 44",
    "predicate": "focus on",
    "object": "memory management for a single training task of large models"
  },
  {
    "subject": "vDNN 43 and SwapAdvisor 44",
    "predicate": "are not directly comparable to",
    "object": "PipeSwitch"
  },
  {
    "subject": "Cluster managers 4548",
    "predicate": "allocate",
    "object": "GPUs to VMs or containers at device granularity"
  },
  {
    "subject": "Several solutions",
    "predicate": "have been proposed to share",
    "object": "a GPU at application granularity using techniques like library interception"
  },
  {
    "subject": "efforts on GPU optimization",
    "predicate": "improve",
    "object": "the performance of running a single task"
  },
  {
    "subject": "efforts on GPU optimization",
    "predicate": "include",
    "object": "tensor fusion"
  },
  {
    "subject": "efforts on GPU optimization",
    "predicate": "include",
    "object": "kernel-level concurrency and scheduling"
  },
  {
    "subject": "We",
    "predicate": "thank",
    "object": "Madan Musuvathi"
  },
  {
    "subject": "We",
    "predicate": "thank",
    "object": "the anonymous reviewers"
  },
  {
    "subject": "Madan Musuvathi and the anonymous reviewers",
    "predicate": "provide",
    "object": "valuable feedback"
  },
  {
    "subject": "Zhihao Bai",
    "predicate": "was supported in part by",
    "object": "an AWS Machine Learning Research Award"
  },
  {
    "subject": "Zhen Zhang",
    "predicate": "was supported in part by",
    "object": "an AWS Machine Learning Research Award"
  },
  {
    "subject": "Xin Jin",
    "predicate": "was supported in part by",
    "object": "an AWS Machine Learning Research Award"
  },
  {
    "subject": "A. Verma, L. Pedrosa, M. Korupolu, D. Oppenheimer, E. Tune, and J. Wilkes",
    "predicate": "authored",
    "object": "Large-scale cluster management at Google with Borg"
  },
  {
    "subject": "Large-scale cluster management at Google with Borg",
    "predicate": "published in",
    "object": "EuroSys"
  },
  {
    "subject": "Large-scale cluster management at Google with Borg",
    "predicate": "published in year",
    "object": "2015"
  },
  {
    "subject": "Nexus",
    "predicate": "is",
    "object": "a GPU cluster engine for accelerating DNN-based video analysis"
  },
  {
    "subject": "Nexus",
    "predicate": "was presented in",
    "object": "ACM SOSP, 2019"
  },
  {
    "subject": "3 H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Philipose, A. Krishnamurthy, and R. Sundaram",
    "predicate": "are authors of",
    "object": "Nexus"
  },
  {
    "subject": "Fried, J. Behrens, A. Belay, and H. Balakrishnan",
    "predicate": "authored",
    "object": "Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads"
  },
  {
    "subject": "Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads",
    "predicate": "published in",
    "object": "USENIX NSDI"
  },
  {
    "subject": "Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads",
    "predicate": "published in year",
    "object": "2019"
  },
  {
    "subject": "CUDA Multi-Process Service",
    "predicate": "version",
    "object": "6"
  },
  {
    "subject": "CUDAMultiProcessServiceOverview.pdf",
    "predicate": "is located at",
    "object": "https://docs.nvidia.com/deploy/pdf"
  },
  {
    "subject": "Salus",
    "predicate": "is",
    "object": "Fine-grained GPU sharing primitives for deep learning applications"
  },
  {
    "subject": "7 P. Yu and M. Chowdhury",
    "predicate": "authored",
    "object": "Salus"
  },
  {
    "subject": "Salus",
    "predicate": "was presented in",
    "object": "Conference on Machine Learning and Systems"
  },
  {
    "subject": "Salus",
    "predicate": "was presented in year",
    "object": "2020"
  },
  {
    "subject": "PipeDream",
    "predicate": "is",
    "object": "generalized pipeline parallelism for DNN training"
  },
  {
    "subject": "PipeDream",
    "predicate": "was presented in",
    "object": "ACM SOSP, 2019"
  },
  {
    "subject": "PipeDream",
    "predicate": "was authored by",
    "object": "D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia"
  },
  {
    "subject": "Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo",
    "predicate": "authored",
    "object": "A generic communication scheduler for distributed DNN training acceleration"
  },
  {
    "subject": "A generic communication scheduler for distributed DNN training acceleration",
    "predicate": "published in",
    "object": "ACM SOSP"
  },
  {
    "subject": "A generic communication scheduler for distributed DNN training acceleration",
    "predicate": "published in year",
    "object": "2019"
  },
  {
    "subject": "Tiresias",
    "predicate": "is",
    "object": "a GPU cluster manager for distributed deep learning"
  },
  {
    "subject": "Tiresias",
    "predicate": "was presented in",
    "object": "USENIX NSDI"
  },
  {
    "subject": "Tiresias",
    "predicate": "was published in",
    "object": "2019"
  },
  {
    "subject": "Authors",
    "predicate": "include",
    "object": "J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo"
  },
  {
    "subject": "Poseidon",
    "predicate": "is",
    "object": "an efficient communication architecture"
  },
  {
    "subject": "Poseidon",
    "predicate": "is used for",
    "object": "distributed deep learning on GPU clusters"
  },
  {
    "subject": "Poseidon",
    "predicate": "was presented in",
    "object": "USENIX ATC, 2017"
  },
  {
    "subject": "H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie, and E. P. Xing",
    "predicate": "are authors of",
    "object": "Poseidon"
  },
  {
    "subject": "Amazon Web Services",
    "predicate": "is",
    "object": "12"
  },
  {
    "subject": "Microsoft Azure",
    "predicate": "is",
    "object": "13"
  },
  {
    "subject": "Google Cloud Platform",
    "predicate": "is",
    "object": "14"
  },
  {
    "subject": "Horovod",
    "predicate": "is described as",
    "object": "fast and easy distributed deep learning in TensorFlow"
  },
  {
    "subject": "15 A. Sergeev and M. Del Balso",
    "predicate": "are authors of",
    "object": "Horovod: fast and easy distributed deep learning in TensorFlow"
  },
  {
    "subject": "Horovod paper",
    "predicate": "was published in",
    "object": "arXiv preprint arXiv:1802.05799"
  },
  {
    "subject": "Horovod paper",
    "predicate": "was published in year",
    "object": "2018"
  },
  {
    "subject": "Su",
    "predicate": "authored",
    "object": "Scaling distributed machine learning with the parameter server"
  },
  {
    "subject": "Scaling distributed machine learning with the parameter server",
    "predicate": "published in",
    "object": "USENIX OSDI"
  },
  {
    "subject": "Scaling distributed machine learning with the parameter server",
    "predicate": "published in year",
    "object": "2014"
  },
  {
    "subject": "Deep residual learning for image recognition",
    "predicate": "author",
    "object": "Sun"
  },
  {
    "subject": "Deep residual learning for image recognition",
    "predicate": "published in",
    "object": "IEEE Conference on Computer Vision and Pattern Recognition"
  },
  {
    "subject": "Deep residual learning for image recognition",
    "predicate": "publication year",
    "object": "2016"
  },
  {
    "subject": "Nvidia data center deep learning product",
    "predicate": "has",
    "object": "performance"
  },
  {
    "subject": "Philly",
    "predicate": "has",
    "object": "20 traces"
  },
  {
    "subject": "philly-traces",
    "predicate": "is hosted on",
    "object": "https://github.com/msr-fiddle/philly-traces"
  },
  {
    "subject": "21",
    "predicate": "is",
    "object": "PyTorch"
  },
  {
    "subject": "22 C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna",
    "predicate": "authored",
    "object": "Rethinking the inception architecture for computer vision"
  },
  {
    "subject": "Rethinking the inception architecture for computer vision",
    "predicate": "was presented in",
    "object": "IEEE Conference on Computer Vision and Pattern Recognition"
  },
  {
    "subject": "Rethinking the inception architecture for computer vision",
    "predicate": "was published in year",
    "object": "2016"
  },
  {
    "subject": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova",
    "predicate": "authored",
    "object": "BERT: Pre-training of deep bidirectional transformers for language understanding"
  },
  {
    "subject": "BERT: Pre-training of deep bidirectional transformers for language understanding",
    "predicate": "was presented in",
    "object": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)"
  },
  {
    "subject": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    "predicate": "was published in",
    "object": "2019"
  },
  {
    "subject": "Gandiva",
    "predicate": "is",
    "object": "Introspective cluster scheduling for deep learning"
  },
  {
    "subject": "Gandiva",
    "predicate": "was presented in",
    "object": "USENIX OSDI"
  },
  {
    "subject": "Gandiva",
    "predicate": "was presented in year",
    "object": "2018"
  },
  {
    "subject": "24 W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P. Patel, X. Peng, H. Zhao, Q. Zhang, et al.",
    "predicate": "are authors of",
    "object": "Gandiva"
  },
  {
    "subject": "25",
    "predicate": "is",
    "object": "TensorFlow"
  },
  {
    "subject": "TensorFlow XLA",
    "predicate": "is",
    "object": "54"
  },
  {
    "subject": "26",
    "predicate": "is",
    "object": "MXNet"
  },
  {
    "subject": "https://mxnet.apache.org",
    "predicate": "is",
    "object": "a website URL"
  },
  {
    "subject": "M. J. Freedman",
    "predicate": "authored",
    "object": "Slaq: quality-driven scheduling for distributed machine learning"
  },
  {
    "subject": "Slaq: quality-driven scheduling for distributed machine learning",
    "predicate": "published in",
    "object": "ACM Symposium on Cloud Computing"
  },
  {
    "subject": "Slaq: quality-driven scheduling for distributed machine learning",
    "predicate": "published in year",
    "object": "2017"
  },
  {
    "subject": "Optimus",
    "predicate": "is",
    "object": "an efficient dynamic resource scheduler for deep learning clusters"
  },
  {
    "subject": "Optimus",
    "predicate": "was presented in",
    "object": "EuroSys"
  },
  {
    "subject": "Optimus",
    "predicate": "was published in",
    "object": "2018"
  },
  {
    "subject": "Authors",
    "predicate": "include",
    "object": "Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo"
  },
  {
    "subject": "Themis",
    "predicate": "is",
    "object": "Fair and efficient GPU cluster scheduling"
  },
  {
    "subject": "Themis",
    "predicate": "was presented in",
    "object": "USENIX NSDI"
  },
  {
    "subject": "Themis",
    "predicate": "was published in",
    "object": "2020"
  },
  {
    "subject": "Themis",
    "predicate": "was authored by",
    "object": "K. Mahajan, A. Balasubramanian, A. Singhvi, S. Venkataraman, A. Akella, A. Phanishayee, and S. Chawla"
  },
  {
    "subject": "HyperSched",
    "predicate": "is a",
    "object": "Dynamic resource reallocation for model development on a deadline"
  },
  {
    "subject": "HyperSched",
    "predicate": "was presented in",
    "object": "ACM Symposium on Cloud Computing"
  },
  {
    "subject": "HyperSched",
    "predicate": "was presented in year",
    "object": "2019"
  },
  {
    "subject": "HyperSched",
    "predicate": "was authored by",
    "object": "R. Liaw, R. Bhardwaj, L. Dunlap, Y. Zou, J. E. Gonzalez, I. Stoica, and A. Tumanov"
  },
  {
    "subject": "CHET",
    "predicate": "is",
    "object": "an optimizing compiler for fully-homomorphic neural-network inferencing"
  },
  {
    "subject": "CHET",
    "predicate": "was presented in",
    "object": "ACM Conference on Programming Language Design and Implementation"
  },
  {
    "subject": "CHET",
    "predicate": "was presented in year",
    "object": "2019"
  },
  {
    "subject": "R. Dathathri, O. Saarikivi, H. Chen, K. Laine, K. Lauter, S. Maleki, M. Musuvathi, and T. Mytkowicz",
    "predicate": "are authors of",
    "object": "CHET"
  },
  {
    "subject": "TVM",
    "predicate": "is",
    "object": "an automated end-to-end optimizing compiler for deep learning"
  },
  {
    "subject": "TVM",
    "predicate": "was presented in",
    "object": "USENIX OSDI"
  },
  {
    "subject": "TVM",
    "predicate": "was presented in year",
    "object": "2018"
  },
  {
    "subject": "T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze, et al.",
    "predicate": "are authors of",
    "object": "TVM"
  },
  {
    "subject": "Gpipe",
    "predicate": "is",
    "object": "efficient training of giant neural networks using pipeline parallelism"
  },
  {
    "subject": "Gpipe",
    "predicate": "is presented in",
    "object": "Advances in Neural Information Processing Systems, 2019"
  },
  {
    "subject": "33 Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, et al.",
    "predicate": "are authors of",
    "object": "Gpipe"
  },
  {
    "subject": "Blink",
    "predicate": "is described as",
    "object": "Fast and generic collectives for distributed ML"
  },
  {
    "subject": "Blink",
    "predicate": "was presented in",
    "object": "Conference on Machine Learning and Systems"
  },
  {
    "subject": "Blink",
    "predicate": "was presented in year",
    "object": "2020"
  },
  {
    "subject": "Authors",
    "predicate": "include",
    "object": "G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica"
  },
  {
    "subject": "NVIDIA Collective Communications Library",
    "predicate": "abbreviated as",
    "object": "NCCL"
  },
  {
    "subject": "36 J. Liu, J. Wu, and D. K. Panda",
    "predicate": "authored",
    "object": "High performance RDMA-based MPI implementation over inniband"
  },
  {
    "subject": "37 Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing",
    "predicate": "authored",
    "object": "More effective distributed ML via a stale synchronous parallel parameter server"
  },
  {
    "subject": "More effective distributed ML via a stale synchronous parallel parameter server",
    "predicate": "published in",
    "object": "Advances in Neural Information Processing Systems"
  },
  {
    "subject": "More effective distributed ML via a stale synchronous parallel parameter server",
    "predicate": "published in year",
    "object": "2013"
  },
  {
    "subject": "A. Awan, C.-H. Chu, H. Subramoni, and D. K. Panda",
    "predicate": "authored",
    "object": "Optimized broadcast for deep learning workloads on dense-GPU inniband clusters: MPI or NCCL?"
  },
  {
    "subject": "Optimized broadcast for deep learning workloads on dense-GPU inniband clusters: MPI or NCCL?",
    "predicate": "was presented in",
    "object": "Proceedings of the 25th European MPI Users Group Meeting"
  },
  {
    "subject": "Proceedings of the 25th European MPI Users Group Meeting",
    "predicate": "occurred in",
    "object": "2018"
  },
  {
    "subject": "A. Vishnu, C. Siegel, T. Warfel, and V. Amatya",
    "predicate": "authored",
    "object": "GossipGraD: Scalable deep learning using gossip communication based asynchronous gradient descent"
  },
  {
    "subject": "GossipGraD",
    "predicate": "is published in",
    "object": "CoRR"
  },
  {
    "subject": "41 Z. Zhang, C. Chang, H. Lin, Y. Wang, R. Arora, and X. Jin",
    "predicate": "authored",
    "object": "Is network the bottleneck of distributed training?"
  },
  {
    "subject": "Is network the bottleneck of distributed training?",
    "predicate": "published in",
    "object": "ACM SIGCOMM Workshop on Network Meets AI ML (NetAI)"
  },
  {
    "subject": "Is network the bottleneck of distributed training?",
    "predicate": "published in date",
    "object": "August 2020"
  },
  {
    "subject": "Y. Chen, Z. Liu, B. Ren, and X. Jin",
    "predicate": "authored",
    "object": "On efficient constructions of checkpoints"
  },
  {
    "subject": "On efficient constructions of checkpoints",
    "predicate": "presented in",
    "object": "International Conference on Machine Learning (ICML)"
  },
  {
    "subject": "International Conference on Machine Learning (ICML)",
    "predicate": "held in",
    "object": "July 2020"
  },
  {
    "subject": "vDNN",
    "predicate": "is",
    "object": "Virtualized deep neural networks for scalable, memory-efficient neural network design"
  },
  {
    "subject": "vDNN",
    "predicate": "was presented in",
    "object": "2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)"
  },
  {
    "subject": "Authors",
    "predicate": "include",
    "object": "M. Rhu, N. Gimelshein, J. Clemons, A. Zulqar, and S. W. Keckler"
  },
  {
    "subject": "vDNN",
    "predicate": "was published in year",
    "object": "2016"
  },
  {
    "subject": "SwapAdvisor",
    "predicate": "is authored by",
    "object": "C.-C. Huang, G. Jin, and J. Li"
  },
  {
    "subject": "SwapAdvisor",
    "predicate": "pushes deep learning beyond",
    "object": "the GPU memory limit"
  },
  {
    "subject": "SwapAdvisor",
    "predicate": "uses",
    "object": "smart swapping"
  },
  {
    "subject": "SwapAdvisor",
    "predicate": "was published in",
    "object": "ACM ASPLOS"
  },
  {
    "subject": "SwapAdvisor",
    "predicate": "was published in year",
    "object": "2020"
  },
  {
    "subject": "NVIDIA Container Runtime",
    "predicate": "is for",
    "object": "Docker"
  },
  {
    "subject": "nvidia-docker",
    "predicate": "is hosted on",
    "object": "https://github.com/NVIDIA/nvidia-docker"
  },
  {
    "subject": "Mesos",
    "predicate": "is",
    "object": "a platform for fine-grained resource sharing in the data center"
  },
  {
    "subject": "Mesos",
    "predicate": "was presented in",
    "object": "USENIX NSDI, 2011"
  },
  {
    "subject": "47 B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph, R. H. Katz, S. Shenker, and I. Stoica",
    "predicate": "are authors of",
    "object": "Mesos: A platform for fine-grained resource sharing in the data center"
  },
  {
    "subject": "Apache Hadoop YARN",
    "predicate": "is",
    "object": "Yet another resource negotiator"
  },
  {
    "subject": "Apache Hadoop YARN",
    "predicate": "was presented in",
    "object": "ACM Symposium on Cloud Computing"
  },
  {
    "subject": "Apache Hadoop YARN",
    "predicate": "was presented in year",
    "object": "2013"
  },
  {
    "subject": "V. K. Vavilapalli, A. C. Murthy, C. Douglas, S. Agarwal, M. Konar, R. Evans, T. Graves, J. Lowe, H. Shah, S. Seth, et al.",
    "predicate": "are authors of",
    "object": "Apache Hadoop YARN"
  },
  {
    "subject": "49 G. Giunta, R. Montella, G. Agrillo, and G. Coviello",
    "predicate": "authored",
    "object": "A GPGPU transparent virtualization component for high performance computing clouds"
  },
  {
    "subject": "A GPGPU transparent virtualization component for high performance computing clouds",
    "predicate": "was presented in",
    "object": "European Conference on Parallel Processing"
  },
  {
    "subject": "A GPGPU transparent virtualization component for high performance computing clouds",
    "predicate": "was presented in year",
    "object": "2010"
  },
  {
    "subject": "V. T. Ravi, M. Becchi, G. Agrawal, and S. Chakradhar",
    "predicate": "authored",
    "object": "Supporting GPU sharing in cloud environments with a transparent runtime consolidation framework"
  },
  {
    "subject": "Supporting GPU sharing in cloud environments with a transparent runtime consolidation framework",
    "predicate": "was presented in",
    "object": "Proceedings of the 20th international symposium on High performance distributed computing"
  },
  {
    "subject": "Proceedings of the 20th international symposium on High performance distributed computing",
    "predicate": "occurred in",
    "object": "2011"
  },
  {
    "subject": "GViM",
    "predicate": "is",
    "object": "GPU-accelerated virtual machines"
  },
  {
    "subject": "GViM",
    "predicate": "was presented in",
    "object": "Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing"
  },
  {
    "subject": "Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing",
    "predicate": "was published in",
    "object": "2009"
  },
  {
    "subject": "V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan",
    "predicate": "are authors of",
    "object": "GViM: GPU-accelerated virtual machines"
  },
  {
    "subject": "rCUDA",
    "predicate": "reduces",
    "object": "the number of GPU-based accelerators in high performance clusters"
  },
  {
    "subject": "51 J. Duato, A. J. Pena, F. Silla, R. Mayo, and E. S. Quintana-Ort",
    "predicate": "authored",
    "object": "rCUDA: Reducing the number of GPU-based accelerators in high performance clusters"
  },
  {
    "subject": "rCUDA",
    "predicate": "was presented in",
    "object": "2010 International Conference on High Performance Computing Simulation"
  },
  {
    "subject": "2010 International Conference on High Performance Computing Simulation",
    "predicate": "occurred in",
    "object": "2010"
  },
  {
    "subject": "vCUDA",
    "predicate": "is",
    "object": "GPU-accelerated high-performance computing in virtual machines"
  },
  {
    "subject": "Sun and K. Li",
    "predicate": "authored",
    "object": "vCUDA: GPU-accelerated high-performance computing in virtual machines"
  },
  {
    "subject": "vCUDA",
    "predicate": "published in",
    "object": "IEEE Transactions on Computers"
  },
  {
    "subject": "MXNet",
    "predicate": "is",
    "object": "a flexible and efficient machine learning library"
  },
  {
    "subject": "MXNet",
    "predicate": "is designed for",
    "object": "heterogeneous distributed systems"
  },
  {
    "subject": "55 T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang",
    "predicate": "are authors of",
    "object": "MXNet"
  },
  {
    "subject": "MXNet paper",
    "predicate": "was published in",
    "object": "2015"
  },
  {
    "subject": "MXNet paper",
    "predicate": "was published as",
    "object": "arXiv preprint arXiv:1512.01274"
  },
  {
    "subject": "56 C. Gregg, J. Dorn, K. Hazelwood, and K. Skadron",
    "predicate": "wrote",
    "object": "Fine-grained resource sharing for concurrent GPGPU kernels"
  },
  {
    "subject": "Fine-grained resource sharing for concurrent GPGPU kernels",
    "predicate": "was presented at",
    "object": "4th USENIX Workshop on Hot Topics in Parallelism"
  },
  {
    "subject": "4th USENIX Workshop on Hot Topics in Parallelism",
    "predicate": "occurred in",
    "object": "2012"
  },
  {
    "subject": "57 S. Pai, M. J. Thazhuthaveetil, and R. Govindarajan",
    "predicate": "authored",
    "object": "Improving GPGPU concurrency with elastic kernels"
  },
  {
    "subject": "Improving GPGPU concurrency with elastic kernels",
    "predicate": "published in",
    "object": "ACM SIGARCH Computer Architecture News"
  },
  {
    "subject": "Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia, and A. Aiken",
    "predicate": "authored",
    "object": "TASO: optimizing deep learning computation with automatic generation of graph substitutions"
  },
  {
    "subject": "TASO: optimizing deep learning computation with automatic generation of graph substitutions",
    "predicate": "was presented in",
    "object": "ACM SOSP, 2019"
  }
]